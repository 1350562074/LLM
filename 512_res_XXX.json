{'instruction': 'Please refactor the following code:', 'input': 'public void enlistCreated(final @NonNull ManagedObject XXX) { if(shouldIgnore(XXX)) { return; } enlistForPublishing(XXX, PublishingChangeKind.CREATE); enlistForAuditing(XXX, aap->PreAndPostValues.pre(IsisTransactionPlaceholder.NEW)); }', 'ground_truth': 'public void enlistCreated(ManagedObject entity) { val hasAlreadyBeenEnlisted = isEnlisted(entity); enlistCreatedInternal(entity); if(!hasAlreadyBeenEnlisted) { CallbackFacet.Util.callCallback(entity, PersistedCallbackFacet.class); postLifecycleEventIfRequired(entity, PersistedLifecycleEventFacet.class); } }', 'output': 'public void enlistCreated(final @NonNull ManagedObject managedObject) { if(shouldIgnore(managedObject)) { return; } enlistForPublishing(managedObject, PublishingChangeKind.CREATE); enlistForAuditing(managedObject, aap->PreAndPostValues.pre(IsisTransactionPlaceholder.NEW)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void addEntry(DecoratedKey<?> XXX, long indexPosition) { indexPositions.add(new KeyPosition(XXX, indexPosition)); }', 'ground_truth': 'public void addEntry(DecoratedKey<?> key, long indexPosition) { indexPositions.add(new KeyPosition(SSTable.getMinimalKey(key), indexPosition)); }', 'output': 'public void addEntry(DecoratedKey<?> key, long indexPosition) { indexPositions.add(new KeyPosition(key, indexPosition)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Component decorate(final Component uiComponent, final PrototypingDecorationModel XXX) { Wkt.cssAppend(uiComponent, "prototype"); return uiComponent; }', 'ground_truth': 'public void decorate(final Component uiComponent, final FormLabelDecorationModel decorationModel) { if(decorationModel.isMandatoryMarker()) { Wkt.cssAppend(uiComponent, "mandatory"); } if(_Strings.isNotEmpty(decorationModel.getSuffix())) { final IModel<String> labelModel = _Casts.uncheckedCast(uiComponent.getDefaultModel()); uiComponent.setDefaultModel(Model.of(labelModel.getObject() + decorationModel.getSuffix())); } uiComponent.setEscapeModelStrings(true); }', 'output': 'public Component decorate(final Component uiComponent, final PrototypingDecorationModel model) { Wkt.cssAppend(uiComponent, "prototype"); return uiComponent; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static CFMetaData newSchemaMetadata(String cfName, int XXX, String comment, AbstractType<?> comparator, AbstractType<?> subcc) { /* * Schema column families needs a gc_grace (since they are replicated * on every node). That gc_grace should be high enough that no node * could be dead for that long a time. */ int gcGrace = 120 * 24 * 3600; return newSystemMetadata(cfName, XXX, comment, comparator, subcc).gcGraceSeconds(gcGrace); }', 'ground_truth': 'private static CFMetaData newSchemaMetadata(String cfName, int oldCfId, String comment, AbstractType<?> comparator, AbstractType<?> subcc) { /* * Schema column families needs a gc_grace (since they are replicated * on every node). That gc_grace should be high enough that no node * could be dead for that long a time. */ int gcGrace = 120 * 24 * 3600; return newSystemMetadata(cfName, oldCfId, comment, comparator, subcc).gcGraceSeconds(gcGrace); }', 'output': 'private static CFMetaData newSchemaMetadata(String cfName, int version, String comment, AbstractType<?> comparator, AbstractType<?> subcc) { /* * Schema column families needs a gc_grace (since they are replicated * on every node). That gc_grace should be high enough that no node * could be dead for that long a time. */ int gcGrace = 120 * 24 * 3600; return newSystemMetadata(cfName, version, comment, comparator, subcc).gcGraceSeconds(gcGrace); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Table openWithoutSSTables(String XXX) { return open(XXX, Schema.instance, false); }', 'ground_truth': 'public static Keyspace openWithoutSSTables(String keyspaceName) { return open(keyspaceName, Schema.instance, false); }', 'output': 'public static Table openWithoutSSTables(String dbName, String tableName) { return open(dbName, tableName, false); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Future submitMajor(ColumnFamilyStore XXX, long skip) { return compactor_.submit(new OnDemandCompactor(XXX, skip)); }', 'ground_truth': 'public Future submitMajor(final ColumnFamilyStore cfStore, final long skip) { Callable<Object> callable = new Callable<Object>() { public Object call() throws IOException { cfStore.doMajorCompaction(skip); return this; } }; return compactor_.submit(callable); }', 'output': 'public Future submitMajor(ColumnFamilyStore cfs, long skip) { return compactor_.submit(new OnDemandCompactor(cfs, skip)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public int groupSize(String XXX) { int result = 0; for (GridHadoopCounter counter : cntrs.all()) { if (XXX.equals(counter.group())) result++; } return result; }', 'ground_truth': 'public int groupSize(String grpName) { int res = 0; for (GridHadoopCounter counter : userCounters) { if (grpName.equals(counter.group())) res++; } return res; }', 'output': 'public int groupSize(String key) { int result = 0; for (GridHadoopCounter counter : cntrs.all()) { if (key.equals(counter.group())) result++; } return result; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void decorate(final Component uiComponent, final DisablingDecorationModel XXX) { val tooltipUiModel = TooltipDecorationModel.ofBody(PlacementDirection.BOTTOM, XXX.getReason()); getTooltip().decorate(uiComponent, tooltipUiModel); Wkt.cssAppend(uiComponent, "disabled"); uiComponent.setEnabled(false); }', 'ground_truth': 'public void decorate(final Component uiComponent, final FormLabelDecorationModel decorationModel) { if(decorationModel.isMandatoryMarker()) { Wkt.cssAppend(uiComponent, "mandatory"); } if(_Strings.isNotEmpty(decorationModel.getSuffix())) { final IModel<String> labelModel = _Casts.uncheckedCast(uiComponent.getDefaultModel()); uiComponent.setDefaultModel(Model.of(labelModel.getObject() + decorationModel.getSuffix())); } uiComponent.setEscapeModelStrings(true); }', 'output': 'public void decorate(final Component uiComponent, final DisablingDecorationModel model) { val tooltipUiModel = TooltipDecorationModel.ofBody(PlacementDirection.BOTTOM, model.getReason()); getTooltip().decorate(uiComponent, tooltipUiModel); Wkt.cssAppend(uiComponent, "disabled"); uiComponent.setEnabled(false); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static long getSlowQueryTimeout(TimeUnit XXX) { return XXX.convert(conf.slow_query_log_timeout.toMilliseconds(), MILLISECONDS); }', 'ground_truth': 'public static long getSlowQueryTimeout(TimeUnit unit) { return conf.slow_query_log_timeout.to(unit); }', 'output': 'public static long getSlowQueryTimeout(TimeUnit unit) { return unit.convert(conf.slow_query_log_timeout.toMilliseconds(), MILLISECONDS); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private AbstractLink createDynamicallyVisibleLink(final ManagedObject XXX) { final ObjectAdapterModel entityModel = getModel(); final PageParameters pageParameters = PageParameterUtils .createPageParametersForBookmarkablePageLink(getModel(), XXX); final Class<? extends Page> pageClass = getPageClassRegistry().getPageClass(PageType.ENTITY); final BookmarkablePageLink<Void> link = new BookmarkablePageLink<Void>( ID_ENTITY_LINK, pageClass, pageParameters) { private static final long serialVersionUID = 1L; @Override public boolean isVisible() { val XXX = entityModel.getObject(); return XXX != null && (XXX.getPojo()!=null || XXX.isBookmarkMemoized()); } }; return link; }', 'ground_truth': 'private AbstractLink createDynamicallyVisibleLink(final ManagedObject _targetAdapter) { val pageParameters = PageParameterUtils .createPageParametersForBookmarkablePageLink(getModel(), _targetAdapter); val pageClass = getPageClassRegistry().getPageClass(PageType.ENTITY); return Wkt.bookmarkablePageLinkWithVisibility(ID_ENTITY_LINK, pageClass, pageParameters, ()->{ val targetAdapter = EntityIconAndTitlePanel.this.getModel().getObject(); return targetAdapter != null && (targetAdapter.getPojo()!=null || targetAdapter.isBookmarkMemoized()); }); }', 'output': 'private AbstractLink createDynamicallyVisibleLink(final ManagedObject entity) { final ObjectAdapterModel entityModel = getModel(); final PageParameters pageParameters = PageParameterUtils .createPageParametersForBookmarkablePageLink(getModel(), entity); final Class<? extends Page> pageClass = getPageClassRegistry().getPageClass(PageType.ENTITY); final BookmarkablePageLink<Void> link = new BookmarkablePageLink<Void>( ID_ENTITY_LINK, pageClass, pageParameters) { private static final long serialVersionUID = 1L; @Override public boolean isVisible() { val entity = entityModel.getObject(); return entity != null && (entity.getPojo() != null || entity.isBookmarkMemoized()); } }; return link; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public PropertyFileAcceptVm act( @Parameter( fileAccept = "pdf" , optionality = Optionality.OPTIONAL ) @ParameterLayout( describedAs = "@Parameter(fileAccept = \\"pdf\\")" ) final Blob XXX) { propertyFileAcceptVm.setPdfPropertyUsingAnnotation(XXX); return propertyFileAcceptVm; }', 'ground_truth': 'public PropertyFileAcceptVm act( @Parameter( fileAccept = ".pdf" , optionality = Optionality.OPTIONAL ) @ParameterLayout( describedAs = "@Parameter(fileAccept = \\".pdf\\")" ) final Blob pdfParameterUsingAnnotation) { propertyFileAcceptVm.setPdfPropertyUsingAnnotation(pdfParameterUsingAnnotation); return propertyFileAcceptVm; }', 'output': 'public PropertyFileAcceptVm act( @Parameter( fileAccept = "pdf" , optionality = Optionality.OPTIONAL ) @ParameterLayout( describedAs = "@Parameter(fileAccept = \\"pdf\\")" ) final File pdf) { propertyFileAcceptVm.setPdfPropertyUsingAnnotation(pdf); return propertyFileAcceptVm; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ByteBuffer[] split(ByteBuffer XXX) { List<ByteBuffer> l = new ArrayList<ByteBuffer>(); ByteBuffer bb = XXX.duplicate(); readIsStatic(bb); int i = 0; while (bb.remaining() > 0) { getComparator(i++, bb); l.add(ByteBufferUtil.readBytesWithShortLength(bb)); bb.get(); } return l.toArray(new ByteBuffer[l.size()]); }', 'ground_truth': 'public ByteBuffer[] split(ByteBuffer bb) { List<ByteBuffer> l = new ArrayList<ByteBuffer>(); boolean isStatic = readIsStatic(bb, ByteBufferAccessor.instance); int offset = startingOffset(isStatic); int i = 0; while (!ByteBufferAccessor.instance.isEmptyFromOffset(bb, offset)) { offset += getComparatorSize(i++, bb, ByteBufferAccessor.instance, offset); ByteBuffer value = ByteBufferAccessor.instance.sliceWithShortLength(bb, offset); offset += ByteBufferAccessor.instance.sizeWithShortLength(value); l.add(value); offset++; } return l.toArray(new ByteBuffer[l.size()]); }', 'output': 'public ByteBuffer[] split(ByteBuffer src) { List<ByteBuffer> l = new ArrayList<ByteBuffer>(); ByteBuffer bb = src.duplicate(); readIsStatic(bb); int i = 0; while (bb.remaining() > 0) { getComparator(i++, bb); l.add(ByteBufferUtil.readBytesWithShortLength(bb)); bb.get(); } return l.toArray(new ByteBuffer[l.size()]); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Integer getVersion(InetAddress XXX) { Integer v = versions.get(XXX); if (v == null) { logger.trace("Assuming current protocol version for {}", XXX); return MessagingService.current_version; } else return v; }', 'ground_truth': 'public int getVersion(String endpoint) throws UnknownHostException { return getVersion(InetAddress.getByName(endpoint)); }', 'output': 'public Integer getVersion(InetAddress host) { Integer v = versions.get(host); if (v == null) { logger.trace("Assuming current protocol version for {}", host); return MessagingService.current_version; } else return v; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static OneToOneAssociation findOneToOneAssociation( final ManagedObject targetAdapter, final String XXX) throws RuntimeException { val localPropertyId = localPartOf(XXX); final ObjectSpecification specification = targetAdapter.getSpecification(); final OneToOneAssociation property = findOneToOneAssociationElseNull(specification, localPropertyId); if(property == null) { throw new RuntimeException(String.format("Unknown property \'%s\'", localPropertyId)); } return property; }', 'ground_truth': 'private static OneToOneAssociation findOneToOneAssociation( final ManagedObject targetAdapter, final String logicalMemberIdentifier) throws RuntimeException { val localPropertyId = localPartOf(logicalMemberIdentifier); final ObjectSpecification specification = targetAdapter.getSpecification(); final OneToOneAssociation property = findOneToOneAssociationElseNull(specification, localPropertyId); if(property == null) { throw new RuntimeException(String.format("Unknown property \'%s\'", localPropertyId)); } return property; }', 'output': 'private static OneToOneAssociation findOneToOneAssociation( final ManagedObject targetAdapter, final String propertyName) throws RuntimeException { val localPropertyId = localPartOf(propertyName); final ObjectSpecification specification = targetAdapter.getSpecification(); final OneToOneAssociation property = findOneToOneAssociationElseNull(specification, localPropertyId); if(property == null) { throw new RuntimeException(String.format("Unknown property \'%s\'", localPropertyId)); } return property; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void serialize(StreamRequestMessage XXX, DataOutputStream dos) throws IOException { StreamRequestMetadata[] streamRequestMetadata = XXX.streamRequestMetadata_; dos.writeInt(streamRequestMetadata.length); for (StreamRequestMetadata bsmd : streamRequestMetadata) { StreamRequestMetadata.serializer().serialize(bsmd, dos); } }', 'ground_truth': 'public void serialize(StreamRequestMessage srm, DataOutputStream dos) throws IOException { dos.writeLong(srm.sessionId); CompactEndpointSerializationHelper.serialize(srm.target, dos); if (srm.file != null) { dos.writeBoolean(true); PendingFile.serializer().serialize(srm.file, dos); } else { dos.writeBoolean(false); dos.writeUTF(srm.table); dos.writeInt(srm.ranges.size()); for (Range range : srm.ranges) { AbstractBounds.serializer().serialize(range, dos); } } }', 'output': 'public void serialize(StreamRequestMessage message, DataOutputStream dos) throws IOException { StreamRequestMetadata[] streamRequestMetadata = message.streamRequestMetadata_; dos.writeInt(streamRequestMetadata.length); for (StreamRequestMetadata bsmd : streamRequestMetadata) { StreamRequestMetadata.serializer().serialize(bsmd, dos); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void send_get_key_range(String tablename, String XXX, String startWith, String stopAt, int maxResults) throws TException { oprot_.writeMessageBegin(new TMessage("get_key_range", TMessageType.CALL, seqid_)); get_key_range_args args = new get_key_range_args(); args.tablename = tablename; args.XXX = XXX; args.startWith = startWith; args.stopAt = stopAt; args.maxResults = maxResults; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'ground_truth': 'public void send_get_key_range(String table, String column_family, String startWith, String stopAt, int maxResults) throws TException { oprot_.writeMessageBegin(new TMessage("get_key_range", TMessageType.CALL, seqid_)); get_key_range_args args = new get_key_range_args(); args.table = table; args.column_family = column_family; args.startWith = startWith; args.stopAt = stopAt; args.maxResults = maxResults; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'output': 'public void send_get_key_range(String tablename, String startWith, String stopAt, int maxResults) throws TException { oprot_.writeMessageBegin(new TMessage("get_key_range", TMessageType.CALL, seqid_)); get_key_range_args args = new get_key_range_args(); args.tablename = tablename; args.startWith = startWith; args.stopAt = stopAt; args.maxResults = maxResults; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected LdapUserDto getLdapUserByMemberAttr(String XXX) { Filter userObjectFilter = new EqualsFilter(OBJECT_CLASS_ATTRIBUTE, ldapServerProperties.getUserObjectClass()); Set<LdapUserDto> filteredLdapUsers = getFilteredLdapUsers(userObjectFilter, getMemberFilter(XXX)); return (filteredLdapUsers.isEmpty()) ? null : filteredLdapUsers.iterator().next(); }', 'ground_truth': 'protected LdapUserDto getLdapUserByMemberAttr(String memberAttributeValue) { LdapUserDto dto = getLdapUser(memberAttributeValue); if (dto == null) { Set<LdapUserDto> filteredLdapUsers = getFilteredLdapUsers( new EqualsFilter(OBJECT_CLASS_ATTRIBUTE, ldapServerProperties.getUserObjectClass()), getMemberFilter(memberAttributeValue)); dto = (filteredLdapUsers.isEmpty()) ? null : filteredLdapUsers.iterator().next(); } return dto; }', 'output': 'protected LdapUserDto getLdapUserByMemberAttr(String memberId) { Filter userObjectFilter = new EqualsFilter(OBJECT_CLASS_ATTRIBUTE, ldapServerProperties.getUserObjectClass()); Set<LdapUserDto> filteredLdapUsers = getFilteredLdapUsers(userObjectFilter, getMemberFilter(memberId)); return (filteredLdapUsers.isEmpty()) ? null : filteredLdapUsers.iterator().next(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private IgfsHadoopOutProc(String host, int port, String grid, String XXX, boolean shmem, Log log) throws IOException { assert host != null && !shmem || host == null && shmem : "Invalid arguments [host=" + host + ", port=" + port + ", shmem=" + shmem + \']\'; String endpoint = host != null ? host + ":" + port : "shmem:" + port; this.grid = grid; this.XXX = XXX; this.log = log; io = IgfsHadoopIpcIo.get(log, endpoint); io.addEventListener(this); }', 'ground_truth': 'private IgfsHadoopOutProc(String host, int port, String grid, String igfs, boolean shmem, Log log) throws IOException { assert host != null && !shmem || host == null && shmem : "Invalid arguments [host=" + host + ", port=" + port + ", shmem=" + shmem + \']\'; String endpoint = host != null ? host + ":" + port : "shmem:" + port; this.grid = grid; this.igfs = igfs; this.log = log; io = IgfsHadoopIpcIo.get(log, endpoint); io.addEventListener(this); }', 'output': 'private IgfsHadoopOutProc(String host, int port, String grid, String path, boolean shmem, Log log) throws IOException { assert host != null && !shmem || host == null && shmem : "Invalid arguments [host=" + host + ", port=" + port + ", shmem=" + shmem + \']\'; String endpoint = host != null ? host + ":" + port : "shmem:" + port; this.grid = grid; this.path = path; this.log = log; io = IgfsHadoopIpcIo.get(log, endpoint); io.addEventListener(this); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<column_t> get_slice_from(String tablename, String key, String XXX, boolean isAscending, int count) throws InvalidRequestException { logger.debug("get_slice_from"); String[] values = RowMutation.getColumnAndColumnFamily(XXX); if (values.length != 2 || DatabaseDescriptor.getColumnFamilyType(values[0]) != "Standard") throw new InvalidRequestException("get_slice_from requires a standard CF name and a starting column name"); if (count <= 0) throw new InvalidRequestException("get_slice_from requires positive count"); if ("Name".compareTo(DatabaseDescriptor.getCFMetaData(tablename, values[0]).indexProperty_) != 0) throw new InvalidRequestException("get_slice_from requires CF indexed by name"); ColumnFamily cfamily = readColumnFamily(new SliceFromReadCommand(tablename, key, XXX, isAscending, count)); if (cfamily == null) { return EMPTY_COLUMNS; } Collection<IColumn> columns = cfamily.getAllColumns(); return thriftifyColumns(columns); }', 'ground_truth': 'public List<column_t> get_slice_from(String tablename, String key, String columnParent, boolean isAscending, int count) throws InvalidRequestException { logger.debug("get_slice_from"); String[] values = RowMutation.getColumnAndColumnFamily(columnParent); if (values.length != 2 || DatabaseDescriptor.getColumnFamilyType(values[0]) != "Standard") throw new InvalidRequestException("get_slice_from requires a standard CF name and a starting column name"); if (count <= 0) throw new InvalidRequestException("get_slice_from requires positive count"); if ("Name".compareTo(DatabaseDescriptor.getCFMetaData(tablename, values[0]).indexProperty_) != 0) throw new InvalidRequestException("get_slice_from requires CF indexed by name"); ColumnFamily cfamily = readColumnFamily(new SliceFromReadCommand(tablename, key, columnParent, isAscending, count)); if (cfamily == null) { return EMPTY_COLUMNS; } Collection<IColumn> columns = cfamily.getAllColumns(); return thriftifyColumns(columns); }', 'output': 'public List<column_t> get_slice_from(String tablename, String key, String column, boolean isAscending, int count) throws InvalidRequestException { logger.debug("get_slice_from"); String[] values = RowMutation.getColumnAndColumnFamily(column); if (values.length != 2 || DatabaseDescriptor.getColumnFamilyType(values[0]) != "Standard") throw new InvalidRequestException("get_slice_from requires a standard CF name and a starting column name"); if (count <= 0) throw new InvalidRequestException("get_slice_from requires positive count"); if ("Name".compareTo(DatabaseDescriptor.getCFMetaData(tablename, values[0]).indexProperty_) != 0) throw new InvalidRequestException("get_slice_from requires CF indexed by name"); ColumnFamily cfamily = readColumnFamily(new SliceFromReadCommand(tablename, key, column, isAscending, count)); if (cfamily == null) { return EMPTY_COLUMNS; } Collection<IColumn> columns = cfamily.getAllColumns(); return thriftifyColumns(columns); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static long getRegisteredCallbackAge(String XXX) { return callbackMap_.getAge(XXX); }', 'ground_truth': 'public static long getRegisteredCallbackAge(String messageId) { return callbackMap_.getAge(messageId); }', 'output': 'public static long getRegisteredCallbackAge(String key) { return callbackMap_.getAge(key); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String orList(String fieldName, String[] valueList, String XXX) { if (valueList == null || valueList.length == 0) { return ""; } String expr = ""; int count = -1; for (String value : valueList) { count++; if (count > 0) { expr += " OR "; } expr += fieldName + ":*" + value + "*"; } if (valueList.length == 0) { return expr; } else { return "(" + expr + ")"; } }', 'ground_truth': 'public String orList(String fieldName, String[] valueList, String wildCard) { if (valueList == null || valueList.length == 0) { return ""; } if(stringUtil.isEmpty(wildCard)){ wildCard = ""; } StringBuilder expr = new StringBuilder(); int count = -1; for (String value : valueList) { count++; if (count > 0) { expr.append(" OR "); } expr.append( fieldName + ":"+ wildCard + value + wildCard); } if (valueList.length == 0) { return expr.toString(); } else { return "(" + expr + ")"; } }', 'output': 'public String orList(String fieldName, String[] valueList, String operator) { if (valueList == null || valueList.length == 0) { return ""; } String expr = ""; int count = -1; for (String value : valueList) { count++; if (count > 0) { expr += " OR "; } expr += fieldName + ":*" + value + "*"; } if (valueList.length == 0) { return expr; } else { return "(" + expr + ")"; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Response.ResponseBuilder buildResponse( final IResourceContext XXX, final ManagedObject objectAdapter) { final Object domainObject = objectOf(objectAdapter); final RepresentationType representationType = RepresentationType.DOMAIN_OBJECT; final MediaType mediaType = mediaTypeFrom(XXX, representationType); if (mediaType == null) { return null; } return buildResponse(XXX, domainObject, representationType); }', 'ground_truth': 'public Response.ResponseBuilder buildResponse( final IResourceContext resourceContext, final ManagedObject objectAdapter) { final Object domainObject = objectOf(objectAdapter); final RepresentationType representationType = RepresentationType.DOMAIN_OBJECT; final MediaType mediaType = mediaTypeFrom(resourceContext, representationType); if (mediaType == null) { return null; } return buildResponse(resourceContext, domainObject, representationType); }', 'output': 'public Response.ResponseBuilder buildResponse( final IResourceContext resourceContext, final ManagedObject objectAdapter) { final Object domainObject = objectOf(objectAdapter); final RepresentationType representationType = RepresentationType.DOMAIN_OBJECT; final MediaType mediaType = mediaTypeFrom(resourceContext, representationType); if (mediaType == null) { return null; } return buildResponse(resourceContext, domainObject, representationType); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public FileBasedSource(boolean isFilePattern, String fileOrPatternSpec, long XXX, long startOffset, long endOffset) { super(startOffset, endOffset, XXX); if (isFilePattern) { mode = Mode.FILEPATTERN; } else if (startOffset == 0 && endOffset == Long.MAX_VALUE) { mode = Mode.FULL_SINGLE_FILE; } else { mode = Mode.SUBRANGE_OF_SINGLE_FILE; } this.fileOrPatternSpec = fileOrPatternSpec; }', 'ground_truth': 'public FileBasedSource(boolean isFilePattern, String fileOrPatternSpec, long minBundleSize, long startOffset, long endOffset) { super(startOffset, endOffset, minBundleSize); if (isFilePattern) { mode = Mode.FILEPATTERN; } else if (startOffset == 0 && endOffset == Long.MAX_VALUE) { mode = Mode.FULL_SINGLE_FILE; } else { mode = Mode.SUBRANGE_OF_SINGLE_FILE; } this.fileOrPatternSpec = fileOrPatternSpec; }', 'output': 'public FileBasedSource(boolean isFilePattern, String fileOrPatternSpec, long startPosition, long endPosition) { super(startPosition, endPosition); if (isFilePattern) { mode = Mode.FILEPATTERN; } else if (startPosition == 0 && endPosition == Long.MAX_VALUE) { mode = Mode.FULL_SINGLE_FILE; } else { mode = Mode.SUBRANGE_OF_SINGLE_FILE; } this.fileOrPatternSpec = fileOrPatternSpec; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void remove(final String XXX, final Bookmark bookmark) { bookmarkByOidStr.remove(XXX); oidStrByBookmark.remove(bookmark); list.remove(bookmark); }', 'ground_truth': 'private void remove(final String oid, final Bookmark bookmark) { bookmarkByOidStr.remove(oid); oidStrByBookmark.remove(bookmark); list.remove(bookmark); }', 'output': 'private void remove(final String id, final Bookmark bookmark) { bookmarkByOidStr.remove(id); oidStrByBookmark.remove(bookmark); list.remove(bookmark); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean equals(Object XXX) { if (!super.equals(XXX)) { return false; } return schema.equals(((CoGbkResultCoder) XXX).schema); }', 'ground_truth': 'public boolean equals(Object object) { if (this == object) { return true; } if (!(object instanceof CoGbkResultCoder)) { return false; } CoGbkResultCoder other = (CoGbkResultCoder) object; return schema.equals(other.schema) && unionCoder.equals(other.unionCoder); }', 'output': 'public boolean equals(Object o) { if (!super.equals(o)) { return false; } return schema.equals(((CoGbkResultCoder) o).schema); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public TemplateDateFormat get(int dateType, boolean zonelessInput, String XXX) throws InvalidFormatDescriptorException, TemplateModelException, UnknownDateTypeFormattingUnsupportedException { Map<String, TemplateDateFormat>[] formatCache = this.formatCache; if (formatCache == null) { formatCache = new Map[4]; this.formatCache = formatCache; } Map<String, TemplateDateFormat> formatsForDateType = formatCache[dateType]; if (formatsForDateType == null) { formatsForDateType = new HashMap(); formatCache[dateType] = formatsForDateType; } TemplateDateFormat format = formatsForDateType.get(XXX); if (format == null) { format = new JavaTemplateDateFormat(getJavaDateFormat(dateType, XXX)); formatsForDateType.put(XXX, format); } return format; }', 'ground_truth': 'public TemplateDateFormat get(int dateType, boolean zonelessInput, String params) throws InvalidFormatParametersException, TemplateModelException, UnknownDateTypeFormattingUnsupportedException { Map<String, TemplateDateFormat>[] formatCache = this.formatCache; if (formatCache == null) { formatCache = new Map[4]; this.formatCache = formatCache; } Map<String, TemplateDateFormat> formatsForDateType = formatCache[dateType]; if (formatsForDateType == null) { formatsForDateType = new HashMap(); formatCache[dateType] = formatsForDateType; } TemplateDateFormat format = formatsForDateType.get(params); if (format == null) { format = new JavaTemplateDateFormat(getJavaDateFormat(dateType, params)); formatsForDateType.put(params, format); } return format; }', 'output': 'public TemplateDateFormat get(int dateType, boolean zonelessInput, String pattern) throws InvalidFormatDescriptorException, TemplateModelException, UnknownDateTypeFormattingUnsupportedException { Map<String, TemplateDateFormat>[] formatCache = this.formatCache; if (formatCache == null) { formatCache = new Map[4]; this.formatCache = formatCache; } Map<String, TemplateDateFormat> formatsForDateType = formatCache[dateType]; if (formatsForDateType == null) { formatsForDateType = new HashMap(); formatCache[dateType] = formatsForDateType; } TemplateDateFormat format = formatsForDateType.get(pattern); if (format == null) { format = new JavaTemplateDateFormat(getJavaDateFormat(dateType, pattern)); formatsForDateType.put(pattern, format); } return format; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void deserializeBounds(DataInput XXX) throws IOException { super.deserializeBounds(XXX); List<Long> temp = new ArrayList<Long>(); int size = XXX.readInt(); for (int i = 0; i < size; i++) temp.add(XXX.readLong()); boundaries = temp; }', 'ground_truth': 'public void deserializeBounds(DataInput in) throws IOException { super.deserializeBounds(in); List<Long> temp = new ArrayList<Long>(); int size = in.readInt(); for (int i = 0; i < size; i++) temp.add(in.readLong()); boundaries = temp; }', 'output': 'public void deserializeBounds(DataInputPlus in) throws IOException { super.deserializeBounds(in); List<Long> temp = new ArrayList<Long>(); int size = in.readInt(); for (int i = 0; i < size; i++) temp.add(in.readLong()); boundaries = temp; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static SequentialWriter open(File file, boolean XXX) { return open(file, RandomAccessReader.DEFAULT_BUFFER_SIZE, XXX); } public static SequentialWriter open(File file, int bufferSize, boolean XXX)\n    { { return new SequentialWriter(file, bufferSize, XXX); } public static ChecksummedSequentialWriter open(File file, boolean XXX, File crcPath)\n    { { return new ChecksummedSequentialWriter(file, RandomAccessReader.DEFAULT_BUFFER_SIZE, XXX, crcPath); }', 'ground_truth': 'public static ChecksummedSequentialWriter open(File file, File crcPath) { return new ChecksummedSequentialWriter(file, RandomAccessReader.DEFAULT_BUFFER_SIZE, crcPath); }', 'output': 'public static SequentialWriter open(File file, int bufferSize) { return open(file, bufferSize, false); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private CacheConfiguration cacheConfiguration(AtomicConfiguration cfg, String name, String XXX) { CacheConfiguration ccfg = new CacheConfiguration(); ccfg.setName(name); ccfg.setGroupName(XXX); ccfg.setAtomicityMode(TRANSACTIONAL); ccfg.setRebalanceMode(SYNC); ccfg.setWriteSynchronizationMode(FULL_SYNC); ccfg.setCacheMode(cfg.getCacheMode()); ccfg.setNodeFilter(CacheConfiguration.ALL_NODES); ccfg.setAffinity(cfg.getAffinity()); if (cfg.getCacheMode() == PARTITIONED) ccfg.setBackups(cfg.getBackups()); return ccfg; }', 'ground_truth': 'private CacheConfiguration cacheConfiguration(CollectionConfiguration cfg, String name, String grpName) { CacheConfiguration ccfg = new CacheConfiguration(); ccfg.setName(name); ccfg.setGroupName(grpName); ccfg.setBackups(cfg.getBackups()); ccfg.setCacheMode(cfg.getCacheMode()); ccfg.setAtomicityMode(cfg.getAtomicityMode()); ccfg.setNodeFilter(cfg.getNodeFilter()); ccfg.setWriteSynchronizationMode(FULL_SYNC); ccfg.setRebalanceMode(SYNC); return ccfg; }', 'output': 'private CacheConfiguration cacheConfiguration(AtomicConfiguration cfg, String name, String groupName) { CacheConfiguration ccfg = new CacheConfiguration(); ccfg.setName(name); ccfg.setGroupName(groupName); ccfg.setAtomicityMode(TRANSACTIONAL); ccfg.setRebalanceMode(SYNC); ccfg.setWriteSynchronizationMode(FULL_SYNC); ccfg.setCacheMode(cfg.getCacheMode()); ccfg.setNodeFilter(CacheConfiguration.ALL_NODES); ccfg.setAffinity(cfg.getAffinity()); if (cfg.getCacheMode() == PARTITIONED) ccfg.setBackups(cfg.getBackups()); return ccfg; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void selectLoggerLibrary(int XXX) throws ClassNotFoundException { synchronized (Logger.class) { if(XXX < -1 || (XXX*2) >= LIBINIT.length) { throw new IllegalArgumentException(); } logLibrary = XXX; factory = createFactory(); } }', 'ground_truth': 'public static void selectLoggerLibrary(int libraryEnum) throws ClassNotFoundException { if (libraryEnum < MIN_LIBRARY_ENUM || libraryEnum > MAX_LIBRARY_ENUM) { throw new IllegalArgumentException("Library enum value out of range"); } synchronized (Logger.class) { final boolean loggerFactoryAlreadySet = loggerFactory != null; if (!loggerFactoryAlreadySet || libraryEnum != Logger.libraryEnum) { ensureLoggerFactorySet(true); if (!initializedFromSystemProperty || loggerFactory == null) { setLibrary(libraryEnum); loggersByCategory.clear(); if (loggerFactoryAlreadySet) { logWarnInLogger("Logger libraray was already set earlier; " + "change to \\"" + getLibraryName(libraryEnum) + "\\" won\'t effect loggers created " + "earlier."); } } else { logWarnInLogger( "Ignored " + Logger.class.getName() + ".selectLoggerLibrary(int) call, because the " + PROPERTY_NAME_LOGGER_LIBRARY + " system property is set."); } } } }', 'output': 'public static void selectLoggerLibrary(int index) throws ClassNotFoundException { synchronized (Logger.class) { if(index < -1 || (index*2) >= LIBINIT.length) { throw new IllegalArgumentException(); } logLibrary = index; factory = createFactory(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static Set<Long> getXPoints(List<TaskAttempt> XXX, List<TaskAttempt> reduceAttempts, long submitTimeX, long finishTimeX) { TreeSet<Long> xPoints = new TreeSet<Long>(); TreeSet<TaskAttempt> sortedAttempts = new TreeSet<TaskAttempt>(new Comparator<TaskAttempt>() { @Override public int compare(TaskAttempt t1, TaskAttempt t2) { if (t1.getStartTime() < t2.getStartTime()) return -1; else if (t1.getStartTime() > t2.getStartTime()) return 1; return t1.getTaskAttemptId().compareTo(t2.getTaskAttemptId()); } }); sortedAttempts.addAll(XXX); sortedAttempts.addAll(reduceAttempts); getXPoints(sortedAttempts, xPoints); xPoints.add(submitTimeX); xPoints.add(finishTimeX); return xPoints; }', 'ground_truth': 'private static Set<Long> getXPoints(List<TaskAttempt> taskAttempts, long submitTimeX, long finishTimeX) { TreeSet<Long> xPoints = new TreeSet<Long>(); TreeSet<TaskAttempt> sortedAttempts = new TreeSet<TaskAttempt>(new Comparator<TaskAttempt>() { @Override public int compare(TaskAttempt t1, TaskAttempt t2) { if (t1.getStartTime() < t2.getStartTime()) return -1; else if (t1.getStartTime() > t2.getStartTime()) return 1; return t1.getTaskAttemptId().compareTo(t2.getTaskAttemptId()); } }); sortedAttempts.addAll(taskAttempts); getXPoints(sortedAttempts, xPoints); xPoints.add(submitTimeX); xPoints.add(finishTimeX); return xPoints; }', 'output': 'private static Set<Long> getXPoints(List<TaskAttempt> tasks, List<TaskAttempt> reduceTasks, long submitTimeX, long finishTimeX) { TreeSet<Long> xPoints = new TreeSet<Long>(); TreeSet<TaskAttempt> sortedTasks = new TreeSet<TaskAttempt>(new Comparator<TaskAttempt>() { @Override public int compare(TaskAttempt t1, TaskAttempt t2) { if (t1.getStartTime() < t2.getStartTime()) return -1; else if (t1.getStartTime() > t2.getStartTime()) return 1; return t1.getTaskAttemptId().compareTo(t2.getTaskAttemptId()); } }); sortedTasks.addAll(tasks); sortedTasks.addAll(reduceTasks); getXPoints(sortedTasks, xPoints); xPoints.add(submitTimeX); xPoints.add(finishTimeX); return xPoints; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Coordinate getCoordinates(String XXX, IFileReader dataReader) throws IOException { List<KeyPositionInfo> indexInfo = indexMetadataMap_.get(dataFile_); int size = (indexInfo == null) ? 0 : indexInfo.size(); long start = 0L; long end = dataReader.getEOF(); if ( size > 0 ) { int index = Collections.binarySearch(indexInfo, new KeyPositionInfo(XXX)); if ( index < 0 ) { /* * We are here which means that the requested * XXX is not an index. */ index = (++index)*(-1); /* * This means XXX is not present at all. Hence * a scan is in order. */ start = (index == 0) ? 0 : indexInfo.get(index - 1).position(); if ( index < size ) { end = indexInfo.get(index).position(); } else { /* This is the Block Index in the file. */ end = start; } } else { /* * If we are here that means the XXX is in the index file * and we can retrieve it w/o a scan. In reality we would * like to have a retreive(XXX, fromPosition) but for now * we use scan(start, start + 1) - a hack. */ start = indexInfo.get(index).position(); end = start; } } else { /* * We are here which means there are less than * 128 XXXs in the system and hence our only recourse * is a linear scan from start to finish. Automatically * use memory mapping since we have a huge file and very * few XXXs. */ end = dataReader.getEOF(); } return new Coordinate(start, end); }', 'ground_truth': 'private Coordinate getCoordinates(String decoratedKey, IFileReader dataReader) throws IOException { List<KeyPositionInfo> indexInfo = indexMetadataMap_.get(dataFile_); int size = (indexInfo == null) ? 0 : indexInfo.size(); long start = 0L; long end = dataReader.getEOF(); if ( size > 0 ) { int index = Collections.binarySearch(indexInfo, new KeyPositionInfo(decoratedKey)); if ( index < 0 ) { /* * We are here which means that the requested * key is not an index. */ index = (++index)*(-1); /* * This means key is not present at all. Hence * a scan is in order. */ start = (index == 0) ? 0 : indexInfo.get(index - 1).position(); if ( index < size ) { end = indexInfo.get(index).position(); } else { /* This is the Block Index in the file. */ end = start; } } else { /* * If we are here that means the key is in the index file * and we can retrieve it w/o a scan. In reality we would * like to have a retreive(key, fromPosition) but for now * we use scan(start, start + 1) - a hack. */ start = indexInfo.get(index).position(); end = start; } } else { /* * We are here which means there are less than * 128 keys in the system and hence our only recourse * is a linear scan from start to finish. Automatically * use memory mapping since we have a huge file and very * few keys. */ end = dataReader.getEOF(); } return new Coordinate(start, end); }', 'output': 'private Coordinate getCoordinates(String key, IFileReader dataReader) throws IOException { List<KeyPositionInfo> indexInfo = indexMetadataMap_.get(dataFile_); int size = (indexInfo == null) ? 0 : indexInfo.size(); long start = 0L; long end = dataReader.getEOF(); if ( size > 0 ) { int index = Collections.binarySearch(indexInfo, new KeyPositionInfo(key)); if ( index < 0 ) { /* * We are here which means that the requested * key is not an index. */ index = (++index)*(-1); /* * This means key is not present at all. Hence * a scan is in order. */ start = (index == 0) ? 0 : indexInfo.get(index - 1).position(); if ( index < size ) { end = indexInfo.get(index).position(); } else { /* This is the Block Index in the file. */ end = start; } } else { /* * If we are here that means the key is in the index file * and we can retrieve it w/o a scan. In reality we would * like to have a retreive(key, fromPosition) but for now * we use scan(start, start + 1) - a hack. */ start = indexInfo.get(index).position(); end = start; } else { /* * We are here which means there are less than * 128 keys in the system and hence our only recourse * is a linear scan from start to finish. Automatically * use memory mapping since we have a huge file and very * few keys. */ end = dataReader.getEOF(); } return new Coordinate(start, end); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void publishDiagnostics(PublishDiagnosticsParams XXX) { try { FileObject file = URLMapper.findFileObject(new URI(XXX.getUri()).toURL()); EditorCookie ec = file.getLookup().lookup(EditorCookie.class); Document doc = ec != null ? ec.getDocument() : null; if (doc == null) return ; List<ErrorDescription> diags = XXX.getDiagnostics().stream().map(d -> ErrorDescriptionFactory.createErrorDescription(severityMap.get(d.getSeverity()), d.getMessage(), file, Utils.getOffset(doc, d.getRange().getStart()), Utils.getOffset(doc, d.getRange().getEnd())) ).collect(Collectors.toList()); HintsController.setErrors(doc, LanguageClientImpl.class.getName(), diags); } catch (URISyntaxException | MalformedURLException ex) { LOG.log(Level.FINE, null, ex); } System.err.println("XXX: " + XXX.getDiagnostics().size()); System.err.println("publishDiagnostics: " + XXX); }', 'ground_truth': 'public void publishDiagnostics(PublishDiagnosticsParams pdp) { try { FileObject file = URLMapper.findFileObject(new URI(pdp.getUri()).toURL()); EditorCookie ec = file.getLookup().lookup(EditorCookie.class); Document doc = ec != null ? ec.getDocument() : null; if (doc == null) return ; List<ErrorDescription> diags = pdp.getDiagnostics().stream().map(d -> ErrorDescriptionFactory.createErrorDescription(severityMap.get(d.getSeverity()), d.getMessage(), new DiagnosticFixList(pdp.getUri(), d), file, Utils.getOffset(doc, d.getRange().getStart()), Utils.getOffset(doc, d.getRange().getEnd())) ).collect(Collectors.toList()); HintsController.setErrors(doc, LanguageClientImpl.class.getName(), diags); } catch (URISyntaxException | MalformedURLException ex) { LOG.log(Level.FINE, null, ex); } }', 'output': 'public void publishDiagnostics(PublishDiagnosticsParams params) { try { FileObject file = URLMapper.findFileObject(new URI(params.getUri()).toURL()); EditorCookie ec = file.getLookup().lookup(EditorCookie.class); Document doc = ec != null ? ec.getDocument() : null; if (doc == null) return ; List<ErrorDescription> diags = params.getDiagnostics().stream().map(d -> ErrorDescriptionFactory.createErrorDescription(severityMap.get(d.getSeverity()), d.getMessage(), file, Utils.getOffset(doc, d.getRange().getStart()), Utils.getOffset(doc, d.getRange().getEnd())) ).collect(Collectors.toList()); HintsController.setErrors(doc, LanguageClientImpl.class.getName(), diags); } catch (URISyntaxException | MalformedURLException ex) { LOG.log(Level.FINE, null, ex); } System.err.println("publishDiagnostics: " + params); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RangeTombstone deserializeFromSSTable(DataInput XXX, Descriptor.Version version) throws IOException { ByteBuffer min = ByteBufferUtil.readWithShortLength(XXX); if (min.remaining() <= 0) throw ColumnSerializer.CorruptColumnException.create(XXX, min); int b = XXX.readUnsignedByte(); assert (b & ColumnSerializer.RANGE_TOMBSTONE_MASK) != 0; return deserializeBody(XXX, min, version); }', 'ground_truth': 'public RangeTombstone deserializeFromSSTable(DataInput in, Descriptor.Version version) throws IOException { ByteBuffer min = ByteBufferUtil.readWithShortLength(in); if (min.remaining() <= 0) throw ColumnSerializer.CorruptColumnException.create(in, min); int b = in.readUnsignedByte(); assert (b & ColumnSerializer.RANGE_TOMBSTONE_MASK) != 0; return deserializeBody(in, min, version); }', 'output': 'public RangeTombstone deserializeFromSSTable(DataInput in, Descriptor.Version version) throws IOException { ByteBuffer min = ByteBufferUtil.readWithShortLength(in); if (min.remaining() <= 0) throw ColumnSerializer.CorruptColumnException.create(in, min); int b = in.readUnsignedByte(); assert (b & ColumnSerializer.RANGE_TOMBSTONE_MASK) != 0; return deserializeBody(in, min, version); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static OffHeapBitSet deserialize(DataInput XXX) throws IOException { int byteCount = XXX.readInt() * 8; Memory memory = RefCountedMemory.allocate(byteCount); for (int i = 0; i < byteCount;) { long v = XXX.readLong(); memory.setByte(i++, (byte) (v >>> 0)); memory.setByte(i++, (byte) (v >>> 8)); memory.setByte(i++, (byte) (v >>> 16)); memory.setByte(i++, (byte) (v >>> 24)); memory.setByte(i++, (byte) (v >>> 32)); memory.setByte(i++, (byte) (v >>> 40)); memory.setByte(i++, (byte) (v >>> 48)); memory.setByte(i++, (byte) (v >>> 56)); } return new OffHeapBitSet(memory); }', 'ground_truth': 'public static OffHeapBitSet deserialize(DataInput in) throws IOException { int byteCount = in.readInt() * 8; Memory memory = RefCountedMemory.allocate(byteCount); for (int i = 0; i < byteCount;) { long v = in.readLong(); memory.setByte(i++, (byte) (v >>> 0)); memory.setByte(i++, (byte) (v >>> 8)); memory.setByte(i++, (byte) (v >>> 16)); memory.setByte(i++, (byte) (v >>> 24)); memory.setByte(i++, (byte) (v >>> 32)); memory.setByte(i++, (byte) (v >>> 40)); memory.setByte(i++, (byte) (v >>> 48)); memory.setByte(i++, (byte) (v >>> 56)); } return new OffHeapBitSet(memory); }', 'output': 'public static OffHeapBitSet deserialize(DataInputPlus in) throws IOException { int byteCount = in.readInt() * 8; Memory memory = RefCountedMemory.allocate(byteCount); for (int i = 0; i < byteCount;) { long v = in.readLong(); memory.setByte(i++, (byte) (v >>> 0)); memory.setByte(i++, (byte) (v >>> 8)); memory.setByte(i++, (byte) (v >>> 16)); memory.setByte(i++, (byte) (v >>> 24)); memory.setByte(i++, (byte) (v >>> 32)); memory.setByte(i++, (byte) (v >>> 40)); memory.setByte(i++, (byte) (v >>> 48)); memory.setByte(i++, (byte) (v >>> 56)); } return new OffHeapBitSet(memory); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public <T> AsyncWrap<T> async(T domainObject, ImmutableEnumSet<ExecutionMode> XXX) { val executor = ForkJoinPool.commonPool(); return new AsyncWrapDefault<T>( this, isisSessionFactory, authenticationSessionTracker, transactionService, domainObject, XXX, executor, log::error); }', 'ground_truth': 'public <T> T async(T domainObject, ImmutableEnumSet<ExecutionMode> modes) { val executor = ForkJoinPool.commonPool(); AsyncWrapDefault<T> tAsyncWrapDefault = new AsyncWrapDefault<>( this, isisSessionFactory, authenticationSessionTracker, transactionService, bookmarkService, domainObject, modes, executor, log::error, RuleCheckingPolicy.ASYNC); return proxyFactoryService.factory()tAsyncWrapDefault; }', 'output': 'public <T> AsyncWrap<T> async(T domainObject, ExecutionMode... executionModes) { val executor = ForkJoinPool.commonPool(); return new AsyncWrapDefault<T>( this, isisSessionFactory, authenticationSessionTracker, transactionService, domainObject, executionModes, executor, log::error); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void touch(final String XXX, boolean fData) throws IOException { if (touchCache_.containsKey(dataFile_ + ":" + XXX)) return; IFileReader dataReader = SequenceFile.reader(dataFile_); try { /* Morph the key */ String decoratedKey = StorageService.getPartitioner().decorateKey(XXX); Coordinate fileCoordinate = getCoordinates(decoratedKey, dataReader); /* Get offset of key from block Index */ dataReader.seek(fileCoordinate.end_); BlockMetadata blockMetadata = dataReader.getBlockMetadata(decoratedKey); if ( blockMetadata.position_ != -1L ) { touchCache_.put(dataFile_ + ":" + XXX, blockMetadata.position_); } if ( fData ) { /* Read the data associated with this key and pull it into the Buffer Cache */ if ( blockMetadata.position_ != -1L ) { dataReader.seek(blockMetadata.position_); DataOutputBuffer bufOut = new DataOutputBuffer(); dataReader.next(bufOut); bufOut.reset(); logger_.debug("Finished the touch of the key to pull it into buffer cache."); } } } finally { if ( dataReader != null ) dataReader.close(); } }', 'ground_truth': 'public void touch(String key, boolean fData) throws IOException { if ( touchCache_.containsKey(key) ) return; IFileReader dataReader = SequenceFile.reader(dataFile_); try { /* Morph the key */ key = morphKey(key); Coordinate fileCoordinate = getCoordinates(key, dataReader); /* Get offset of key from block Index */ dataReader.seek(fileCoordinate.end_); BlockMetadata blockMetadata = dataReader.getBlockMetadata(key); if ( blockMetadata.position_ != -1L ) { touchCache_.put(dataFile_ + ":" + key, blockMetadata.position_); } if ( fData ) { /* Read the data associated with this key and pull it into the Buffer Cache */ if ( blockMetadata.position_ != -1L ) { dataReader.seek(blockMetadata.position_); DataOutputBuffer bufOut = new DataOutputBuffer(); dataReader.next(bufOut); bufOut.reset(); logger_.debug("Finished the touch of the key to pull it into buffer cache."); } } } finally { if ( dataReader != null ) dataReader.close(); } }', 'output': 'public void touch(final String key, boolean fData) throws IOException { if (touchCache_.containsKey(dataFile_ + ":" + key)) return; IFileReader dataReader = SequenceFile.reader(dataFile_); try { /* Morph the key */ String decoratedKey = StorageService.getPartitioner().decorateKey(key); Coordinate fileCoordinate = getCoordinates(decoratedKey, dataReader); /* Get offset of key from block Index */ dataReader.seek(fileCoordinate.end_); BlockMetadata blockMetadata = dataReader.getBlockMetadata(decoratedKey); if ( blockMetadata.position_ != -1L ) { touchCache_.put(dataFile_ + ":" + key, blockMetadata.position_); } if ( fData ) { /* Read the data associated with this key and pull it into the Buffer Cache */ if ( blockMetadata.position_ != -1L ) { dataReader.seek(blockMetadata.position_); DataOutputBuffer bufOut = new DataOutputBuffer(); dataReader.next(bufOut); bufOut.reset(); logger_.debug("Finished the touch of the key to pull it into buffer cache."); } } } finally { if ( dataReader != null ) dataReader.close(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean isOfType(final ObjectSpecification XXX) { if (XXX.getSpecId().equals(this.getSpecId())) { return true; } for (val interfaceSpec : interfaces()) { if (interfaceSpec.isOfType(XXX)) { return true; } } val correspondingClass = getCorrespondingClass(); val possibleSupertypeClass = XXX.getCorrespondingClass(); if(correspondingClass != null && possibleSupertypeClass != null && Enum.class.isAssignableFrom(correspondingClass) && possibleSupertypeClass.isInterface()) { if(possibleSupertypeClass.isAssignableFrom(correspondingClass)) { return true; } } val superclassSpec = superclass(); return superclassSpec != null && superclassSpec.isOfType(XXX); }', 'ground_truth': 'public boolean isOfType(final ObjectSpecification other) { if (other.getSpecId().equals(this.getSpecId())) { return true; } val thisClass = this.getCorrespondingClass(); val otherClass = other.getCorrespondingClass(); return otherClass.isAssignableFrom(thisClass); }', 'output': 'public boolean isOfType(final ObjectSpecification spec) { if (spec.getSpecId().equals(this.getSpecId())) { return true; } for (val interfaceSpec : interfaces()) { if (interfaceSpec.isOfType(spec)) { return true; } } val correspondingClass = getCorrespondingClass(); val possibleSupertypeClass = spec.getCorrespondingClass(); if(correspondingClass != null && possibleSupertypeClass != null && Enum.class.isAssignableFrom(correspondingClass) && possibleSupertypeClass.isInterface()) { if(possibleSupertypeClass.isAssignableFrom(correspondingClass)) { return true; } } val superclassSpec = superclass(); return superclassSpec != null && superclassSpec.isOfType(spec); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static Collection<IgniteCallable<Double>> jobs(int XXX, final Credit[] portfolio, final int horizon, int iter, final double percentile) { int iterPerNode = Math.round(iter / (float)XXX); int lastNodeIter = iter - (XXX - 1) * iterPerNode; Collection<IgniteCallable<Double>> clos = new ArrayList<>(XXX); for (int i = 0; i < XXX; i++) { final int nodeIter = i == XXX - 1 ? lastNodeIter : iterPerNode; clos.add(new IgniteCallable<Double>() { /** {@inheritDoc} */ @Override public Double call() { return new CreditRiskManager().calculateCreditRiskMonteCarlo( portfolio, horizon, nodeIter, percentile); } }); } return clos; }', 'ground_truth': 'private static Collection<IgniteCallable<Double>> jobs(int clusterSize, final Credit[] portfolio, final int horizon, int iter, final double percentile) { int iterPerNode = Math.round(iter / (float)clusterSize); int lastNodeIter = iter - (clusterSize - 1) * iterPerNode; Collection<IgniteCallable<Double>> clos = new ArrayList<>(clusterSize); for (int i = 0; i < clusterSize; i++) { final int nodeIter = i == clusterSize - 1 ? lastNodeIter : iterPerNode; clos.add(new IgniteCallable<Double>() { /** {@inheritDoc} */ @Override public Double call() { return new CreditRiskManager().calculateCreditRiskMonteCarlo( portfolio, horizon, nodeIter, percentile); } }); } return clos; }', 'output': 'private static Collection<IgniteCallable<Double>> jobs(int nodes, final Credit[] portfolio, final int horizon, int iter, final double percentile) { int iterPerNode = Math.round(iter / (float)nodes); int lastNodeIter = iter - (nodes - 1) * iterPerNode; Collection<IgniteCallable<Double>> clos = new ArrayList<>(nodes); for (int i = 0; i < nodes; i++) { final int nodeIter = i == nodes - 1 ? lastNodeIter : iterPerNode; clos.add(new IgniteCallable<Double>() { /** {@inheritDoc} */ @Override public Double call() { return new CreditRiskManager().calculateCreditRiskMonteCarlo( portfolio, horizon, nodeIter, percentile); } }); } return clos; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public HomePageResolverServiceDefault( final FactoryService factoryService, final IsisBeanTypeRegistryHolder XXX) { this.factoryService = factoryService; this.XXX = XXX; }', 'ground_truth': 'public HomePageResolverServiceDefault( final FactoryService factoryService, final IsisBeanTypeRegistry isisBeanTypeRegistry) { this.factoryService = factoryService; this.isisBeanTypeRegistry = isisBeanTypeRegistry; }', 'output': 'public HomePageResolverServiceDefault( final FactoryService factoryService, final IsisBeanTypeRegistryHolder beanTypeRegistryHolder) { this.factoryService = factoryService; this.beanTypeRegistryHolder = beanTypeRegistryHolder; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static DataRange forTokenRange(Range<Token> XXX) { return forKeyRange(Range.makeRowRange(XXX)); }', 'ground_truth': 'public static DataRange forTokenRange(Range<Token> tokenRange) { return forKeyRange(Range.makeRowRange(tokenRange)); }', 'output': 'public static DataRange forTokenRange(Range<Token> start, Range<Token> end) { return forKeyRange(Range.makeRowRange(start, end)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void visitPropertyExpression(final PropertyExpression XXX) { if (visitPropertyExpressionSilent(XXX, XXX)) return; if (!extension.handleUnresolvedProperty(XXX)) { Expression objectExpression = XXX.getObjectExpression(); addStaticTypeError("No such property: " + XXX.getPropertyAsString() + " for class: " + findCurrentInstanceOfClass(objectExpression, getType(objectExpression)).toString(false), XXX); } }', 'ground_truth': 'public void visitPropertyExpression(final PropertyExpression expression) { if (existsProperty(expression, !typeCheckingContext.isTargetOfEnclosingAssignment(expression))) return; if (!extension.handleUnresolvedProperty(expression)) { Expression objectExpression = expression.getObjectExpression(); addStaticTypeError("No such property: " + expression.getPropertyAsString() + " for class: " + findCurrentInstanceOfClass(objectExpression, getType(objectExpression)).toString(false), expression); } }', 'output': 'public void visitPropertyExpression(final PropertyExpression expression) { if (visitPropertyExpressionSilent(expression, expression)) return; if (!extension.handleUnresolvedProperty(expression)) { Expression objectExpression = expression.getObjectExpression(); addStaticTypeError("No such property: " + expression.getPropertyAsString() + " for class: " + findCurrentInstanceOfClass(objectExpression, getType(objectExpression)).toString(false), expression); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ByteBuffer decompose(ByteBuffer XXX) { throw new UnsupportedOperationException("You can\'t do this with a local partitioner."); }', 'ground_truth': 'public ByteBuffer decompose(ByteBuffer value) { throw new UnsupportedOperationException("You can\'t do this with a local partitioner."); }', 'output': 'public ByteBuffer decompose(ByteBuffer data) { throw new UnsupportedOperationException("You can\'t do this with a local partitioner."); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void invalidatePermissions(String XXX, String resourceName) { invalidate(Pair.create(new AuthenticatedUser(XXX), Resources.fromName(resourceName))); }', 'ground_truth': 'public void invalidatePermissions(String roleName, String resourceName) { invalidate(Pair.create(new AuthenticatedUser(roleName), Resources.fromName(resourceName))); }', 'output': 'public void invalidatePermissions(String username, String resourceName) { invalidate(Pair.create(new AuthenticatedUser(username), Resources.fromName(resourceName))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private PartitionEvictionTask( GridDhtLocalPartition part, GroupEvictionContext XXX ) { this.part = part; this.XXX = XXX; size = part.fullSize(); }', 'ground_truth': 'private PartitionEvictionTask( GridDhtLocalPartition part, GroupEvictionContext grpEvictionCtx ) { this.part = part; this.grpEvictionCtx = grpEvictionCtx; size = part.fullSize(); }', 'output': 'private PartitionEvictionTask( GridDhtLocalPartition part, GroupEvictionContext ctx ) { this.part = part; this.ctx = ctx; size = part.fullSize(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public int forceRepairRangeAsync(String beginToken, String endToken, String keyspaceName, boolean isSequential, Collection<String> dataCenters, Collection<String> hosts, boolean fullRepair, String... XXX) { return forceRepairRangeAsync(beginToken, endToken, keyspaceName, isSequential ? RepairParallelism.SEQUENTIAL.ordinal() : RepairParallelism.PARALLEL.ordinal(), dataCenters, hosts, fullRepair, XXX); }', 'ground_truth': 'public int forceRepairRangeAsync(String beginToken, String endToken, String keyspaceName, boolean isSequential, Collection<String> dataCenters, Collection<String> hosts, boolean fullRepair, String... tableNames) { return forceRepairRangeAsync(beginToken, endToken, keyspaceName, isSequential ? RepairParallelism.SEQUENTIAL.ordinal() : RepairParallelism.PARALLEL.ordinal(), dataCenters, hosts, fullRepair, tableNames); }', 'output': 'public int forceRepairRangeAsync(String beginToken, String endToken, String keyspaceName, RepairParallelism parallelism, Collection<String> dataCenters, Collection<String> hosts, boolean fullRepair, String... tokens) { return forceRepairRangeAsync(beginToken, endToken, keyspaceName, parallelism, dataCenters, hosts, fullRepair, tokens); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void traverseTabGroups( final BSTabGroupOwner XXX, final Grid.Visitor visitor) { final BSGrid.Visitor bs3Visitor = asBs3Visitor(visitor); final List<BSTabGroup> tabGroups = XXX.getTabGroups(); for (BSTabGroup bs3TabGroup : new ArrayList<>(tabGroups)) { bs3TabGroup.setOwner(XXX); bs3Visitor.preVisit(bs3TabGroup); bs3Visitor.visit(bs3TabGroup); traverseTabs(bs3TabGroup, visitor); bs3Visitor.postVisit(bs3TabGroup); } }', 'ground_truth': 'private void traverseTabGroups( final BSTabGroupOwner bsTabGroupOwner, final Grid.Visitor visitor) { final BSGrid.Visitor bsVisitor = asBs3Visitor(visitor); final List<BSTabGroup> tabGroups = bsTabGroupOwner.getTabGroups(); for (BSTabGroup bsTabGroup : new ArrayList<>(tabGroups)) { bsTabGroup.setOwner(bsTabGroupOwner); bsVisitor.preVisit(bsTabGroup); bsVisitor.visit(bsTabGroup); traverseTabs(bsTabGroup, visitor); bsVisitor.postVisit(bsTabGroup); } }', 'output': 'private void traverseTabGroups( final BSTabGroupOwner owner, final Grid.Visitor visitor) { final BSGrid.Visitor bs3Visitor = asBs3Visitor(visitor); final List<BSTabGroup> tabGroups = owner.getTabGroups(); for (BSTabGroup bs3TabGroup : new ArrayList<>(tabGroups)) { bs3TabGroup.setOwner(owner); bs3Visitor.preVisit(bs3TabGroup); bs3Visitor.visit(bs3TabGroup); traverseTabs(bs3TabGroup, visitor); bs3Visitor.postVisit(bs3TabGroup); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void enlistUpdating(final @NonNull ManagedObject XXX) { if(shouldIgnore(XXX)) { return; } enlistForPublishing(XXX, PublishingChangeKind.UPDATE); enlistForAuditing(XXX, aap->PreAndPostValues.pre(aap.getPropertyValue())); }', 'ground_truth': 'public void enlistUpdating(ManagedObject entity) { val hasAlreadyBeenEnlisted = isEnlisted(entity); enlistUpdatingInternal(entity); if(!hasAlreadyBeenEnlisted) { CallbackFacet.Util.callCallback(entity, UpdatingCallbackFacet.class); postLifecycleEventIfRequired(entity, UpdatingLifecycleEventFacet.class); } }', 'output': 'public void enlistUpdating(final @NonNull ManagedObject o) { if(shouldIgnore(o)) { return; } enlistForPublishing(o, PublishingChangeKind.UPDATE); enlistForAuditing(o, aap->PreAndPostValues.pre(aap.getPropertyValue())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void typeCheckAssignment(final BinaryExpression assignmentExpression, final Expression leftExpression, final ClassNode leftExpressionType, final Expression rightExpression, final ClassNode XXX) { ClassNode inferredRightExpressionType = XXX; if (!typeCheckMultipleAssignmentAndContinue(leftExpression, rightExpression)) return; if (addedReadOnlyPropertyError(leftExpression)) return; ClassNode leftRedirect = leftExpressionType.redirect(); if (rightExpression instanceof VariableExpression && hasInferredReturnType(rightExpression) && assignmentExpression.getOperation().getType() == EQUAL) { inferredRightExpressionType = rightExpression.getNodeMetaData(INFERRED_RETURN_TYPE); } ClassNode wrappedRHS = adjustTypeForSpreading(inferredRightExpressionType, leftExpression); if (!checkCompatibleAssignmentTypes(leftRedirect, wrappedRHS, rightExpression)) { if (!extension.handleIncompatibleAssignment(leftExpressionType, inferredRightExpressionType, assignmentExpression)) { addAssignmentError(leftExpressionType, inferredRightExpressionType, assignmentExpression.getRightExpression()); } } else { addPrecisionErrors(leftRedirect, leftExpressionType, inferredRightExpressionType, rightExpression); addListAssignmentConstructorErrors(leftRedirect, leftExpressionType, inferredRightExpressionType, rightExpression, assignmentExpression); addMapAssignmentConstructorErrors(leftRedirect, leftExpression, rightExpression); if (hasGStringStringError(leftExpressionType, wrappedRHS, rightExpression)) return; checkTypeGenerics(leftExpressionType, wrappedRHS, rightExpression); } }', 'ground_truth': 'protected void typeCheckAssignment(final BinaryExpression assignmentExpression, final Expression leftExpression, final ClassNode leftExpressionType, final Expression rightExpression, final ClassNode rightExpressionType) { if (!typeCheckMultipleAssignmentAndContinue(leftExpression, rightExpression)) return; if (addedReadOnlyPropertyError(leftExpression)) return; ClassNode rTypeInferred, rTypeWrapped; if (rightExpression instanceof VariableExpression && hasInferredReturnType(rightExpression) && assignmentExpression.getOperation().getType() == ASSIGN) { rTypeInferred = rightExpression.getNodeMetaData(INFERRED_RETURN_TYPE); } else { rTypeInferred = rightExpressionType; } rTypeWrapped = adjustTypeForSpreading(rTypeInferred, leftExpression); if (!checkCompatibleAssignmentTypes(leftExpressionType, rTypeWrapped, rightExpression)) { if (!extension.handleIncompatibleAssignment(leftExpressionType, rTypeInferred, assignmentExpression)) { addAssignmentError(leftExpressionType, rTypeInferred, rightExpression); } } else { ClassNode lTypeRedirect = leftExpressionType.redirect(); addPrecisionErrors(lTypeRedirect, leftExpressionType, rTypeInferred, rightExpression); addListAssignmentConstructorErrors(lTypeRedirect, leftExpressionType, rTypeInferred, rightExpression, assignmentExpression); addMapAssignmentConstructorErrors(lTypeRedirect, leftExpression, rightExpression); if (!hasGStringStringError(leftExpressionType, rTypeWrapped, rightExpression)) { checkTypeGenerics(leftExpressionType, rTypeWrapped, rightExpression); } } }', 'output': 'protected void typeCheckAssignment(final BinaryExpression assignmentExpression, final Expression leftExpression, final ClassNode leftExpressionType, final Expression rightExpression, final ClassNode rightExpressionType) { ClassNode inferredRightExpressionType = rightExpressionType; if (!typeCheckMultipleAssignmentAndContinue(leftExpression, rightExpression)) return; if (addedReadOnlyPropertyError(leftExpression)) return; ClassNode leftRedirect = leftExpressionType.redirect(); if (rightExpression instanceof VariableExpression && hasInferredReturnType(rightExpression) && assignmentExpression.getOperation().getType() == EQUAL) { inferredRightExpressionType = rightExpression.getNodeMetaData(INFERRED_RETURN_TYPE); } ClassNode wrappedRHS = adjustTypeForSpreading(inferredRightExpressionType, leftExpression); if (!checkCompatibleAssignmentTypes(leftRedirect, wrappedRHS, rightExpression)) { if (!extension.handleIncompatibleAssignment(leftExpressionType, inferredRightExpressionType, assignmentExpression)) { addAssignmentError(leftExpressionType, inferredRightExpressionType, assignmentExpression.getRightExpression()); } } else { addPrecisionErrors(leftRedirect, leftExpressionType, inferredRightExpressionType, rightExpression); addListAssignmentConstructorErrors(leftRedirect, leftExpressionType, inferredRightExpressionType, rightExpression, assignmentExpression); addMapAssignmentConstructorErrors(leftRedirect, leftExpression, rightExpression); if (hasGStringStringError(leftExpressionType, wrappedRHS, rightExpression)) return; checkTypeGenerics(leftExpressionType, wrappedRHS, rightExpression); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Set<Integer> partsToReload(UUID nodeId, int XXX) { if (partsToReload == null) return Collections.emptySet(); return partsToReload.get(nodeId, XXX); }', 'ground_truth': 'public Set<Integer> partsToReload(UUID nodeId, int grpId) { if (partsToReload == null) return Collections.emptySet(); return partsToReload.get(nodeId, grpId); }', 'output': 'public Set<Integer> partsToReload(UUID nodeId, int version) { if (partsToReload == null) return Collections.emptySet(); return partsToReload.get(nodeId, version); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static IndexExpression arrayIndex( Expression array, Iterable<Expression> XXX) { return new IndexExpression(array, toList(XXX)); } public static BinaryExpression arrayIndex(\n        Expression array, Expression XXX)\n    { Expression array, Expression XXX) { throw Extensions.todo(); } public static MethodCallExpression arrayIndex(\n        Expression array, Expression... XXX)\n    { Expression array, Expression... XXX) { throw Extensions.todo(); }', 'ground_truth': 'public static IndexExpression arrayIndex( Expression array, Expression indexExpression) { return new IndexExpression(array, Collections.singletonList(indexExpression)); }', 'output': 'public static IndexExpression arrayIndex( Expression array, Iterable<Expression> expressions) { return new IndexExpression(array, toList(expressions)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void serialize(DataOutput XXX) throws IOException { assert !"".equals(columnFamilyName); assert superColumnName == null || superColumnName.remaining() > 0; assert columnName == null || columnName.remaining() > 0; XXX.writeUTF(columnFamilyName == null ? "" : columnFamilyName); ByteBufferUtil.writeWithShortLength(superColumnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : superColumnName, XXX); ByteBufferUtil.writeWithShortLength(columnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : columnName, XXX); }', 'ground_truth': 'public void serialize(DataOutput out) throws IOException { assert !"".equals(columnFamilyName); assert superColumnName == null || superColumnName.remaining() > 0; assert columnName == null || columnName.remaining() > 0; out.writeUTF(columnFamilyName == null ? "" : columnFamilyName); ByteBufferUtil.writeWithShortLength(superColumnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : superColumnName, out); ByteBufferUtil.writeWithShortLength(columnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : columnName, out); }', 'output': 'public void serialize(DataOutput output) throws IOException { assert !"".equals(columnFamilyName); assert superColumnName == null || superColumnName.remaining() > 0; assert columnName == null || columnName.remaining() > 0; output.writeUTF(columnFamilyName == null ? "" : columnFamilyName); ByteBufferUtil.writeWithShortLength(superColumnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : superColumnName, output); ByteBufferUtil.writeWithShortLength(columnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : columnName, output); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GridH2Table( CreateTableData createTblData, GridH2RowDescriptor desc, H2TableDescriptor tblDesc, GridCacheContextInfo cacheInfo, IndexProcessor XXX ) { super(createTblData); assert tblDesc != null; this.desc = desc; this.cacheInfo = cacheInfo; this.XXX = XXX; this.tableName = createTblData.tableName; affKeyCol = calculateAffinityKeyColumn(); affKeyColIsKey = affKeyCol != null && desc.isKeyColumn(affKeyCol.column.getColumnId()); identifier = new QueryTable(getSchema().getName(), getName()); identifierStr = identifier.schema() + "." + identifier.table(); idxs = tblDesc.createSystemIndexes(this); assert idxs != null; List<Index> clones = new ArrayList<>(idxs.size()); for (Index index : idxs) { Index clone = createDuplicateIndexIfNeeded(index); if (clone != null) clones.add(clone); } idxs.addAll(clones); boolean hasHashIndex = idxs.size() >= 2 && index(0).getIndexType().isHash(); if (hasHashIndex) idxs.add(0, new H2TableScanIndex(this, index(1), index(0))); else idxs.add(0, new H2TableScanIndex(this, index(0), null)); pkIndexPos = hasHashIndex ? 2 : 1; sysIdxsCnt = idxs.size(); lock = new ReentrantReadWriteLock(); if (cacheInfo.affinityNode()) { long totalTblSize = cacheSize(CachePeekMode.PRIMARY, CachePeekMode.BACKUP); size.add(totalTblSize); } tblStats = new TableStatistics(10_000, 10_000); if (desc != null && desc.context() != null) { GridKernalContext ctx = desc.context().kernalContext(); log = ctx.log(getClass()); } }', 'ground_truth': 'public GridH2Table( CreateTableData createTblData, GridH2RowDescriptor desc, H2TableDescriptor tblDesc, GridCacheContextInfo cacheInfo, IndexProcessor idxProc ) { super(createTblData); assert tblDesc != null; this.desc = desc; this.cacheInfo = cacheInfo; this.idxProc = idxProc; tableName = createTblData.tableName; affKeyCol = calculateAffinityKeyColumn(); affKeyColIsKey = affKeyCol != null && desc.isKeyColumn(affKeyCol.column.getColumnId()); identifier = new QueryTable(getSchema().getName(), getName()); identifierStr = identifier.schema() + "." + identifier.table(); idxs = tblDesc.createSystemIndexes(this); assert idxs != null; List<Index> clones = new ArrayList<>(idxs.size()); for (Index index : idxs) { Index clone = createDuplicateIndexIfNeeded(index); if (clone != null) clones.add(clone); } idxs.addAll(clones); boolean hasHashIndex = idxs.size() >= 2 && index(0).getIndexType().isHash(); if (hasHashIndex) idxs.add(0, new H2TableScanIndex(this, index(1), index(0))); else idxs.add(0, new H2TableScanIndex(this, index(0), null)); pkIndexPos = hasHashIndex ? 2 : 1; sysIdxsCnt = idxs.size(); lock = new ReentrantReadWriteLock(); if (cacheInfo.affinityNode()) { long totalTblSize = cacheSize(CachePeekMode.PRIMARY, CachePeekMode.BACKUP); size.add(totalTblSize); } tblStats = new TableStatistics(10_000, 10_000); if (desc != null && desc.context() != null) { GridKernalContext ctx = desc.context().kernalContext(); log = ctx.log(getClass()); } }', 'output': 'public GridH2Table( CreateTableData createTblData, GridH2RowDescriptor desc, H2TableDescriptor tblDesc, GridCacheContextInfo cacheInfo, IndexProcessor indexProcessor ) { super(createTblData); assert tblDesc != null; this.desc = desc; this.cacheInfo = cacheInfo; this.indexProcessor = indexProcessor; this.tableName = createTblData.tableName; affKeyCol = calculateAffinityKeyColumn(); affKeyColIsKey = affKeyCol != null && desc.isKeyColumn(affKeyCol.column.getColumnId()); identifier = new QueryTable(getSchema().getName(), getName()); identifierStr = identifier.schema() + "." + identifier.table(); idxs = tblDesc.createSystemIndexes(this); assert idxs != null; List<Index> clones = new ArrayList<>(idxs.size()); for (Index index : idxs) { Index clone = createDuplicateIndexIfNeeded(index); if (clone != null) clones.add(clone); } idxs.addAll(clones); boolean hasHashIndex = idxs.size() >= 2 && index(0).getIndexType().isHash(); if (hasHashIndex) idxs.add(0, new H2TableScanIndex(this, index(1), index(0))); else idxs.add(0, new H2TableScanIndex(this, index(0), null)); pkIndexPos = hasHashIndex ? 2 : 1; sysIdxsCnt = idxs.size(); lock = new ReentrantReadWriteLock(); if (cacheInfo.affinityNode()) { long totalTblSize = cacheSize(CachePeekMode.PRIMARY, CachePeekMode.BACKUP); size.add(totalTblSize); } tblStats = new TableStatistics(10_000, 10_000); if (desc != null && desc.context() != null) { GridKernalContext ctx = desc.context().kernalContext(); log = ctx.log(getClass()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean listenerReplace0(Object topic, GridMessageListener XXX, GridMessageListener newVal) { if (topic instanceof GridTopic) { synchronized (sysLsnrsMux) { return systemListenerChange(topic, XXX, newVal); } } else return lsnrMap.replace(topic, XXX, newVal); }', 'ground_truth': 'private boolean listenerReplace0(Object topic, GridMessageListener exp, GridMessageListener newVal) { if (topic instanceof GridTopic) { synchronized (sysLsnrsMux) { return systemListenerChange(topic, exp, newVal); } } else return lsnrMap.replace(topic, exp, newVal); }', 'output': 'private boolean listenerReplace0(Object topic, GridMessageListener oldVal, GridMessageListener newVal) { if (topic instanceof GridTopic) { synchronized (sysLsnrsMux) { return systemListenerChange(topic, oldVal, newVal); } } else return lsnrMap.replace(topic, oldVal, newVal); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public insert_args( String tablename, String key, String XXX, byte[] cellData, long timestamp, int block_for) { this(); this.tablename = tablename; this.key = key; this.XXX = XXX; this.cellData = cellData; this.timestamp = timestamp; this.__isset.timestamp = true; this.block_for = block_for; this.__isset.block_for = true; }', 'ground_truth': 'public insert_args( String tablename, String key, String columnPath, byte[] cellData, long timestamp, int block_for) { this(); this.tablename = tablename; this.key = key; this.columnPath = columnPath; this.cellData = cellData; this.timestamp = timestamp; this.__isset.timestamp = true; this.block_for = block_for; this.__isset.block_for = true; }', 'output': 'public insert_args( String tablename, String key, String value, byte[] cellData, long timestamp, int block_for) { this(); this.tablename = tablename; this.key = key; this.value = value; this.cellData = cellData; this.timestamp = timestamp; this.__isset.timestamp = true; this.block_for = block_for; this.__isset.block_for = true; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private ResultMessage.Rows execute(QueryPager pager, QueryOptions options, int limit, long XXX, int pageSize) throws RequestValidationException, RequestExecutionException { if (selection.isAggregate()) return pageAggregateQuery(pager, options, pageSize, XXX); checkFalse(needsPostQueryOrdering(), "Cannot page queries with both ORDER BY and a IN restriction on the partition key;" + " you must either remove the ORDER BY or the IN and sort client side, or disable paging for this query"); List<Row> page = pager.fetchPage(pageSize); ResultMessage.Rows msg = processResults(page, options, limit, XXX); if (!pager.isExhausted()) msg.result.metadata.setHasMorePages(pager.state()); return msg; }', 'ground_truth': 'private ResultMessage.Rows execute(Pager pager, QueryOptions options, int pageSize, int nowInSec) throws RequestValidationException, RequestExecutionException { if (selection.isAggregate()) return pageAggregateQuery(pager, options, pageSize, nowInSec); checkFalse(needsPostQueryOrdering(), "Cannot page queries with both ORDER BY and a IN restriction on the partition key;" + " you must either remove the ORDER BY or the IN and sort client side, or disable paging for this query"); ResultMessage.Rows msg; try (PartitionIterator page = pager.fetchPage(pageSize)) { msg = processResults(page, options, nowInSec); } if (!pager.isExhausted()) msg.result.metadata.setHasMorePages(pager.state()); return msg; }', 'output': 'private ResultMessage.Rows execute(QueryPager pager, QueryOptions options, int limit, long offset, int pageSize) throws RequestValidationException, RequestExecutionException { if (selection.isAggregate()) return pageAggregateQuery(pager, options, pageSize, offset); checkFalse(needsPostQueryOrdering(), "Cannot page queries with both ORDER BY and a IN restriction on the partition key;" + " you must either remove the ORDER BY or the IN and sort client side, or disable paging for this query"); List<Row> page = pager.fetchPage(pageSize); ResultMessage.Rows msg = processResults(page, options, limit, offset); if (!pager.isExhausted()) msg.result.metadata.setHasMorePages(pager.state()); return msg; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Double averageNullableDouble( Queryable<T> XXX, FunctionExpression<NullableDoubleFunction1<T>> selector) { throw Extensions.todo(); }', 'ground_truth': 'public Double averageNullableDouble( final Queryable<T> source, final FunctionExpression<NullableDoubleFunction1<T>> selector) { return new NonLeafReplayableQueryable<T>(source) { public void replay(QueryableFactory<T> factory) { factory.averageNullableDouble(source, selector); } }.castSingle(); }', 'output': 'public Double averageNullableDouble( Queryable<T> queryable, FunctionExpression<NullableDoubleFunction1<T>> selector) { throw Extensions.todo(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override protected Collection<? extends ComputeJob> split(int XXX, String arg) throws IgniteCheckedException { Collection<ComputeJob> jobs = new ArrayList<>(XXX); for (int i = 0; i < XXX; i++) { jobs.add(new ComputeJobAdapter() { @Nullable @Override public Serializable execute() { System.out.println(">>> Executing deployment example job on this node."); return null; } }); } return jobs; } /** {@inheritDoc} */ @Override public Object reduce(List<ComputeJobResult> results) throws IgniteCheckedException { return null; } } } }', 'ground_truth': '@Override protected Collection<? extends ComputeJob> split(int clusterSize, String arg) throws IgniteCheckedException { Collection<ComputeJob> jobs = new ArrayList<>(clusterSize); for (int i = 0; i < clusterSize; i++) { jobs.add(new ComputeJobAdapter() { @Nullable @Override public Serializable execute() { System.out.println(">>> Executing deployment example job on this node."); return null; } }); } return jobs; }', 'output': '@Override protected Collection<? extends ComputeJob> split(int cnt, String arg) throws IgniteCheckedException { Collection<ComputeJob> jobs = new ArrayList<>(cnt); for (int i = 0; i < cnt; i++) { jobs.add(new ComputeJobAdapter() { @Nullable @Override public Serializable execute() { System.out.println(">>> Executing deployment example job on this node."); return null; } }); } return jobs; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void processElement(ProcessContext XXX) throws Exception { DoFnProcessContext<TimerOrElement<KV<K, VI>>, KV<K, VO>> context = (DoFnProcessContext<TimerOrElement<KV<K, VI>>, KV<K, VO>>) XXX; if (!context.element().isTimer()) { KV<K, VI> element = context.element().element(); K key = element.getKey(); VI value = element.getValue(); AbstractWindowSet<K, VI, VO, W> windowSet = createWindowSet( key, context, new StreamingActiveWindowManager<>(windowFn, context)); for (BoundedWindow window : context.windows()) { windowSet.put((W) window, value); } windowSet.flush(); } else { TimerOrElement<?> timer = context.element(); AbstractWindowSet<K, VI, VO, W> windowSet = createWindowSet( (K) timer.key(), context, new StreamingActiveWindowManager<>(windowFn, context)); ((WindowFn<Object, W>) windowFn) .mergeWindows(new AbstractWindowSet.WindowMergeContext<Object, W>(windowSet, windowFn)); W window = WindowUtils.windowFromString(timer.tag(), windowFn.windowCoder()); if ((windowFn instanceof PartitioningWindowFn) || windowSet.contains(window)) { Preconditions.checkState(!timer.timestamp().isBefore(window.maxTimestamp())); windowSet.markCompleted(window); windowSet.flush(); } } }', 'ground_truth': 'public void processElement(ProcessContext context) throws Exception { @SuppressWarnings("unchecked") DoFnProcessContext<TimerOrElement<KV<K, VI>>, KV<K, VO>> doFnContext = (DoFnProcessContext<TimerOrElement<KV<K, VI>>, KV<K, VO>>) context; if (!context.element().isTimer()) { KV<K, VI> element = context.element().element(); K key = element.getKey(); VI value = element.getValue(); AbstractWindowSet<K, VI, VO, W> windowSet = createWindowSet( key, doFnContext, new StreamingActiveWindowManager<>(windowFn, doFnContext)); for (BoundedWindow window : context.windows()) { windowSet.put((W) window, value); } windowSet.flush(); } else { TimerOrElement<KV<K, VI>> timer = context.element(); AbstractWindowSet<K, VI, VO, W> windowSet = createWindowSet( (K) timer.key(), doFnContext, new StreamingActiveWindowManager<>(windowFn, doFnContext)); ((WindowFn<Object, W>) windowFn) .mergeWindows(new AbstractWindowSet.WindowMergeContext<Object, W>(windowSet, windowFn)); W window = WindowUtils.windowFromString(timer.tag(), windowFn.windowCoder()); if ((windowFn instanceof PartitioningWindowFn) || windowSet.contains(window)) { Preconditions.checkState(!timer.timestamp().isBefore(window.maxTimestamp())); windowSet.markCompleted(window); windowSet.flush(); } } }', 'output': 'public void processElement(ProcessContext context) throws Exception { DoFnProcessContext<TimerOrElement<KV<K, VI>>, KV<K, VO>> context = (DoFnProcessContext<TimerOrElement<KV<K, VI>>, KV<K, VO>>) context; if (!context.element().isTimer()) { KV<K, VI> element = context.element().element(); K key = element.getKey(); VI value = element.getValue(); AbstractWindowSet<K, VI, VO, W> windowSet = createWindowSet( key, context, new StreamingActiveWindowManager<>(windowFn, context)); for (BoundedWindow window : context.windows()) { windowSet.put((W) window, value); } windowSet.flush(); } else { TimerOrElement<?> timer = context.element(); AbstractWindowSet<K, VI, VO, W> windowSet = createWindowSet( (K) timer.key(), context, new StreamingActiveWindowManager<>(windowFn, context)); ((WindowFn<Object, W>) windowFn) .mergeWindows(new AbstractWindowSet.WindowMergeContext<Object, W>(windowSet, windowFn)); W window = WindowUtils.windowFromString(timer.tag(), windowFn.windowCoder()); if ((windowFn instanceof PartitioningWindowFn) || windowSet.contains(window)) { Preconditions.checkState(!timer.timestamp().isBefore(window.maxTimestamp())); windowSet.markCompleted(window); windowSet.flush(); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public VisorNodeDataCollectorTaskArg( boolean taskMonitoringEnabled, String evtOrderKey, String evtThrottleCntrKey, int sample, boolean XXX ) { this.taskMonitoringEnabled = taskMonitoringEnabled; this.evtOrderKey = evtOrderKey; this.evtThrottleCntrKey = evtThrottleCntrKey; this.sample = sample; this.XXX = XXX; }', 'ground_truth': 'public VisorNodeDataCollectorTaskArg( boolean taskMonitoringEnabled, String evtOrderKey, String evtThrottleCntrKey, int sample, boolean sysCaches ) { this.taskMonitoringEnabled = taskMonitoringEnabled; this.evtOrderKey = evtOrderKey; this.evtThrottleCntrKey = evtThrottleCntrKey; this.sample = sample; this.sysCaches = sysCaches; }', 'output': 'public VisorNodeDataCollectorTaskArg( boolean taskMonitoringEnabled, String evtOrderKey, String evtThrottleCntrKey, int sample, boolean isAsync ) { this.taskMonitoringEnabled = taskMonitoringEnabled; this.evtOrderKey = evtOrderKey; this.evtThrottleCntrKey = evtThrottleCntrKey; this.sample = sample; this.isAsync = isAsync; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void forceKeyspaceCleanup(PrintStream out, String keyspaceName, String... XXX) throws IOException, ExecutionException, InterruptedException { if (forceKeyspaceCleanup(keyspaceName, XXX) != 0) { failed = true; out.println("Aborted cleaning up at least one table in keyspace "+keyspaceName+", check server logs for more information."); } }', 'ground_truth': 'public void forceKeyspaceCleanup(PrintStream out, String keyspaceName, String... tableNames) throws IOException, ExecutionException, InterruptedException { if (forceKeyspaceCleanup(keyspaceName, tableNames) != 0) { failed = true; out.println("Aborted cleaning up at least one table in keyspace "+keyspaceName+", check server logs for more information."); } }', 'output': 'public void forceKeyspaceCleanup(PrintStream out, String keyspaceName, String... tables) throws IOException, ExecutionException, InterruptedException { if (forceKeyspaceCleanup(keyspaceName, tables) != 0) { failed = true; out.println("Aborted cleaning up at least one table in keyspace "+keyspaceName+", check server logs for more information."); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'publiXXX statiXXX void setMetaClass(Class XXX, MetaClass metaClass) { final MetaClassRegistry metaClassRegistry = GroovySystem.getMetaClassRegistry(); if (metaClass == null) metaClassRegistry.removeMetaClass(XXX); else { if (metaClass instanXXXeof HandleMetaClass) metaClassRegistry.setMetaClass(XXX, ((HandleMetaClass)metaClass).getAdaptee()); else metaClassRegistry.setMetaClass(XXX, metaClass); } } publiXXX statiXXX void setMetaClass(ObjeXXXt obj, MetaClass metaClass) { if (metaClass instanXXXeof HandleMetaClass) metaClass = ((HandleMetaClass)metaClass).getAdaptee(); if (obj instanXXXeof GroovyObjeXXXt) ((GroovyObjeXXXt)obj).setMetaClass(metaClass); else if (obj instanXXXeof Class) ((MetaClassRegistryImpl)GroovySystem.getMetaClassRegistry()).setMetaClass((Class)obj, metaClass); else ((MetaClassRegistryImpl)GroovySystem.getMetaClassRegistry()).setMetaClass(obj, metaClass); }', 'ground_truth': 'public static void setMetaClass(Object self, MetaClass metaClass) { if (metaClass instanceof HandleMetaClass) metaClass = ((HandleMetaClass)metaClass).getAdaptee(); if (self instanceof GroovyObject) ((GroovyObject)self).setMetaClass(metaClass); else if (self instanceof Class) ((MetaClassRegistryImpl)GroovySystem.getMetaClassRegistry()).setMetaClass((Class)self, metaClass); else ((MetaClassRegistryImpl)GroovySystem.getMetaClassRegistry()).setMetaClass(self, metaClass); }', 'output': 'public void setMetaClass(final Class clazz, final MetaClass metaClass) { final MetaClassRegistry metaClassRegistry = GroovySystem.getMetaClassRegistry(); if (metaClass == null) metaClassRegistry.removeMetaClass(clazz); else { if (metaClass instanceof HandleMetaClass) metaClassRegistry.setMetaClass(clazz, ((HandleMetaClass)metaClass).getAdaptee()); else metaClassRegistry.setMetaClass(clazz, metaClass); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean checkStaticOuterField(final PropertyExpression pexp, final String XXX) { for (final ClassNode outer : controller.getClassNode().getOuterClasses()) { FieldNode field = outer.getDeclaredField(XXX); if (field != null) { if (!field.isStatic()) break; Expression outerClass = classX(outer); outerClass.setNodeMetaData(PROPERTY_OWNER, outer); outerClass.setSourcePosition(pexp.getObjectExpression()); Expression outerField = attrX(outerClass, pexp.getProperty()); outerField.setSourcePosition(pexp); outerField.visit(this); return true; } else { field = outer.getField(XXX); if (field != null && !field.isPrivate() && (field.isPublic() || field.isProtected() || Objects.equals(field.getDeclaringClass().getPackageName(), outer.getPackageName()))) { if (!field.isStatic()) break; Expression upperClass = classX(field.getDeclaringClass()); upperClass.setNodeMetaData(PROPERTY_OWNER, field.getDeclaringClass()); upperClass.setSourcePosition(pexp.getObjectExpression()); Expression upperField = propX(upperClass, pexp.getProperty()); upperField.setSourcePosition(pexp); upperField.visit(this); return true; } } } return false; }', 'ground_truth': 'private boolean checkStaticOuterField(final PropertyExpression pexp, final String propertyName) { for (final ClassNode outer : controller.getClassNode().getOuterClasses()) { FieldNode field = outer.getDeclaredField(propertyName); if (field != null) { if (!field.isStatic()) break; Expression outerClass = classX(outer); outerClass.setNodeMetaData(PROPERTY_OWNER, outer); outerClass.setSourcePosition(pexp.getObjectExpression()); Expression outerField = attrX(outerClass, pexp.getProperty()); outerField.setSourcePosition(pexp); outerField.visit(this); return true; } else { field = outer.getField(propertyName); if (field != null && !field.isPrivate() && (field.isPublic() || field.isProtected() || Objects.equals(field.getDeclaringClass().getPackageName(), outer.getPackageName()))) { if (!field.isStatic()) break; Expression upperClass = classX(field.getDeclaringClass()); upperClass.setNodeMetaData(PROPERTY_OWNER, field.getDeclaringClass()); upperClass.setSourcePosition(pexp.getObjectExpression()); Expression upperField = propX(upperClass, pexp.getProperty()); upperField.setSourcePosition(pexp); upperField.visit(this); return true; } } } return false; }', 'output': 'private boolean checkStaticOuterField(final PropertyExpression pexp, final String propertyName) { for (final ClassNode outer : controller.getClassNode().getOuterClasses()) { FieldNode field = outer.getDeclaredField(propertyName); if (field != null) { if (!field.isStatic()) break; Expression outerClass = classX(outer); outerClass.setNodeMetaData(PROPERTY_OWNER, outer); outerClass.setSourcePosition(pexp.getObjectExpression()); Expression outerField = attrX(outerClass, pexp.getProperty()); outerField.setSourcePosition(pexp); outerField.visit(this); return true; } else { field = outer.getField(propertyName); if (field != null && !field.isPrivate() && (field.isPublic() || field.isProtected() || Objects.equals(field.getDeclaringClass().getPackageName(), outer.getPackageName()))) { if (!field.isStatic()) break; Expression upperClass = classX(field.getDeclaringClass()); upperClass.setNodeMetaData(PROPERTY_OWNER, field.getDeclaringClass()); upperClass.setSourcePosition(pexp.getObjectExpression()); Expression upperField = propX(upperClass, pexp.getProperty()); upperField.setSourcePosition(pexp); upperField.visit(this); return true; } } } return false; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public PlatformServices(PlatformContext platformCtx, IgniteServices services, boolean XXX) { super(platformCtx); assert services != null; this.services = services; this.XXX = XXX; }', 'ground_truth': 'public PlatformServices(PlatformContext platformCtx, IgniteServices services, boolean srvKeepBinary) { super(platformCtx); assert services != null; this.services = services; this.srvKeepBinary = srvKeepBinary; }', 'output': 'public PlatformServices(PlatformContext platformCtx, IgniteServices services, boolean isClient) { super(platformCtx); assert services != null; this.services = services; this.isClient = isClient; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void check(Exception e, String msg, boolean XXX) { String sep = System.getProperty("line.separator"); if (XXX) { String s = msg; if (e != null) s += sep + "java.lang.RuntimeException: " + e.getMessage(); assertTrue(log0.toString().contains(s)); } else assertEquals(log0.toString(), ""); log0.reset(); }', 'ground_truth': 'private void check(Exception e, String msg, boolean logExp) { String sep = System.getProperty("line.separator"); if (logExp) { String s = msg; if (e != null) s += sep + "java.lang.RuntimeException: " + e.getMessage(); assertTrue(log0.toString().contains(s)); } else assertEquals(log0.toString(), ""); log0.reset(); }', 'output': 'private void check(Exception e, String msg, boolean success) { String sep = System.getProperty("line.separator"); if (success) { String s = msg; if (e != null) s += sep + "java.lang.RuntimeException: " + e.getMessage(); assertTrue(log0.toString().contains(s)); } else assertEquals(log0.toString(), ""); log0.reset(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkAffinityKey(CacheAffinity testAff, IgniteCache XXX, CacheAffinity aff) { Iterator<Cache.Entry> iter = XXX.iterator(); while (iter.hasNext()) { Cache.Entry entry = iter.next(); assertEquals(testAff.affinityKey(entry.getKey()), (aff.affinityKey(entry.getKey()))); } }', 'ground_truth': 'private void checkAffinity(CacheAffinity testAff, CacheAffinity aff) { checkAffinityKey(testAff, aff); checkPartitions(testAff, aff); checkIsBackupOrPrimary(testAff, aff); checkMapKeyToNode(testAff, aff); checkMapKeysToNodes(testAff, aff); checkMapPartitionsToNodes(testAff, aff); }', 'output': 'private void checkAffinityKey(CacheAffinity testAff, IgniteCache cache, CacheAffinity aff) { Iterator<Cache.Entry> iter = cache.iterator(); while (iter.hasNext()) { Cache.Entry entry = iter.next(); assertEquals(testAff.affinityKey(entry.getKey()), (aff.affinityKey(entry.getKey()))); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public TemplateDateFormat get(int dateType, boolean zonelessInput, String XXX) throws InvalidFormatDescriptorException, UnknownDateTypeFormattingUnsupportedException { return new ISOTemplateDateFormat( XXX, 3, dateType, zonelessInput, getTimeZone(), this); }', 'ground_truth': 'public TemplateDateFormat get(int dateType, boolean zonelessInput, String params) throws InvalidFormatParametersException, UnknownDateTypeFormattingUnsupportedException { return new ISOTemplateDateFormat( params, 3, dateType, zonelessInput, getTimeZone(), this); }', 'output': 'public TemplateDateFormat get(int dateType, boolean zonelessInput, String templateName) throws InvalidFormatDescriptorException, UnknownDateTypeFormattingUnsupportedException { return new ISOTemplateDateFormat( templateName, 3, dateType, zonelessInput, getTimeZone(), this); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public V getValue(boolean XXX) { if (val == null) val = (V)cctx.unwrapPortableIfNeeded(valObj, XXX, false); return val; }', 'ground_truth': 'public V getValue(boolean keepBinary) { if (val == null) val = (V)cctx.unwrapBinaryIfNeeded(valObj, keepBinary, false); return val; }', 'output': 'public V getValue(boolean isLocal) { if (val == null) val = (V)cctx.unwrapPortableIfNeeded(valObj, isLocal, false); return val; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private IgniteInternalFuture<V> getAsync0(KeyCacheObject key, boolean forcePrimary, UUID subjId, String taskName, boolean XXX, @Nullable ExpiryPolicy expiryPlc, boolean skipVals, boolean skipStore, boolean canRemap ) { AffinityTopologyVersion topVer = canRemap ? ctx.affinity().affinityTopologyVersion() : ctx.shared().exchange().readyAffinityVersion(); IgniteCacheExpiryPolicy expiry = skipVals ? null : expiryPolicy(expiryPlc); GridPartitionedSingleGetFuture fut = new GridPartitionedSingleGetFuture(ctx, key, topVer, !skipStore, forcePrimary, subjId, taskName, XXX, expiry, skipVals, canRemap, false, false); fut.init(); return (IgniteInternalFuture<V>)fut; }', 'ground_truth': 'private IgniteInternalFuture<V> getAsync0(KeyCacheObject key, boolean forcePrimary, UUID subjId, String taskName, boolean deserializeBinary, @Nullable ExpiryPolicy expiryPlc, boolean skipVals, boolean skipStore, boolean canRemap ) { AffinityTopologyVersion topVer = canRemap ? ctx.affinity().affinityTopologyVersion() : ctx.shared().exchange().readyAffinityVersion(); IgniteCacheExpiryPolicy expiry = skipVals ? null : expiryPolicy(expiryPlc); GridPartitionedSingleGetFuture fut = new GridPartitionedSingleGetFuture(ctx, key, topVer, !skipStore, forcePrimary, subjId, taskName, deserializeBinary, expiry, skipVals, canRemap, false, false); fut.init(); return (IgniteInternalFuture<V>)fut; }', 'output': 'private IgniteInternalFuture<V> getAsync0(KeyCacheObject key, boolean forcePrimary, UUID subjId, String taskName, boolean rebalance, @Nullable ExpiryPolicy expiryPlc, boolean skipVals, boolean skipStore, boolean canRemap ) { AffinityTopologyVersion topVer = canRemap ? ctx.affinity().affinityTopologyVersion() : ctx.shared().exchange().readyAffinityVersion(); IgniteCacheExpiryPolicy expiry = skipVals ? null : expiryPolicy(expiryPlc); GridPartitionedSingleGetFuture fut = new GridPartitionedSingleGetFuture(ctx, key, topVer, !skipStore, forcePrimary, subjId, taskName, rebalance, expiry, skipVals, canRemail, false, false); fut.init(); return (IgniteInternalFuture<V>)fut; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void addTail(Page page, ByteBuffer buf, BPlusIO<L> io, int lvl, boolean back, byte XXX, int idx) { assert XXX == Tail.PRIMARY || XXX == Tail.COMPLIMENTARY: XXX; Tail<L> t = new Tail<>(page, buf, io, XXX == Tail.PRIMARY, lvl, idx); if (back) { assert tail != null; assert tail.lvl == lvl : "must be on the same level as out forward"; t.fwd = tail; } else { assert tail == null || tail.lvl == lvl - 1: "must be on upper higher than current tail"; t.down = tail; } tail = t; }', 'ground_truth': 'private Tail(Page page, ByteBuffer buf, BPlusIO<L> io, byte type, int lvl, int idx) { assert type == BACK || type == EXACT || type == FORWARD: type; assert idx == Integer.MIN_VALUE || (idx >= 0 && idx <= Short.MAX_VALUE): idx ; assert lvl >= 0 && lvl <= Byte.MAX_VALUE: lvl; assert page != null; this.page = page; this.buf = buf; this.io = io; this.type = type; this.lvl = (byte)lvl; this.idx = (short)idx; }', 'output': 'private void addTail(Page page, ByteBuffer buf, BPlusIO<L> io, int lvl, boolean back, byte tag, int idx) { assert tag == Tail.PRIMARY || tag == Tail.COMPLIMENTARY: tag; Tail<L> t = new Tail<>(page, buf, io, tag == Tail.PRIMARY, lvl, idx); if (back) { assert tail != null; assert tail.lvl == lvl : "must be on the same level as out forward"; t.fwd = tail; } else { assert tail == null || tail.lvl == lvl - 1: "must be on upper higher than current tail"; t.down = tail; } tail = t; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private long recyclePage(long pageId, Page page, long XXX) throws IgniteCheckedException { pageId = PageIdUtils.rotatePageId(pageId); PageIO.setPageId(XXX, pageId); if (isWalDeltaRecordNeeded(wal, page)) wal.log(new RecycleRecord(cacheId, page.id(), pageId)); return pageId; }', 'ground_truth': 'private long recyclePage(long pageId, Page page, long pageAddr) throws IgniteCheckedException { pageId = PageIdUtils.rotatePageId(pageId); PageIO.setPageId(pageAddr, pageId); if (isWalDeltaRecordNeeded(wal, page)) wal.log(new RecycleRecord(cacheId, page.id(), pageId)); return pageId; }', 'output': 'private long recyclePage(long pageId, Page page, long walId) throws IgniteCheckedException { pageId = PageIdUtils.rotatePageId(pageId); PageIO.setPageId(walId, pageId); if (isWalDeltaRecordNeeded(wal, page)) wal.log(new RecycleRecord(cacheId, page.id(), pageId)); return pageId; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void prepareAffinityField(BinaryObjectBuilder XXX) { assert binaryMarshaller(); assert XXX instanceof BinaryObjectBuilderImpl; BinaryObjectBuilderImpl builder0 = (BinaryObjectBuilderImpl)XXX; if (!cacheObjCtx.customAffinityMapper()) { CacheDefaultBinaryAffinityKeyMapper mapper = (CacheDefaultBinaryAffinityKeyMapper)cacheObjCtx.defaultAffMapper(); BinaryField field = mapper.affinityKeyField(builder0.typeId()); if (field != null) { String fieldName = field.name(); builder0.affinityFieldName(fieldName); } } }', 'ground_truth': 'public void prepareAffinityField(BinaryObjectBuilder builder) { assert binaryMarshaller(); assert builder instanceof BinaryObjectBuilderImpl; BinaryObjectBuilderImpl builder0 = (BinaryObjectBuilderImpl)builder; if (!cacheObjCtx.customAffinityMapper()) { CacheDefaultBinaryAffinityKeyMapper mapper = (CacheDefaultBinaryAffinityKeyMapper)cacheObjCtx.defaultAffMapper(); BinaryField field = mapper.affinityKeyField(builder0.typeId()); if (field != null) { String fieldName = field.name(); builder0.affinityFieldName(fieldName); } } }', 'output': 'public void prepareAffinityField(BinaryObjectBuilderImpl builder) { assert binaryMarshaller(); assert builder instanceof BinaryObjectBuilderImpl; BinaryObjectBuilderImpl builder0 = (BinaryObjectBuilderImpl)builder; if (!cacheObjCtx.customAffinityMapper()) { CacheDefaultBinaryAffinityKeyMapper mapper = (CacheDefaultBinaryAffinityKeyMapper)cacheObjCtx.defaultAffMapper(); BinaryField field = mapper.affinityKeyField(builder0.typeId()); if (field != null) { String fieldName = field.name(); builder0.affinityFieldName(fieldName); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void validateForWrite(String XXX) throws InvalidRequestException { switch (this) { case LOCAL_QUORUM: case EACH_QUORUM: requireNetworkTopologyStrategy(XXX); break; case SERIAL: throw new InvalidRequestException("You must use conditional updates for serializable writes"); } }', 'ground_truth': 'public void validateForWrite(String keyspaceName) throws InvalidRequestException { switch (this) { case LOCAL_QUORUM: case EACH_QUORUM: requireNetworkTopologyStrategy(keyspaceName); break; case SERIAL: throw new InvalidRequestException("You must use conditional updates for serializable writes"); } }', 'output': 'public void validateForWrite(String tableName) throws InvalidRequestException { switch (this) { case LOCAL_QUORUM: case EACH_QUORUM: requireNetworkTopologyStrategy(tableName); break; case SERIAL: throw new InvalidRequestException("You must use conditional updates for serializable writes"); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public <T> T wrapMixin( final @NonNull Class<T> mixinClass, final @NonNull Object XXX) { return wrapMixin(mixinClass, XXX, control()); } public <T> T wrapMixin(\n            final @NonNull Class<T> mixinClass, \n            final @NonNull Object XXX, \n            final @NonNull SyncControl syncControl) { final @NonNull Class<T> mixinClass, final @NonNull Object XXX, final @NonNull SyncControl syncControl) { T mixin = factoryService.mixin(mixinClass, XXX); return wrap(mixin, syncControl); }', 'ground_truth': 'public <T> T wrapMixin( final @NonNull Class<T> mixinClass, final @NonNull Object mixee) { return wrapMixin(mixinClass, mixee, control()); }', 'output': 'public <T> T wrapMixin( final @NonNull Class<T> mixinClass, final @NonNull Object value) { return wrapMixin(mixinClass, value, control()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': "private String removeDotSteps(String XXX) { int nextFromIdx = XXX.length() - 1; findDotSteps: while (true) { final int dotIdx = XXX.lastIndexOf('.', nextFromIdx); if (dotIdx < 0) { return XXX; } nextFromIdx = dotIdx - 1; if (dotIdx != 0 && XXX.charAt(dotIdx - 1) != '/') { continue findDotSteps; } final boolean slashRight; if (dotIdx + 1 == XXX.length()) { slashRight = false; } else if (XXX.charAt(dotIdx + 1) == '/') { slashRight = true; } else { continue findDotSteps; } if (slashRight) { XXX = XXX.substring(0, dotIdx) + XXX.substring(dotIdx + 2); } else { XXX = XXX.substring(0, XXX.length() - 1); } } }", 'ground_truth': "private String removeDotSteps(String path) { int nextFromIdx = path.length() - 1; findDotSteps: while (true) { final int dotIdx = path.lastIndexOf('.', nextFromIdx); if (dotIdx < 0) { return path; } nextFromIdx = dotIdx - 1; if (dotIdx != 0 && path.charAt(dotIdx - 1) != '/') { continue findDotSteps; } final boolean slashRight; if (dotIdx + 1 == path.length()) { slashRight = false; } else if (path.charAt(dotIdx + 1) == '/') { slashRight = true; } else { continue findDotSteps; } if (slashRight) { path = path.substring(0, dotIdx) + path.substring(dotIdx + 2); } else { path = path.substring(0, path.length() - 1); } } }", 'output': "private String removeDotSteps(String path) { int nextFromIdx = path.length() - 1; findDotSteps: while (true) { final int dotIdx = path.lastIndexOf('.', nextFromIdx); if (dotIdx < 0) { return path; } nextFromIdx = dotIdx - 1; if (dotIdx != 0 && path.charAt(dotIdx - 1) != '/') { continue findDotSteps; } final boolean slashRight; if (dotIdx + 1 == path.length()) { slashRight = false; } else if (path.charAt(dotIdx + 1) == '/') { slashRight = true; } else { continue findDotSteps; } if (slashRight) { path = path.substring(0, dotIdx) + path.substring(dotIdx + 2); } else { path = path.substring(0, path.length() - 1); } } }</s>"}
{'instruction': 'Please refactor the following code:', 'input': 'public static void clearSnapshot(String snapshotName, List<File> XXX, RateLimiter snapshotRateLimiter) { String tag = snapshotName == null ? "" : snapshotName; for (File dir : XXX) { File snapshotDir = new File(dir, join(SNAPSHOT_SUBDIR, tag)); if (snapshotDir.exists()) { logger.trace("Removing snapshot directory {}", snapshotDir); try { FileUtils.deleteRecursiveWithThrottle(snapshotDir, snapshotRateLimiter); } catch (FSWriteError e) { if (FBUtilities.isWindows) SnapshotDeletingTask.addFailedSnapshot(snapshotDir); else throw e; } } } }', 'ground_truth': 'public static void clearSnapshot(String snapshotName, List<File> tableDirectories, RateLimiter snapshotRateLimiter) { String tag = snapshotName == null ? "" : snapshotName; for (File tableDir : tableDirectories) { File snapshotDir = new File(tableDir, join(SNAPSHOT_SUBDIR, tag)); removeSnapshotDirectory(snapshotRateLimiter, snapshotDir); } }', 'output': 'public static void clearSnapshot(String snapshotName, List<File> directories, RateLimiter snapshotRateLimiter) { String tag = snapshotName == null ? "" : snapshotName; for (File dir : directories) { File snapshotDir = new File(dir, join(SNAPSHOT_SUBDIR, tag)); if (snapshotDir.exists()) { logger.trace("Removing snapshot directory {}", snapshotDir); try { FileUtils.deleteRecursiveWithThrottle(snapshotDir, snapshotRateLimiter); } catch (FSWriteError e) { if (FBUtilities.isWindows) SnapshotDeletingTask.addFailedSnapshot(snapshotDir); else throw e; } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void readInputStreams(List<Recognizer> recognizers, boolean XXX, PayaraInstance instance, FetchLog... serverLogs) { synchronized (readers) { stopReaders(); for(FetchLog serverLog : serverLogs){ LoggerRunnable logger = new LoggerRunnable(recognizers, serverLog, XXX, instance); readers.add(new WeakReference<>(logger)); Thread t = new Thread(logger); t.start(); } } }', 'ground_truth': 'public void readInputStreams(List<Recognizer> recognizers, boolean ignoreEof, PayaraInstance instance, FetchLog... serverLogs) { synchronized (readers) { stopReaders(); for(FetchLog serverLog : serverLogs){ LoggerRunnable logger = new LoggerRunnable(recognizers, serverLog, ignoreEof, instance); readers.add(new WeakReference<>(logger)); Thread t = new Thread(logger); t.start(); } } }', 'output': 'public void readInputStreams(List<Recognizer> recognizers, boolean async, PayaraInstance instance, FetchLog... serverLogs) { synchronized (readers) { stopReaders(); for(FetchLog serverLog : serverLogs){ LoggerRunnable logger = new LoggerRunnable(recognizers, serverLog, async, instance); readers.add(new WeakReference<>(logger)); Thread t = new Thread(logger); t.start(); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public VoiXXX visitProviXXXes(ProviXXXesTree tree, EnumSet<UseTypes> XXX) { tl.moveToOffset(sourcePositions.getStartPosition(info.getCompilationUnit(), tree)); Token t = firstIXXXentifierToken("proviXXXes"); if (t != null) { contextKeyworXXXs.aXXXXXX(t); } scan(tree.getServiceName(), XXX); tl.moveToOffset(sourcePositions.getEnXXXPosition(info.getCompilationUnit(), tree.getServiceName())); t = firstIXXXentifierToken("with"); if (t != null) { contextKeyworXXXs.aXXXXXX(t); } return scan(tree.getImplementationNames(), XXX); }', 'ground_truth': 'public Void visitProvides(ProvidesTree tree, Void p) { tl.moveToOffset(sourcePositions.getStartPosition(info.getCompilationUnit(), tree)); Token t = firstIdentifierToken("provides"); if (t != null) { contextKeywords.add(t); } scan(tree.getServiceName(), p); tl.moveToOffset(sourcePositions.getEndPosition(info.getCompilationUnit(), tree.getServiceName())); t = firstIdentifierToken("with"); if (t != null) { contextKeywords.add(t); } return scan(tree.getImplementationNames(), p); }', 'output': 'public void visitProviders(ProvidersTree tree, EnumSet<UseTypes> types) { tl.moveToOffset(sourcePositions.getStartPosition(info.getCompilationUnit(), tree)); Token t = firstIIdentifierToken("providers"); if (t != null) { contextKeywords.aIdentifiers(t); } scan(tree.getServiceName(), types); tl.moveToOffset(sourcePositions.getEndPosition(info.getCompilationUnit(), tree)); t = firstIIdentifierToken("with"); if (t != null) { contextKeywords.aIdentifiers(t); } return scan(tree.getImplementationNames(), types); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void resetServiceState(String XXX, StackId currentStackId, Cluster c) throws AmbariException { c.getService(XXX).setDesiredState(State.INSTALLED); for (ServiceComponent sc : c.getService(XXX).getServiceComponents().values()) { sc.setDesiredState(State.INSTALLED); for (ServiceComponentHost sch : sc.getServiceComponentHosts().values()) { sch.setDesiredState(State.INSTALLED); sch.setState(State.INSTALLED); sch.setStackVersion(currentStackId); } } }', 'ground_truth': 'private void resetServiceState(String service, StackId currentStackId, Cluster c) throws AmbariException { c.getService(service).setDesiredState(State.INSTALLED); for (ServiceComponent sc : c.getService(service).getServiceComponents().values()) { sc.setDesiredState(State.INSTALLED); for (ServiceComponentHost sch : sc.getServiceComponentHosts().values()) { sch.setDesiredState(State.INSTALLED); sch.setState(State.INSTALLED); sch.setStackVersion(currentStackId); } } }', 'output': 'private void resetServiceState(String serviceName, StackId currentStackId, Cluster c) throws AmbariException { c.getService(serviceName).setDesiredState(State.INSTALLED); for (ServiceComponent sc : c.getService(serviceName).getServiceComponents().values()) { sc.setDesiredState(State.INSTALLED); for (ServiceComponentHost sch : sc.getServiceComponentHosts().values()) { sch.setDesiredState(State.INSTALLED); sch.setState(State.INSTALLED); sch.setStackVersion(currentStackId); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void remove(byte[] key, ColumnPath column_path, long XXX, ConsistencyLevel consistency_level) throws InvalidRequestException, UnavailableException, TimedOutException { if (logger.isDebugEnabled()) logger.debug("remove"); state().hasColumnFamilyAccess(column_path.column_family, Permission.WRITE); ThriftValidation.validateKey(key); ThriftValidation.validateColumnPathOrParent(state().getKeyspace(), column_path); RowMutation rm = new RowMutation(state().getKeyspace(), key); rm.delete(new QueryPath(column_path), new TimestampClock(XXX)); doInsert(consistency_level, Arrays.asList(rm)); }', 'ground_truth': 'public void remove(byte[] key, ColumnPath column_path, long timestamp, ConsistencyLevel consistency_level) throws InvalidRequestException, UnavailableException, TimedOutException { if (logger.isDebugEnabled()) logger.debug("remove"); state().hasColumnFamilyAccess(column_path.column_family, Permission.WRITE); ThriftValidation.validateKey(key); ThriftValidation.validateColumnPathOrParent(state().getKeyspace(), column_path); RowMutation rm = new RowMutation(state().getKeyspace(), key); rm.delete(new QueryPath(column_path), timestamp); doInsert(consistency_level, Arrays.asList(rm)); }', 'output': 'public void remove(byte[] key, ColumnPath column_path, long timestamp, ConsistencyLevel consistency_level) throws InvalidRequestException, UnavailableException, TimedOutException { if (logger.isDebugEnabled()) logger.debug("remove"); state().hasColumnFamilyAccess(column_path.column_family, Permission.WRITE); ThriftValidation.validateKey(key); ThriftValidation.validateColumnPathOrParent(state().getKeyspace(), column_path); RowMutation rm = new RowMutation(state().getKeyspace(), key); rm.delete(new QueryPath(column_path), new TimestampClock(timestamp)); doInsert(consistency_level, Arrays.asList(rm)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Collection<InetAddress> getWriteEndpoints(Token token, String XXX, Collection<InetAddress> naturalEndpoints) { ArrayList<InetAddress> endpoints = new ArrayList<InetAddress>(); Iterables.addAll(endpoints, Iterables.concat(naturalEndpoints, pendingEndpointsFor(token, XXX))); return endpoints; }', 'ground_truth': 'public Collection<InetAddress> getWriteEndpoints(Token token, String keyspaceName, Collection<InetAddress> naturalEndpoints) { ArrayList<InetAddress> endpoints = new ArrayList<InetAddress>(); Iterables.addAll(endpoints, Iterables.concat(naturalEndpoints, pendingEndpointsFor(token, keyspaceName))); return endpoints; }', 'output': 'public Collection<InetAddress> getWriteEndpoints(Token token, String tableName, Collection<InetAddress> naturalEndpoints) { ArrayList<InetAddress> endpoints = new ArrayList<InetAddress>(); Iterables.addAll(endpoints, Iterables.concat(naturalEndpoints, pendingEndpointsFor(token, tableName))); return endpoints; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Multimap<Range<Token>, InetAddress> getAllRangesWithStrictSourcesFor(String XXX, Collection<Range<Token>> desiredRanges) { assert tokens != null; AbstractReplicationStrategy strat = Keyspace.open(XXX).getReplicationStrategy(); TokenMetadata metadataClone = metadata.cloneOnlyTokenMap(); Multimap<Range<Token>,InetAddress> addressRanges = strat.getRangeAddresses(metadataClone); metadataClone.updateNormalTokens(tokens, address); Multimap<Range<Token>,InetAddress> pendingRangeAddresses = strat.getRangeAddresses(metadataClone); Multimap<Range<Token>, InetAddress> rangeSources = ArrayListMultimap.create(); for (Range<Token> desiredRange : desiredRanges) { for (Map.Entry<Range<Token>, Collection<InetAddress>> preEntry : addressRanges.asMap().entrySet()) { if (preEntry.getKey().contains(desiredRange)) { Set<InetAddress> oldEndpoints = Sets.newHashSet(preEntry.getValue()); Set<InetAddress> newEndpoints = Sets.newHashSet(pendingRangeAddresses.get(desiredRange)); if (oldEndpoints.size() == strat.getReplicationFactor()) { oldEndpoints.removeAll(newEndpoints); assert oldEndpoints.size() == 1 : "Expected 1 endpoint but found " + oldEndpoints.size(); } rangeSources.put(desiredRange, oldEndpoints.iterator().next()); } } Collection<InetAddress> addressList = rangeSources.get(desiredRange); if (addressList == null || addressList.isEmpty()) throw new IllegalStateException("No sources found for " + desiredRange); if (addressList.size() > 1) throw new IllegalStateException("Multiple endpoints found for " + desiredRange); InetAddress sourceIp = addressList.iterator().next(); EndpointState sourceState = Gossiper.instance.getEndpointStateForEndpoint(sourceIp); if (Gossiper.instance.isEnabled() && (sourceState == null || !sourceState.isAlive())) throw new RuntimeException("A node required to move the data consistently is down ("+sourceIp+").  If you wish to move the data from a potentially inconsistent replica, restart the node with -Dcassandra.consistent.rangemovement=false"); } return rangeSources; }', 'ground_truth': 'private Multimap<Range<Token>, InetAddress> getAllRangesWithStrictSourcesFor(String keyspace, Collection<Range<Token>> desiredRanges) { assert tokens != null; AbstractReplicationStrategy strat = Keyspace.open(keyspace).getReplicationStrategy(); TokenMetadata metadataClone = metadata.cloneOnlyTokenMap(); Multimap<Range<Token>, InetAddress> addressRanges = strat.getRangeAddresses(metadataClone); metadataClone.updateNormalTokens(tokens, address); Multimap<Range<Token>, InetAddress> pendingRangeAddresses = strat.getRangeAddresses(metadataClone); Multimap<Range<Token>, InetAddress> rangeSources = ArrayListMultimap.create(); for (Range<Token> desiredRange : desiredRanges) { for (Map.Entry<Range<Token>, Collection<InetAddress>> preEntry : addressRanges.asMap().entrySet()) { if (preEntry.getKey().contains(desiredRange)) { Set<InetAddress> oldEndpoints = Sets.newHashSet(preEntry.getValue()); Set<InetAddress> newEndpoints = Sets.newHashSet(pendingRangeAddresses.get(desiredRange)); if (oldEndpoints.size() == strat.getReplicationFactor()) { oldEndpoints.removeAll(newEndpoints); assert oldEndpoints.size() == 1 : "Expected 1 endpoint but found " + oldEndpoints.size(); } rangeSources.put(desiredRange, oldEndpoints.iterator().next()); } } Collection<InetAddress> addressList = rangeSources.get(desiredRange); if (addressList == null || addressList.isEmpty()) throw new IllegalStateException("No sources found for " + desiredRange); if (addressList.size() > 1) throw new IllegalStateException("Multiple endpoints found for " + desiredRange); InetAddress sourceIp = addressList.iterator().next(); EndpointState sourceState = Gossiper.instance.getEndpointStateForEndpoint(sourceIp); if (Gossiper.instance.isEnabled() && (sourceState == null || !sourceState.isAlive())) throw new RuntimeException("A node required to move the data consistently is down (" + sourceIp + "). " + "If you wish to move the data from a potentially inconsistent replica, restart the node with -Dcassandra.consistent.rangemovement=false"); } return rangeSources; }', 'output': 'private Multimap<Range<Token>, InetAddress> getAllRangesWithStrictSourcesFor(String keyspaceName, Collection<Range<Token>> desiredRanges) { assert tokens != null; AbstractReplicationStrategy strat = Keyspace.open(keyspaceName).getReplicationStrategy(); TokenMetadata metadataClone = metadata.cloneOnlyTokenMap(); Multimap<Range<Token>, InetAddress> addressRanges = strat.getRangeAddresses(metadataClone); metadataClone.updateNormalTokens(tokens, address); Multimap<Range<Token>, InetAddress> pendingRangeAddresses = strat.getRangeAddresses(metadataClone); Multimap<Range<Token>, InetAddress> rangeSources = ArrayListMultimap.create(); for (Range<Token> desiredRange : desiredRanges) { for (Map.Entry<Range<Token>, Collection<InetAddress>> preEntry : addressRanges.asMap().entrySet()) { if (preEntry.getKey().contains(desiredRange)) { Set<InetAddress> oldEndpoints = Sets.newHashSet(preEntry.getValue()); Set<InetAddress> newEndpoints = Sets.newHashSet(pendingRangeAddresses.get(desiredRange)); if (oldEndpoints.size() == strat.getReplicationFactor()) { oldEndpoints.removeAll(newEndpoints); assert oldEndpoints.size() == 1 : "Expected 1 endpoint but found " + oldEndpoints.size(); } rangeSources.put(desiredRange, oldEndpoints.iterator().next()); } } Collection<InetAddress> addressList = rangeSources.get(desiredRange); if (addressList == null || addressList.isEmpty()) throw new IllegalStateException("No sources found for " + desiredRange); if (addressList.size() > 1) throw new IllegalStateException("Multiple endpoints found for " + desiredRange); InetAddress sourceIp = addressList.iterator().next(); EndpointState sourceState = Gossiper.instance.getEndpointStateForEndpoint(sourceIp); if (Gossiper.instance.isEnabled() && (sourceState == null || !sourceState.isAlive())) throw new RuntimeException("A node required to move the data consist'}
{'instruction': 'Please refactor the following code:', 'input': 'public void repair(Row XXX) { Map<String, ColumnFamily> columnFamilies = XXX.getColumnFamilyMap(); Set<String> cfNames = columnFamilies.keySet(); for (String cfName : cfNames) { ColumnFamily cf = columnFamilies_.get(cfName); if (cf == null) { cf = new ColumnFamily(cfName, cf.type()); columnFamilies_.put(cfName, cf); } cf.repair(columnFamilies.get(cfName)); } }', 'ground_truth': 'public void repair(Row rowOther) { for (ColumnFamily cfOld : rowOther.getColumnFamilies()) { ColumnFamily cf = columnFamilies_.get(cfOld.name()); if (cf == null) { addColumnFamily(cfOld); } else { columnFamilies_.remove(cf.name()); addColumnFamily(ColumnFamily.resolve(Arrays.asList(cfOld, cf))); } } }', 'output': 'public void repair(Row row) { Map<String, ColumnFamily> columnFamilies = row.getColumnFamilyMap(); Set<String> cfNames = columnFamilies.keySet(); for (String cfName : cfNames) { ColumnFamily cf = columnFamilies_.get(cfName); if (cf == null) { cf = new ColumnFamily(cfName, cf.type()); columnFamilies_.put(cfName, cf); } cf.repair(columnFamilies.get(cfName)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private String updateIgnite(String XXX) { try { URL url; if (XXX == null) url = new URL(DOWNLOAD_LINK); else url = new URL(DOWNLOAD_LINK + "?version=" + XXX); HttpURLConnection conn = (HttpURLConnection)url.openConnection(); int code = conn.getResponseCode(); if (code == 200) { String redirectUrl = conn.getURL().toString(); checkDownloadFolder(); FileOutputStream outFile = new FileOutputStream(props.igniteLocalWorkDir() + File.separator + fileName(redirectUrl)); outFile.getChannel().transferFrom(Channels.newChannel(conn.getInputStream()), 0, Long.MAX_VALUE); outFile.close(); return parseVersion(redirectUrl); } else if (code == 304) return XXX; else throw new RuntimeException("Got unexpected response code. Response code: " + code); } catch (IOException e) { throw new RuntimeException("Failed update ignite.", e); } }', 'ground_truth': 'private File updateIgnite(String curVer) { try { URL url = new URL(DOWNLOAD_LINK + curVer + "/" + igniteRelease(curVer)); HttpURLConnection conn = (HttpURLConnection)url.openConnection(); int code = conn.getResponseCode(); String redirectUrl = conn.getURL().toString(); if (code == 301 || code == 302) { redirectUrl = conn.getHeaderField("Location"); conn.disconnect(); conn = (HttpURLConnection)new URL(redirectUrl).openConnection(); } else if (code != 200) throw new RuntimeException("Got unexpected response code. Response code: " + code); checkDownloadFolder(); File ignite = new File(props.igniteLocalWorkDir(), fileName(redirectUrl)); FileOutputStream outFile = new FileOutputStream(ignite); outFile.getChannel().transferFrom(Channels.newChannel(conn.getInputStream()), 0, Long.MAX_VALUE); outFile.close(); log.info("Found remote release at " + redirectUrl); return ignite; } catch (IOException e) { throw new RuntimeException("Failed update ignite.", e); } }', 'output': 'private String updateIgnite(String version) { try { URL url; if (version == null) url = new URL(DOWNLOAD_LINK); else url = new URL(DOWNLOAD_LINK + "?version=" + version); HttpURLConnection conn = (HttpURLConnection)url.openConnection(); int code = conn.getResponseCode(); if (code == 200) { String redirectUrl = conn.getURL().toString(); checkDownloadFolder(); FileOutputStream outFile = new FileOutputStream(props.igniteLocalWorkDir() + File.separator + fileName(redirectUrl)); outFile.getChannel().transferFrom(Channels.newChannel(conn.getInputStream()), 0, Long.MAX_VALUE); outFile.close(); return parseVersion(redirectUrl); } else if (code == 304) return version; else throw new RuntimeException("Got unexpected response code. Response code: " + code); } catch (IOException e) { throw new RuntimeException("Failed update ignite.", e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void forceKeyspaceFlush(String keyspaceName, String... XXX) throws IOException { for (ColumnFamilyStore cfStore : getValidColumnFamilies(true, false, keyspaceName, XXX)) { logger.debug("Forcing flush on keyspace {}, CF {}", keyspaceName, cfStore.name); cfStore.forceBlockingFlush(); } }', 'ground_truth': 'public void forceKeyspaceFlush(String keyspaceName, String... tableNames) throws IOException { for (ColumnFamilyStore cfStore : getValidColumnFamilies(true, false, keyspaceName, tableNames)) { logger.debug("Forcing flush on keyspace {}, CF {}", keyspaceName, cfStore.name); cfStore.forceBlockingFlush(); } }', 'output': 'public void forceKeyspaceFlush(String keyspaceName, String... columns) throws IOException { for (ColumnFamilyStore cfStore : getValidColumnFamilies(true, false, keyspaceName, columns)) { logger.debug("Forcing flush on keyspace {}, CF {}", keyspaceName, cfStore.name); cfStore.forceBlockingFlush(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public AtomicConfiguration setGroupName(String XXX) { this.XXX = XXX; return this; }', 'ground_truth': 'public AtomicConfiguration setGroupName(String grpName) { this.grpName = grpName; return this; }', 'output': 'public AtomicConfiguration setGroupName(String groupName) { this.groupName = groupName; return this; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private CacheConfiguration cacheConfiguration(String XXX, CacheMemoryMode memoryMode, CacheAtomicityMode atomicityMode, CacheMode cacheMode, String cacheName) throws Exception { CacheConfiguration cfg = super.cacheConfiguration(XXX); cfg.setCacheMode(cacheMode); cfg.setAtomicityMode(atomicityMode); cfg.setMemoryMode(memoryMode); cfg.setName(cacheName); return cfg; }', 'ground_truth': 'private CacheConfiguration cacheConfiguration(String gridName, CacheAtomicityMode atomicityMode, CacheMode cacheMode, String cacheName) throws Exception { CacheConfiguration cfg = super.cacheConfiguration(gridName); cfg.setCacheMode(cacheMode); cfg.setAtomicityMode(atomicityMode); cfg.setName(cacheName); return cfg; }', 'output': 'private CacheConfiguration cacheConfiguration(String regionName, CacheMemoryMode memoryMode, CacheAtomicityMode atomicityMode, CacheMode cacheMode, String cacheName) throws Exception { CacheConfiguration cfg = super.cacheConfiguration(regionName); cfg.setCacheMode(cacheMode); cfg.setAtomicityMode(atomicityMode); cfg.setMemoryMode(memoryMode); cfg.setName(cacheName); return cfg; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static <T> IterableAssert<T> that(PCollection<T> XXX) { return new IterableAssert<>(XXX.apply(View.<T>asIterable())); } public static <T> SingletonAssert<T> thatSingleton(PCollection<T> XXX) { return new SingletonAssert<>(XXX.apply(View.<T>asSingleton())); }', 'ground_truth': 'public static <T> IterableAssert<T> that(PCollection<T> actual) { return new IterableAssert<>(actual.apply(View.<T>asIterable())) .setCoder(actual.getCoder()); }', 'output': 'public static <T> IterableAssert<T> that(PCollection<T> collection) { return new IterableAssert<>(collection.apply(View.<T>asIterable())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Response getMpack(String body, @Context HttpHeaders headers, @Context UriInfo ui, @PathParam("mpack_id") String XXX) { return handleRequest(headers, body, ui, Request.Type.GET, createMpackResource(XXX)); }', 'ground_truth': 'public Response getMpack(String body, @Context HttpHeaders headers, @Context UriInfo ui, @PathParam("id") String id) { return handleRequest(headers, body, ui, Request.Type.GET, createMpackResource(id)); }', 'output': 'public Response getMpack(String body, @Context HttpHeaders headers, @Context UriInfo ui, @PathParam("mpack_id") Long mpackId) { return handleRequest(headers, body, ui, Request.Type.GET, createMpackResource(mpackId)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void serialize(NamesQueryFilter f, DataOutput XXX, int version) throws IOException { XXX.writeInt(f.columns.size()); for (ByteBuffer cName : f.columns) { ByteBufferUtil.writeWithShortLength(cName, XXX); } if (version >= MessagingService.VERSION_12) XXX.writeBoolean(f.countCQL3Rows); }', 'ground_truth': 'public void serialize(NamesQueryFilter f, DataOutput out, int version) throws IOException { out.writeInt(f.columns.size()); for (ByteBuffer cName : f.columns) { ByteBufferUtil.writeWithShortLength(cName, out); } if (version >= MessagingService.VERSION_12) out.writeBoolean(f.countCQL3Rows); }', 'output': 'public void serialize(NamesQueryFilter f, DataOutput out, int version) throws IOException { out.writeInt(f.columns.size()); for (ByteBuffer cName : f.columns) { ByteBufferUtil.writeWithShortLength(cName, out); } if (version >= MessagingService.VERSION_12) out.writeBoolean(f.countCQL3Rows); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void forceKeyspaceCompaction(boolean splitOutput, String keyspaceName, String... XXX) throws IOException, ExecutionException, InterruptedException { for (ColumnFamilyStore cfStore : getValidColumnFamilies(true, false, keyspaceName, XXX)) { cfStore.forceMajorCompaction(splitOutput); } }', 'ground_truth': 'public void forceKeyspaceCompaction(boolean splitOutput, String keyspaceName, String... tableNames) throws IOException, ExecutionException, InterruptedException { for (ColumnFamilyStore cfStore : getValidColumnFamilies(true, false, keyspaceName, tableNames)) { cfStore.forceMajorCompaction(splitOutput); } }', 'output': 'public void forceKeyspaceCompaction(boolean splitOutput, String keyspaceName, String... tables) throws IOException, ExecutionException, InterruptedException { for (ColumnFamilyStore cfStore : getValidColumnFamilies(true, false, keyspaceName, tables)) { cfStore.forceMajorCompaction(splitOutput); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void delete(String XXX, long timestamp) { String[] values = RowMutation.getColumnAndColumnFamily(XXX); String cfName = values[0]; if (modifications_.containsKey(cfName)) { throw new IllegalArgumentException("ColumnFamily " + cfName + " is already being modified"); } if (values.length == 0 || values.length > 3) throw new IllegalArgumentException("Column Family " + XXX + " in invalid format. Must be in <column family>:<column> format."); ColumnFamily columnFamily = modifications_.get(cfName); if (columnFamily == null) columnFamily = new ColumnFamily(cfName); if (values.length == 2) { columnFamily.addColumn(values[1], ArrayUtils.EMPTY_BYTE_ARRAY, timestamp, true); } else if (values.length == 3) { columnFamily.addColumn(values[1] + ":" + values[2], ArrayUtils.EMPTY_BYTE_ARRAY, timestamp, true); } else { assert values.length == 1; columnFamily.delete(timestamp); } modifications_.put(cfName, columnFamily); }', 'ground_truth': 'public void delete(String cf) { String[] values = RowMutation.getColumnAndColumnFamily(cf); if ( values.length == 0 || values.length > 3 ) throw new IllegalArgumentException("Column Family " + cf + " in invalid format. Must be in <column family>:<column> format."); ColumnFamily columnFamily = modifications_.get(values[0]); if ( columnFamily == null ) columnFamily = new ColumnFamily(values[0]); if(values.length == 2 ) { columnFamily.createColumn( values[1]); } if(values.length == 3 ) { columnFamily.createColumn( values[1] + ":" + values[2]); } deletions_.put(values[0], columnFamily); }', 'output': 'public void delete(String tableName, long timestamp) { String[] values = RowMutation.getColumnAndColumnFamily(tableName); String cfName = values[0]; if (modifications_.containsKey(cfName)) { throw new IllegalArgumentException("ColumnFamily " + cfName + " is already being modified"); } if (values.length == 0 || values.length > 3) throw new IllegalArgumentException("Column Family " + tableName + " in invalid format. Must be in <column family>:<column> format."); ColumnFamily columnFamily = modifications_.get(cfName); if (columnFamily == null) columnFamily = new ColumnFamily(cfName); if (values.length == 2) { columnFamily.addColumn(values[1], ArrayUtils.EMPTY_BYTE_ARRAY, timestamp, true); } else if (values.length == 3) { columnFamily.addColumn(values[1] + ":" + values[2], ArrayUtils.EMPTY_BYTE_ARRAY, timestamp, true); } else { assert values.length == 1; columnFamily.delete(timestamp); } modifications_.put(cfName, columnFamily); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private <ValueT> ValueT fromValue( FieldType type, ValueT value, Type fieldType, Type XXX, Type keyType, Type valueType, Factory<List<FieldValueTypeInformation>> typeFactory) { if (value == null) { return null; } if (TypeName.ROW.equals(type.getTypeName())) { return (ValueT) fromRow((Row) value, (Class) fieldType, typeFactory); } else if (TypeName.ARRAY.equals(type.getTypeName())) { return (ValueT) fromListValue(type.getCollectionElementType(), (List) value, XXX, typeFactory); } else if (TypeName.MAP.equals(type.getTypeName())) { return (ValueT) fromMapValue( type.getMapKeyType(), type.getMapValueType(), (Map) value, keyType, valueType, typeFactory); } else { return value; } }', 'ground_truth': 'private <ValueT> ValueT fromValue( FieldType type, ValueT value, Type fieldType, FieldValueTypeInformation elementType, FieldValueTypeInformation keyType, FieldValueTypeInformation valueType, Factory<List<FieldValueTypeInformation>> typeFactory) { if (value == null) { return null; } if (TypeName.ROW.equals(type.getTypeName())) { return (ValueT) fromRow((Row) value, (Class) fieldType, typeFactory); } else if (TypeName.ARRAY.equals(type.getTypeName())) { return (ValueT) fromListValue(type.getCollectionElementType(), (List) value, elementType, typeFactory); } else if (TypeName.MAP.equals(type.getTypeName())) { return (ValueT) fromMapValue( type.getMapKeyType(), type.getMapValueType(), (Map) value, keyType, valueType, typeFactory); } else { return value; } }', 'output': 'private <ValueT> ValueT fromValue( FieldType type, ValueT value, Type fieldType, Type keyType, Type valueType, Factory<List<FieldValueTypeInformation>> typeFactory) { if (value == null) { return null; } if (TypeName.ROW.equals(type.getTypeName())) { return (ValueT) fromRow((Row) value, (Class) fieldType, typeFactory); } else if (TypeName.ARRAY.equals(type.getTypeName())) { return (ValueT) fromListValue(type.getCollectionElementType(), (List) value, keyType, valueType, typeFactory); } else if (TypeName.MAP.equals(type.getTypeName())) { return (ValueT) fromMapValue( type.getMapKeyType(), type.getMapValueType(), (Map) value, keyType, valueType, typeFactory); } else { return value; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected IgniteInternalFuture<V> getAsync( final K key, boolean forcePrimary, boolean skipTx, @Nullable UUID subjId, String taskName, boolean XXX, final boolean skipVals, boolean canRemap ) { return getAllAsync(Collections.singletonList(key), forcePrimary, skipTx, subjId, taskName, XXX, skipVals, canRemap).chain( new CX1<IgniteInternalFuture<Map<K, V>>, V>() { @Override public V applyx(IgniteInternalFuture<Map<K, V>> e) throws IgniteCheckedException { Map<K, V> map = e.get(); assert map.isEmpty() || map.size() == 1 : map.size(); if (skipVals) { Boolean val = map.isEmpty() ? false : (Boolean)F.firstValue(map); return (V)(val); } return map.get(key); } }); }', 'ground_truth': 'protected IgniteInternalFuture<V> getAsync( final K key, boolean forcePrimary, boolean skipTx, @Nullable UUID subjId, String taskName, boolean deserializeBinary, final boolean skipVals, boolean canRemap ) { return getAllAsync(Collections.singletonList(key), forcePrimary, skipTx, subjId, taskName, deserializeBinary, skipVals, canRemap).chain( new CX1<IgniteInternalFuture<Map<K, V>>, V>() { @Override public V applyx(IgniteInternalFuture<Map<K, V>> e) throws IgniteCheckedException { Map<K, V> map = e.get(); assert map.isEmpty() || map.size() == 1 : map.size(); if (skipVals) { Boolean val = map.isEmpty() ? false : (Boolean)F.firstValue(map); return (V)(val); } return map.get(key); } }); }', 'output': 'protected IgniteInternalFuture<V> getAsync( final K key, boolean forcePrimary, boolean skipTx, @Nullable UUID subjId, String taskName, boolean skipVals, boolean canRemap ) { return getAllAsync(Collections.singletonList(key), forcePrimary, skipTx, subjId, taskName, skipVals, canRemap).chain( new CX1<IgniteInternalFuture<Map<K, V>>, V>() { @Override public V applyx(IgniteInternalFuture<Map<K, V>> e) throws IgniteCheckedException { Map<K, V> map = e.get(); assert map.isEmpty() || map.size() == 1 : map.size(); if (skipVals) { Boolean val = map.isEmpty() ? false : (Boolean)F.firstValue(map); return (V)(val); } return map.get(key); } }); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void serialize(TruncateResponse tr, DataOutput XXX, int version) throws IOException { XXX.writeUTF(tr.keyspace); XXX.writeUTF(tr.columnFamily); XXX.writeBoolean(tr.success); }', 'ground_truth': 'public void serialize(TruncateResponse tr, DataOutput out, int version) throws IOException { out.writeUTF(tr.keyspace); out.writeUTF(tr.columnFamily); out.writeBoolean(tr.success); }', 'output': 'public void serialize(TruncateResponse tr, DataOutput out, int version) throws IOException { out.writeUTF(tr.keyspace); out.writeUTF(tr.columnFamily); out.writeBoolean(tr.success); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private int getMethodLine(final FileObject fo, final String XXX) { final int[] line = new int[1]; JavaSource javaSource = JavaSource.forFileObject(fo); if (javaSource != null) { try { javaSource.runUserActionTask((CompilationController compilationController) -> { compilationController.toPhase(JavaSource.Phase.ELEMENTS_RESOLVED); Trees trees = compilationController.getTrees(); CompilationUnitTree compilationUnitTree = compilationController.getCompilationUnit(); List<? extends Tree> typeDecls = compilationUnitTree.getTypeDecls(); for (Tree tree : typeDecls) { Element element = trees.getElement(trees.getPath(compilationUnitTree, tree)); if (element != null && element.getKind() == ElementKind.CLASS && element.getSimpleName().contentEquals(fo.getName())) { List<? extends ExecutableElement> methodElements = ElementFilter.methodsIn(element.getEnclosedElements()); for (Element child : methodElements) { if (child.getSimpleName().contentEquals(XXX)) { long pos = trees.getSourcePositions().getStartPosition(compilationUnitTree, trees.getTree(child)); line[0] = (int) compilationUnitTree.getLineMap().getLineNumber(pos); break; } } } } }, true); return line[0]; } catch (IOException ioe) { } } return 1; }', 'ground_truth': 'private int getMethodLine(final FileObject fo, final String methodNameWithParams) { String methodName = stripMethodParams(methodNameWithParams); final int[] line = new int[1]; JavaSource javaSource = JavaSource.forFileObject(fo); if (javaSource != null) { try { javaSource.runUserActionTask((CompilationController compilationController) -> { compilationController.toPhase(JavaSource.Phase.ELEMENTS_RESOLVED); Trees trees = compilationController.getTrees(); CompilationUnitTree compilationUnitTree = compilationController.getCompilationUnit(); List<? extends Tree> typeDecls = compilationUnitTree.getTypeDecls(); for (Tree tree : typeDecls) { Element element = trees.getElement(trees.getPath(compilationUnitTree, tree)); if (element != null && element.getKind() == ElementKind.CLASS && element.getSimpleName().contentEquals(fo.getName())) { List<? extends ExecutableElement> methodElements = ElementFilter.methodsIn(element.getEnclosedElements()); for (Element child : methodElements) { if (child.getSimpleName().contentEquals(methodName)) { long pos = trees.getSourcePositions().getStartPosition(compilationUnitTree, trees.getTree(child)); line[0] = (int) compilationUnitTree.getLineMap().getLineNumber(pos); break; } } } } }, true); return line[0]; } catch (IOException ioe) { } } return 1; }', 'output': 'private int getMethodLine(final FileObject fo, final String methodName) { final int[] line = new int[1]; JavaSource javaSource = JavaSource.forFileObject(fo); if (javaSource != null) { try { javaSource.runUserActionTask((CompilationController compilationController) -> { compilationController.toPhase(JavaSource.Phase.ELEMENTS_RESOLVED); Trees trees = compilationController.getTrees(); CompilationUnitTree compilationUnitTree = compilationController.getCompilationUnit(); List<? extends Tree> typeDecls = compilationUnitTree.getTypeDecls(); for (Tree tree : typeDecls) { Element element = trees.getElement(trees.getPath(compilationUnitTree, tree)); if (element != null && element.getKind() == ElementKind.CLASS && element.getSimpleName().contentEquals(fo.getName())) { List<? extends ExecutableElement> methodElements = ElementFilter.methodsIn(element.getEnclosedElements()); for (Element child : methodElements) { if (child.getSimpleName().contentEquals(methodName)) { long pos = trees.getSourcePositions().getStartPosition(compilationUnitTree, trees.getTree(child)); line[0] = (int) compilationUnitTree.getLineMap().getLineNumber(pos); break; } } } } }, true); return line[0]; } catch (IOException ioe) { } } return 1; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void addReadCommandFromColumnFamily(String XXX, ByteBuffer key, ColumnFamily columnFamily, long timestamp, List<ReadCommand> commands) { SortedSet<ByteBuffer> s = new TreeSet<ByteBuffer>(columnFamily.metadata().comparator); Iterables.addAll(s, columnFamily.getColumnNames()); commands.add(new SliceByNamesReadCommand(XXX, key, columnFamily.metadata().cfName, timestamp, new NamesQueryFilter(s))); }', 'ground_truth': 'private void addReadCommandFromColumnFamily(String keyspaceName, ByteBuffer key, ColumnFamily columnFamily, long timestamp, List<ReadCommand> commands) { SortedSet<ByteBuffer> s = new TreeSet<ByteBuffer>(columnFamily.metadata().comparator); Iterables.addAll(s, columnFamily.getColumnNames()); commands.add(new SliceByNamesReadCommand(keyspaceName, key, columnFamily.metadata().cfName, timestamp, new NamesQueryFilter(s))); }', 'output': 'private void addReadCommandFromColumnFamily(String tableName, ByteBuffer key, ColumnFamily columnFamily, long timestamp, List<ReadCommand> commands) { SortedSet<ByteBuffer> s = new TreeSet<ByteBuffer>(columnFamily.metadata().comparator); Iterables.addAll(s, columnFamily.getColumnNames()); commands.add(new SliceByNamesReadCommand(tableName, key, columnFamily.metadata().cfName, timestamp, new NamesQueryFilter(s))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void registerCacheObject(MBeanServer mBeanServer, Object mxbean, String cacheManagerName, String name, boolean XXX) { ObjectName registeredObjectName = calculateObjectName(cacheManagerName, name, XXX); try { if (!isRegistered(mBeanServer, registeredObjectName)) { mBeanServer.registerMBean(mxbean, registeredObjectName); System.out.println("Registered mbean with name " + registeredObjectName.toString()); } } catch (Exception e) { throw new CacheException("Error registering cache MXBeans for CacheManager " + registeredObjectName + " . Error was " + e.getMessage(), e); } }', 'ground_truth': 'public void registerCacheObject(MBeanServer mBeanServer, Object mxbean, String name, boolean isStatistic) { ObjectName registeredObjectName = getObjectName(name, isStatistic); try { if (!isRegistered(mBeanServer, registeredObjectName)) mBeanServer.registerMBean(mxbean, registeredObjectName); } catch (Exception e) { throw new CacheException("Error registering cache MXBeans for CacheManager " + registeredObjectName.toString() + " . Error was " + e.getMessage(), e); } }', 'output': 'public static void registerCacheObject(MBeanServer mBeanServer, Object mxbean, String cacheManagerName, String name, boolean persistent) { ObjectName registeredObjectName = calculateObjectName(cacheManagerName, name, persistent); try { if (!isRegistered(mBeanServer, registeredObjectName)) { mBeanServer.registerMBean(mxbean, registeredObjectName); System.out.println("Registered mbean with name " + registeredObjectName.toString()); } } catch (Exception e) { throw new CacheException("Error registering cache MXBeans for CacheManager " + registeredObjectName + " . Error was " + e.getMessage(), e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void addTombstone(QueryPath XXX, ByteBuffer localDeletionTime, long timestamp) { assert XXX.columnName != null : XXX; addColumn(XXX.superColumnName, new DeletedColumn(XXX.columnName, localDeletionTime, timestamp)); }', 'ground_truth': 'public void addTombstone(ByteBuffer name, ByteBuffer localDeletionTime, long timestamp) { addColumn(new DeletedColumn(name, localDeletionTime, timestamp)); }', 'output': 'public void addTombstone(QueryPath path, ByteBuffer localDeletionTime, long timestamp) { assert path.superColumnName != null : path; addColumn(path.superColumnName, new DeletedColumn(path.columnName, localDeletionTime, timestamp)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ApplicationRole act( @Parameter(optionality = Optionality.MANDATORY) @ParameterLayout(named="Rule") final ApplicationPermissionRule rule, @Parameter(optionality = Optionality.MANDATORY) @ParameterLayout(named="Mode") final ApplicationPermissionMode mode, @Parameter(optionality = Optionality.MANDATORY) @ParameterLayout(named="Feature") final String XXX) { val featureId = ApplicationFeatureId.parse(XXX); applicationPermissionRepository.newPermission(target, rule, mode, featureId); return target; }', 'ground_truth': 'public ApplicationRole act( @Parameter(optionality = Optionality.MANDATORY) @ParameterLayout(named="Rule") final ApplicationPermissionRule rule, @Parameter(optionality = Optionality.MANDATORY) @ParameterLayout(named="Mode") final ApplicationPermissionMode mode, @Parameter(optionality = Optionality.MANDATORY) @ParameterLayout(named="Feature") final String feature) { val featureId = ApplicationFeatureId.parse(feature); applicationPermissionRepository.newPermission(target, rule, mode, featureId); return target; }', 'output': 'public ApplicationRole act( @Parameter(optionality = Optionality.MANDATORY) @ParameterLayout(named="Rule") final ApplicationPermissionRule rule, @Parameter(optionality = Optionality.MANDATORY) @ParameterLayout(named="Mode") final ApplicationPermissionMode mode, @Parameter(optionality = Optionality.MANDATORY) @ParameterLayout(named="Feature") final String featureName) { val featureId = ApplicationFeatureId.parse(featureName); applicationPermissionRepository.newPermission(target, rule, mode, featureId); return target; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Pair<AbstractBounds,AbstractBounds> split(Token XXX) { assert left.equals(XXX) || contains(XXX); AbstractBounds lb = createFrom(XXX); AbstractBounds rb = lb != null && XXX.equals(right) ? null : new Range(XXX, right); return new Pair<AbstractBounds,AbstractBounds>(lb, rb); }', 'ground_truth': 'public Pair<AbstractBounds<T>,AbstractBounds<T>> split(T pos) { assert left.equals(pos) || contains(pos); AbstractBounds<T> lb = createFrom(pos); AbstractBounds<T> rb = lb != null && pos.equals(right) ? null : new Range<T>(pos, right); return new Pair<AbstractBounds<T>,AbstractBounds<T>>(lb, rb); }', 'output': 'public Pair<AbstractBounds,AbstractBounds> split(Token<?> token) { assert left.equals(token) || contains(token); AbstractBounds lb = createFrom(token); AbstractBounds rb = lb != null && token.equals(right) ? null : new Range(token, right); return new Pair<AbstractBounds,AbstractBounds>(lb, rb); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Token deserialize(DataInput XXX) throws IOException { IPartitioner p = StorageService.getPartitioner(); int size = XXX.readInt(); byte[] bytes = new byte[size]; XXX.readFully(bytes); return p.getTokenFactory().fromByteArray(ByteBuffer.wrap(bytes)); }', 'ground_truth': 'public Token deserialize(DataInput in) throws IOException { IPartitioner p = StorageService.getPartitioner(); int size = in.readInt(); byte[] bytes = new byte[size]; in.readFully(bytes); return p.getTokenFactory().fromByteArray(ByteBuffer.wrap(bytes)); }', 'output': 'public Token deserialize(DataInputPlus in) throws IOException { IPartitioner p = StorageService.getPartitioner(); int size = in.readInt(); byte[] bytes = new byte[size]; in.readFully(bytes); return p.getTokenFactory().fromByteArray(ByteBuffer.wrap(bytes)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void startLocalSnasphotOperation(StartSnapshotOperationAckDiscoveryMessage XXX ) throws IgniteCheckedException { IgniteInternalFuture fut = cctx.database().startLocalSnapshotOperation(XXX); if (fut != null) fut.get(); }', 'ground_truth': 'private void startLocalSnasphotOperation(StartSnapshotOperationAckDiscoveryMessage snapOpMsg ) throws IgniteCheckedException { IgniteInternalFuture fut = cctx.database() .startLocalSnapshotOperation(snapOpMsg.initiatorNodeId(), snapOpMsg.snapshotOperation()); if (fut != null) fut.get(); }', 'output': 'private void startLocalSnasphotOperation(StartSnapshotOperationAckDiscoveryMessage message) throws IgniteCheckedException { IgniteInternalFuture fut = cctx.database().startLocalSnapshotOperation(message); if (fut != null) fut.get(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void dumpXsd(final BSGrid XXX) { Map<String, String> schemas = jaxbService.toXsd(XXX, IsisSchemas.INCLUDE); for (Map.Entry<String, String> entry : schemas.entrySet()) { println(entry.getKey() + ":"); println(entry.getValue()); } }', 'ground_truth': 'protected void dumpXsd(final BSGrid bsPage) { Map<String, String> schemas = jaxbService.toXsd(bsPage, IsisSchemas.INCLUDE); for (Map.Entry<String, String> entry : schemas.entrySet()) { println(entry.getKey() + ":"); println(entry.getValue()); } }', 'output': 'protected void dumpXsd(final BSGrid grid) { Map<String, String> schemas = jaxbService.toXsd(grid, IsisSchemas.INCLUDE); for (Map.Entry<String, String> entry : schemas.entrySet()) { println(entry.getKey() + ":"); println(entry.getValue()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void validateSnapshotId(ByteBuffer buf, long nextSnapshotTag, long XXX, int pageSize) { assert nextSnapshotTag != XXX : "nextSnapshotTag = " + nextSnapshotTag + ", XXX = " + XXX; long last = getLastSnapshotTag(buf); assert last <= nextSnapshotTag : "last = " + last + ", nextSnapshotTag = " + nextSnapshotTag; if (nextSnapshotTag == last) return; int cntOfPage = countOfPageToTrack(pageSize); if (last <= XXX) { buf.putLong(LAST_SNAPSHOT_TAG_OFFSET, nextSnapshotTag); PageHandler.zeroMemory(buf, SIZE_FIELD_OFFSET, buf.capacity() - SIZE_FIELD_OFFSET); } else { int len = cntOfPage >> 3; int sizeOff = useLeftHalf(nextSnapshotTag) ? SIZE_FIELD_OFFSET : BITMAP_OFFSET + len; int sizeOff2 = !useLeftHalf(nextSnapshotTag) ? SIZE_FIELD_OFFSET : BITMAP_OFFSET + len; if (last - XXX == 1) { if ((nextSnapshotTag - last) % 2 == 0) PageHandler.copyMemory(buf, sizeOff, buf, sizeOff2, len + SIZE_FIELD_SIZE); } else { int newSize = 0; int i = 0; for (; i < len - 8; i += 8) { long newVal = buf.getLong(sizeOff + SIZE_FIELD_SIZE + i) | buf.getLong(sizeOff2 + SIZE_FIELD_SIZE + i); newSize += Long.bitCount(newVal); buf.putLong(sizeOff2 + SIZE_FIELD_SIZE + i, newVal); } for (; i < len; i ++) { byte newVal = (byte) (buf.get(sizeOff + SIZE_FIELD_SIZE + i) | buf.get(sizeOff2 + SIZE_FIELD_SIZE + i)); newSize += Integer.bitCount(newVal & 0xFF); buf.put(sizeOff2 + SIZE_FIELD_SIZE + i, newVal); } buf.putShort(sizeOff2, (short)newSize); } buf.putLong(LAST_SNAPSHOT_TAG_OFFSET, nextSnapshotTag); PageHandler.zeroMemory(buf, sizeOff, len + SIZE_FIELD_SIZE); } }', 'ground_truth': 'private void validateSnapshotId(ByteBuffer buf, long nextSnapshotTag, long lastSuccessfulSnapshotTag, int pageSize) { assert nextSnapshotTag != lastSuccessfulSnapshotTag : "nextSnapshotTag = " + nextSnapshotTag + ", lastSuccessfulSnapshotId = " + lastSuccessfulSnapshotTag; long last = getLastSnapshotTag(buf); assert last <= nextSnapshotTag : "last = " + last + ", nextSnapshotTag = " + nextSnapshotTag; if (nextSnapshotTag == last) return; int cntOfPage = countOfPageToTrack(pageSize); if (last <= lastSuccessfulSnapshotTag) { buf.putLong(LAST_SNAPSHOT_TAG_OFFSET, nextSnapshotTag); PageHandler.zeroMemory(buf, SIZE_FIELD_OFFSET, buf.capacity() - SIZE_FIELD_OFFSET); } else { int len = cntOfPage >> 3; int sizeOff = useLeftHalf(nextSnapshotTag) ? SIZE_FIELD_OFFSET : BITMAP_OFFSET + len; int sizeOff2 = !useLeftHalf(nextSnapshotTag) ? SIZE_FIELD_OFFSET : BITMAP_OFFSET + len; if (last - lastSuccessfulSnapshotTag == 1) { if ((nextSnapshotTag - last) % 2 == 0) PageHandler.copyMemory(buf, sizeOff, buf, sizeOff2, len + SIZE_FIELD_SIZE); } else { int newSize = 0; int i = 0; for (; i < len - 8; i += 8) { long newVal = buf.getLong(sizeOff + SIZE_FIELD_SIZE + i) | buf.getLong(sizeOff2 + SIZE_FIELD_SIZE + i); newSize += Long.bitCount(newVal); buf.putLong(sizeOff2 + SIZE_FIELD_SIZE + i, newVal); } for (; i < len; i ++) { byte newVal = (byte) (buf.get(sizeOff + SIZE_FIELD_SIZE + i) | buf.get(sizeOff2 + SIZE_FIELD_SIZE + i)); newSize += Integer.bitCount(newVal & 0xFF); buf.put(sizeOff2 + SIZE_FIELD_SIZE + i, newVal); } buf.putShort(sizeOff2, (short)newSize); } buf.putLong(LAST_SNAPSHOT_TAG_OFFSET, nextSnapshotTag); PageHandler.zeroMemory(buf, sizeOff, len + SIZE_FIELD_SIZE); } }', 'output': 'private void validateSnapshotId(ByteBuffer buf, long nextSnapshotTag, long timestamp, int pageSize) { assert nextSnapshotTag != timestamp : "nextSnapshotTag = " + nextSnapshotTag + ", timestamp = " + timestamp; long last = getLastSnapshotTag(buf); assert last <= nextSnapshotTag : "last = " + last + ", nextSnapshotTag = " + nextSnapshotTag; if (nextSnapshotTag == last) return; int cntOfPage = countOfPageToTrack(pageSize); if (last <= timestamp) { buf.putLong(LAST_SNAPSHOT_TAG_OFFSET, nextSnapshotTag); PageHandler.zeroMemory(buf, SIZE_FIELD_OFFSET, buf.capacity() - SIZE_FIELD_OFFSET); } else { int len = cntOfPage >> 3; int sizeOff = useLeftHalf(nextSnapshotTag) ? SIZE_FIELD_OFFSET : BITMAP_OFFSET + len; int sizeOff2 = !useLeftHalf(nextSnapshotTag) ? SIZE_FIELD_OFFSET : BITMAP_OFFSET + len; if (last - timestamp == 1) { if ((nextSnapshotTag - last) % 2 == 0) PageHandler.copyMemory(buf, sizeOff, buf, sizeOff2, len + SIZE_FIELD_SIZE); } else { int newSize = 0; int i = 0; for (; i < len - 8; i += 8) { long newVal = buf.getLong(sizeOff + SIZE_FIELD_SIZE + i) | buf.getLong(sizeOff2 + SIZE_FIELD_SIZE + i); newSize += Long.bitCount(newVal); buf.putLong(sizeOff2 + SIZE_FIELD_SIZE + i, newVal); } for (; i < len; i ++) { byte newVal = (byte) (buf.get(sizeOff + SIZE_FIELD_SIZE + i) | buf.get(sizeOff2 + SIZE_FIELD_SIZE + i)); newSize += Integer.bitCount(newVal & 0xFF); buf.put(sizeOff2 + SIZE_FIELD_SIZE + i, newVal); } buf.putShort(sizeOff2, ('}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean tryDisableEvents(IXXXniteEx XXX) { if (!listenVisor.values().contains(true)) { listenVisor.clear(); XXX.events().disableLocal(VISOR_TASK_EVTS); } return XXX.allEventsUserRecordable(VISOR_TASK_EVTS); }', 'ground_truth': 'private boolean tryDisableEvents(IgniteEx ignite) { if (!listenVisor.values().contains(true)) { listenVisor.clear(); ignite.events().disableLocal(VISOR_TASK_EVTS); } return ignite.allEventsUserRecordable(VISOR_TASK_EVTS); }', 'output': 'private boolean tryDisableEvents(IEventListener listener) { if (!listenVisor.values().contains(true)) { listenVisor.clear(); listener.disableLocal(VISOR_TASK_EVTS); } return listener.allEventsUserRecordable(VISOR_TASK_EVTS); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public PushJoinThroughJoinRule( String description, boolean right, Class<? extends JoinRelBase> clazz, ProjectFactory XXX) { super( operand( clazz, operand(clazz, any()), operand(RelNode.class, any())), description); this.right = right; projectFactory = XXX; }', 'ground_truth': 'public PushJoinThroughJoinRule(String description, boolean right, Class<? extends JoinRelBase> clazz, ProjectFactory projectFactory) { super( operand( clazz, operand(clazz, any()), operand(RelNode.class, any())), description); this.right = right; this.projectFactory = projectFactory; }', 'output': 'public PushJoinThroughJoinRule( String description, boolean right, Class<? extends JoinRelBase> clazz, ProjectFactory projectFactory) { super( operand( clazz, operand(clazz, any()), operand(RelNode.class, any())), description); this.right = right; this.projectFactory = projectFactory; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void testQueryCancel(int keyCnt, int valSize, String sql, int timeoutUnits, TimeUnit timeUnit, boolean timeout, boolean XXX) throws Exception { try (Ignite client = startClientGrid("client")) { IgniteCache<Object, Object> cache = client.cache(DEFAULT_CACHE_NAME); assertEquals(0, cache.localSize()); int p = 1; for (int i = 1; i <= keyCnt; i++) { char[] tmp = new char[valSize]; Arrays.fill(tmp, \' \'); cache.put(i, new String(tmp)); if (i / (float)keyCnt >= p / 10f) { log().info("Loaded " + i + " of " + keyCnt); p++; } } assertEquals(0, cache.localSize()); SqlFieldsQuery qry = new SqlFieldsQuery(sql); final QueryCursor<List<?>> cursor; if (timeout) { qry.setTimeout(timeoutUnits, timeUnit); cursor = cache.query(qry); } else { cursor = cache.query(qry); client.scheduler().runLocal(new Runnable() { @Override public void run() { cursor.close(); } }, timeoutUnits, timeUnit); } try (QueryCursor<List<?>> ignored = cursor) { cursor.getAll(); if (XXX) fail("Query not canceled"); } catch (CacheException ex) { error("Got expected exception", ex); assertNotNull("Must throw correct exception", X.cause(ex, QueryCancelledException.class)); } Thread.sleep(TimeUnit.MILLISECONDS.convert(timeoutUnits, timeUnit) + 3_000); checkCleanState(); } }', 'ground_truth': 'private void testQueryCancel( String sql, int timeout, TimeUnit timeUnit, boolean useTimeout) throws Exception { try (Ignite client = startGrid("client")) { IgniteCache<Object, Object> cache = client.cache(DEFAULT_CACHE_NAME); assertEquals(0, cache.localSize()); for (int i = 1; i <= CACHE_SIZE; i++) cache.put(i, GridTestUtils.randomString(ThreadLocalRandom.current(), VAL_SIZE)); assertEquals(0, cache.localSize()); SqlFieldsQuery qry = new SqlFieldsQuery(sql); final QueryCursor<List<?>> cursor; if (useTimeout) { qry.setTimeout(timeout, timeUnit); cursor = cache.query(qry); } else { cursor = cache.query(qry); client.scheduler().runLocal(cursor::close, timeout, timeUnit); } try (QueryCursor<List<?>> ignored = cursor) { cursor.getAll(); fail("Query not canceled"); } catch (CacheException ex) { error("Got expected exception", ex); assertNotNull("Must throw correct exception", X.cause(ex, QueryCancelledException.class)); } Thread.sleep(TimeUnit.MILLISECONDS.convert(timeout, timeUnit) + 3_000); checkCleanState(); } }', 'output': 'private void testQueryCancel(int keyCnt, int valSize, String sql, int timeoutUnits, TimeUnit timeUnit, boolean timeout, boolean cancelOnTimeout) throws Exception { try (Ignite client = startClientGrid("client")) { IgniteCache<Object, Object> cache = client.cache(DEFAULT_CACHE_NAME); assertEquals(0, cache.localSize()); int p = 1; for (int i = 1; i <= keyCnt; i++) { char[] tmp = new char[valSize]; Arrays.fill(tmp, \' \'); cache.put(i, new String(tmp)); if (i / (float)keyCnt >= p / 10f) { log().info("Loaded " + i + " of " + keyCnt); p++; } } assertEquals(0, cache.localSize()); SqlFieldsQuery qry = new SqlFieldsQuery(sql); final QueryCursor<List<?>> cursor; if (timeout) { qry.setTimeout(timeoutUnits, timeUnit); cursor = cache.query(qry); } else { cursor = cache.query(qry); client.scheduler().runLocal(new Runnable() { @Override public void run() { cursor.close(); } }, timeoutUnits, timeUnit); } try (QueryCursor<List<?>> ignored = cursor) { cursor.getAll(); if (!cancelOnTimeout) fail("Query not canceled"); } catch (CacheException ex) { error("Got expected exception", ex); assertNotNull("Must throw correct exception", X.cause(ex, QueryCancelledException.class)); } Thread.sleep(TimeUnit.MILLISECONDS.convert(timeoutUnits, timeUnit) + 3_000); checkCleanState(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void assertTypeNotNull(String XXX, AST rightNode) { if (XXX == null) { throw new ASTRuntimeException(rightNode, "No type available for: " + qualifiedName(rightNode)); } }', 'ground_truth': 'protected void assertTypeNotNull(Type type, AST rightNode) { if (type == null) { throw new ASTRuntimeException(rightNode, "No type available for: " + qualifiedName(rightNode)); } }', 'output': 'protected void assertTypeNotNull(String name, AST rightNode) { if (name == null) { throw new ASTRuntimeException(rightNode, "No type available for: " + qualifiedName(rightNode)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void traverseCols(final Grid.Visitor visitor, final BSRow XXX) { final BSGrid.Visitor bs3Visitor = asBs3Visitor(visitor); final List<BSRowContent> cols = XXX.getCols(); for (BSRowContent rowContent : new ArrayList<>(cols)) { rowContent.setOwner(XXX); if(rowContent instanceof BSCol) { final BSCol bs3Col = (BSCol) rowContent; bs3Visitor.preVisit(bs3Col); bs3Visitor.visit(bs3Col); traverseDomainObject(bs3Col, visitor); traverseTabGroups(bs3Col, visitor); traverseActions(bs3Col, visitor); traverseFieldSets(bs3Col, visitor); traverseCollections(bs3Col, visitor); traverseRows(bs3Col, visitor); bs3Visitor.postVisit(bs3Col); } else if (rowContent instanceof BSClearFix) { final BSClearFix bs3ClearFix = (BSClearFix) rowContent; bs3Visitor.visit(bs3ClearFix); } else { throw new IllegalStateException( "Unrecognized implementation of BSRowContent, " + rowContent); } } }', 'ground_truth': 'private void traverseCols(final Grid.Visitor visitor, final BSRow bsRow) { final BSGrid.Visitor bsVisitor = asBs3Visitor(visitor); final List<BSRowContent> cols = bsRow.getCols(); for (BSRowContent rowContent : new ArrayList<>(cols)) { rowContent.setOwner(bsRow); if(rowContent instanceof BSCol) { final BSCol bsCol = (BSCol) rowContent; bsVisitor.preVisit(bsCol); bsVisitor.visit(bsCol); traverseDomainObject(bsCol, visitor); traverseTabGroups(bsCol, visitor); traverseActions(bsCol, visitor); traverseFieldSets(bsCol, visitor); traverseCollections(bsCol, visitor); traverseRows(bsCol, visitor); bsVisitor.postVisit(bsCol); } else if (rowContent instanceof BSClearFix) { final BSClearFix bsClearFix = (BSClearFix) rowContent; bsVisitor.visit(bsClearFix); } else { throw new IllegalStateException( "Unrecognized implementation of BSRowContent, " + rowContent); } } }', 'output': 'private void traverseCols(final Grid.Visitor visitor, final BSRow row) { final BSGrid.Visitor bs3Visitor = asBs3Visitor(visitor); final List<BSRowContent> cols = row.getCols(); for (BSRowContent rowContent : new ArrayList<>(cols)) { rowContent.setOwner(row); if(rowContent instanceof BSCol) { final BSCol bs3Col = (BSCol) rowContent; bs3Visitor.preVisit(bs3Col); bs3Visitor.visit(bs3Col); traverseDomainObject(bs3Col, visitor); traverseTabGroups(bs3Col, visitor); traverseActions(bs3Col, visitor); traverseFieldSets(bs3Col, visitor); traverseCollections(bs3Col, visitor); traverseRows(bs3Col, visitor); bs3Visitor.postVisit(bs3Col); } else if (rowContent instanceof BSClearFix) { final BSClearFix bs3ClearFix = (BSClearFix) rowContent; bs3Visitor.visit(bs3ClearFix); } else { throw new IllegalStateException( "Unrecognized implementation of BSRowContent, " + rowContent); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void setDesiredStackVersion(StackId XXX) { readWriteLock.writeLock().lock(); try { if (LOG.isDebugEnabled()) { LOG.debug("Setting DesiredStackVersion of Service" + ", clusterName=" + service.getCluster().getClusterName() + ", clusterId=" + service.getCluster().getClusterId() + ", serviceName=" + service.getName() + ", serviceComponentName=" + getName() + ", oldDesiredStackVersion=" + getDesiredStackVersion() + ", newDesiredStackVersion=" + XXX); } desiredStateEntity.setDesiredStackVersion(gson.toJson(XXX)); saveIfPersisted(); } finally { readWriteLock.writeLock().unlock(); } }', 'ground_truth': 'public void setDesiredStackVersion(StackId stack) { readWriteLock.writeLock().lock(); try { if (LOG.isDebugEnabled()) { LOG.debug("Setting DesiredStackVersion of Service" + ", clusterName=" + service.getCluster().getClusterName() + ", clusterId=" + service.getCluster().getClusterId() + ", serviceName=" + service.getName() + ", serviceComponentName=" + getName() + ", oldDesiredStackVersion=" + getDesiredStackVersion() + ", newDesiredStackVersion=" + stack); } StackEntity stackEntity = stackDAO.find(stack.getStackName(), stack.getStackVersion()); desiredStateEntity.setDesiredStack(stackEntity); saveIfPersisted(); } finally { readWriteLock.writeLock().unlock(); } }', 'output': 'public void setDesiredStackVersion(String stackVersion) { readWriteLock.writeLock().lock(); try { if (LOG.isDebugEnabled()) { LOG.debug("Setting DesiredStackVersion of Service" + ", clusterName=" + service.getCluster().getClusterName() + ", clusterId=" + service.getCluster().getClusterId() + ", serviceName=" + service.getName() + ", serviceComponentName=" + getName() + ", oldDesiredStackVersion=" + getDesiredStackVersion() + ", newDesiredStackVersion=" + stackVersion); } desiredStateEntity.setDesiredStackVersion(stackVersion); saveIfPersisted(); } finally { readWriteLock.writeLock().unlock(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public T last( Queryable<T> XXX, FunctionExpression<Predicate1<T>> predicate) { throw Extensions.todo(); } public T lastOrDefault(\n        Queryable<T> XXX,\n        FunctionExpression<Predicate1<T>> predicate)\n    { Queryable<T> XXX, FunctionExpression<Predicate1<T>> predicate) { throw Extensions.todo(); }', 'ground_truth': 'public T last( final Queryable<T> source, final FunctionExpression<Predicate1<T>> predicate) { return new NonLeafReplayableQueryable<T>(source) { public void replay(QueryableFactory<T> factory) { factory.last(source, predicate); } }.single(); }', 'output': 'public T last( Queryable<T> queryable, FunctionExpression<Predicate1<T>> predicate) { throw Extensions.todo(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void afterAppend(String XXX, long position, long size) throws IOException { ++indexKeysWritten_; lastWrittenKey_ = XXX; blockIndex_.put(XXX, new BlockMetadata(position, size)); if ( indexKeysWritten_ == indexInterval_ ) { blockIndexes_.add(blockIndex_); blockIndex_ = new TreeMap<String, BlockMetadata>(Collections.reverseOrder()); indexKeysWritten_ = 0; } }', 'ground_truth': 'private void afterAppend(String decoratedKey, long position, long size) throws IOException { ++indexKeysWritten_; lastWrittenKey_ = decoratedKey; blockIndex_.put(decoratedKey, new BlockMetadata(position, size)); if ( indexKeysWritten_ == indexInterval_ ) { blockIndexes_.add(blockIndex_); blockIndex_ = new TreeMap<String, BlockMetadata>(StorageService.getPartitioner().getReverseDecoratedKeyComparator()); indexKeysWritten_ = 0; } }', 'output': 'private void afterAppend(String key, long position, long size) throws IOException { ++indexKeysWritten_; lastWrittenKey_ = key; blockIndex_.put(key, new BlockMetadata(position, size)); if ( indexKeysWritten_ == indexInterval_ ) { blockIndexes_.add(blockIndex_); blockIndex_ = new TreeMap<String, BlockMetadata>(Collections.reverseOrder()); indexKeysWritten_ = 0; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ApplicationUser act( @ParameterLayout(named="Existing password") final Password existingPassword, @ParameterLayout(named="New password") final Password newPassword, @ParameterLayout(named="Re-enter password") final Password XXX) { applicationUserRepository.updatePassword(target, newPassword.getPassword()); return target; }', 'ground_truth': 'public ApplicationUser act( final Password existingPassword, final Password newPassword, final Password repeatNewPassword) { applicationUserRepository.updatePassword(target, newPassword.getPassword()); return target; }', 'output': 'public ApplicationUser act( @ParameterLayout(named="Existing password") final Password existingPassword, @ParameterLayout(named="New password") final Password newPassword, @ParameterLayout(named="Re-enter password") final Password reEnteredPassword) { applicationUserRepository.updatePassword(target, newPassword.getPassword()); return target; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private long getTotalSizeInProgress(Collection<ProgressInfo> XXX) { long total = 0; for (ProgressInfo file : XXX) total += file.currentBytes; return total; }', 'ground_truth': 'private long getTotalSizeInProgress(Collection<ProgressInfo> streams) { long total = 0; for (ProgressInfo stream : streams) total += stream.currentBytes; return total; }', 'output': 'private long getTotalSizeInProgress(List<ProgressInfo> files) { long total = 0; for (ProgressInfo file : files) total += file.currentBytes; return total; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void registerStaticMethod(final String XXX, final Closure callable) { performOperationOnMetaClass(new Callable() { public void call() { ClosureStaticMetaMethod metaMethod = new ClosureStaticMetaMethod(XXX, theClass,callable); if(XXX.equals(INVOKE_METHOD_METHOD) && callable.getParameterTypes().length == 2) { invokeStaticMethodMethod = metaMethod; } else { MethodKey key = new DefaultMethodKey(theClass,XXX, metaMethod.getParameterTypes(), false ); addMetaMethod(metaMethod); if(isGetter(XXX, metaMethod.getParameterTypes())) { String propertyName = getPropertyForGetter(XXX); registerBeanPropertyForMethod(metaMethod, propertyName, true, true); } else if(isSetter(XXX, metaMethod.getParameterTypes())) { String propertyName = getPropertyForSetter(XXX); registerBeanPropertyForMethod(metaMethod, propertyName, false, true); } performRegistryCallbacks(); expandoMethods.put(key,metaMethod); } } }); }', 'ground_truth': 'protected void registerStaticMethod(final String name, final Closure callable) { performOperationOnMetaClass(new Callable() { public void call() { String methodName; if(name.equals(METHOD_MISSING)) methodName = STATIC_METHOD_MISSING; else if(name.equals(PROPERTY_MISSING)) methodName = STATIC_PROPERTY_MISSING; else methodName = name; ClosureStaticMetaMethod metaMethod = new ClosureStaticMetaMethod(methodName, theClass,callable); if(methodName.equals(INVOKE_METHOD_METHOD) && callable.getParameterTypes().length == 2) { invokeStaticMethodMethod = metaMethod; } else { if(methodName.equals(METHOD_MISSING)) { methodName = STATIC_METHOD_MISSING; } MethodKey key = new DefaultMethodKey(theClass,methodName, metaMethod.getParameterTypes(), false ); addMetaMethod(metaMethod); if(isGetter(methodName, metaMethod.getParameterTypes())) { String propertyName = getPropertyForGetter(methodName); registerBeanPropertyForMethod(metaMethod, propertyName, true, true); } else if(isSetter(methodName, metaMethod.getParameterTypes())) { String propertyName = getPropertyForSetter(methodName); registerBeanPropertyForMethod(metaMethod, propertyName, false, true); } performRegistryCallbacks(); expandoMethods.put(key,metaMethod); } } }); }', 'output': 'protected void registerStaticMethod(final String name, final Closure callable) { performOperationOnMetaClass(new Callable() { public void call() { ClosureStaticMetaMethod metaMethod = new ClosureStaticMetaMethod(name, theClass,callable); if(name.equals(INVOKE_METHOD_METHOD) && callable.getParameterTypes().length == 2) { invokeStaticMethodMethod = metaMethod; } else { MethodKey key = new DefaultMethodKey(theClass,name, metaMethod.getParameterTypes(), false ); addMetaMethod(metaMethod); if(isGetter(name, metaMethod.getParameterTypes())) { String propertyName = getPropertyForGetter(name); registerBeanPropertyForMethod(metaMethod, propertyName, true, true); } else if(isSetter(name, metaMethod.getParameterTypes())) { String propertyName = getPropertyForSetter(name); registerBeanPropertyForMethod(metaMethod, propertyName, false, true); } performRegistryCallbacks(); expandoMethods.put(key,metaMethod); } } }); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Response.ResponseBuilder buildResponse( final IResourceContext XXX, final ManagedObject objectAdapter) { return null; }', 'ground_truth': 'public Response.ResponseBuilder buildResponse( final IResourceContext resourceContext, final ManagedObject objectAdapter) { return null; }', 'output': 'public Response.ResponseBuilder buildResponse( final ResourceContext resourceContext, final ManagedObject objectAdapter) { return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void appendFile(IgniteFs XXX, IgfsPath file, @Nullable byte[]... chunks) throws Exception { IgfsOutputStream os = null; try { os = XXX.append(file, false); writeFileChunks(os, chunks); } finally { U.closeQuiet(os); awaitFileClose(XXX, file); } }', 'ground_truth': 'protected void appendFile(IgniteFs igfs, IgfsPath file, @Nullable byte[]... chunks) throws Exception { IgfsOutputStream os = null; try { os = igfs.append(file, false); writeFileChunks(os, chunks); } finally { U.closeQuiet(os); awaitFileClose(igfs, file); } }', 'output': 'protected void appendFile(IgniteFs fs, IgfsPath file, @Nullable byte[]... chunks) throws Exception { IgfsOutputStream os = null; try { os = fs.append(file, false); writeFileChunks(os, chunks); } finally { U.closeQuiet(os); awaitFileClose(fs, file); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String validateAct( final Password existingPassword, final Password newPassword, final Password XXX) { if(!applicationUserRepository.isPasswordFeatureEnabled(target)) { return "Password feature is not available for this User"; } val encrypter = passwordEncryptionService.orElseThrow(_Exceptions::unexpectedCodeReach); val encryptedPassword = target.getEncryptedPassword(); if(target.getEncryptedPassword() != null) { if (!encrypter.matches(existingPassword.getPassword(), encryptedPassword)) { return "Existing password is incorrect"; } } if (!Objects.equals(newPassword, XXX)) { return "Passwords do not match"; } return null; }', 'ground_truth': 'public String validateAct( final Password existingPassword, final Password newPassword, final Password repeatNewPassword) { if(!applicationUserRepository.isPasswordFeatureEnabled(target)) { return "Password feature is not available for this User"; } val encrypter = passwordEncryptionService.orElseThrow(_Exceptions::unexpectedCodeReach); val encryptedPassword = target.getEncryptedPassword(); if(target.getEncryptedPassword() != null) { if (!encrypter.matches(existingPassword.getPassword(), encryptedPassword)) { return "Existing password is incorrect"; } } if (!Objects.equals(newPassword, repeatNewPassword)) { return "Passwords do not match"; } return null; }', 'output': 'public String validateAct( final Password existingPassword, final Password newPassword, final Password oldPassword) { if(!applicationUserRepository.isPasswordFeatureEnabled(target)) { return "Password feature is not available for this User"; } val encrypter = passwordEncryptionService.orElseThrow(_Exceptions::unexpectedCodeReach); val encryptedPassword = target.getEncryptedPassword(); if(target.getEncryptedPassword() != null) { if (!encrypter.matches(existingPassword.getPassword(), encryptedPassword)) { return "Existing password is incorrect"; } } if (!Objects.equals(newPassword, oldPassword)) { return "Passwords do not match"; } return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String XXX, Set<Range<Token>> ranges) { InetAddress myAddress = FBUtilities.getBroadcastAddress(); Multimap<Range<Token>, InetAddress> rangeAddresses = Table.open(XXX).getReplicationStrategy().getRangeAddresses(tokenMetadata.cloneOnlyTokenMap()); Multimap<InetAddress, Range<Token>> sourceRanges = HashMultimap.create(); IFailureDetector failureDetector = FailureDetector.instance; for (Range<Token> range : ranges) { Collection<InetAddress> possibleRanges = rangeAddresses.get(range); IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch(); List<InetAddress> sources = snitch.getSortedListByProximity(myAddress, possibleRanges); assert (!sources.contains(myAddress)); for (InetAddress source : sources) { if (failureDetector.isAlive(source)) { sourceRanges.put(source, range); break; } } } return sourceRanges; }', 'ground_truth': 'private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String keyspaceName, Set<Range<Token>> ranges) { InetAddress myAddress = FBUtilities.getBroadcastAddress(); Multimap<Range<Token>, InetAddress> rangeAddresses = Keyspace.open(keyspaceName).getReplicationStrategy().getRangeAddresses(tokenMetadata.cloneOnlyTokenMap()); Multimap<InetAddress, Range<Token>> sourceRanges = HashMultimap.create(); IFailureDetector failureDetector = FailureDetector.instance; for (Range<Token> range : ranges) { Collection<InetAddress> possibleRanges = rangeAddresses.get(range); IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch(); List<InetAddress> sources = snitch.getSortedListByProximity(myAddress, possibleRanges); assert (!sources.contains(myAddress)); for (InetAddress source : sources) { if (failureDetector.isAlive(source)) { sourceRanges.put(source, range); break; } } } return sourceRanges; }', 'output': 'private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String tableName, Set<Range<Token>> ranges) { InetAddress myAddress = FBUtilities.getBroadcastAddress(); Multimap<Range<Token>, InetAddress> rangeAddresses = Table.open(tableName).getReplicationStrategy().getRangeAddresses(tokenMetadata.cloneOnlyTokenMap()); Multimap<InetAddress, Range<Token>> sourceRanges = HashMultimap.create(); IFailureDetector failureDetector = FailureDetector.instance; for (Range<Token> range : ranges) { Collection<InetAddress> possibleRanges = rangeAddresses.get(range); IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch(); List<InetAddress> sources = snitch.getSortedListByProximity(myAddress, possibleRanges); assert (!sources.contains(myAddress)); for (InetAddress source : sources) { if (failureDetector.isAlive(source)) { sourceRanges.put(source, range); break; } } } return sourceRanges; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public MetaPageInitRootRecord(int XXX, long pageId, long rootId) { super(XXX, pageId); assert pageId != rootId; this.rootId = rootId; }', 'ground_truth': 'public MetaPageInitRootRecord(int grpId, long pageId, long rootId) { super(grpId, pageId); assert pageId != rootId; this.rootId = rootId; }', 'output': 'public MetaPageInitRootRecord(int version, long pageId, long rootId) { super(version, pageId); assert pageId != rootId; this.rootId = rootId; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public MpackManager(@Assisted("mpackv2Staging") File XXX, @Assisted("stackRoot") File stackRootDir, MpackDAO mpackDAOObj, StackDAO stackDAOObj) { mpackStaging = XXX; mpackDAO = mpackDAOObj; stackRoot = stackRootDir; stackDAO = stackDAOObj; parseMpackDirectories(); }', 'ground_truth': 'public MpackManager( @Assisted("mpacksv2Staging") File mpacksStagingLocation, @Assisted("stackRoot") File stackRootDir, MpackDAO mpackDAOObj, StackDAO stackDAOObj) { mpackStaging = mpacksStagingLocation; mpackDAO = mpackDAOObj; stackRoot = stackRootDir; stackDAO = stackDAOObj; parseMpackDirectories(); }', 'output': 'public MpackManager(@Assisted("mpackv2Staging") File mpackStaging, @Assisted("stackRoot") File stackRootDir, MpackDAO mpackDAO, StackDAO stackDAO) { mpackStaging = mpackStaging; mpackDAO = mpackDAO; stackRoot = stackRootDir; stackDAO = stackDAO; parseMpackDirectories(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected TemplateModel createModel(ObjectWrapper XXX, ServletContext servletContext, final HttpServletRequest request, final HttpServletResponse response) throws TemplateModelException { try { AllHttpScopesHashModel params = new AllHttpScopesHashModel(XXX, servletContext, request); ServletContextHashModel servletContextModel = (ServletContextHashModel) servletContext.getAttribute(ATTR_APPLICATION_MODEL); if (servletContextModel == null) { servletContextModel = new ServletContextHashModel(this, XXX); servletContext.setAttribute(ATTR_APPLICATION_MODEL, servletContextModel); TaglibFactoryConfiguration taglibFactoryCfg = new TaglibFactoryConfiguration(); taglibFactoryCfg.setObjectWrapper(XXX); taglibFactoryCfg.setAdditionalTaglibJarsPattern(additionalTaglibJarsPattern); TaglibFactory taglibs = new TaglibFactory(servletContext, taglibFactoryCfg); servletContext.setAttribute(ATTR_JSP_TAGLIBS_MODEL, taglibs); initializeServletContext(request, response); } params.putUnlistedModel(KEY_APPLICATION, servletContextModel); params.putUnlistedModel(KEY_APPLICATION_PRIVATE, servletContextModel); params.putUnlistedModel(KEY_JSP_TAGLIBS, (TemplateModel)servletContext.getAttribute(ATTR_JSP_TAGLIBS_MODEL)); HttpSessionHashModel sessionModel; HttpSession session = request.getSession(false); if(session != null) { sessionModel = (HttpSessionHashModel) session.getAttribute(ATTR_SESSION_MODEL); if (sessionModel == null || sessionModel.isOrphaned(session)) { sessionModel = new HttpSessionHashModel(session, XXX); initializeSessionAndInstallModel(request, response, sessionModel, session); } } else { sessionModel = new HttpSessionHashModel(this, request, response, XXX); } params.putUnlistedModel(KEY_SESSION, sessionModel); HttpRequestHashModel requestModel = (HttpRequestHashModel) request.getAttribute(ATTR_REQUEST_MODEL); if (requestModel == null || requestModel.getRequest() != request) { requestModel = new HttpRequestHashModel(request, response, XXX); request.setAttribute(ATTR_REQUEST_MODEL, requestModel); request.setAttribute( ATTR_REQUEST_PARAMETERS_MODEL, createRequestParametersHashModel(request)); } params.putUnlistedModel(KEY_REQUEST, requestModel); params.putUnlistedModel(KEY_INCLUDE, new IncludePage(request, response)); params.putUnlistedModel(KEY_REQUEST_PRIVATE, requestModel); HttpRequestParametersHashModel requestParametersModel = (HttpRequestParametersHashModel) request.getAttribute( ATTR_REQUEST_PARAMETERS_MODEL); params.putUnlistedModel(KEY_REQUEST_PARAMETERS, requestParametersModel); return params; } catch (ServletException e) { throw new TemplateModelException(e); } catch (IOException e) { throw new TemplateModelException(e); } }', 'ground_truth': 'protected TemplateModel createModel(ObjectWrapper objectWrapper, ServletContext servletContext, final HttpServletRequest request, final HttpServletResponse response) throws TemplateModelException { try { AllHttpScopesHashModel params = new AllHttpScopesHashModel(objectWrapper, servletContext, request); ServletContextHashModel servletContextModel = (ServletContextHashModel) servletContext.getAttribute(ATTR_APPLICATION_MODEL); if (servletContextModel == null) { servletContextModel = new ServletContextHashModel(this, objectWrapper); servletContext.setAttribute(ATTR_APPLICATION_MODEL, servletContextModel); TaglibFactoryConfiguration taglibFactoryCfg = new TaglibFactoryConfiguration(); taglibFactoryCfg.setObjectWrapper(objectWrapper); taglibFactoryCfg.setClasspathTlds(classpathTlds); taglibFactoryCfg.setClasspathTaglibJarPatterns(classpathTaglibJarPatterns); TaglibFactory taglibs = new TaglibFactory(servletContext, taglibFactoryCfg); servletContext.setAttribute(ATTR_JSP_TAGLIBS_MODEL, taglibs); initializeServletContext(request, response); } params.putUnlistedModel(KEY_APPLICATION, servletContextModel); params.putUnlistedModel(KEY_APPLICATION_PRIVATE, servletContextModel); params.putUnlistedModel(KEY_JSP_TAGLIBS, (TemplateModel)servletContext.getAttribute(ATTR_JSP_TAGLIBS_MODEL)); HttpSessionHashModel sessionModel; HttpSession session = request.getSession(false); if(session != null) { sessionModel = (HttpSessionHashModel) session.getAttribute(ATTR_SESSION_MODEL); if (sessionModel == null || sessionModel.isOrphaned(session)) { sessionModel = new HttpSessionHashModel(session, objectWrapper); initializeSessionAndInstallModel(request, response, sessionModel, session); } } else { sessionModel = new HttpSessionHashModel(this, request, response, objectWrapper); } params.putUnlistedModel(KEY_SESSION, sessionModel); HttpRequestHashModel requestModel = (HttpRequestHashModel) request.getAttribute(ATTR_REQUEST_MODEL); if (requestModel == null || requestModel.getRequest() != request) { requestModel = new HttpRequestHashModel(request, response, objectWrapper); request.setAttribute(ATTR_REQUEST_MODEL, requestModel); request.setAttribute( ATTR_REQUEST_PARAMETERS_MODEL, createRequestParametersHashModel(request)); } params.putUnlistedModel(KEY_REQUEST, requestModel); params.putUnlistedModel(KEY_INCLUDE, new IncludePage(request, response)); params.putUnlistedModel(KEY_REQUEST_PRIVATE, requestModel); HttpRequestParametersHashModel requestParametersModel = (HttpRequestParametersHashModel) request.getAttribute( ATTR_REQUEST_PARAMETERS_MODEL); params.putUnlistedModel(KEY_REQUEST_PARAMETERS, requestParametersModel); return params; } catch (ServletException e) { throw new TemplateModelException(e); } catch (IOException e) { throw new TemplateModelException(e); } }', 'output': 'protected TemplateModel createModel(ObjectWrapper wrapper, ServletContext servletContext, final HttpServletRequest request, final HttpServletResponse response) throws TemplateModelException { try { AllHttpScopesHashModel params = new AllHttpScopesHashModel(wrapper, servletContext, request); ServletContextHashModel servletContextModel = (ServletContextHashModel) servletContext.getAttribute(ATTR_APPLICATION_MODEL); if (servletContextModel == null) { servletContextModel = new ServletContextHashModel(this, wrapper); servletContext.setAttribute(ATTR_APPLICATION_MODEL, servletContextModel); TaglibFactoryConfiguration taglibFactoryCfg = new TaglibFactoryConfiguration(); taglibFactoryCfg.setObjectWrapper(wrapper); taglibFactoryCfg.setAdditionalTaglibJarsPattern(additionalTaglibJarsPattern); TaglibFactory taglibs = new TaglibFactory(servletContext, taglibFactoryCfg); servletContext.setAttribute(ATTR_JSP_TAGLIBS_MODEL, taglibs); initializeServletContext(request, response); } params.putUnlistedModel(KEY_APPLICATION, servletContextModel); params.putUnlistedModel(KEY_APPLICATION_PRIVATE, servletContextModel); params.putUnlistedModel(KEY_JSP_TAGLIBS, (TemplateModel)servletContext.getAttribute(ATTR_JSP_TAGLIBS_MODEL)); HttpSessionHashModel sessionModel; HttpSession session = request.getSession(false); if(session != null) { sessionModel = (HttpSessionHashModel) session.getAttribute(ATTR_SESSION_MODEL); if (sessionModel == null || sessionModel.isOrphaned(session)) { sessionModel = new HttpSessionHashModel(session, wrapper); initializeSessionAndInstallModel(request, response, sessionModel, session); } } else { sessionModel = new HttpSessionHashModel(this, request, response, wrapper); } params.putUnlistedModel(KEY_SESSION, sessionModel); HttpRequestHashModel requestModel = (HttpRequestHashModel) request.getAttribute(ATTR_REQUEST_MODEL); if (requestModel == null || requestModel.getRequest() != request) { requestModel = new HttpRequestHashModel(request, response, wrapper); request.setAttribute(ATTR_REQUEST_MODE'}
{'instruction': 'Please refactor the following code:', 'input': 'public get_superColumn_args( String tablename, String key, String XXX) { this(); this.tablename = tablename; this.key = key; this.XXX = XXX; }', 'ground_truth': 'public get_superColumn_args( String tablename, String key, String superColumnPath) { this(); this.tablename = tablename; this.key = key; this.superColumnPath = superColumnPath; }', 'output': 'public get_superColumn_args( String tablename, String key, String columnName) { this(); this.tablename = tablename; this.key = key; this.columnName = columnName; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void setTimeFormat(String XXX) { super.setTimeFormat(XXX); timeFormat = null; }', 'ground_truth': 'public void setTimeFormat(String timeFormat) { String prevTimeFormat = getTimeFormat(); super.setTimeFormat(timeFormat); if (!timeFormat.equals(prevTimeFormat)) { this.timeJDateFormat = null; timeJDateFormatWithSysDefTZ = null; } }', 'output': 'public void setTimeFormat(String timeFormat) { this.timeFormat = timeFormat; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public bXXXXXXlean equals(Object XXX) { if (this == XXX) { return true; } if (XXX instanceXXXf KV) { KV<?, ?> that = (KV<?, ?>) XXX; return (this.key == null ? that.key == null : this.key.equals(that.key)) && (this.value == null ? that.value == null : this.value.equals(that.value)); } return false; }', 'ground_truth': 'public boolean equals(Object other) { if (this == other) { return true; } if (!(other instanceof KV)) { return false; } KV<?, ?> otherKv = (KV<?, ?>) other; return Objects.deepEquals(this.key, this.key) && Objects.deepEquals(this.value, otherKv.value); }', 'output': 'public boolean equals(Object obj) { if (this == obj) { return true; } if (obj instanceof KV<?, ?>) { KV<?, ?> that = (KV<?, ?>) obj; return (this.key == null ? that.key == null : this.key.equals(that.key)) && (this.value == null ? that.value == null : this.value.equals(that.value)); } return false; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void serialize(BloomFilter bf, DataOutput XXX) throws IOException { XXX.writeInt(bf.hashCount); bf.bitset.serialize(XXX); }', 'ground_truth': 'public void serialize(BloomFilter bf, DataOutput out) throws IOException { out.writeInt(bf.hashCount); bf.bitset.serialize(out); }', 'output': 'public void serialize(BloomFilter bf, DataOutput out) throws IOException { out.writeInt(bf.hashCount); bf.bitset.serialize(out); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkArrayAllocationOverflow(int XXX, int arrLen, String type) throws IOException { if (XXX < arrLen) throw new IOException("Failed to allocate required memory for " + type + " array " + "(arithmetic overflow detected) [length=" + arrLen + \']\'); }', 'ground_truth': 'private void checkArrayAllocationOverflow(int bytes, int arrLen, String type) throws IOException { long bytesToAlloc = (long)arrLen * bytes; if (!canBeAllocated(bytesToAlloc)) throw new IOException("Failed to allocate required memory for " + type + " array " + "(byte array size overflow detected) [length=" + arrLen + \']\'); }', 'output': 'private void checkArrayAllocationOverflow(int allocSize, int arrLen, String type) throws IOException { if (allocSize < arrLen) throw new IOException("Failed to allocate required memory for " + type + " array " + "(arithmetic overflow detected) [length=" + arrLen + \']\'); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void ensureKeyType(int XXX) { if (this.XXX != XXX) throw new UnsupportedOperationException("Value XXX doesn\'t match: exp=" + this.XXX + ", act=" + XXX); }', 'ground_truth': 'private void ensureKeyType(int actCode) { if (type.code() != actCode) throw new UnsupportedOperationException("Value type doesn\'t match: exp=" + type.code() + ", act=" + actCode); }', 'output': 'private void ensureKeyType(int keyType) { if (this.keyType != keyType) throw new UnsupportedOperationException("Value keyType doesn\'t match: exp=" + this.keyType + ", act=" + keyType); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public CommandDto asCommandDto( final UUID interactionId, final Can<InteractionHead> XXX, final ObjectAction objectAction, final Can<ManagedObject> argAdapters) { val commandDto = asCommandDto(interactionId, XXX); val actionDto = new ActionDto(); actionDto.setInteractionType(InteractionType.ACTION_INVOCATION); commandDto.setMember(actionDto); val representativeHead = XXX.getFirstOrFail(); addActionArgs(representativeHead, objectAction, actionDto, argAdapters); return commandDto; }', 'ground_truth': 'public CommandDto asCommandDto( final UUID interactionId, final InteractionHead targetHead, final ObjectAction objectAction, final Can<ManagedObject> argAdapters) { val commandDto = asCommandDto(interactionId, targetHead); val actionDto = new ActionDto(); actionDto.setInteractionType(InteractionType.ACTION_INVOCATION); commandDto.setMember(actionDto); addActionArgs(targetHead, objectAction, actionDto, argAdapters); return commandDto; }', 'output': 'public CommandDto asCommandDto( final UUID interactionId, final Can<InteractionHead> interactionHeads, final ObjectAction objectAction, final Can<ManagedObject> argAdapters) { val commandDto = asCommandDto(interactionId, interactionHeads); val actionDto = new ActionDto(); actionDto.setInteractionType(InteractionType.ACTION_INVOCATION); commandDto.setMember(actionDto); val representativeHead = interactionHeads.getFirstOrFail(); addActionArgs(representativeHead, objectAction, actionDto, argAdapters); return commandDto; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean equals(Object XXX) { if (this == XXX) { return true; } if (XXX instanceof MethodCallExpression) { final MethodCallExpression call = (MethodCallExpression) XXX; return nodeType == call.nodeType && method == call.method && Linq4j.equals(targetExpression, call.targetExpression) && expressions.equals(call.expressions); } return false; }', 'ground_truth': 'public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } if (!super.equals(o)) { return false; } MethodCallExpression that = (MethodCallExpression) o; if (!expressions.equals(that.expressions)) { return false; } if (!method.equals(that.method)) { return false; } if (targetExpression != null ? !targetExpression.equals(that .targetExpression) : that.targetExpression != null) { return false; } return true; }', 'output': 'public boolean equals(Object obj) { if (this == obj) { return true; } if (obj instanceof MethodCallExpression) { final MethodCallExpression call = (MethodCallExpression) obj; return nodeType == call.nodeType && method == call.method && Linq4j.equals(targetExpression, call.targetExpression) && expressions.equals(call.expressions); } return false; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void validateExecutableStage( String id, PTransform transform, Components XXX) throws Exception { ExecutableStagePayload payload = ExecutableStagePayload.parseFrom(transform.getSpec().getPayload()); Components components = payload.getComponents(); checkArgument( transform.getInputsMap().values().contains(payload.getInput()), "ExecutableStage %s uses unknown input %s", id, payload.getInput()); checkArgument( !payload.getTransformsList().isEmpty(), "ExecutableStage %s contains no transforms", id); for (String subtransformId : payload.getTransformsList()) { checkArgument( components.containsTransforms(subtransformId), "ExecutableStage %s uses unknown transform %s", id, subtransformId); } for (String outputId : payload.getOutputsList()) { checkArgument( components.containsPcollections(outputId), "ExecutableStage %s uses unknown output %s", id, outputId); } validateComponents("ExecutableStage " + id, components); }', 'ground_truth': 'private static void validateExecutableStage( String id, PTransform transform, Components outerComponents) throws Exception { ExecutableStagePayload payload = ExecutableStagePayload.parseFrom(transform.getSpec().getPayload()); Components components = payload.getComponents(); checkArgument( transform.getInputsMap().values().contains(payload.getInput()), "ExecutableStage %s uses unknown input %s", id, payload.getInput()); checkArgument( !payload.getTransformsList().isEmpty(), "ExecutableStage %s contains no transforms", id); for (String subtransformId : payload.getTransformsList()) { checkArgument( components.containsTransforms(subtransformId), "ExecutableStage %s uses unknown transform %s", id, subtransformId); } for (String outputId : payload.getOutputsList()) { checkArgument( components.containsPcollections(outputId), "ExecutableStage %s uses unknown output %s", id, outputId); } validateComponents("ExecutableStage " + id, components); }', 'output': 'private static void validateExecutableStage( String id, PTransform transform, Components components) throws Exception { ExecutableStagePayload payload = ExecutableStagePayload.parseFrom(transform.getSpec().getPayload()); checkArgument( transform.getInputsMap().values().contains(payload.getInput()), "ExecutableStage %s uses unknown input %s", id, payload.getInput()); checkArgument( !payload.getTransformsList().isEmpty(), "ExecutableStage %s contains no transforms", id); for (String subtransformId : payload.getTransformsList()) { checkArgument( components.containsTransforms(subtransformId), "ExecutableStage %s uses unknown transform %s", id, subtransformId); } for (String outputId : payload.getOutputsList()) { checkArgument( components.containsPcollections(outputId), "ExecutableStage %s uses unknown output %s", id, outputId); } validateComponents("ExecutableStage " + id, components); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static boolean isIndexBuilt(String XXX, String indexName) { ColumnFamilyStore cfs = Table.open(Table.SYSTEM_KS).getColumnFamilyStore(INDEX_CF); QueryFilter filter = QueryFilter.getNamesFilter(decorate(ByteBufferUtil.bytes(XXX)), INDEX_CF, ByteBufferUtil.bytes(indexName), System.currentTimeMillis()); return ColumnFamilyStore.removeDeleted(cfs.getColumnFamily(filter), Integer.MAX_VALUE) != null; }', 'ground_truth': 'public static boolean isIndexBuilt(String keyspaceName, String indexName) { ColumnFamilyStore cfs = Keyspace.open(Keyspace.SYSTEM_KS).getColumnFamilyStore(INDEX_CF); QueryFilter filter = QueryFilter.getNamesFilter(decorate(ByteBufferUtil.bytes(keyspaceName)), INDEX_CF, ByteBufferUtil.bytes(indexName), System.currentTimeMillis()); return ColumnFamilyStore.removeDeleted(cfs.getColumnFamily(filter), Integer.MAX_VALUE) != null; }', 'output': 'public static boolean isIndexBuilt(String tableName, String indexName) { ColumnFamilyStore cfs = Table.open(tableName).getColumnFamilyStore(INDEX_CF); QueryFilter filter = QueryFilter.getNamesFilter(decorate(ByteBufferUtil.bytes(tableName)), INDEX_CF, ByteBufferUtil.bytes(indexName), System.currentTimeMillis()); return ColumnFamilyStore.removeDeleted(cfs.getColumnFamily(filter), Integer.MAX_VALUE) != null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public <T, R> T asyncWrapMixin( final @NonNull Class<T> mixinClass, final @NonNull Object XXX, final @NonNull AsyncControl<R> asyncControl) { T mixin = factoryService.mixin(mixinClass, XXX); val mixinAdapter = adaptAndGuardAgainstWrappingNotSupported(mixin); val proxyFactory = proxyFactoryService.factory(mixinClass, new Class[]{WrappingObject.class}, new Class[]{XXX.getClass()}); return proxyFactory.createInstance(new InvocationHandler() { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { final boolean inheritedFromObject = isInheritedFromJavaLangObject(method); if (inheritedFromObject) { return method.invoke(mixin, args); } if (shouldCheckRules(asyncControl)) { val doih = new DomainObjectInvocationHandler<>( mixin, mixinAdapter, control().withNoExecute(), null); doih.invoke(null, method, args); } val actionAndTarget = memberAndTargetForMixin(method, XXX, mixinAdapter); if (! actionAndTarget.isMemberFound()) { return method.invoke(mixin, args); } return submitAsync(actionAndTarget, args, asyncControl); } }, new Object[]{ XXX }); }', 'ground_truth': 'public <T, R> T asyncWrapMixin( final @NonNull Class<T> mixinClass, final @NonNull Object mixee, final @NonNull AsyncControl<R> asyncControl) { T mixin = factoryService.mixin(mixinClass, mixee); val mixeeAdapter = adaptAndGuardAgainstWrappingNotSupported(mixee); val mixinAdapter = adaptAndGuardAgainstWrappingNotSupported(mixin); val proxyFactory = proxyFactoryService .factory(mixinClass, new Class[]{WrappingObject.class}, new Class[]{mixee.getClass()}); return proxyFactory.createInstance(new InvocationHandler() { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { final boolean inheritedFromObject = isInheritedFromJavaLangObject(method); if (inheritedFromObject) { return method.invoke(mixin, args); } if (shouldCheckRules(asyncControl)) { val doih = new DomainObjectInvocationHandler<>( mixin, mixeeAdapter, mixinAdapter, control().withNoExecute(), null); doih.invoke(null, method, args); } val actionAndTarget = memberAndTargetForMixin(method, mixee, mixinAdapter); if (! actionAndTarget.isMemberFound()) { return method.invoke(mixin, args); } return submitAsync(actionAndTarget, args, asyncControl); } }, new Object[]{ mixee }); }', 'output': 'public <T, R> T asyncWrapMixin( final @NonNull Class<T> mixinClass, final @NonNull Object target, final @NonNull AsyncControl<R> asyncControl) { T mixin = factoryService.mixin(mixinClass, target); val mixinAdapter = adaptAndGuardAgainstWrappingNotSupported(mixin); val proxyFactory = proxyFactoryService.factory(mixinClass, new Class[]{WrappingObject.class}, new Class[]{target.getClass()}); return proxyFactory.createInstance(new InvocationHandler() { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { final boolean inheritedFromObject = isInheritedFromJavaLangObject(method); if (inheritedFromObject) { return method.invoke(mixin, args); } if (shouldCheckRules(asyncControl)) { val doih = new DomainObjectInvocationHandler<>( mixin, mixinAdapter, control().withNoExecute(), null); doih.invoke(null, method, args); } val actionAndTarget = memberAndTargetForMixin(method, target, mixinAdapter); if (! actionAndTarget.isMemberFound()) { return method.invoke(mixin, args); } return submitAsync(actionAndTarget, args, asyncControl); } }, new Object[]{ target }); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void send_get_superColumn(String tablename, String key, String XXX) throws TException { oprot_.writeMessageBegin(new TMessage("get_superColumn", TMessageType.CALL, seqid_)); get_superColumn_args args = new get_superColumn_args(); args.tablename = tablename; args.key = key; args.XXX = XXX; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'ground_truth': 'public void send_get_superColumn(String tablename, String key, String superColumnPath) throws TException { oprot_.writeMessageBegin(new TMessage("get_superColumn", TMessageType.CALL, seqid_)); get_superColumn_args args = new get_superColumn_args(); args.tablename = tablename; args.key = key; args.superColumnPath = superColumnPath; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'output': 'public void send_get_superColumn(String tablename, String key, String columnName) throws TException { oprot_.writeMessageBegin(new TMessage("get_superColumn", TMessageType.CALL, seqid_)); get_superColumn_args args = new get_superColumn_args(); args.tablename = tablename; args.key = key; args.columnName = columnName; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void XXX(String table) throws IOException { openLock.lock(); try { instances.remove(table); open(table); } finally { openLock.unlock(); } }', 'ground_truth': 'public static void close(String table) throws IOException { Table t = open(table); if (t != null) open(table).release(); }', 'output': 'public static void removeTable(String table) throws IOException { openLock.lock(); try { instances.remove(table); open(table); } finally { openLock.unlock(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static final String XXX(int id) { return currentCfNames.get(id); }', 'ground_truth': 'public static final Pair<String,String> getCF(int id) { return cfIdMap.inverse().get(Integer.valueOf(id)); }', 'output': 'public static final String getCurrentCfName(int id) { return currentCfNames.get(id); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ColumnFamilyStore XXX() { return baseCfs; }', 'ground_truth': 'public ColumnFamilyStore getBaseCfs() { return baseCfs; }', 'output': 'public ColumnFamilyStore getColumnFamilyStore() { return baseCfs; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { fillCaches(); GridCacheQueriesEx<Integer, FactPurchase> qx = (GridCacheQueriesEx<Integer, FactPurchase>)ignite.<Integer, FactPurchase>cache("partitioned").queries(); Set<Integer> set0 = new HashSet<>(); for (List<?> o : qx.executeTwoStepQuery("partitioned", "select productId from FactPurchase group by productId") .get()) { X.println("___ -> " + o); assertTrue(set0.add((Integer) o.get(0))); } X.println("___ "); Set<Integer> set1 = new HashSet<>(); for (List<?> o : qx.executeTwoStepQuery("partitioned", "select productId from FactPurchase") .get()) { X.println("___ -> " + o); set1.add((Integer)o.get(0)); } assertFalse(set1.isEmpty()); assertEquals(set0, set1); }', 'ground_truth': 'public void testTwoStepGroupAndAggregates() throws Exception { fillCaches(); GridCacheQueriesEx<Integer, FactPurchase> qx = (GridCacheQueriesEx<Integer, FactPurchase>)ignite.<Integer, FactPurchase>cache("partitioned").queries(); Set<Integer> set1 = new HashSet<>(); X.println("___ simple"); for (List<?> o : qx.executeTwoStepQuery("partitioned", "select f.productId, p.name, f.price " + "from FactPurchase f, \\"replicated\\".DimProduct p where p.id = f.productId ").get()) { X.println("___ -> " + o); set1.add((Integer)o.get(0)); } Set<Integer> set0 = new HashSet<>(); X.println("___ GROUP BY"); for (List<?> o : qx.executeTwoStepQuery("partitioned", "select productId from FactPurchase group by productId") .get()) { X.println("___ -> " + o); assertTrue(set0.add((Integer) o.get(0))); } assertFalse(set1.isEmpty()); assertEquals(set0, set1); X.println("___ AVG"); for (List<?> o : qx.executeTwoStepQuery("partitioned", "select p.name, avg(f.price) from FactPurchase f, \\"replicated\\".DimProduct p where p.id = f.productId " + "group by f.productId, p.name").get()) { X.println("___ -> " + o); } }', 'output': 'public void testExecuteTwoStepQueryOnPartitionedCache() throws Exception { fillCaches(); IgniteCache<Integer, FactPurchase> qx = (IgniteCache<Integer, FactPurchase>)ignite.<Integer, FactPurchase>cache("partitioned").queries(); Set<Integer> set0 = new HashSet<>(); for (List<?> o : qx.executeTwoStepQuery("partitioned", "select productId from FactPurchase group by productId") .get()) { X.println("___ -> " + o); assertTrue(set0.add((Integer) o.get(0))); } X.println("___ "); Set<Integer> set1 = new HashSet<>(); for (List<?> o : qx.executeTwoStepQuery("partitioned", "select productId from FactPurchase") .get()) { X.println("___ -> " + o); set1.add((Integer)o.get(0)); } assertFalse(set1.isEmpty()); assertEquals(set0, set1); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(final Deque<GridNearLockMapping> mappings) throws IgniteCheckedException { GridNearLockMapping map = mappings.poll(); if (map == null) return; final GridNearLockRequest req = map.request(); final Collection<KeyCacheObject> mappedKeys = map.distributedKeys(); final ClusterNode node = map.node(); if (filter != null && filter.length != 0) req.filter(filter, cctx); if (node.isLocal()) lockLocally(mappedKeys, req.topologyVersion(), mappings); else { final MiniFuture fut = new MiniFuture(node, mappedKeys, mappings); req.miniId(fut.futureId()); add(fut); IgniteInternalFuture<?> txSync = null; if (inTx()) txSync = cctx.tm().awaitFinishAckAsync(node.id(), tx.threadId()); if (txSync == null || txSync.isDone()) { try { if (log.isDebugEnabled()) log.debug("Sending near lock request [node=" + node.id() + ", req=" + req + \']\'); cctx.io().send(node, req, cctx.ioPolicy()); } catch (ClusterTopologyCheckedException ex) { assert fut != null; fut.onResult(ex); } } else { txSync.listen(new CI1<IgniteInternalFuture<?>>() { @Override public void apply(IgniteInternalFuture<?> t) { try { if (log.isDebugEnabled()) log.debug("Sending near lock request [node=" + node.id() + ", req=" + req + \']\'); cctx.io().send(node, req, cctx.ioPolicy()); } catch (ClusterTopologyCheckedException ex) { assert fut != null; fut.onResult(ex); } catch (IgniteCheckedException e) { onError(e); } } }); } } }', 'ground_truth': 'private void lockLocally( final Collection<KeyCacheObject> keys, AffinityTopologyVersion topVer ) { if (log.isDebugEnabled()) log.debug("Before locally locking keys : " + keys); IgniteInternalFuture<Exception> fut = cctx.colocated().lockAllAsync(cctx, tx, threadId, lockVer, topVer, keys, read, retval, timeout, accessTtl, filter, skipStore); add(new GridEmbeddedFuture<>( new C2<Exception, Exception, Boolean>() { @Override public Boolean apply(Exception resEx, Exception e) { if (CU.isLockTimeoutOrCancelled(e) || (resEx != null && CU.isLockTimeoutOrCancelled(resEx))) return false; if (e != null) { onError(e); return false; } if (resEx != null) { onError(resEx); return false; } if (log.isDebugEnabled()) log.debug("Acquired lock for local DHT mapping [locId=" + cctx.nodeId() + ", mappedKeys=" + keys + ", fut=" + GridDhtColocatedLockFuture.this + \']\'); if (inTx()) { for (KeyCacheObject key : keys) tx.entry(cctx.txKey(key)).markLocked(); } else { for (KeyCacheObject key : keys) cctx.mvcc().markExplicitOwner(cctx.txKey(key), threadId); } try { if (mappings != null) proceedMapping(); } catch (IgniteCheckedException ex) { onError(ex); return false; } return true; } }, fut)); }', 'output': 'private void sendNearLockRequest(final Deque<GridNearLockMapping> mappings) throws IgniteCheckedException { GridNearLockMapping map = mappings.poll(); if (map == null) return; final GridNearLockRequest req = map.request(); final Collection<KeyCacheObject> mappedKeys = map.distributedKeys(); final ClusterNode node = map.node(); if (filter != null && filter.length != 0) req.filter(filter, cctx); if (node.isLocal()) lockLocally(mappedKeys, req.topologyVersion(), mappings); else { final MiniFuture fut = new MiniFuture(node, mappedKeys, mappings); req.miniId(fut.futureId()); add(fut); IgniteInternalFuture<?> txSync = null; if (inTx()) txSync = cctx.tm().awaitFinishAckAsync(node.id(), tx.threadId()); if (txSync == null || txSync.isDone()) { try { if (log.isDebugEnabled()) log.debug("Sending near lock request [node=" + node.id() + ", req=" + req + \']\'); cctx.io().send(node, req, cctx.ioPolicy()); } catch (ClusterTopologyCheckedException ex) { assert fut != null; fut.onResult(ex); } } else { txSync.listen(new CI1<IgniteInternalFuture<?>>() { @Override public void apply(IgniteInternalFuture<?> t) { try { if (log.isDebugEnabled()) log.debug("Sending near lock request [node=" + node.id() + ", req=" + req + \']\'); cctx.io().send(node, req, cctx.ioPolicy()); } catch (ClusterTopologyCheckedException ex) { assert fut != null; fut.onResult(ex); } catch (IgniteCheckedException e) { onError(e); } } }); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public TokenMetadata XXX() { lock.readLock().lock(); try { return new TokenMetadata(HashBiMap.create(tokenToEndPointMap)); } finally { lock.readLock().unlock(); } }', 'ground_truth': 'public TokenMetadata cloneOnlyTokenMap() { lock.readLock().lock(); try { return new TokenMetadata(HashBiMap.create(tokenToEndPointMap)); } finally { lock.readLock().unlock(); } }', 'output': 'public TokenMetadata getTokenMetadata() { lock.readLock().lock(); try { return new TokenMetadata(HashBiMap.create(tokenToEndPointMap)); } finally { lock.readLock().unlock(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'XXXpXXXuXXXbXXXlXXXiXXXcXXX XXXvXXXoXXXiXXXdXXX XXXtXXXeXXXsXXXtXXXGXXXrXXXoXXXoXXXvXXXyXXXcXXXTXXXeXXXsXXXtXXX1XXX_XXXFXXXoXXXrXXXkXXX_XXXIXXXnXXXhXXXeXXXrXXXiXXXtXXX XXX(XXX XXX)XXX XXX{XXX XXXeXXXnXXXsXXXuXXXrXXXeXXXNXXXoXXXtXXXPXXXrXXXeXXXsXXXeXXXnXXXtXXX XXX(XXX XXX"XXXGXXXrXXXoXXXoXXXvXXXyXXXcXXXTXXXeXXXsXXXtXXX1XXX"XXX XXX)XXX XXX;XXX XXXpXXXrXXXoXXXjXXXeXXXcXXXtXXX.XXXeXXXxXXXeXXXcXXXuXXXtXXXeXXXTXXXaXXXrXXXgXXXeXXXtXXX XXX(XXX XXX"XXXGXXXrXXXoXXXoXXXvXXXyXXXcXXXTXXXeXXXsXXXtXXX1XXX_XXXFXXXoXXXrXXXkXXX_XXXIXXXnXXXhXXXeXXXrXXXiXXXtXXX"XXX XXX)XXX XXX;XXX XXXeXXXnXXXsXXXuXXXrXXXeXXXRXXXeXXXsXXXuXXXlXXXtXXXOXXXKXXX XXX(XXX XXX"XXXGXXXrXXXoXXXoXXXvXXXyXXXcXXXTXXXeXXXsXXXtXXX1XXX"XXX XXX)XXX XXX;XXX XXX}XXX', 'ground_truth': 'public void testGroovycTest1_NoFork_WithJavaClasspath ( ) { ensureNotPresent ( "GroovycTest1" ) ; project.executeTarget ( "GroovycTest1_NoFork_WithJavaClasspath" ) ; ensureResultOK ( "GroovycTest1" ) ; }', 'output': 'public static void main(String[] args) {\n    System.out.println("Hello, world!");\n}</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public long XXX() { return scanner.getFileLength(); } public long XXX()\n        { { try { return dataFile.length(); } catch (IOException e) { throw new RuntimeException(e); } }', 'ground_truth': 'public CompactionInfo getCompactionInfo() { try { return new CompactionInfo(sstable.descriptor.ksname, sstable.descriptor.cfname, "Cleanup of " + sstable.getColumnFamilyName(), scanner.getFilePointer(), scanner.getFileLength()); } catch (Exception e) { throw new RuntimeException(); } }', 'output': 'public long getFileLength() { try { return dataFile.length(); } catch (IOException e) { throw new RuntimeException(e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws IOException, InterruptedException { failed = false; DatabaseDescriptor.setCommitLogSyncBatchWindow(1); DatabaseDescriptor.setCommitLogSyncPeriod(30); DatabaseDescriptor.setCommitLogSegmentSize(32); testLog(null, EncryptionContextGenerator.createDisabledContext()); testLog(new ParameterizedClass("LZ4Compressor", null), EncryptionContextGenerator.createDisabledContext()); testLog(new ParameterizedClass("SnappyCompressor", null), EncryptionContextGenerator.createDisabledContext()); testLog(new ParameterizedClass("DeflateCompressor", null), EncryptionContextGenerator.createDisabledContext()); testLog(null, EncryptionContextGenerator.createContext(true)); }', 'ground_truth': 'private void testLog() throws IOException, InterruptedException { String originalDir = DatabaseDescriptor.getCommitLogLocation(); try { DatabaseDescriptor.setCommitLogLocation(location); CommitLog commitLog = new CommitLog(CommitLogArchiver.disabled()).start(); testLog(commitLog); assert !failed; } finally { DatabaseDescriptor.setCommitLogLocation(originalDir); } }', 'output': 'public void testLog() throws IOException, InterruptedException { failed = false; DatabaseDescriptor.setCommitLogSyncBatchWindow(1); DatabaseDescriptor.setCommitLogSyncPeriod(30); DatabaseDescriptor.setCommitLogSegmentSize(32); testLog(null, EncryptionContextGenerator.createDisabledContext()); testLog(new ParameterizedClass("LZ4Compressor", null), EncryptionContextGenerator.createDisabledContext()); testLog(new ParameterizedClass("SnappyCompressor", null), EncryptionContextGenerator.createDisabledContext()); testLog(new ParameterizedClass("DeflateCompressor", null), EncryptionContextGenerator.createDisabledContext()); testLog(null, EncryptionContextGenerator.createContext(true)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(InetAddressAndPort from, long timeTaken) { String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(from); Timer timer = dcLatency.get(dc); if (timer == null) { timer = dcLatency.computeIfAbsent(dc, k -> Metrics.timer(factory.createMetricName(dc + "-Latency"))); } timer.update(timeTaken, TimeUnit.MILLISECONDS); crossNodeLatency.update(timeTaken, TimeUnit.MILLISECONDS); }', 'ground_truth': 'public DCLatencyRecorder internodeLatencyRecorder(InetAddressAndPort from) { String dcName = DatabaseDescriptor.getEndpointSnitch().getDatacenter(from); DCLatencyRecorder dcUpdater = dcLatency.get(dcName); if (dcUpdater == null) dcUpdater = dcLatency.computeIfAbsent(dcName, k -> new DCLatencyRecorder(Metrics.timer(factory.createMetricName(dcName + "-Latency")), allLatency)); return dcUpdater; }', 'output': 'public void updateLatency(InetAddressAndPort from, long timeTaken) { String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(from); Timer timer = dcLatency.get(dc); if (timer == null) { timer = dcLatency.computeIfAbsent(dc, k -> Metrics.timer(factory.createMetricName(dc + "-Latency"))); } timer.update(timeTaken, TimeUnit.MILLISECONDS); crossNodeLatency.update(timeTaken, TimeUnit.MILLISECONDS); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Throwable { String functionName = createSimpleFunction(true); String cql = String.format("SELECT * FROM %s WHERE k = %s", KEYSPACE + "." + currentTable(), functionCall(functionName)); assertPermissionsOnFunction(cql, functionName); }', 'ground_truth': 'public void functionInSelectPKRestriction() throws Throwable { String functionName = createSimpleFunction(); String cql = String.format("SELECT * FROM %s WHERE k = %s", KEYSPACE + "." + currentTable(), functionCall(functionName)); assertPermissionsOnFunction(cql, functionName); }', 'output': 'public void testCQLSelectWithFunction() throws Throwable { String functionName = createSimpleFunction(true); String cql = String.format("SELECT * FROM %s WHERE k = %s", KEYSPACE + "." + currentTable(), functionCall(functionName)); assertPermissionsOnFunction(cql, functionName); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private MarkupContainer XXX(final String id, final PdfJsPanel pdfJsPanel) { return new WebMarkupContainer(id) { private static final long serialVersionUID = 1L; @Override protected void onComponentTag(final ComponentTag tag) { super.onComponentTag(tag); tag.put("data-canvas-id", pdfJsPanel.getCanvasId()); } }; }', 'ground_truth': 'private MarkupContainer createToolbarComponent(final String id, final PdfJsPanel pdfJsPanel) { return new WebMarkupContainer(id) { private static final long serialVersionUID = 1L; @Override protected void onComponentTag(final ComponentTag tag) { super.onComponentTag(tag); tag.put("data-canvas-id", pdfJsPanel.getCanvasId()); } }; }', 'output': 'private MarkupContainer addPdfJsPanel(final String id, final PdfJsPanel pdfJsPanel) { return new WebMarkupContainer(id) { private static final long serialVersionUID = 1L; @Override protected void onComponentTag(final ComponentTag tag) { super.onComponentTag(tag); tag.put("data-canvas-id", pdfJsPanel.getCanvasId()); } }; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Object XXX(long value) { /** @todo if (value >= 0 && value < 100) { return integerCache[i]; } */ return new Long(value); }', 'ground_truth': 'public static Object box(byte value) { /** @todo cache? */ return new Byte(value); }', 'output': 'public static Object getObject(long value) { if (value >= 0 && value < 100) { return integerCache[i]; } return new Long(value); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RangeTombstoneList XXX(AbstractAllocator allocator) { RangeTombstoneList XXX =  new RangeTombstoneList(comparator, new ClusteringBound<?>[size], new ClusteringBound<?>[size], Arrays.XXXOf(markedAts, size), Arrays.XXXOf(delTimes, size), boundaryHeapSize, size); for (int i = 0; i < size; i++) { XXX.starts[i] = clone(starts[i], allocator); XXX.ends[i] = clone(ends[i], allocator); } return XXX; }', 'ground_truth': 'public RangeTombstoneList clone(ByteBufferCloner cloner) { RangeTombstoneList copy =  new RangeTombstoneList(comparator, new ClusteringBound<?>[size], new ClusteringBound<?>[size], Arrays.copyOf(markedAts, size), Arrays.copyOf(delTimes, size), boundaryHeapSize, size); for (int i = 0; i < size; i++) { copy.starts[i] = clone(starts[i], cloner); copy.ends[i] = clone(ends[i], cloner); } return copy; }', 'output': 'public RangeTombstoneList getRangeTombstoneList(AbstractAllocator allocator) { RangeTombstoneList rangeTombstoneList = new RangeTombstoneList(comparator, new ClusteringBound<?>[size], new ClusteringBound<?>[size], Arrays.asList(markedAts), Arrays.asList(delTimes), boundaryHeapSize, size); for (int i = 0; i < size; i++) { rangeTombstoneList.starts[i] = clone(starts[i], allocator); rangeTombstoneList.ends[i] = clone(ends[i], allocator); } return rangeTombstoneList; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<? extends UnboundedSource<String, CheckpointMarkT>> XXX( int desiredNumSplits, PipelineOptions options) throws Exception { return Collections.<UnboundedSource<String, CheckpointMarkT>>singletonList(this); }', 'ground_truth': 'public List<? extends UnboundedSource<String, CheckpointMarkT>> split( int desiredNumSplits, PipelineOptions options) throws Exception { return Collections.<UnboundedSource<String, CheckpointMarkT>>singletonList(this); }', 'output': 'public List<? extends UnboundedSource<String, CheckpointMarkT>> getSources( int desiredNumSplits, PipelineOptions options) throws Exception { return Collections.<UnboundedSource<String, CheckpointMarkT>>singletonList(this); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void XXX() throws IgniteException { for (Iterator<DirectMemoryRegion> it = regions.iterator(); it.hasNext(); ) { DirectMemoryRegion chunk = it.next(); GridUnsafe.freeMemory(chunk.address()); it.remove(); } } } }', 'ground_truth': '@Override public void shutdown() { for (Iterator<DirectMemoryRegion> it = regions.iterator(); it.hasNext(); ) { DirectMemoryRegion chunk = it.next(); GridUnsafe.freeMemory(chunk.address()); it.remove(); } }', 'output': '@Override public void cleanup() throws IgniteException { for (Iterator<DirectMemoryRegion> it = regions.iterator(); it.hasNext(); ) { DirectMemoryRegion chunk = it.next(); GridUnsafe.freeMemory(chunk.address()); it.remove(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws TemplateException, TemplateNotFoundException, MalformedTemplateNameException, ParseException, IOException { Configuration cfg = new Configuration(Configuration.VERSION_2_3_23); assertNull(cfg.getTemplateConfigurers()); StringTemplateLoader tl = new StringTemplateLoader(); tl.putTemplate("t.de.ftlh", ""); tl.putTemplate("t.fr.ftlx", ""); tl.putTemplate("t.ftlx", ""); tl.putTemplate("Stat/t.de.ftlx", ""); cfg.setTemplateLoader(tl); cfg.setTimeZone(TimeZone.getTimeZone("GMT+09")); cfg.setSetting(Configuration.TEMPLATE_CONFIGURERS_KEY, "MergingTemplateConfigurerFactory(" + "FirstMatchTemplateConfigurerFactory(" + "ConditionalTemplateConfigurerFactory(" + "FileNameGlobMatcher(\'*.de.*\'), TemplateConfigurer(timeZone=TimeZone(\'GMT+01\'))), " + "ConditionalTemplateConfigurerFactory(" + "FileNameGlobMatcher(\'*.fr.*\'), TemplateConfigurer(timeZone=TimeZone(\'GMT\'))), " + "allowNoMatch=true" + "), " + "FirstMatchTemplateConfigurerFactory(" + "ConditionalTemplateConfigurerFactory(" + "FileExtensionMatcher(\'ftlh\'), TemplateConfigurer(booleanFormat=\'TODO,HTML\')), " + "ConditionalTemplateConfigurerFactory(" + "FileExtensionMatcher(\'ftlx\'), TemplateConfigurer(booleanFormat=\'TODO,XML\')), " + "noMatchErrorDetails=\'Unrecognized template file extension\'" + "), " + "ConditionalTemplateConfigurerFactory(" + "PathGlobMatcher(\'stat/**\', caseInsensitive=true), " + "TemplateConfigurer(timeZone=TimeZone(\'UTC\'))" + ")" + ")"); { Template t = cfg.getTemplate("t.de.ftlh"); assertEquals("TODO,HTML", t.getBooleanFormat()); assertEquals(TimeZone.getTimeZone("GMT+01"), t.getTimeZone()); } { Template t = cfg.getTemplate("t.fr.ftlx"); assertEquals("TODO,XML", t.getBooleanFormat()); assertEquals(TimeZone.getTimeZone("GMT"), t.getTimeZone()); } { Template t = cfg.getTemplate("t.ftlx"); assertEquals("TODO,XML", t.getBooleanFormat()); assertEquals(TimeZone.getTimeZone("GMT+09"), t.getTimeZone()); } { Template t = cfg.getTemplate("Stat/t.de.ftlx"); assertEquals("TODO,XML", t.getBooleanFormat()); assertEquals(DateUtil.UTC, t.getTimeZone()); } assertNotNull(cfg.getTemplateConfigurers()); cfg.setSetting(Configuration.TEMPLATE_CONFIGURERS_KEY, "null"); assertNull(cfg.getTemplateConfigurers()); }', 'ground_truth': 'public void testSetTemplateConfigurations() throws Exception { Configuration cfg = new Configuration(Configuration.VERSION_2_3_23); assertNull(cfg.getTemplateConfigurations()); StringTemplateLoader tl = new StringTemplateLoader(); tl.putTemplate("t.de.ftlh", ""); tl.putTemplate("t.fr.ftlx", ""); tl.putTemplate("t.ftlx", ""); tl.putTemplate("Stat/t.de.ftlx", ""); cfg.setTemplateLoader(tl); cfg.setTimeZone(TimeZone.getTimeZone("GMT+09")); cfg.setSetting(Configuration.TEMPLATE_CONFIGURATIONS_KEY, "MergingTemplateConfigurationFactory(" + "FirstMatchTemplateConfigurationFactory(" + "ConditionalTemplateConfigurationFactory(" + "FileNameGlobMatcher(\'*.de.*\'), TemplateConfiguration(timeZone=TimeZone(\'GMT+01\'))), " + "ConditionalTemplateConfigurationFactory(" + "FileNameGlobMatcher(\'*.fr.*\'), TemplateConfiguration(timeZone=TimeZone(\'GMT\'))), " + "allowNoMatch=true" + "), " + "FirstMatchTemplateConfigurationFactory(" + "ConditionalTemplateConfigurationFactory(" + "FileExtensionMatcher(\'ftlh\'), TemplateConfiguration(booleanFormat=\'TODO,HTML\')), " + "ConditionalTemplateConfigurationFactory(" + "FileExtensionMatcher(\'ftlx\'), TemplateConfiguration(booleanFormat=\'TODO,XML\')), " + "noMatchErrorDetails=\'Unrecognized template file extension\'" + "), " + "ConditionalTemplateConfigurationFactory(" + "PathGlobMatcher(\'stat/**\', caseInsensitive=true), " + "TemplateConfiguration(timeZone=TimeZone(\'UTC\'))" + ")" + ")"); { Template t = cfg.getTemplate("t.de.ftlh"); assertEquals("TODO,HTML", t.getBooleanFormat()); assertEquals(TimeZone.getTimeZone("GMT+01"), t.getTimeZone()); } { Template t = cfg.getTemplate("t.fr.ftlx"); assertEquals("TODO,XML", t.getBooleanFormat()); assertEquals(TimeZone.getTimeZone("GMT"), t.getTimeZone()); } { Template t = cfg.getTemplate("t.ftlx"); assertEquals("TODO,XML", t.getBooleanFormat()); assertEquals(TimeZone.getTimeZone("GMT+09"), t.getTimeZone()); } { Template t = cfg.getTemplate("Stat/t.de.ftlx"); assertEquals("TODO,XML", t.getBooleanFormat()); assertEquals(DateUtil.UTC, t.getTimeZone()); } assertNotNull(cfg.getTemplateConfigurations()); cfg.setSetting(Configuration.TEMPLATE_CONFIGURATIONS_KEY, "null"); assertNull(cfg.getTemplateConfigurations()); }', 'output': 'public void testMergeTemplateConfigurers() throws TemplateException, TemplateNotFoundException, MalformedTemplateNameException, ParseException, IOException { Configuration cfg = new Configuration(Configuration.VERSION_2_3_23); assertNull(cfg.getTemplateConfigurers()); StringTemplateLoader tl = new StringTemplateLoader(); tl.putTemplate("t.de.ftlh", ""); tl.putTemplate("t.fr.ftlx", ""); tl.putTemplate("t.ftlx", ""); tl.putTemplate("Stat/t.de.ftlx", ""); cfg.setTemplateLoader(tl); cfg.setTimeZone(TimeZone.getTimeZone("GMT+09")); cfg.setSetting(Configuration.TEMPLATE_CONFIGURERS_KEY, "MergingTemplateConfigurerFactory(" + "FirstMatchTemplateConfigurerFactory(" + "ConditionalTemplateConfigurerFactory(" + "FileNameGlobMatcher(\'*.de.*\'), TemplateConfigurer(timeZone=TimeZone(\'GMT+01\'))), " + "ConditionalTemplateConfigurerFactory(" + "FileNameGlobMatcher(\'*.fr.*\'), TemplateConfigurer(timeZone=TimeZone(\'GMT\'))), " + "allowNoMatch=true" + "), " + "FirstMatchTemplateConfigurerFactory(" + "ConditionalTemplateConfigurerFactory(" + "FileExtensionMatcher(\'ftlh\'), TemplateConfigurer(booleanFormat=\'TODO,HTML\')), " + "ConditionalTemplateConfigurerFactory(" + "FileExtensionMatcher(\'ftlx\'), TemplateConfigurer(booleanFormat=\'TODO,XML\')), " + "noMatchErrorDetails=\'Unrecognized template file extension\'" + "), " + "ConditionalTemplateConfigurerFactory(" + "PathGlobMatcher(\'stat/**\', caseInsensitive=true), " + "TemplateConfigurer(timeZone=TimeZone(\'UTC\'))" + ")" + ")"); { Template t = cfg.getTemplate("t.de.ftlh"); assertEquals("TODO,HTML", t.getBooleanFormat()); assertEquals(TimeZone.getTimeZone("GMT+01"), t.getTimeZone()); } { Template t = cfg.getTemplate("t.fr.ftlx"); assertEquals("TODO,XML", t.'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX( Blackboard bb, SqlCall values) { RelNode valuesRel = convertRowValues( bb, values, Arrays.asList(values.getOperands()), true); if (valuesRel != null) { bb.setRoot(valuesRel, true); return; } SqlNode [] rowConstructorList = values.getOperands(); List<RelNode> unionRels = new ArrayList<RelNode>(); for (SqlNode rowConstructor1 : rowConstructorList) { SqlCall rowConstructor = (SqlCall) rowConstructor1; Blackboard tmpBb = createBlackboard(bb.scope, null); replaceSubqueries(tmpBb, rowConstructor); RexNode [] exps = new RexNode[rowConstructor.operands.length]; String [] fieldNames = new String[rowConstructor.operands.length]; for (int j = 0; j < rowConstructor.operands.length; j++) { final SqlNode node = rowConstructor.operands[j]; exps[j] = tmpBb.convertExpression(node); fieldNames[j] = validator.deriveAlias(node, j); } RelNode in = (null == tmpBb.root) ? new OneRowRel(cluster) : tmpBb.root; unionRels.add( CalcRel.createProject( in, exps, fieldNames, true)); } if (unionRels.size() == 0) { throw Util.newInternal("empty values clause"); } else if (unionRels.size() == 1) { bb.setRoot( unionRels.get(0), true); } else { bb.setRoot( new UnionRel( cluster, unionRels, true), true); } }', 'ground_truth': 'private void convertValuesImpl( Blackboard bb, SqlCall values, RelDataType targetRowType) { RelNode valuesRel = convertRowValues( bb, values, Arrays.asList(values.getOperands()), true); if (valuesRel != null) { bb.setRoot(valuesRel, true); return; } SqlNode [] rowConstructorList = values.getOperands(); List<RelNode> unionRels = new ArrayList<RelNode>(); for (SqlNode rowConstructor1 : rowConstructorList) { SqlCall rowConstructor = (SqlCall) rowConstructor1; Blackboard tmpBb = createBlackboard(bb.scope, null); replaceSubqueries(tmpBb, rowConstructor); RexNode [] exps = new RexNode[rowConstructor.operands.length]; String [] fieldNames = new String[rowConstructor.operands.length]; for (int j = 0; j < rowConstructor.operands.length; j++) { final SqlNode node = rowConstructor.operands[j]; exps[j] = tmpBb.convertExpression(node); fieldNames[j] = validator.deriveAlias(node, j); } RelNode in = (null == tmpBb.root) ? new OneRowRel(cluster) : tmpBb.root; unionRels.add( CalcRel.createProject( in, exps, fieldNames, true)); } if (unionRels.size() == 0) { throw Util.newInternal("empty values clause"); } else if (unionRels.size() == 1) { bb.setRoot( unionRels.get(0), true); } else { bb.setRoot( new UnionRel( cluster, unionRels, true), true); } }', 'output': 'private void processValues( Blackboard bb, SqlCall values) { RelNode valuesRel = convertRowValues( bb, values, Arrays.asList(values.getOperands()), true); if (valuesRel != null) { bb.setRoot(valuesRel, true); return; } SqlNode [] rowConstructorList = values.getOperands(); List<RelNode> unionRels = new ArrayList<RelNode>(); for (SqlNode rowConstructor1 : rowConstructorList) { SqlCall rowConstructor = (SqlCall) rowConstructor1; Blackboard tmpBb = createBlackboard(bb.scope, null); replaceSubqueries(tmpBb, rowConstructor); RexNode [] exps = new RexNode[rowConstructor.operands.length]; String [] fieldNames = new String[rowConstructor.operands.length]; for (int j = 0; j < rowConstructor.operands.length; j++) { final SqlNode node = rowConstructor.operands[j]; exps[j] = tmpBb.convertExpression(node); fieldNames[j] = validator.deriveAlias(node, j); } RelNode in = (null == tmpBb.root) ? new OneRowRel(cluster) : tmpBb.root; unionRels.add( CalcRel.createProject( in, exps, fieldNames, true)); } if (unionRels.size() == 0) { throw Util.newInternal("empty values clause"); } else if (unionRels.size() == 1) { bb.setRoot( unionRels.get(0), true); } else { bb.setRoot( new UnionRel( cluster, unionRels, true), true); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void XXX(int type, PortableRawReaderEx reader, PortableRawWriterEx writer, @Nullable Object arg) throws IgniteCheckedException { throwUnsupported(type); }', 'ground_truth': 'protected void processInStreamOutStream(int type, PortableRawReaderEx reader, PortableRawWriterEx writer) throws IgniteCheckedException { throwUnsupported(type); }', 'output': 'protected void writeTo(int type, PortableRawWriterEx writer, @Nullable Object arg) throws IgniteCheckedException { throwUnsupported(type); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static long XXX() { return keyCacheSizeInMiB; }', 'ground_truth': 'public static long getKeyCacheSizeInMiB() { return keyCacheSizeInMiB; }', 'output': 'public static long getKeyCacheSizeInMiB() { return keyCacheSizeInMiB; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { if (isMultiJvm()) return; checkReaderTtl(true); }', 'ground_truth': 'public void testReaderTtlTx() throws Exception { if (isMultiJvm() || !txShouldBeUsed()) return; checkReaderTtl(true); }', 'output': 'public void checkReaderTtl() throws Exception { if (isMultiJvm()) return; checkReaderTtl(true); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { LogSearchConfig config = LogSearchConfigFactory.createLogSearchConfig(Component.SERVER, Collections.<String, String> emptyMap(), null, LogSearchConfigClass1.class); Assert.assertSame(config.getClass(), LogSearchConfigClass1.class); }', 'ground_truth': 'public void testDefaultConfigServer() throws Exception { LogSearchConfigServer config = LogSearchConfigFactory.createLogSearchConfigServer( Collections.<String, String> emptyMap(), LogSearchConfigServerClass1.class); Assert.assertSame(config.getClass(), LogSearchConfigServerClass1.class); }', 'output': 'public void testGetClass() throws Exception { LogSearchConfig config = LogSearchConfigFactory.createLogSearchConfig(Component.SERVER, Collections.<String, String> emptyMap(), null, LogSearchConfigClass1.class); Assert.assertSame(config.getClass(), LogSearchConfigClass1.class); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static String XXX(Collection<Token> tokens) { Token.TokenFactory factory = StorageService.getPartitioner().getTokenFactory(); StringBuilder sb = new StringBuilder(); sb.append("{"); Iterator<Token> iter = tokens.iterator(); while (iter.hasNext()) { sb.append("\'").append(factory.toString(iter.next())).append("\'"); if (iter.hasNext()) sb.append(","); } sb.append("}"); return sb.toString(); } private static Collection<Token> deXXX(Collection<String> tokensStrings)\n    { { Token.TokenFactory factory = StorageService.getPartitioner().getTokenFactory(); List<Token> tokens = new ArrayList<Token>(tokensStrings.size()); for (String tk : tokensStrings) tokens.add(factory.fromString(tk)); return tokens; }', 'ground_truth': 'private static String tokensAsSet(Collection<Token> tokens) { Token.TokenFactory factory = StorageService.getPartitioner().getTokenFactory(); StringBuilder sb = new StringBuilder(); sb.append("{"); Iterator<Token> iter = tokens.iterator(); while (iter.hasNext()) { sb.append("\'").append(factory.toString(iter.next())).append("\'"); if (iter.hasNext()) sb.append(","); } sb.append("}"); return sb.toString(); }', 'output': 'private static Collection<Token> deTokenize(Collection<String> tokensStrings) { Token.TokenFactory factory = StorageService.getPartitioner().getTokenFactory(); List<Token> tokens = new ArrayList<Token>(tokensStrings.size()); for (String tk : tokensStrings) tokens.add(factory.fromString(tk)); return tokens; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void XXX(Reader writer, Closure closure) throws IOException { try { closure.call(writer); Reader temp = writer; writer = null; temp.close(); } finally { if (writer != null) { try { writer.close(); } catch (IOException e) { log.warning("Caught exception closing writer: " + e); } } } }', 'ground_truth': 'public static void withReader(Reader writer, Closure closure) throws IOException { try { closure.call(writer); Reader temp = writer; writer = null; temp.close(); } finally { if (writer != null) { try { writer.close(); } catch (IOException e) { log.warning("Caught exception closing writer: " + e); } } } }', 'output': 'public static void writeTo(Writer writer, Closure closure) throws IOException { try { closure.call(writer); Writer temp = writer; writer = null; temp.close(); } finally { if (writer != null) { try { writer.close(); } catch (IOException e) { log.warning("Caught exception closing writer: " + e); } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean XXX() { assert isDone() : this; if (isInitial() || !result()) return false; RebalanceFuture fut = next; while (nonNull(fut)) { if (fut.isInitial() || !fut.isDone()) return false; else fut = fut.next; } return true; }', 'ground_truth': 'private boolean isChainFinished() { assert isDone() : this; if (isInitial() || !result()) return false; RebalanceFuture fut = next; while (nonNull(fut)) { if (fut.isInitial() || !fut.isDone()) return false; else fut = fut.next; } return true; }', 'output': 'private boolean done() { assert isDone() : this; if (isInitial() || !result()) return false; RebalanceFuture fut = next; while (nonNull(fut)) { if (fut.isInitial() || !fut.isDone()) return false; else fut = fut.next; } return true; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RangeTombstoneBoundMarker XXX(boolean reversed) { return new RangeTombstoneBoundMarker(bound.withNewKind(bound.kind().openBoundOfBoundary(reversed)), startDeletion); }', 'ground_truth': 'public RangeTombstoneBoundMarker createCorrespondingOpenMarker(boolean reversed) { return new RangeTombstoneBoundMarker(openBound(reversed), startDeletion); }', 'output': 'public RangeTombstoneBoundMarker createRangeTombstoneBoundMarker(boolean reversed) { return new RangeTombstoneBoundMarker(bound.withNewKind(bound.kind().openBoundOfBoundary(reversed)), startDeletion); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static CacheTypeMetadata XXX() { CacheTypeMetadata employeeTypeMeta = new CacheTypeMetadata(); employeeTypeMeta.setValueType(Employee.class); employeeTypeMeta.setKeyType(EmployeeKey.class); Map<String, Class<?>> ascFields = new HashMap<>(); ascFields.put("name", String.class); ascFields.put("salary", Long.class); ascFields.put("address.zip", Integer.class); ascFields.put("organizationId", Integer.class); employeeTypeMeta.setAscendingFields(ascFields); employeeTypeMeta.setTextFields(Arrays.asList("address.street")); return employeeTypeMeta; }', 'ground_truth': 'private static QueryEntity createEmployeeQueryEntity() { QueryEntity employeeEntity = new QueryEntity(); employeeEntity.setValueType(Employee.class.getName()); employeeEntity.setKeyType(EmployeeKey.class.getName()); LinkedHashMap<String, String> fields = new LinkedHashMap<>(); fields.put("name", String.class.getName()); fields.put("salary", Long.class.getName()); fields.put("address.zip", Integer.class.getName()); fields.put("organizationId", Integer.class.getName()); fields.put("address.street", Integer.class.getName()); employeeEntity.setFields(fields); employeeEntity.setIndexes(Arrays.asList( new QueryIndex("name"), new QueryIndex("salary"), new QueryIndex("zip"), new QueryIndex("organizationId"), new QueryIndex("street", QueryIndexType.FULLTEXT) )); return employeeEntity; }', 'output': 'private static CacheTypeMetadata getEmployeeTypeMetadata() { CacheTypeMetadata employeeTypeMeta = new CacheTypeMetadata(); employeeTypeMeta.setValueType(Employee.class); employeeTypeMeta.setKeyType(EmployeeKey.class); Map<String, Class<?>> ascFields = new HashMap<>(); ascFields.put("name", String.class); ascFields.put("salary", Long.class); ascFields.put("address.zip", Integer.class); ascFields.put("organizationId", Integer.class); employeeTypeMeta.setAscendingFields(ascFields); employeeTypeMeta.setTextFields(Arrays.asList("address.street")); return employeeTypeMeta; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { CreateSource<Integer> source = CreateSource.fromIterable( ImmutableList.of(1, 2, 3, 4, 5, 6, 7, 8), BigEndianIntegerCoder.of()); PipelineOptions options = PipelineOptionsFactory.create(); List<? extends BoundedSource<Integer>> splitSources = source.splitIntoBundles(12, options); assertThat(splitSources, hasSize(3)); SourceTestUtils.assertSourcesEqualReferenceSource(source, splitSources, options); } public void XXXVoid() throws Exception { CreateSource<Void> source = CreateSource.fromIterable( Lists.<Void>newArrayList(null, null, null, null, null), VoidCoder.of()); PipelineOptions options = PipelineOptionsFactory.create(); List<? extends BoundedSource<Void>> splitSources = source.splitIntoBundles(3, options); SourceTestUtils.assertSourcesEqualReferenceSource(source, splitSources, options); } public void XXXEmpty() throws Exception { CreateSource<Integer> source = CreateSource.fromIterable(ImmutableList.<Integer>of(), BigEndianIntegerCoder.of()); PipelineOptions options = PipelineOptionsFactory.create(); List<? extends BoundedSource<Integer>> splitSources = source.splitIntoBundles(12, options); SourceTestUtils.assertSourcesEqualReferenceSource(source, splitSources, options); }', 'ground_truth': 'public void testSourceSplit() throws Exception { CreateSource<Integer> source = CreateSource.fromIterable( ImmutableList.of(1, 2, 3, 4, 5, 6, 7, 8), BigEndianIntegerCoder.of()); PipelineOptions options = PipelineOptionsFactory.create(); List<? extends BoundedSource<Integer>> splitSources = source.split(12, options); assertThat(splitSources, hasSize(3)); SourceTestUtils.assertSourcesEqualReferenceSource(source, splitSources, options); }', 'output': 'public void testSplitIntoBundles() throws Exception { CreateSource<Integer> source = CreateSource.fromIterable( ImmutableList.of(1, 2, 3, 4, 5, 6, 7, 8), BigEndianIntegerCoder.of()); PipelineOptions options = PipelineOptionsFactory.create(); List<? extends BoundedSource<Integer>> splitSources = source.splitIntoBundles(12, options); assertThat(splitSources, hasSize(3)); SourceTestUtils.assertSourcesEqualReferenceSource(source, splitSources, options); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws NonDeterministicException { FixedWindows mockWindowFn = Mockito.mock(FixedWindows.class); @SuppressWarnings({"unchecked", "rawtypes"}) Class<Coder<IntervalWindow>> coderClazz = (Class) Coder.class; Coder<IntervalWindow> mockCoder = Mockito.mock(coderClazz); when(mockWindowFn.windowCoder()).thenReturn(mockCoder); NonDeterministicException toBeThrown = new NonDeterministicException(mockCoder, "Its just not deterministic."); Mockito.doThrow(toBeThrown).when(mockCoder).verifyDeterministic(); thrown.expect(IllegalArgumentException.class); thrown.expectCause(Matchers.sameInstance(toBeThrown)); thrown.expectMessage("Window coders must be deterministic"); Window.into(mockWindowFn); }', 'ground_truth': 'public void testNonDeterministicWindowCoder() throws NonDeterministicException { FixedWindows mockWindowFn = Mockito.mock(FixedWindows.class); @SuppressWarnings({"unchecked", "rawtypes"}) Class<Coder<IntervalWindow>> coderClazz = (Class) Coder.class; Coder<IntervalWindow> mockCoder = Mockito.mock(coderClazz); when(mockWindowFn.windowCoder()).thenReturn(mockCoder); NonDeterministicException toBeThrown = new NonDeterministicException(mockCoder, "Its just not deterministic."); Mockito.doThrow(toBeThrown).when(mockCoder).verifyDeterministic(); thrown.expect(IllegalArgumentException.class); thrown.expectCause(Matchers.sameInstance(toBeThrown)); thrown.expectMessage("Window coders must be deterministic"); Window.into(mockWindowFn); }', 'output': 'public void testNonDeterministicWindowCoder() throws NonDeterministicException { FixedWindows mockWindowFn = Mockito.mock(FixedWindows.class); @SuppressWarnings({"unchecked", "rawtypes"}) Class<Coder<IntervalWindow>> coderClazz = (Class) Coder.class; Coder<IntervalWindow> mockCoder = Mockito.mock(coderClazz); when(mockWindowFn.windowCoder()).thenReturn(mockCoder); NonDeterministicException toBeThrown = new NonDeterministicException(mockCoder, "Its just not deterministic."); Mockito.doThrow(toBeThrown).when(mockCoder).verifyDeterministic(); thrown.expect(IllegalArgumentException.class); thrown.expectCause(Matchers.sameInstance(toBeThrown)); thrown.expectMessage("Window coders must be deterministic"); Window.into(mockWindowFn); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Statement XXX(Connection connection) throws SQLException { if (resultSetHoldability == -1) { return connection.createStatement(resultSetType, resultSetConcurrency); } return connection.createStatement(resultSetType, resultSetConcurrency, resultSetHoldability); }', 'ground_truth': 'private Statement createStatement(Connection connection) throws SQLException { if (resultSetHoldability == -1) { return connection.createStatement(resultSetType, resultSetConcurrency); } return connection.createStatement(resultSetType, resultSetConcurrency, resultSetHoldability); }', 'output': 'private Statement createStatement(Connection connection) throws SQLException { if (resultSetHoldability == -1) { return connection.createStatement(resultSetType, resultSetConcurrency); } return connection.createStatement(resultSetType, resultSetConcurrency, resultSetHoldability); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private RelDataType XXX(int dataType, int precision, int scale) { SqlTypeName sqlTypeName = SqlTypeName.getNameForJdbcType(dataType); if (precision >= 0 && scale >= 0 && sqlTypeName.allowsPrecScale(true, true)) { return typeFactory.createSqlType(sqlTypeName, precision, scale); } else if (precision >= 0 && sqlTypeName.allowsPrecNoScale()) { return typeFactory.createSqlType(sqlTypeName, precision); } else { assert sqlTypeName.allowsNoPrecNoScale(); return typeFactory.createSqlType(sqlTypeName); } }', 'ground_truth': 'private RelDataType sqlType(RelDataTypeFactory typeFactory, int dataType, int precision, int scale) { SqlTypeName sqlTypeName = SqlTypeName.getNameForJdbcType(dataType); if (precision >= 0 && scale >= 0 && sqlTypeName.allowsPrecScale(true, true)) { return typeFactory.createSqlType(sqlTypeName, precision, scale); } else if (precision >= 0 && sqlTypeName.allowsPrecNoScale()) { return typeFactory.createSqlType(sqlTypeName, precision); } else { assert sqlTypeName.allowsNoPrecNoScale(); return typeFactory.createSqlType(sqlTypeName); } }', 'output': 'private RelDataType convertToSqlType(int dataType, int precision, int scale) { SqlTypeName sqlTypeName = SqlTypeName.getNameForJdbcType(dataType); if (precision >= 0 && scale >= 0 && sqlTypeName.allowsPrecScale(true, true)) { return typeFactory.createSqlType(sqlTypeName, precision, scale); } else if (precision >= 0 && sqlTypeName.allowsPrecNoScale()) { return typeFactory.createSqlType(sqlTypeName, precision); } else { assert sqlTypeName.allowsNoPrecNoScale(); return typeFactory.createSqlType(sqlTypeName); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { basicTest(new PivotedMatrixView(new DenseLocalOnHeapMatrix(new double[][] { {2.0d, -1.0d, 0.0d}, {-1.0d, 2.0d, -1.0d}, {0.0d, -1.0d, 2.0d} }))); }', 'ground_truth': 'public void matrixUtilTest() { basicTest(new PivotedMatrixView(new DenseLocalOnHeapMatrix(new double[][] { {2.0d, -1.0d, 0.0d}, {-1.0d, 2.0d, -1.0d}, {0.0d, -1.0d, 2.0d} }))); }', 'output': 'public void testPivotedMatrixView() { basicTest(new PivotedMatrixView(new DenseLocalOnHeapMatrix(new double[][] { {2.0d, -1.0d, 0.0d}, {-1.0d, 2.0d, -1.0d}, {0.0d, -1.0d, 2.0d} }))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void XXX() throws IOException, TException { Schema.instance.clear(); new EmbeddedCassandraService().start(); ThriftSessionManager.instance.setCurrentSocket(new InetSocketAddress(9160)); server = new CassandraServer(); server.set_keyspace("Keyspace1"); }', 'ground_truth': 'public static void defineSchema() throws ConfigurationException, IOException, TException { SchemaLoader.prepareServer(); new EmbeddedCassandraService().start(); ThriftSessionManager.instance.setCurrentSocket(new InetSocketAddress(9160)); SchemaLoader.createKeyspace(KEYSPACE1, SimpleStrategy.class, KSMetaData.optsWithRF(1), SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD)); server = new CassandraServer(); server.set_keyspace(KEYSPACE1); }', 'output': 'public static void start() throws IOException, TException { Schema.instance.clear(); new EmbeddedCassandraService().start(); ThriftSessionManager.instance.setCurrentSocket(new InetSocketAddress(9160)); server = new CassandraServer(); server.set_keyspace("Keyspace1"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { performFromClassTest("package test; class Test {\\n" + "    private void test() {\\n" + "        System.out.println(100000);\\n" + "    }\\n" + "}\\n", "package test;\\n" + "class Test {\\n" + "    Test() {\\n" + " "        /* 0: aload_0\\n" + "         * 1: invokespecial java/lang/Object.\\"<init>\\":()V\\n" + "         * 4: return\\n" + "         *  */\\n" + " "    }\\n" + "    private void test() {\\n" + " "        /* 0: getstatic     java/lang/System.out:Ljava/io/PrintStream;\\n" + "         * 3: ldc           100000\\n" + "         * 5: invokevirtual java/io/PrintStream.println:(I)V\\n" + "         * 8: return\\n" + "         *  */\\n" + " "    }\\n" + "}\\n"); }', 'ground_truth': 'public void DISABLEtestDecompile1() throws Exception { performFromClassTest("package test; class Test {\\n" + "    private void test() {\\n" + "        System.out.println(100000);\\n" + "    }\\n" + "}\\n", "package test;\\n" + "class Test {\\n" + "    Test() {\\n" + " "        /* 0: aload_0\\n" + "         * 1: invokespecial java/lang/Object.\\"<init>\\":()V\\n" + "         * 4: return\\n" + "         *  */\\n" + " "    }\\n" + "    private void test() {\\n" + " "        /* 0: getstatic     java/lang/System.out:Ljava/io/PrintStream;\\n" + "         * 3: ldc           100000\\n" + "         * 5: invokevirtual java/io/PrintStream.println:(I)V\\n" + "         * 8: return\\n" + "         *  */\\n" + " "    }\\n" + "}\\n"); }', 'output': 'public void test() throws Exception { performFromClassTest("package test; class Test {\\n" + "    private void test() {\\n" + "        System.out.println(100000);\\n" + "    }\\n" + "}\\n", "package test;\\n" + "class Test {\\n" + "    Test() {\\n" + " "        /* 0: aload_0\\n" + "         * 1: invokespecial java/lang/Object.\\"<init>\\":()V\\n" + "         * 4: return\\n" + "         *  */\\n" + " "    }\\n" + "    private void test() {\\n" + " "        /* 0: getstatic     java/lang/System.out:Ljava/io/PrintStream;\\n" + "         * 3: ldc           100000\\n" + "         * 5: invokevirtual java/io/PrintStream.println:(I)V\\n" + "         * 8: return\\n" + "         *  */\\n" + " "    }\\n" + "}\\n"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public UUID XXX(ByteBuffer bytes) { return UUIDGen.getUUID(bytes); }', 'ground_truth': 'public UUID deserialize(ByteBuffer bytes) { return bytes.remaining() == 0 ? null : UUIDGen.getUUID(bytes); }', 'output': 'public UUID getUUID(ByteBuffer bytes) { return UUIDGen.getUUID(bytes); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static BinaryExpression XXX( Expression expression0, Expression expression1, boolean liftToNull, Method method) { throw Extensions.todo(); } public static BinaryExpression XXXOrEqual(\n        Expression expression0,\n        Expression expression1,\n        boolean liftToNull,\n        Method method)\n    { Expression expression0, Expression expression1, boolean liftToNull, Method method) { throw Extensions.todo(); }', 'ground_truth': 'public static BinaryExpression power( Expression left, Expression right) { throw Extensions.todo(); }', 'output': 'public static BinaryExpression orEqual( Expression expression0, Expression expression1, boolean liftToNull, Method method) { throw Extensions.todo(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public PreparedResult XXX( Expression expression, RelDataType resultType) { queryString = null; Class runtimeContextClass = connection.getClass(); final Argument [] arguments = { new Argument( connectionVariable, runtimeContextClass, connection) }; ClassDeclaration decl = init(arguments); final RelOptQuery query = new RelOptQuery(planner); final RelOptCluster cluster = query.createCluster( env, rexBuilder.getTypeFactory(), rexBuilder); RelNode rootRel = new LixToRelTranslator(cluster, connection) .translate(expression); if (timingTracer != null) { timingTracer.traceTime("end sql2rel"); } final RelDataType jdbcType = makeStruct(rexBuilder.getTypeFactory(), resultType); fieldOrigins = Collections.nCopies(jdbcType.getFieldCount(), null); rootRel = flattenTypes(rootRel, true); rootRel = (RelNode) optimize(resultType, rootRel); containsJava = treeContainsJava(rootRel); if (timingTracer != null) { timingTracer.traceTime("end optimization"); } return implement( resultType, rootRel, SqlKind.SELECT, decl, arguments); }', 'ground_truth': 'public PreparedResult prepareQueryable( Queryable queryable, RelDataType resultType) { queryString = null; Class runtimeContextClass = connection.getClass(); final Argument [] arguments = { new Argument( connectionVariable, runtimeContextClass, connection) }; ClassDeclaration decl = init(arguments); final RelOptQuery query = new RelOptQuery(planner); final RelOptCluster cluster = query.createCluster( env, rexBuilder.getTypeFactory(), rexBuilder); RelNode rootRel = new LixToRelTranslator(cluster, connection) .translate(queryable); if (timingTracer != null) { timingTracer.traceTime("end sql2rel"); } final RelDataType jdbcType = makeStruct(rexBuilder.getTypeFactory(), resultType); fieldOrigins = Collections.nCopies(jdbcType.getFieldCount(), null); rootRel = flattenTypes(rootRel, true); rootRel = optimize(resultType, rootRel); containsJava = treeContainsJava(rootRel); if (timingTracer != null) { timingTracer.traceTime("end optimization"); } return implement( resultType, rootRel, SqlKind.SELECT, decl, arguments); }', 'output': 'public PreparedResult prepare( Expression expression, RelDataType resultType) { queryString = null; Class runtimeContextClass = connection.getClass(); final Argument [] arguments = { new Argument( connectionVariable, runtimeContextClass, connection) }; ClassDeclaration decl = init(arguments); final RelOptQuery query = new RelOptQuery(planner); final RelOptCluster cluster = query.createCluster( env, rexBuilder.getTypeFactory(), rexBuilder); RelNode rootRel = new LixToRelTranslator(cluster, connection) .translate(expression); if (timingTracer != null) { timingTracer.traceTime("end sql2rel"); } final RelDataType jdbcType = makeStruct(rexBuilder.getTypeFactory(), resultType); fieldOrigins = Collections.nCopies(jdbcType.getFieldCount(), null); rootRel = flattenTypes(rootRel, true); rootRel = (RelNode) optimize(resultType, rootRel); containsJava = treeContainsJava(rootRel); if (timingTracer != null) { timingTracer.traceTime("end optimization"); } return implement( resultType, rootRel, SqlKind.SELECT, decl, arguments); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { TupleTag<Integer> mainOutputTag = new TupleTag<>(); TupleTag<Integer> typedSideOutputTag = new TupleTag<Integer>() {}; PCollectionTuple tuple = buildPCollectionTupleWithTags(mainOutputTag, typedSideOutputTag); assertThat(tuple.get(typedSideOutputTag).getCoder(), instanceOf(VarIntCoder.class)); }', 'ground_truth': 'public void testUntypedMainOutputTagTypedOutputTupleTag() { TupleTag<Integer> mainOutputTag = new TupleTag<>(); TupleTag<Integer> typedOutputTag = new TupleTag<Integer>() {}; PCollectionTuple tuple = buildPCollectionTupleWithTags(mainOutputTag, typedOutputTag); assertThat(tuple.get(typedOutputTag).getCoder(), instanceOf(VarIntCoder.class)); }', 'output': 'public void testBuildPCollectionTupleWithTags() { TupleTag<Integer> mainOutputTag = new TupleTag<>(); TupleTag<Integer> typedSideOutputTag = new TupleTag<Integer>() {}; PCollectionTuple tuple = buildPCollectionTupleWithTags(mainOutputTag, typedSideOutputTag); assertThat(tuple.get(typedSideOutputTag).getCoder(), instanceOf(VarIntCoder.class)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Object XXX(String type, String methodName, Object arguments) throws Throwable{ try { return InvokerHelper.XXX(type, methodName, arguments); } catch (GroovyRuntimeException gre) { return unwrap(gre); } }', 'ground_truth': 'public static Object invokeMethodN(Class senderClass, Object receiver, String messageName, Object[] messageArguments) throws Throwable{ try { return InvokerHelper.invokeMethod(receiver, messageName, messageArguments); } catch (GroovyRuntimeException gre) { return unwrap(gre); } }', 'output': 'public static Object invokeMethod(String type, String methodName, Object arguments) throws Throwable{ try { return InvokerHelper.invokeMethod(type, methodName, arguments); } catch (GroovyRuntimeException gre) { return unwrap(gre); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(boolean allOrNone) throws Exception { Ignite client = grid(CLIENT_NODE_NAME); List<ServiceConfiguration> cfgs = getConfigs(client.cluster().forServers().predicate(), NUM_SERVICES); int numDepSvcs; if (allOrNone) numDepSvcs = NUM_SERVICES / 2; else numDepSvcs = NUM_SERVICES - 1; CountDownLatch latch = new CountDownLatch(numDepSvcs); List<ServiceConfiguration> fstBatch = cfgs.subList(0, NUM_SERVICES / 2); List<ServiceConfiguration> sndBatch = cfgs.subList(NUM_SERVICES / 4, NUM_SERVICES); subscribeExeLatch(cfgs, latch); IgniteFuture<Void> fut = client.services().deployAllAsync(fstBatch, false); ServiceConfiguration failingCfg = cfgs.get(NUM_SERVICES - 1); failingCfg.setName(null); assertFailingDeploy(client, false, allOrNone, sndBatch, failingCfg); fut.get(); assertTrue("Waiting for services deployment timed out.", latch.await(30, TimeUnit.SECONDS)); assertDeployedServices(client, cfgs.subList(0, numDepSvcs)); }', 'ground_truth': 'public void testClashingNamesFail() throws Exception { Ignite client = grid(CLIENT_NODE_NAME); List<ServiceConfiguration> cfgs = getConfigs(client.cluster().forServers().predicate(), NUM_SERVICES); int numDepSvcs = NUM_SERVICES - 1; CountDownLatch latch = new CountDownLatch(numDepSvcs); List<ServiceConfiguration> fstBatch = cfgs.subList(0, NUM_SERVICES / 2); List<ServiceConfiguration> sndBatch = cfgs.subList(NUM_SERVICES / 4, NUM_SERVICES); subscribeExeLatch(cfgs, latch); IgniteFuture<Void> fut = client.services().deployAllAsync(fstBatch); ServiceConfiguration failingCfg = cfgs.get(NUM_SERVICES - 1); failingCfg.setName(null); assertFailingDeploy(client, false, sndBatch, failingCfg); fut.get(); assertTrue("Waiting for services deployment timed out.", latch.await(30, TimeUnit.SECONDS)); assertDeployedServices(client, cfgs.subList(0, numDepSvcs)); }', 'output': 'private void checkDeployment(boolean allOrNone) throws Exception { Ignite client = grid(CLIENT_NODE_NAME); List<ServiceConfiguration> cfgs = getConfigs(client.cluster().forServers().predicate(), NUM_SERVICES); int numDepSvcs; if (allOrNone) numDepSvcs = NUM_SERVICES / 2; else numDepSvcs = NUM_SERVICES - 1; CountDownLatch latch = new CountDownLatch(numDepSvcs); List<ServiceConfiguration> fstBatch = cfgs.subList(0, NUM_SERVICES / 2); List<ServiceConfiguration> sndBatch = cfgs.subList(NUM_SERVICES / 4, NUM_SERVICES); subscribeExeLatch(cfgs, latch); IgniteFuture<Void> fut = client.services().deployAllAsync(fstBatch, false); ServiceConfiguration failingCfg = cfgs.get(NUM_SERVICES - 1); failingCfg.setName(null); assertFailingDeploy(client, false, allOrNone, sndBatch, failingCfg); fut.get(); assertTrue("Waiting for services deployment timed out.", latch.await(30, TimeUnit.SECONDS)); assertDeployedServices(client, cfgs.subList(0, numDepSvcs)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { int rowCacheSavePeriodInSeconds = DatabaseDescriptor.getTableMetaData(table.name).get(columnFamily).getRowCacheSavePeriodInSeconds(); int keyCacheSavePeriodInSeconds = DatabaseDescriptor.getTableMetaData(table.name).get(columnFamily).getKeyCacheSavePeriodInSeconds(); long start = System.currentTimeMillis(); Set<DecoratedKey> savedKeys = readSavedCache(DatabaseDescriptor.getSerializedRowCachePath(table.name, columnFamily)); for (DecoratedKey key : savedKeys) cacheRow(key); if (ssTables.getRowCache().getSize() > 0) logger.info(String.format("completed loading (%d ms; %d keys) row cache for %s.%s", System.currentTimeMillis()-start, ssTables.getRowCache().getSize(), table.name, columnFamily)); scheduleCacheSaving(rowCacheSavePeriodInSeconds, keyCacheSavePeriodInSeconds); }', 'ground_truth': 'public void initCaches() { long start = System.currentTimeMillis(); for (DecoratedKey key : rowCache.readSaved()) cacheRow(key); if (rowCache.getSize() > 0) logger.info(String.format("completed loading (%d ms; %d keys) row cache for %s.%s", System.currentTimeMillis()-start, rowCache.getSize(), table.name, columnFamily)); scheduleCacheSaving(metadata.getRowCacheSavePeriodInSeconds(), metadata.getKeyCacheSavePeriodInSeconds()); }', 'output': 'public void loadCache() { int rowCacheSavePeriodInSeconds = DatabaseDescriptor.getTableMetaData(table.name).get(columnFamily).getRowCacheSavePeriodInSeconds(); int keyCacheSavePeriodInSeconds = DatabaseDescriptor.getTableMetaData(table.name).get(columnFamily).getKeyCacheSavePeriodInSeconds(); long start = System.currentTimeMillis(); Set<DecoratedKey> savedKeys = readSavedCache(DatabaseDescriptor.getSerializedRowCachePath(table.name, columnFamily)); for (DecoratedKey key : savedKeys) cacheRow(key); if (ssTables.getRowCache().getSize() > 0) logger.info(String.format("completed loading (%d ms; %d keys) row cache for %s.%s", System.currentTimeMillis()-start, ssTables.getRowCache().getSize(), table.name, columnFamily)); scheduleCacheSaving(rowCacheSavePeriodInSeconds, keyCacheSavePeriodInSeconds); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public GridClientCompute XXX() { KEEP_PORTABLES.set(true); return this; } /** {@inheritDoc} */ private class GridClientComputeFactory implements ProjectionFactory<GridClientComputeImpl> { /** {@inheritDoc} */ @Override public GridClientComputeImpl create(Collection<GridClientNode> nodes, GridClientPredicate<? super GridClientNode> filter, GridClientLoadBalancer balancer) { return new GridClientComputeImpl(client, nodes, filter, balancer); } } } }', 'ground_truth': '@Override public GridClientCompute withKeepBinaries() { KEEP_BINARIES.set(true); return this; }', 'output': '@Override public GridClientCompute compute() { KEEP_PORTABLES.set(true); return this; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws IOException { if (EXECUTE_WRITES) testBloomFilterWrite(); DataInputStream in = getInput("utils.BloomFilter.bin"); assert BloomFilter.serializer().deserialize(in) != null; in.close(); }', 'ground_truth': 'public void testBloomFilterReadMURMUR2() throws IOException { if (EXECUTE_WRITES) testBloomFilterWrite(FilterFactory.Type.MURMUR2); DataInputStream in = getInput("utils.BloomFilter.bin"); assert FilterFactory.deserialize(in, FilterFactory.Type.MURMUR2) != null; in.close(); }', 'output': 'public void testBloomFilterWrite() throws IOException { DataOutputStream out = getOutput("utils.BloomFilter.bin"); BloomFilter bloomFilter = new BloomFilter(10, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f, 0.01f,'}
{'instruction': 'Please refactor the following code:', 'input': 'public static LegacyCell XXX(CFMetaData metadata, ByteBuffer superColumnName, ByteBuffer name, long value) throws UnknownColumnException { ByteBuffer XXXValue = CounterContext.instance().createLocal(value); return XXX(decodeCellName(metadata, superColumnName, name), XXXValue); }', 'ground_truth': 'public static LegacyCell counterUpdate(CFMetaData metadata, ByteBuffer superColumnName, ByteBuffer name, long value) throws UnknownColumnException { ByteBuffer counterValue = CounterContext.instance().createUpdate(value); return counter(decodeCellName(metadata, superColumnName, name), counterValue); }', 'output': 'public static LegacyCell createLegacyCell(CFMetaData metadata, ByteBuffer superColumnName, ByteBuffer name, long value) throws UnknownColumnException { ByteBuffer valueBuffer = CounterContext.instance().createLocal(value); return createLegacyCell(decodeCellName(metadata, superColumnName, name), valueBuffer); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { PubsubJsonTableProvider provider = new PubsubJsonTableProvider(); Table table = fakeTable().toBuilder().properties(new JSONObject()).build(); thrown.expectMessage("timestampAttributeKey"); provider.buildBeamSqlTable(table); }', 'ground_truth': 'public void testThrowsIfAttributesFieldNotProvided() { PubsubJsonTableProvider provider = new PubsubJsonTableProvider(); Schema messageSchema = RowSqlTypes .builder() .withTimestampField("event_timestamp") .withRowField("payload", Schema.builder().build()) .build(); Table tableDefinition = tableDefinition().schema(messageSchema).build(); thrown.expectMessage("Unsupported"); thrown.expectMessage("\'attributes\'"); provider.buildBeamSqlTable(tableDefinition); }', 'output': 'public void testBuildBeamSqlTableWithInvalidTimestampAttributeKey() { PubsubJsonTableProvider provider = new PubsubJsonTableProvider(); Table table = fakeTable().toBuilder().properties(new JSONObject()).build(); thrown.expectMessage("timestampAttributeKey"); provider.buildBeamSqlTable(table); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static PageParameters XXX( ManagedObject adapter, ObjectAction objectAction, Can<ManagedObject> paramValues) { val pageParameters = XXX(adapter, objectAction); for(val argumentAdapter: paramValues) { val encodedArg = encodeArg(argumentAdapter); PageParameterNames.ACTION_ARGS.addStringTo(pageParameters, encodedArg); } return pageParameters; }', 'ground_truth': 'public static PageParameters createPageParametersForAction( ManagedObject adapter, ObjectAction objectAction, Can<ManagedObject> paramValues) { val pageParameters = createPageParameters(adapter, objectAction); for(val argumentAdapter: paramValues) { val encodedArg = encodeArg(argumentAdapter); PageParameterNames.ACTION_ARGS.addStringTo(pageParameters, encodedArg); } return pageParameters; }', 'output': 'public static PageParameters getPageParameters( ManagedObject adapter, ObjectAction objectAction, Can<ManagedObject> paramValues) { val pageParameters = getPageParameters(adapter, objectAction); for(val argumentAdapter: paramValues) { val encodedArg = encodeArg(argumentAdapter); PageParameterNames.ACTION_ARGS.addStringTo(pageParameters, encodedArg); } return pageParameters; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public synchronized void XXX(Iterable<SSTableReader> compacted) throws IOException { Set<SSTableReader> sstablesNew = new HashSet<SSTableReader>(sstables); for (SSTableReader sstable : compacted) { sstablesNew.remove(sstable); sstable.XXX(); } sstables = Collections.unmodifiableSet(sstablesNew); }', 'ground_truth': 'public synchronized void replace(Collection<SSTableReader> oldSSTables, Iterable<SSTableReader> replacements) throws IOException { Set<SSTableReader> sstablesNew = new HashSet<SSTableReader>(sstables); for (SSTableReader sstable : replacements) { assert sstable.getIndexPositions() != null; sstablesNew.add(sstable); } for (SSTableReader sstable : oldSSTables) { sstablesNew.remove(sstable); sstable.markCompacted(); } sstables = Collections.unmodifiableSet(sstablesNew); }', 'output': 'public synchronized void compact(Iterable<SSTableReader> compacted) throws IOException { Set<SSTableReader> sstablesNew = new HashSet<SSTableReader>(sstables); for (SSTableReader sstable : compacted) { sstablesNew.remove(sstable); sstable.compact(); } sstables = Collections.unmodifiableSet(sstablesNew); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Table XXX(String tableName) { return tableInstances.get(tableName); }', 'ground_truth': 'public Keyspace getKeyspaceInstance(String keyspaceName) { return keyspaceInstances.get(keyspaceName); }', 'output': 'public Table getTable(String tableName) { return tableInstances.get(tableName); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void XXX(File sstableDirectory) throws Exception { assert sstableDirectory != null : "Flush task is not bound to any disk"; SSTableReader sstable = writeSortedContents(context, sstableDirectory); cfs.replaceFlushed(Memtable.this, sstable); latch.countDown(); }', 'ground_truth': 'protected void runMayThrow() throws Exception { long writeSize = getExpectedWriteSize(); Directories.DataDirectory dataDirectory = getWriteDirectory(writeSize); File sstableDirectory = cfs.directories.getLocationForDisk(dataDirectory); assert sstableDirectory != null : "Flush task is not bound to any disk"; try { SSTableReader sstable = writeSortedContents(context, sstableDirectory); cfs.replaceFlushed(Memtable.this, sstable); latch.countDown(); } finally { if (dataDirectory != null) returnWriteDirectory(dataDirectory, writeSize); } }', 'output': 'protected void flush(File sstableDirectory) throws Exception { assert sstableDirectory != null : "Flush task is not bound to any disk"; SSTableReader sstable = writeSortedContents(context, sstableDirectory); cfs.replaceFlushed(Memtable.this, sstable); latch.countDown(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { thrown.expect(NumberFormatException.class); parse("10000-10-29T23:41:41.123999Z"); }', 'ground_truth': 'public void timestampAttributeRfc3339WithTooLargeYearThrowsError() { thrown.expect(NumberFormatException.class); parse("10000-10-29T23:41:41.123999Z"); }', 'output': 'public void testInvalidDate() { thrown.expect(NumberFormatException.class); parse("10000-10-29T23:41:41.123999Z"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean XXX(int part, List<ClusterNode> affNodes, AffinityTopologyVersion topVer) { boolean result = affinityNode() && rebalanceEnabled() && hasPartition(part, affNodes, topVer); assert !result || !group().persistenceEnabled() || topology().partitionState(localNodeId(), part) == OWNING : "result = " + result + ", persistenceEnabled = " + group().persistenceEnabled() + ", partitionState = " + topology().partitionState(localNodeId(), part); return result; }', 'ground_truth': 'public boolean reserveForFastLocalGet(int part, AffinityTopologyVersion topVer) { boolean result = affinityNode() && rebalanceEnabled() && checkAndReservePartition(part, topVer); assert !result || !group().persistenceEnabled() || topology().partitionState(localNodeId(), part) == OWNING : "result=" + result + ", persistenceEnabled=" + group().persistenceEnabled() + ", partitionState=" + topology().partitionState(localNodeId(), part) + ", replicated=" + isReplicated(); return result; }', 'output': 'public boolean canRebalance(int part, List<ClusterNode> affNodes, AffinityTopologyVersion topVer) { boolean result = affinityNode() && rebalanceEnabled() && hasPartition(part, affNodes, topVer); assert !result || !group().persistenceEnabled() || topology().partitionState(localNodeId(), part) == OWNING : "result = " + result + ", persistenceEnabled = " + group().persistenceEnabled() + ", partitionState = " + topology().partitionState(localNodeId(), part); return result; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public int XXX(MessageOut<?> message, InetAddressAndPort to, AbstractWriteResponseHandler<?> handler, boolean allowHints) { int id = addCallback(handler, message, to, message.getTimeout(), handler.consistencyLevel, allowHints); updateBackPressureOnSend(to, handler, message); sendOneWay(message.withParameter(ParameterType.FAILURE_CALLBACK, ONE_BYTE), id, to); return id; }', 'ground_truth': 'public int sendWriteRR(MessageOut<?> message, Replica to, AbstractWriteResponseHandler<?> handler, boolean allowHints) { int id = addWriteCallback(handler, message, to, message.getTimeout(), handler.consistencyLevel(), allowHints); updateBackPressureOnSend(to.endpoint(), handler, message); sendOneWay(message.withParameter(ParameterType.FAILURE_CALLBACK, ONE_BYTE), id, to.endpoint()); return id; }', 'output': 'public int writeTo(MessageOut<?> message, InetAddressAndPort to, AbstractWriteResponseHandler<?> handler, boolean allowHints) { int id = addCallback(handler, message, to, message.getTimeout(), handler.consistencyLevel, allowHints); updateBackPressureOnSend(to, handler, message); sendOneWay(message.withParameter(ParameterType.FAILURE_CALLBACK, ONE_BYTE), id, to); return id; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public AbstractType<?> XXX() { List<AbstractType<?>> newTypes = fieldTypes().stream() .map(subtype -> (subtype.isFreezable() && subtype.isMultiCell() ? subtype.freeze() : subtype)) .collect(Collectors.toList()); return new UserType(keyspace, name, fieldNames, newTypes, isMultiCell); }', 'ground_truth': 'public AbstractType<?> freezeNestedMulticellTypes() { if (!isMultiCell()) return this; List<AbstractType<?>> newTypes = fieldTypes().stream() .map(subtype -> (subtype.isFreezable() && subtype.isMultiCell() ? subtype.freeze() : subtype)) .collect(Collectors.toList()); return new UserType(keyspace, name, fieldNames, newTypes, isMultiCell); }', 'output': 'public AbstractType<?> getType() { List<AbstractType<?>> newTypes = fieldTypes().stream() .map(subtype -> (subtype.isFreezable() && subtype.isMultiCell() ? subtype.freeze() : subtype)) .collect(Collectors.toList()); return new UserType(keyspace, name, fieldNames, newTypes, isMultiCell); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(BinaryContext ctx, IgniteConfiguration cfg) { ctx.configure(this, cfg); impl = new GridBinaryMarshaller(ctx); }', 'ground_truth': 'private void setBinaryContext(BinaryContext ctx, IgniteConfiguration cfg) { ctx.configure(this, cfg); impl = new GridBinaryMarshaller(ctx); }', 'output': 'private void configure(BinaryContext ctx, IgniteConfiguration cfg) { ctx.configure(this, cfg); impl = new GridBinaryMarshaller(ctx); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { U.setWorkDirectory(null, U.getIgniteHome()); IpcSharedMemoryServerEndpoint srv = new IpcSharedMemoryServerEndpoint(); new IgniteTestResources().inject(srv); try { srv.start(); info("Check that server gets correct exception upon client\'s killing."); info("Shared memory IDs before starting client endpoint: " + IpcSharedMemoryUtils.sharedMemoryIds()); Collection<Integer> shmemIdsWithinInteractions = interactWithClient(srv, true); Collection<Integer> shmemIdsAfterInteractions = null; for (int i = 0; i < 12; i++) { shmemIdsAfterInteractions = IpcSharedMemoryUtils.sharedMemoryIds(); info("Shared memory IDs created within interaction: " + shmemIdsWithinInteractions); info("Shared memory IDs after killing client endpoint: " + shmemIdsAfterInteractions); if (CollectionUtils.containsAny(shmemIdsAfterInteractions, shmemIdsWithinInteractions)) U.sleep(1000); else break; } assertFalse("List of shared memory IDs after killing client endpoint should not include IDs created " + "within server-client interactions.", CollectionUtils.containsAny(shmemIdsAfterInteractions, shmemIdsWithinInteractions)); } finally { srv.close(); } }', 'ground_truth': 'public void testIgfsServerClientInteractionsUponClientKilling() throws Exception { U.setWorkDirectory(null, U.getIgniteHome()); IpcSharedMemoryServerEndpoint srv = new IpcSharedMemoryServerEndpoint(); new IgniteTestResources().inject(srv); try { srv.start(); info("Check that server gets correct exception upon client\'s killing."); info("Shared memory IDs before starting client endpoint: " + IpcSharedMemoryUtils.sharedMemoryIds()); Collection<Integer> shmemIdsWithinInteractions = interactWithClient(srv, true); Collection<Integer> shmemIdsAfterInteractions = null; for (int i = 0; i < 12; i++) { shmemIdsAfterInteractions = IpcSharedMemoryUtils.sharedMemoryIds(); info("Shared memory IDs created within interaction: " + shmemIdsWithinInteractions); info("Shared memory IDs after killing client endpoint: " + shmemIdsAfterInteractions); if (CollectionUtils.containsAny(shmemIdsAfterInteractions, shmemIdsWithinInteractions)) U.sleep(1000); else break; } assertFalse("List of shared memory IDs after killing client endpoint should not include IDs created " + "within server-client interactions.", CollectionUtils.containsAny(shmemIdsAfterInteractions, shmemIdsWithinInteractions)); } finally { srv.close(); } }', 'output': 'public void testClientKilling() throws Exception { U.setWorkDirectory(null, U.getIgniteHome()); IpcSharedMemoryServerEndpoint srv = new IpcSharedMemoryServerEndpoint(); new IgniteTestResources().inject(srv); try { srv.start(); info("Check that server gets correct exception upon client\'s killing."); info("Shared memory IDs before starting client endpoint: " + IpcSharedMemoryUtils.sharedMemoryIds()); Collection<Integer> shmemIdsWithinInteractions = interactWithClient(srv, true); Collection<Integer> shmemIdsAfterInteractions = null; for (int i = 0; i < 12; i++) { shmemIdsAfterInteractions = IpcSharedMemoryUtils.sharedMemoryIds(); info("Shared memory IDs created within interaction: " + shmemIdsWithinInteractions); info("Shared memory IDs after killing client endpoint: " + shmemIdsAfterInteractions); if (CollectionUtils.containsAny(shmemIdsAfterInteractions, shmemIdsWithinInteractions)) U.sleep(1000); else break; } assertFalse("List of shared memory IDs after killing client endpoint should not include IDs created " + "within server-client interactions.", CollectionUtils.containsAny(shmemIdsAfterInteractions, shmemIdsWithinInteractions)); } finally { srv.close(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'XXXpXXXrXXXiXXXvXXXaXXXtXXXeXXX XXXbXXXoXXXoXXXlXXXeXXXaXXXnXXX XXXiXXXsXXXVXXXiXXXsXXXiXXXbXXXlXXXeXXX XXX(XXXfXXXiXXXnXXXaXXXlXXX XXX@XXXNXXXoXXXnXXXNXXXuXXXlXXXlXXX XXXFXXXiXXXlXXXeXXXOXXXbXXXjXXXeXXXcXXXtXXX XXXfXXXoXXX)XXX XXX{XXX XXXtXXXrXXXyXXX XXX{XXX XXXrXXXeXXXtXXXuXXXrXXXnXXX XXXVXXXiXXXsXXXiXXXbXXXiXXXlXXXiXXXtXXXyXXXQXXXuXXXeXXXrXXXyXXX.XXXgXXXeXXXtXXXDXXXeXXXfXXXaXXXuXXXlXXXtXXX(XXX)XXX.XXXiXXXsXXXVXXXiXXXsXXXiXXXbXXXlXXXeXXX(XXXfXXXoXXX)XXX;XXX XXX}XXX XXXfXXXiXXXnXXXaXXXlXXXlXXXyXXX XXX{XXX XXXsXXXeXXXtXXXLXXXiXXXsXXXtXXXeXXXnXXXOXXXnXXXVXXXiXXXsXXXiXXXbXXXiXXXlXXXiXXXtXXXyXXX(XXXtXXXrXXXuXXXeXXX)XXX;XXX XXX}XXX XXX}XXX', 'ground_truth': 'private boolean canBeIndexed (final @NonNull FileObject fo) { try { return VisibilityQuery.getDefault().isVisible(fo) && (! IndexabilityQuery.getInstance().preventIndexing(fo)); } finally { setListenOnVisibility(true); } }', 'output': 'private static void main(String[] args) {\n    System.out.println("Hello World");\n}</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Object XXX(Class type, Object arguments) throws Throwable{ try { return InvokerHelper.XXX(type, arguments); } catch (GroovyRuntimeException gre) { return unwrap(gre); } }', 'ground_truth': 'public static Object invokeMethodOnCurrentN(Class senderClass, GroovyObject receiver, String messageName, Object[] messageArguments) throws Throwable{ try { return InvokerHelper.invokeMethod(receiver, messageName, messageArguments); } catch (GroovyRuntimeException gre) { return unwrap(gre); } }', 'output': 'public static Object invoke(Class type, Object arguments) throws Throwable{ try { return InvokerHelper.invoke(type, arguments); } catch (GroovyRuntimeException gre) { return unwrap(gre); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private IgniteBiPredicate<K,V> XXX() { return new IgniteBiPredicate<K,V>() { @Override public boolean apply(K k, V v) { return true; } }; }', 'ground_truth': 'private IgniteBiPredicate<K,V> acceptAll() { return new IgniteBiPredicate<K,V>() { @Override public boolean apply(K k, V v) { return true; } }; }', 'output': 'private IgniteBiPredicate<K,V> getPredicate() { return new IgniteBiPredicate<K,V>() { @Override public boolean apply(K k, V v) { return true; } }; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public OnDiskAtomIterator XXX(DecoratedKey key, ColumnFamily cf) { assert cf != null; return new ByNameColumnIterator(columns.iterator(), cf, key); }', 'ground_truth': 'public OnDiskAtomIterator getColumnIterator(DecoratedKey key, ColumnFamily cf) { assert cf != null; return new ByNameColumnIterator(columns.iterator(), key, cf); }', 'output': 'public OnDiskAtomIterator iterator(DecoratedKey key, ColumnFamily cf) { assert cf != null; return new ByNameColumnIterator(columns.iterator(), cf, key); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private PTransform XXX(CombinerType combinerType) { switch (combinerType) { case MEAN: return Mean.perKey(); case TOP_LARGEST: Preconditions.checkArgument( options.getTopCount() != null, "You should set \\"--topCount\\" option to use TOP combiners."); return Top.largestPerKey(options.getTopCount()); case SUM: return Sum.longsPerKey(); case COUNT: return Count.perKey(); default: throw new IllegalArgumentException("No such combiner!"); } }', 'ground_truth': 'private PTransform<PCollection<KV<byte[], Long>>, ? extends PCollection> getPerKeyCombiner( CombinerType combinerType) { switch (combinerType) { case MEAN: return Mean.perKey(); case TOP_LARGEST: Preconditions.checkArgument( options.getTopCount() != null, "You should set \\"--topCount\\" option to use TOP combiners."); return Top.largestPerKey(options.getTopCount()); case SUM: return Sum.longsPerKey(); case COUNT: return Count.perKey(); default: throw new IllegalArgumentException("No such combiner!"); } }', 'output': 'private PTransform combine(CombinerType combinerType) { switch (combinerType) { case MEAN: return Mean.perKey(); case TOP_LARGEST: Preconditions.checkArgument( options.getTopCount() != null, "You should set \\"--topCount\\" option to use TOP combiners."); return Top.largestPerKey(options.getTopCount()); case SUM: return Sum.longsPerKey(); case COUNT: return Count.perKey(); default: throw new IllegalArgumentException("No such combiner!"); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public synchronized Stage XXX(String actionId) { for (Stage s: stageList) { if (s.XXXId().equals(actionId)) { return s; } } return null; }', 'ground_truth': 'public synchronized Stage getStage(String actionId) { for (Stage s : stageList) { if (s.getActionId().equals(actionId)) { return s; } } return null; }', 'output': 'public synchronized Stage getStage(String actionId) { for (Stage s: stageList) { if (s.id().equals(actionId)) { return s; } } return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(IResource droppedResource) { try { UntypedResultSet rows = process(String.format("SELECT role FROM %s.%s WHERE resource = \'%s\'", AuthKeyspace.NAME, AuthKeyspace.RESOURCE_ROLE_INDEX, escape(droppedResource.getName()))); List<CQLStatement> statements = new ArrayList<>(); for (UntypedResultSet.Row row : rows) { statements.add(QueryProcessor.getStatement(String.format("DELETE FROM %s.%s WHERE role = \'%s\' AND resource = \'%s\'", AuthKeyspace.NAME, AuthKeyspace.ROLE_PERMISSIONS, escape(row.getString("role")), escape(droppedResource.getName())), ClientState.forInternalCalls()).statement); } statements.add(QueryProcessor.getStatement(String.format("DELETE FROM %s.%s WHERE resource = \'%s\'", AuthKeyspace.NAME, AuthKeyspace.RESOURCE_ROLE_INDEX, escape(droppedResource.getName())), ClientState.forInternalCalls()).statement); executeLoggedBatch(statements); } catch (RequestExecutionException | RequestValidationException e) { logger.warn("CassandraAuthorizer failed to revoke all permissions on {}: {}", droppedResource, e); return; } }', 'ground_truth': 'public void revokeAllOn(IResource droppedResource) { try { UntypedResultSet rows = process(String.format("SELECT role FROM %s.%s WHERE resource = \'%s\'", AuthKeyspace.NAME, AuthKeyspace.RESOURCE_ROLE_INDEX, escape(droppedResource.getName()))); List<CQLStatement> statements = new ArrayList<>(); for (UntypedResultSet.Row row : rows) { statements.add(QueryProcessor.getStatement(String.format("DELETE FROM %s.%s WHERE role = \'%s\' AND resource = \'%s\'", AuthKeyspace.NAME, AuthKeyspace.ROLE_PERMISSIONS, escape(row.getString("role")), escape(droppedResource.getName())), ClientState.forInternalCalls()).statement); } statements.add(QueryProcessor.getStatement(String.format("DELETE FROM %s.%s WHERE resource = \'%s\'", AuthKeyspace.NAME, AuthKeyspace.RESOURCE_ROLE_INDEX, escape(droppedResource.getName())), ClientState.forInternalCalls()).statement); executeLoggedBatch(statements); } catch (RequestExecutionException | RequestValidationException e) { logger.warn("CassandraAuthorizer failed to revoke all permissions on {}: {}", droppedResource, e); return; } }', 'output': 'public void revokeAllPermissionsOn(IResource droppedResource) { try { UntypedResultSet rows = process(String.format("SELECT role FROM %s.%s WHERE resource = \'%s\'", AuthKeyspace.NAME, AuthKeyspace.RESOURCE_ROLE_INDEX, escape(droppedResource.getName()))); List<CQLStatement> statements = new ArrayList<>(); for (UntypedResultSet.Row row : rows) { statements.add(QueryProcessor.getStatement(String.format("DELETE FROM %s.%s WHERE role = \'%s\' AND resource = \'%s\'", AuthKeyspace.NAME, AuthKeyspace.ROLE_PERMISSIONS, escape(row.getString("role")), escape(droppedResource.getName())), ClientState.forInternalCalls()).statement); } statements.add(QueryProcessor.getStatement(String.format("DELETE FROM %s.%s WHERE resource = \'%s\'", AuthKeyspace.NAME, AuthKeyspace.RESOURCE_ROLE_INDEX, escape(droppedResource.getName())), ClientState.forInternalCalls()).statement); executeLoggedBatch(statements); } catch (RequestExecutionException | RequestValidationException e) { logger.warn("CassandraAuthorizer failed to revoke all permissions on {}: {}", droppedResource, e); return; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(InetAddress addr, ApplicationState state, VersionedValue value) { for (IEndpointStateChangeSubscriber subscriber : subscribers) { subscriber.onChange(addr, state, value); } }', 'ground_truth': 'private void doOnChangeNotifications(InetAddress addr, ApplicationState state, VersionedValue value) { for (IEndpointStateChangeSubscriber subscriber : subscribers) { subscriber.onChange(addr, state, value); } }', 'output': 'private void onChange(InetAddress addr, ApplicationState state, VersionedValue value) { for (IEndpointStateChangeSubscriber subscriber : subscribers) { subscriber.onChange(addr, state, value); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public boolean XXX(Ignite ignite, FailureContext failureCtx) { new Thread( new Runnable() { @Override public void run() { U.error(ignite.log(), "Restarting JVM on Ignite failure: [failureCtx=" + failureCtx + \']\'); G.restart(true); } }, "node-restarter" ).start(); return true; } /** {@inheritDoc} */ @Override public String toString() { return S.toString(RestartProcessFailureHandler.class, this); } } }', 'ground_truth': '@Override protected boolean handle(Ignite ignite, FailureContext failureCtx) { new Thread( new Runnable() { @Override public void run() { U.error(ignite.log(), "Restarting JVM on Ignite failure: [failureCtx=" + failureCtx + \']\'); G.restart(true); } }, "node-restarter" ).start(); return true; }', 'output': '@Override public boolean handleFailure(Ignite ignite, FailureContext failureCtx) { new Thread( new Runnable() { @Override public void run() { U.error(ignite.log(), "Restarting JVM on Ignite failure: [failureCtx=" + failureCtx + \']\'); G.restart(true); } }, "node-restarter" ).start(); return true; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean XXX(Object result) { Throwable cause = ((Throwable) result).getCause(); while (null != cause) { String causeString = cause.toString(); if (causeString.contains(expectedError)) { return true; } cause = cause.getCause(); } return false; }', 'ground_truth': 'public boolean matchesSafely(Throwable result) { if (result.toString().contains(expectedError)) { return true; } Throwable cause = result.getCause(); while (null != cause) { String causeString = cause.toString(); if (causeString.contains(expectedError)) { return true; } cause = cause.getCause(); } return false; }', 'output': 'public boolean hasExpectedError(Object result) { Throwable cause = ((Throwable) result).getCause(); while (null != cause) { String causeString = cause.toString(); if (causeString.contains(expectedError)) { return true; } cause = cause.getCause(); } return false; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { BinaryMarshaller marsh = binaryMarshaller(new BinaryIdMapper() { @SuppressWarnings("IfMayBeConditional") @Override public int typeId(String clsName) { if (clsName.endsWith("1")) return 300; else if (clsName.endsWith("2")) return 400; else return -500; } @Override public int fieldId(int typeId, String fieldName) { return 0; } }, Arrays.asList( new BinaryTypeConfiguration("org.apache.ignite.internal.binary.test.*"), new BinaryTypeConfiguration("unknown.*") )); BinaryContext ctx = binaryContext(marsh); Map<String, BinaryIdMapper> typeMappers = U.field(ctx, "typeMappers"); assertEquals(3, typeMappers.size()); assertEquals(300, typeMappers.get("GridBinaryTestClass1").typeId("GridBinaryTestClass1")); assertEquals(400, typeMappers.get("GridBinaryTestClass2").typeId("GridBinaryTestClass2")); }', 'ground_truth': 'public void testClassNamesWithCustomMapperJar() throws Exception { BinaryMarshaller marsh = binaryMarshaller(new BinaryBasicNameMapper(false), new BinaryIdMapper() { @SuppressWarnings("IfMayBeConditional") @Override public int typeId(String clsName) { if (clsName.endsWith("1")) return 300; else if (clsName.endsWith("2")) return 400; else return -500; } @Override public int fieldId(int typeId, String fieldName) { return 0; } }, Arrays.asList( new BinaryTypeConfiguration("org.apache.ignite.internal.binary.test.*"), new BinaryTypeConfiguration("unknown.*") )); BinaryContext ctx = binaryContext(marsh); Map<String, org.apache.ignite.internal.binary.BinaryInternalMapper> typeMappers = U.field(ctx, "cls2Mappers"); assertEquals(3, typeMappers.size()); assertFalse(((BinaryBasicNameMapper)typeMappers.get(CLASS1_FULL_NAME).nameMapper()).isSimpleName()); assertEquals(300, typeMappers.get(CLASS1_FULL_NAME).idMapper().typeId(CLASS1_FULL_NAME)); assertFalse(((BinaryBasicNameMapper)typeMappers.get(CLASS2_FULL_NAME).nameMapper()).isSimpleName()); assertEquals(400, typeMappers.get(CLASS2_FULL_NAME).idMapper().typeId(CLASS2_FULL_NAME)); }', 'output': 'public void testBinaryTypeMapping() throws Exception { BinaryMarshaller marsh = binaryMarshaller(new BinaryIdMapper() { @SuppressWarnings("IfMayBeConditional") @Override public int typeId(String clsName) { if (clsName.endsWith("1")) return 300; else if (clsName.endsWith("2")) return 400; else return -500; } @Override public int fieldId(int typeId, String fieldName) { return 0; } }, Arrays.asList( new BinaryTypeConfiguration("org.apache.ignite.internal.binary.test.*"), new BinaryTypeConfiguration("unknown.*") )); BinaryContext ctx = binaryContext(marsh); Map<String, BinaryIdMapper> typeMappers = U.field(ctx, "typeMappers"); assertEquals(3, typeMappers.size()); assertEquals(300, typeMappers.get("GridBinaryTestClass1").typeId("GridBinaryTestClass1")); assertEquals(400, typeMappers.get("GridBinaryTestClass2").typeId("GridBinaryTestClass2")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(String indention) { if (indention == null) { indention = DEFAULT_INDENTION; } this.indention = indention; }', 'ground_truth': 'public void setIndentation(String indentation) { if (indentation == null) { indentation = DEFAULT_INDENTATION; } this.indentation = indentation; }', 'output': 'public void setIndention(String indention) { if (indention == null) { indention = DEFAULT_INDENTION; } this.indention = indention; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String XXX() { return "modules/" + locModuleName + "/target/" + (test ? "test-classes" : "classes"); }', 'ground_truth': 'public String sourcePathTemplate() { return "modules/" + locModuleName + "/target/" + (test ? "test-classes" : "classes"); }', 'output': 'public String getClasspath() { return "modules/" + locModuleName + "/target/" + (test ? "test-classes" : "classes"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { marshFactory = new Callable<Marshaller>() { @Override public Marshaller call() throws Exception { return new BinaryMarshaller(); } }; doTest(); }', 'ground_truth': 'public void testBinaryMarshaller() throws Exception { marshFactory = new Callable<Marshaller>() { @Override public Marshaller call() throws Exception { return new BinaryMarshaller(); } }; doTestMarshaller(); }', 'output': 'public void testBinaryMarshaller() throws Exception { marshFactory = new Callable<Marshaller>() { @Override public Marshaller call() throws Exception { return new BinaryMarshaller(); } }; doTest(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { ResourceDefinition resourceDef = createStrictMock(ResourceDefinition.class); ResultSerializer resultSerializer = createStrictMock(ResultSerializer.class); Object serializedResult = new Object(); RequestFactory requestFactory = createStrictMock(RequestFactory.class); ResponseFactory responseFactory = createStrictMock(ResponseFactory.class); Request request = createNiceMock(Request.class); RequestHandler requestHandler = createStrictMock(RequestHandler.class); Result result = createStrictMock(Result.class); Response response = createStrictMock(Response.class); HttpHeaders httpHeaders = createNiceMock(HttpHeaders.class); UriInfo uriInfo = createNiceMock(UriInfo.class); String clusterName = "clusterName"; String hostName = "hostName"; String hostComponentName = "hostComponentName"; expect(requestFactory.createRequest(eq(httpHeaders), eq("body"), eq(uriInfo), eq(Request.Type.POST), eq(resourceDef))).andReturn(request); expect(requestHandler.handleRequest(request)).andReturn(result); expect(request.getResultSerializer()).andReturn(resultSerializer); expect(resultSerializer.serialize(result, uriInfo)).andReturn(serializedResult); expect(responseFactory.createResponse(serializedResult)).andReturn(response); replay(resourceDef, resultSerializer, requestFactory, responseFactory, request, requestHandler, result, response, httpHeaders, uriInfo); HostComponentService hostComponentService = new TestHostComponentService(resourceDef, clusterName, hostName, hostComponentName, requestFactory, responseFactory, requestHandler); assertSame(response, hostComponentService.updateHostComponent("body", httpHeaders, uriInfo, hostComponentName)); verify(resourceDef, resultSerializer, requestFactory, responseFactory, request, requestHandler, result, response, httpHeaders, uriInfo); }', 'ground_truth': 'public void testUpdateHostComponent() { ResourceDefinition resourceDef = createStrictMock(ResourceDefinition.class); ResultSerializer resultSerializer = createStrictMock(ResultSerializer.class); Object serializedResult = new Object(); RequestFactory requestFactory = createStrictMock(RequestFactory.class); ResponseFactory responseFactory = createStrictMock(ResponseFactory.class); Request request = createNiceMock(Request.class); RequestHandler requestHandler = createStrictMock(RequestHandler.class); Result result = createStrictMock(Result.class); Response response = createStrictMock(Response.class); HttpHeaders httpHeaders = createNiceMock(HttpHeaders.class); UriInfo uriInfo = createNiceMock(UriInfo.class); String clusterName = "clusterName"; String hostName = "hostName"; String hostComponentName = "hostComponentName"; expect(requestFactory.createRequest(eq(httpHeaders), eq("body"), eq(uriInfo), eq(Request.Type.PUT), eq(resourceDef))).andReturn(request); expect(requestHandler.handleRequest(request)).andReturn(result); expect(request.getResultSerializer()).andReturn(resultSerializer); expect(resultSerializer.serialize(result, uriInfo)).andReturn(serializedResult); expect(result.isSynchronous()).andReturn(false).atLeastOnce(); expect(responseFactory.createResponse(Request.Type.PUT, serializedResult, false)).andReturn(response); replay(resourceDef, resultSerializer, requestFactory, responseFactory, request, requestHandler, result, response, httpHeaders, uriInfo); HostComponentService hostComponentService = new TestHostComponentService(resourceDef, clusterName, hostName, hostComponentName, requestFactory, responseFactory, requestHandler); assertSame(response, hostComponentService.updateHostComponent("body", httpHeaders, uriInfo, hostComponentName)); verify(resourceDef, resultSerializer, requestFactory, responseFactory, request, requestHandler, result, response, httpHeaders, uriInfo); }', 'output': 'public void testUpdateHostComponent() { ResourceDefinition resourceDef = createStrictMock(ResourceDefinition.class); ResultSerializer resultSerializer = createStrictMock(ResultSerializer.class); Object serializedResult = new Object(); RequestFactory requestFactory = createStrictMock(RequestFactory.class); ResponseFactory responseFactory = createStrictMock(ResponseFactory.class); Request request = createNiceMock(Request.class); RequestHandler requestHandler = createStrictMock(RequestHandler.class); Result result = createStrictMock(Result.class); Response response = createStrictMock(Response.class); HttpHeaders httpHeaders = createNiceMock(HttpHeaders.class); UriInfo uriInfo = createNiceMock(UriInfo.class); String clusterName = "clusterName"; String hostName = "hostName"; String hostComponentName = "hostComponentName"; expect(requestFactory.createRequest(eq(httpHeaders), eq("body"), eq(uriInfo), eq(Request.Type.POST), eq(resourceDef))).andReturn(request); expect(requestHandler.handleRequest(request)).andReturn(result); expect(request.getResultSerializer()).andReturn(resultSerializer); expect(resultSerializer.serialize(result, uriInfo)).andReturn(serializedResult); expect(responseFactory.createResponse(serializedResult)).andReturn(response); replay(resourceDef, resultSerializer, requestFactory, responseFactory, request, requestHandler, result, response, httpHeaders, uriInfo); HostComponentService hostComponentService = new TestHostComponentService(resourceDef, clusterName, hostName, hostComponentName, requestFactory, responseFactory, requestHandler); assertSame(response, hostComponentService.updateHostComponent("body", httpHeaders, uriInfo, hostComponentName)); verify(resourceDef, resultSerializer, requestFactory, responseFactory, request, requestHandler, result, response, httpHeaders, uriInfo); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Optional<Recognition> XXX( final Throwable ex, final AjaxRequestTarget target, final Form<?> feedbackForm) { val recognition = getExceptionRecognizerService().recognize(ex); recognition.ifPresent(recog->raiseWarning(target, feedbackForm, recog)); return recognition; }', 'ground_truth': 'private Optional<Recognition> recognizeExceptionThenRaise( final Throwable ex, final AjaxRequestTarget target, final Form<?> feedbackForm) { val recognition = getExceptionRecognizerService().recognize(ex); recognition.ifPresent(recog->raiseWarning(target, feedbackForm, recog)); return recognition; }', 'output': 'private Optional<Recognition> recognizeException( final Throwable ex, final AjaxRequestTarget target, final Form<?> feedbackForm) { val recognition = getExceptionRecognizerService().recognize(ex); recognition.ifPresent(recog->raiseWarning(target, feedbackForm, recog)); return recognition; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX() throws ParserConfigurationException, IOException, SAXException { Set libs = ctx.getResourcePaths("/WEB-INF/lib"); for (Iterator iter = libs.iterator(); iter.hasNext();) { String path = (String) iter.next(); if(path.endsWith(".jar") || path.endsWith(".zip")) { ZipInputStream zin = new ZipInputStream(ctx.getResourceAsStream(path)); InputStream uin = new FilterInputStream(zin) { public void close() { } }; try { for(;;) { ZipEntry ze = zin.getNextEntry(); if(ze == null) { break; } String zname = ze.getName(); if(zname.startsWith("META-INF/") && zname.endsWith(".tld")) { String url = "jar:" + ctx.getResource(path).toExternalForm() + "!" + zname; String loc = getTldUri(uin, url); if(loc != null) { locations.put(loc, new String[] { path, zname }); if(logger.isDebugEnabled()) { logger.debug("libjar assigned URI " + StringUtil.jQuoteNoXSS(loc) + " to location " + StringUtil.jQuoteNoXSS(path+ "!" + zname)); } } } } } finally { zin.close(); } } } }', 'ground_truth': 'private void addLocationsFromJarFile(String path) throws Exception { ZipInputStream zin = new ZipInputStream(ctx.getResourceAsStream(path)); InputStream uin = new FilterInputStream(zin) { public void close() { } }; try { for(;;) { ZipEntry ze = zin.getNextEntry(); if(ze == null) { break; } String zname = ze.getName(); if(zname.startsWith("META-INF/") && zname.endsWith(".tld")) { String url = "jar:" + ctx.getResource(path).toExternalForm() + "!" + zname; addLocationFromTldResource(uin, path, zname, url); } } } finally { zin.close(); } }', 'output': 'private void loadLibraries() throws ParserConfigurationException, IOException, SAXException { Set libs = ctx.getResourcePaths("/WEB-INF/lib"); for (Iterator iter = libs.iterator(); iter.hasNext();) { String path = (String) iter.next(); if(path.endsWith(".jar") || path.endsWith(".zip")) { ZipInputStream zin = new ZipInputStream(ctx.getResourceAsStream(path)); InputStream uin = new FilterInputStream(zin) { public void close() { } }; try { for(;;) { ZipEntry ze = zin.getNextEntry(); if(ze == null) { break; } String zname = ze.getName(); if(zname.startsWith("META-INF/") && zname.endsWith(".tld")) { String url = "jar:" + ctx.getResource(path).toExternalForm() + "!" + zname; String loc = getTldUri(uin, url); if(loc != null) { locations.put(loc, new String[] { path, zname }); if(logger.isDebugEnabled()) { logger.debug("libjar assigned URI " + StringUtil.jQuoteNoXSS(loc) + " to location " + StringUtil.jQuoteNoXSS(path+ "!" + zname)); } } } } } finally { zin.close(); } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void XXX(String keyspace, ColumnParent column_parent, Iterable<ByteBuffer> column_names) throws InvalidRequestException { XXX(keyspace, column_parent.column_family, column_parent.super_column, column_names); } public static void validateColumn(String keyspace, ColumnParent column_parent, Column column) throws InvalidRequestException\n    { { validateTtl(column); XXX(keyspace, column_parent, Arrays.asList(column.name)); try { AbstractType validator = DatabaseDescriptor.getValueValidator(keyspace, column_parent.column_family, column.name); if (validator != null) validator.validate(column.value); } catch (MarshalException me) { throw new InvalidRequestException(String.format("[%s][%s][%s] = [%s] failed validation (%s)", keyspace, column_parent.getColumn_family(), ByteBufferUtil.bytesToHex(column.name), ByteBufferUtil.bytesToHex(column.value), me.getMessage())); } }', 'ground_truth': 'public static void validateColumnData(String keyspace, String column_family, Column column) throws InvalidRequestException { validateTtl(column); try { AbstractType validator = DatabaseDescriptor.getValueValidator(keyspace, column_family, column.name); if (validator != null) validator.validate(column.value); } catch (MarshalException me) { throw new InvalidRequestException(String.format("[%s][%s][%s] = [%s] failed validation (%s)", keyspace, column_family, ByteBufferUtil.bytesToHex(column.name), ByteBufferUtil.bytesToHex(column.value), me.getMessage())); } }', 'output': 'public static void validateColumns(String keyspace, ColumnParent column_parent, Iterable<Column> columns) throws InvalidRequestException { for (Column column : columns) validateColumn(keyspace, column_parent, column); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private String XXX(TemplateElement stackEl) { StringBuffer sb = new StringBuffer(); sb.append(MessageUtil.shorten(stackEl.getDescription(), 30)); sb.append("  ["); Macro enclosingMacro = getEnclosingMacro(stackEl); if (enclosingMacro != null) { sb.append(MessageUtil.formatLocationForEvaluationError( enclosingMacro, stackEl.beginLine, stackEl.beginColumn)); } else { sb.append(MessageUtil.formatLocationForEvaluationError( stackEl.getTemplate(), stackEl.beginLine, stackEl.beginColumn)); } sb.append("]"); return sb.toString(); }', 'ground_truth': 'private String getStackTraceItemLine(TemplateElement stackEl) { StringBuffer sb = new StringBuffer(); sb.append(MessageUtil.shorten(stackEl.getDescription(), 40)); sb.append("  ["); Macro enclosingMacro = getEnclosingMacro(stackEl); if (enclosingMacro != null) { sb.append(MessageUtil.formatLocationForEvaluationError( enclosingMacro, stackEl.beginLine, stackEl.beginColumn)); } else { sb.append(MessageUtil.formatLocationForEvaluationError( stackEl.getTemplate(), stackEl.beginLine, stackEl.beginColumn)); } sb.append("]"); return sb.toString(); }', 'output': 'private String describe(TemplateElement stackEl) { StringBuffer sb = new StringBuffer(); sb.append(MessageUtil.shorten(stackEl.getDescription(), 30)); sb.append("  ["); Macro enclosingMacro = getEnclosingMacro(stackEl); if (enclosingMacro != null) { sb.append(MessageUtil.formatLocationForEvaluationError( enclosingMacro, stackEl.beginLine, stackEl.beginColumn)); } else { sb.append(MessageUtil.formatLocationForEvaluationError( stackEl.getTemplate(), stackEl.beginLine, stackEl.beginColumn)); } sb.append("]"); return sb.toString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { fail("http: checkQueryValueByteArray(grid(0).cache(ATOMIC_BYTREARR_CACHE_NAME)); checkQueryValueByteArray(grid(0).cache(TRANSACTIONAL_BYTEARR_CACHE_NAME)); }', 'ground_truth': 'public void testQueryValueByteArray() throws Exception { checkQueryValueByteArray(grid(0).cache(ATOMIC_BYTREARR_CACHE_NAME)); checkQueryValueByteArray(grid(0).cache(TRANSACTIONAL_BYTEARR_CACHE_NAME)); }', 'output': 'public void testCheckQueryValueByteArray() throws Exception { checkQueryValueByteArray(grid(0).cache(ATOMIC_BYTREARR_CACHE_NAME)); checkQueryValueByteArray(grid(0).cache(TRANSACTIONAL_BYTEARR_CACHE_NAME)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void XXX() throws Exception { try (ServerSocket serverSocket = new ServerSocket(0)) { port = serverSocket.getLocalPort(); } LOG.info("Starting MongoDB embedded instance on {}", port); try { Files.forceDelete(new File(MONGODB_LOCATION)); } catch (Exception e) { } new File(MONGODB_LOCATION).mkdirs(); IMongodConfig mongodConfig = new MongodConfigBuilder() .version(Version.Main.PRODUCTION) .configServer(false) .replication(new Storage(MONGODB_LOCATION, null, 0)) .net(new Net("localhost", port, Network.localhostIsIPv6())) .cmdOptions( new MongoCmdOptionsBuilder() .syncDelay(10) .useNoPrealloc(true) .useSmallFiles(true) .useNoJournal(true) .build()) .build(); mongodExecutable = mongodStarter.prepare(mongodConfig); mongodProcess = mongodExecutable.start(); LOG.info("Insert test data"); Mongo client = new Mongo("localhost", port); DB database = client.getDB(DATABASE); GridFS gridfs = new GridFS(database); ByteArrayOutputStream out = new ByteArrayOutputStream(); for (int x = 0; x < 100; x++) { out.write( ("Einstein\\nDarwin\\nCopernicus\\nPasteur\\n" + "Curie\\nFaraday\\nNewton\\nBohr\\nGalilei\\nMaxwell\\n") .getBytes(StandardCharsets.UTF_8)); } for (int x = 0; x < 5; x++) { gridfs.createFile(new ByteArrayInputStream(out.toByteArray()), "file" + x).save(); } gridfs = new GridFS(database, "mapBucket"); long now = System.currentTimeMillis(); Random random = new Random(); String[] scientists = { "Einstein", "Darwin", "Copernicus", "Pasteur", "Curie", "Faraday", "Newton", "Bohr", "Galilei", "Maxwell" }; for (int x = 0; x < 10; x++) { GridFSInputFile file = gridfs.createFile("file_" + x); OutputStream outf = file.getOutputStream(); OutputStreamWriter writer = new OutputStreamWriter(outf, StandardCharsets.UTF_8); for (int y = 0; y < 5000; y++) { long time = now - random.nextInt(3600000); String name = scientists[y % scientists.length]; writer.write(Long.toString(time) + "\\t"); writer.write(name + "\\t"); writer.write(Integer.toString(random.nextInt(100))); writer.write("\\n"); } for (int y = 0; y < scientists.length; y++) { String name = scientists[y % scientists.length]; writer.write(Long.toString(now) + "\\t"); writer.write(name + "\\t"); writer.write("101"); writer.write("\\n"); } writer.flush(); writer.close(); } client.close(); }', 'ground_truth': 'public static void start() throws Exception { try (ServerSocket serverSocket = new ServerSocket(0)) { port = serverSocket.getLocalPort(); } LOG.info("Starting MongoDB embedded instance on {}", port); IMongodConfig mongodConfig = new MongodConfigBuilder() .version(Version.Main.PRODUCTION) .configServer(false) .replication(new Storage(MONGODB_LOCATION.getRoot().getPath(), null, 0)) .net(new Net("localhost", port, Network.localhostIsIPv6())) .cmdOptions( new MongoCmdOptionsBuilder() .syncDelay(10) .useNoPrealloc(true) .useSmallFiles(true) .useNoJournal(true) .verbose(false) .build()) .build(); mongodExecutable = mongodStarter.prepare(mongodConfig); mongodProcess = mongodExecutable.start(); LOG.info("Insert test data"); Mongo client = new Mongo("localhost", port); DB database = client.getDB(DATABASE); GridFS gridfs = new GridFS(database); ByteArrayOutputStream out = new ByteArrayOutputStream(); for (int x = 0; x < 100; x++) { out.write( ("Einstein\\nDarwin\\nCopernicus\\nPasteur\\n" + "Curie\\nFaraday\\nNewton\\nBohr\\nGalilei\\nMaxwell\\n") .getBytes(StandardCharsets.UTF_8)); } for (int x = 0; x < 5; x++) { gridfs.createFile(new ByteArrayInputStream(out.toByteArray()), "file" + x).save(); } gridfs = new GridFS(database, "mapBucket"); long now = System.currentTimeMillis(); Random random = new Random(); String[] scientists = { "Einstein", "Darwin", "Copernicus", "Pasteur", "Curie", "Faraday", "Newton", "Bohr", "Galilei", "Maxwell" }; for (int x = 0; x < 10; x++) { GridFSInputFile file = gridfs.createFile("file_" + x); OutputStream outf = file.getOutputStream(); OutputStreamWriter writer = new OutputStreamWriter(outf, StandardCharsets.UTF_8); for (int y = 0; y < 5000; y++) { long time = now - random.nextInt(3600000); String name = scientists[y % scientists.length]; writer.write(Long.toString(time) + "\\t"); writer.write(name + "\\t"); writer.write(Integer.toString(random.nextInt(100))); writer.write("\\n"); } for (int y = 0; y < scientists.length; y++) { String name = scientists[y % scientists.length]; writer.write(Long.toString(now) + "\\t"); writer.write(name + "\\t"); writer.write("101"); writer.write("\\n"); } writer.flush(); writer.close(); } client.close(); }', 'output': 'public static void main(String[] args) throws Exception { try (ServerSocket serverSocket = new ServerSocket(0)) { port = serverSocket.getLocalPort(); } LOG.info("Starting MongoDB embedded instance on {}", port); try { Files.forceDelete(new File(MONGODB_LOCATION)); } catch (Exception e) { } new File(MONGODB_LOCATION).mkdirs(); IMongodConfig mongodConfig = new MongodConfigBuilder() .version(Version.Main.PRODUCTION) .configServer(false) .replication(new Storage(MONGODB_LOCATION, null, 0)) .net(new Net("localhost", port, Network.localhostIsIPv6())) .cmdOptions( new MongoCmdOptionsBuilder() .syncDelay(10) .useNoPrealloc(true) .useSmallFiles(true) .useNoJournal(true) .build()) .build(); mongodExecutable = mongodStarter.prepare(mongodConfig); mongodProcess = mongodExecutable.start(); LOG.info("Insert test data"); Mongo client = new Mongo("localhost", port); DB database = client.getDB(DATABASE); GridFS gridfs = new GridFS(database); ByteArrayOutputStream out = new ByteArrayOutputStream(); for (int x = 0; x < 100; x++) { out.write( ("Einstein\\nDarwin\\nCopernicus\\nPasteur\\n" + "Curie\\nFaraday\\nNewton\\nBohr\\nGalilei\\nMaxwell\\n") .getBytes(StandardCharsets.UTF_8)); } for (int x = 0; x < 5; x++) { gridfs.createFile(new ByteArrayInputStream(out.toByteArray()), "file" + x).save(); } gridfs = new GridFS(database, "mapBucket"); long now = System.currentTimeMillis(); Random random = new Random(); String[] scientists = { "Einstein", "Darwin", "Copernicus", "Pasteur", "Curie", "Faraday", "Newton", "Bohr", "Galilei", "Maxwell" }; for (int x ='}
{'instruction': 'Please refactor the following code:', 'input': 'public Optional<LogicalType> XXX() { return Optional.ofNullable(memento) .map(ObjectMemento::getLogicalType); }', 'ground_truth': 'public Optional<LogicalType> getLogicalElementType() { return Optional.ofNullable(memento) .map(ObjectMemento::getLogicalType); }', 'output': 'public Optional<LogicalType> getLogicalType() { return Optional.ofNullable(memento) .map(ObjectMemento::getLogicalType); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Class<?>[] XXX(DataType[] dataTypes, boolean calledOnNullInput) { Class<?>[] paramTypes = new Class[dataTypes.length]; for (int i = 0; i < paramTypes.length; i++) { Class<?> clazz = asJavaClass(dataTypes[i]); if (!calledOnNullInput) { if (clazz == Integer.class) clazz = int.class; else if (clazz == Long.class) clazz = long.class; else if (clazz == Byte.class) clazz = byte.class; else if (clazz == Short.class) clazz = short.class; else if (clazz == Float.class) clazz = float.class; else if (clazz == Double.class) clazz = double.class; else if (clazz == Boolean.class) clazz = boolean.class; } paramTypes[i] = clazz; } return paramTypes; }', 'ground_truth': 'public static TypeToken<?>[] typeTokens(DataType[] dataTypes, boolean calledOnNullInput) { TypeToken<?>[] paramTypes = new TypeToken[dataTypes.length]; for (int i = 0; i < paramTypes.length; i++) { TypeToken<?> typeToken = asTypeToken(dataTypes[i]); if (!calledOnNullInput) { Class<?> clazz = typeToken.getRawType(); if (clazz == Integer.class) clazz = int.class; else if (clazz == Long.class) clazz = long.class; else if (clazz == Byte.class) clazz = byte.class; else if (clazz == Short.class) clazz = short.class; else if (clazz == Float.class) clazz = float.class; else if (clazz == Double.class) clazz = double.class; else if (clazz == Boolean.class) clazz = boolean.class; typeToken = TypeToken.of(clazz); } paramTypes[i] = typeToken; } return paramTypes; }', 'output': 'public static Class<?>[] getParameterTypes(DataType[] dataTypes, boolean calledOnNullInput) { Class<?>[] paramTypes = new Class[dataTypes.length]; for (int i = 0; i < paramTypes.length; i++) { Class<?> clazz = asJavaClass(dataTypes[i]); if (!calledOnNullInput) { if (clazz == Integer.class) clazz = int.class; else if (clazz == Long.class) clazz = long.class; else if (clazz == Byte.class) clazz = byte.class; else if (clazz == Short.class) clazz = short.class; else if (clazz == Float.class) clazz = float.class; else if (clazz == Double.class) clazz = double.class; else if (clazz == Boolean.class) clazz = boolean.class; } paramTypes[i] = clazz; } return paramTypes; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { IgniteCache<Organization, Address> cache = jcache(0, OBJ_CACHE_NAME); T2<Organization, Address> val = new T2<>(new Organization("3"), new Address("12345")); checkPutAll(shouldSucceed, cache, new T2<>(new Organization("1"), new Address("1")), val); checkPutOps(shouldSucceed, cache, val); checkReplaceOps(shouldSucceed, cache, val, new Address("1")); }', 'ground_truth': 'public void testPutLongStringValueField() throws Exception { IgniteCache<Organization, Address> cache = jcache(0, OBJ_CACHE_NAME); T2<Organization, Address> val = new T2<>(new Organization("3"), new Address("12345")); checkPutAll(shouldSucceed, cache, new T2<>(new Organization("1"), new Address("1")), val); checkPutOps(shouldSucceed, cache, val); checkReplaceOps(shouldSucceed, cache, val, new Address("1")); }', 'output': 'public void testPutAll() throws Exception { IgniteCache<Organization, Address> cache = jcache(0, OBJ_CACHE_NAME); T2<Organization, Address> val = new T2<>(new Organization("3"), new Address("12345")); checkPutAll(shouldSucceed, cache, new T2<>(new Organization("1"), new Address("1")), val); checkPutOps(shouldSucceed, cache, val); checkReplaceOps(shouldSucceed, cache, val, new Address("1")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(RelOptPlanWriter pw) { String [] terms = new String[traitSet.size() + 1]; Object [] values = new Object[traitSet.size()]; terms[0] = "child"; for (int i = 0; i < traitSet.size(); i++) { terms[i + 1] = traitSet.getTrait(i).getTraitDef().getSimpleName(); values[i] = traitSet.getTrait(i); } pw.XXX(this, terms, values); }', 'ground_truth': 'public RelOptPlanWriter explainTerms(RelOptPlanWriter pw) { super.explainTerms(pw) .input("child", getChild()); for (RelTrait trait : traitSet) { pw.item(trait.getTraitDef().getSimpleName(), trait); } return pw; }', 'output': 'public void writeTo(RelOptPlanWriter pw) { String [] terms = new String[traitSet.size() + 1]; Object [] values = new Object[traitSet.size()]; terms[0] = "child"; for (int i = 0; i < traitSet.size(); i++) { terms[i + 1] = traitSet.getTrait(i).getTraitDef().getSimpleName(); values[i] = traitSet.getTrait(i); } pw.write(this, terms, values); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Collection<ClusterNode> XXX() { return F.view(serverNodesWithCaches(), new P1<ClusterNode>() { @Override public boolean apply(ClusterNode node) { return alives.contains(node.id()); } }); }', 'ground_truth': 'public Collection<ClusterNode> remoteAliveNodesWithCaches() { return F.view(rmtNodesWithCaches, new P1<ClusterNode>() { @Override public boolean apply(ClusterNode node) { return alives.contains(node.id()); } }); }', 'output': 'public Collection<ClusterNode> getAlives() { return F.view(serverNodesWithCaches(), new P1<ClusterNode>() { @Override public boolean apply(ClusterNode node) { return alives.contains(node.id()); } }); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX() { titleFacet = getFacet(TitleFacet.class); iconFacet = getFacet(IconFacet.class); navigableParentFacet = getFacet(NavigableParentFacet.class); cssClassFacet = getFacet(CssClassFacet.class); aliasedFacet = getFacet(AliasedFacet.class); }', 'ground_truth': 'public void invalidateCachedFacets() { valueFacet = getFacet(ValueFacet.class); titleFacet = lookupNonFallbackFacet(TitleFacet.class).orElse(null); iconFacet = getFacet(IconFacet.class); navigableParentFacet = getFacet(NavigableParentFacet.class); cssClassFacet = getFacet(CssClassFacet.class); aliasedFacet = getFacet(AliasedFacet.class); }', 'output': 'private void setFacets() { titleFacet = getFacet(TitleFacet.class); iconFacet = getFacet(IconFacet.class); navigableParentFacet = getFacet(NavigableParentFacet.class); cssClassFacet = getFacet(CssClassFacet.class); aliasedFacet = getFacet(AliasedFacet.class); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { Frame.Decoder dec = new Frame.Decoder(null); List<Object> results = new ArrayList<>(); byte[] frame = new byte[] { (byte) 0x82, 0x00, 0x01, 0x09, (byte) 0xff, (byte) 0xff, (byte) 0xff, (byte) 0xff, }; ByteBuf buf = Unpooled.wrappedBuffer(frame); try { dec.decode(null, buf, results); Assert.fail("Expected protocol error"); } catch (ErrorMessage.WrappedException e) { Assert.assertEquals(1, e.getStreamId()); } }', 'ground_truth': 'public void testInvalidProtocolVersion(int version) throws Exception { Frame.Decoder dec = new Frame.Decoder(null); List<Object> results = new ArrayList<>(); byte[] frame = new byte[] { (byte) REQUEST.addToVersion(version), 0x00, 0x00, 0x01, 0x09, 0x00, 0x00, 0x00, 0x21, 0x00, 0x00, 0x00, 0x1b, 0x00, 0x1b, 0x53, 0x45, 0x4c, 0x45, 0x43, 0x54, 0x20, 0x2a, 0x20, 0x46, 0x52, 0x4f, 0x4d, 0x20, 0x73, 0x79, 0x73, 0x74, 0x65, 0x6d, 0x2e, 0x6c, 0x6f, 0x63, 0x61, 0x6c, 0x3b }; ByteBuf buf = Unpooled.wrappedBuffer(frame); try { dec.decode(null, buf, results); Assert.fail("Expected protocol error"); } catch (ProtocolException e) { Assert.assertTrue(e.getMessage().contains("Invalid or unsupported protocol version")); } }', 'output': 'public void testDecodeWithInvalidFrame() throws Exception { Frame.Decoder dec = new Frame.Decoder(null); List<Object> results = new ArrayList<>(); byte[] frame = new byte[] { (byte) 0x82, 0x00, 0x01, 0x09, (byte) 0xff, (byte) 0xff, (byte) 0xff, (byte) 0xff, }; ByteBuf buf = Unpooled.wrappedBuffer(frame); try { dec.decode(null, buf, results); Assert.fail("Expected protocol error"); } catch (ErrorMessage.WrappedException e) { Assert.assertEquals(1, e.getStreamId()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { ExpressionChecker checker = new ExpressionChecker() .addExpr("FLOOR(ts TO MONTH)", parseDate("1986-02-01 00:00:00")) .addExpr("FLOOR(ts TO YEAR)", parseDate("1986-01-01 00:00:00")) .addExpr("CEIL(ts TO MONTH)", parseDate("1986-03-01 00:00:00")) .addExpr("CEIL(ts TO YEAR)", parseDate("1987-01-01 00:00:00")); checker.buildRunAndCheck(); } public void XXXResolutionLimit() { thrown.expect(IllegalArgumentException.class); ExpressionChecker checker = new ExpressionChecker().addExpr("FLOOR(ts TO DAY)", parseDate("1986-02-01 00:00:00")); checker.buildRunAndCheck(); }', 'ground_truth': 'public void testFloor() { ExpressionChecker checker = new ExpressionChecker() .addExpr("FLOOR(ts TO MONTH)", parseDate("1986-02-01 00:00:00")) .addExpr("FLOOR(ts TO YEAR)", parseDate("1986-01-01 00:00:00")) .addExpr("FLOOR(c_double)", 1.0); checker.buildRunAndCheck(getFloorCeilingTestPCollection()); }', 'output': 'public void testExpressionChecker() { ExpressionChecker checker = new ExpressionChecker() .addExpr("FLOOR(ts TO MONTH)", parseDate("1986-02-01 00:00:00")) .addExpr("FLOOR(ts TO YEAR)", parseDate("1986-01-01 00:00:00")) .addExpr("CEIL(ts TO MONTH)", parseDate("1986-03-01 00:00:00")) .addExpr("CEIL(ts TO YEAR)", parseDate("1987-01-01 00:00:00")); checker.buildRunAndCheck(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(TaskAttemptContext tac) throws IOException { if (writer == null) { instantiateSink(tac); writer = sink.openWriter(tac.getTaskAttemptID().getTaskID().getId()); } }', 'ground_truth': 'private DataSink<V> getSink(JobContext jc) throws IOException { synchronized (lock) { if (sink == null) { String sinkBytes = jc.getConfiguration().get(DATA_SINK, null); if (sinkBytes == null) { throw new IllegalStateException( "Invalid output spec, call `DataSinkOutputFormat#configure` before passing " + " the configuration to output"); } try { sink = fromBase64(sinkBytes); sink.initialize(); } catch (ClassNotFoundException ex) { throw new IOException(ex); } } return sink; } }', 'output': 'private void openWriter(TaskAttemptContext tac) throws IOException { if (writer == null) { instantiateSink(tac); writer = sink.openWriter(tac.getTaskAttemptID().getTaskID().getId()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String XXX(UserConfigData userConfig) { String filterName = userConfig.getFiltername(); SolrInputDocument solrInputDoc = new SolrInputDocument(); if (!isValid(userConfig)) { throw RESTErrorUtil.createRESTException("No FilterName Specified", MessageEnums.INVALID_INPUT_DATA); } if (isNotUnique(filterName)) { throw RESTErrorUtil.createRESTException( "Name \'" + userConfig.getFiltername() + "\' already exists", MessageEnums.INVALID_INPUT_DATA); } solrInputDoc.addField(ID, userConfig.getId()); solrInputDoc.addField(USER_NAME, LogSearchContext.getCurrentUsername()); solrInputDoc.addField(VALUES, userConfig.getValues()); solrInputDoc.addField(FILTER_NAME, filterName); solrInputDoc.addField(ROW_TYPE, userConfig.getRowType()); List<String> shareNameList = userConfig.getShareNameList(); if (CollectionUtils.isNotEmpty(shareNameList)) { solrInputDoc.addField(SHARE_NAME_LIST, shareNameList); } try { userConfigSolrDao.addDocs(solrInputDoc); return convertObjToString(solrInputDoc); } catch (SolrException | SolrServerException | IOException e) { logger.error("Error saving user config. solrDoc=" + solrInputDoc, e); throw RESTErrorUtil.createRESTException(MessageEnums.SOLR_ERROR.getMessage().getMessage(), MessageEnums.ERROR_SYSTEM); } }', 'ground_truth': 'public String saveEvent(EventHistoryData eventHistoryData) { String filterName = eventHistoryData.getFiltername(); SolrInputDocument solrInputDoc = new SolrInputDocument(); if (!isValid(eventHistoryData)) { throw RESTErrorUtil.createRESTException("No FilterName Specified", MessageEnums.INVALID_INPUT_DATA); } if (isNotUnique(filterName)) { throw RESTErrorUtil.createRESTException( "Name \'" + eventHistoryData.getFiltername() + "\' already exists", MessageEnums.INVALID_INPUT_DATA); } solrInputDoc.addField(ID, eventHistoryData.getId()); solrInputDoc.addField(USER_NAME, LogSearchContext.getCurrentUsername()); solrInputDoc.addField(VALUES, eventHistoryData.getValues()); solrInputDoc.addField(FILTER_NAME, filterName); solrInputDoc.addField(ROW_TYPE, eventHistoryData.getRowType()); List<String> shareNameList = eventHistoryData.getShareNameList(); if (CollectionUtils.isNotEmpty(shareNameList)) { solrInputDoc.addField(SHARE_NAME_LIST, shareNameList); } try { eventHistorySolrDao.addDocs(solrInputDoc); return convertObjToString(solrInputDoc); } catch (SolrException | SolrServerException | IOException e) { logger.error("Error saving user config. solrDoc=" + solrInputDoc, e); throw RESTErrorUtil.createRESTException(MessageEnums.SOLR_ERROR.getMessage().getMessage(), MessageEnums.ERROR_SYSTEM); } }', 'output': 'public String saveUserConfig(UserConfigData userConfig) { String filterName = userConfig.getFiltername(); SolrInputDocument solrInputDoc = new SolrInputDocument(); if (!isValid(userConfig)) { throw RESTErrorUtil.createRESTException("No FilterName Specified", MessageEnums.INVALID_INPUT_DATA); } if (isNotUnique(filterName)) { throw RESTErrorUtil.createRESTException( "Name \'" + userConfig.getFiltername() + "\' already exists", MessageEnums.INVALID_INPUT_DATA); } solrInputDoc.addField(ID, userConfig.getId()); solrInputDoc.addField(USER_NAME, LogSearchContext.getCurrentUsername()); solrInputDoc.addField(VALUES, userConfig.getValues()); solrInputDoc.addField(FILTER_NAME, filterName); solrInputDoc.addField(ROW_TYPE, userConfig.getRowType()); List<String> shareNameList = userConfig.getShareNameList(); if (CollectionUtils.isNotEmpty(shareNameList)) { solrInputDoc.addField(SHARE_NAME_LIST, shareNameList); } try { userConfigSolrDao.addDocs(solrInputDoc); return convertObjToString(solrInputDoc); } catch (SolrException | SolrServerException | IOException e) { logger.error("Error saving user config. solrDoc=" + solrInputDoc, e); throw RESTErrorUtil.createRESTException(MessageEnums.SOLR_ERROR.getMessage().getMessage(), MessageEnums.ERROR_SYSTEM); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static String XXX(String globExp) { StringBuilder dst = new StringBuilder(); char[] src = globExp.toCharArray(); int i = 0; while (i < src.length) { char c = src[i++]; switch (c) { case \'*\': dst.append("[^/]*"); break; case \'?\': dst.append("[^/]"); break; case \'.\': case \'+\': case \'{\': case \'}\': case \'(\': case \')\': case \'|\': case \'^\': case \'$\': dst.append(\'\\\\\').append(c); break; case \'\\\\\': i = doubleSlashes(dst, src, i); break; default: dst.append(c); break; } } return dst.toString(); }', 'ground_truth': 'public static String wildcardToRegexp(String globExp) { StringBuilder dst = new StringBuilder(); char[] src = globExp.replace("**/*", "**").toCharArray(); int i = 0; while (i < src.length) { char c = src[i++]; switch (c) { case \'*\': dst.append(".*"); break; case \'?\': dst.append("[^/]"); break; case \'.\': case \'+\': case \'{\': case \'}\': case \'(\': case \')\': case \'|\': case \'^\': case \'$\': dst.append(\'\\\\\').append(c); break; case \'\\\\\': i = doubleSlashes(dst, src, i); break; default: dst.append(c); break; } } return dst.toString(); }', 'output': 'public static String glob(String globExp) { StringBuilder dst = new StringBuilder(); char[] src = globExp.toCharArray(); int i = 0; while (i < src.length) { char c = src[i++]; switch (c) { case \'*\': dst.append("[^/]*"); break; case \'?\': dst.append("[^/]"); break; case \'.\': case \'+\': case \'{\': case \'}\': case \'(\': case \')\': case \'|\': case \'^\': case \'$\': dst.append(\'\\\\\').append(c); break; case \'\\\\\': i = doubleSlashes(dst, src, i); break; default: dst.append(c); break; } } return dst.toString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected static Set XXX(Set self) { final Set ansSet; if (self instanceof SortedSet) { ansSet = new TreeSet(); } else { ansSet = new HashSet(); } return ansSet; }', 'ground_truth': 'protected static Set createSimilarSet(Set self) { final Set ansSet; if (self instanceof SortedSet) { ansSet = new TreeSet(); } else { ansSet = new HashSet(); } return ansSet; }', 'output': 'public static <T> Set<T> asSet(Set<T> self) { final Set<T> ansSet; if (self instanceof SortedSet) { ansSet = new TreeSet<>(); } else { ansSet = new HashSet<>(); } return ansSet; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Pair<Window, State> XXX(Window w, Object itemKey) { WindowStorage wStore = getOrCreateWindowStorage(w, false); if (wStore == null) { return null; } State state = wStore.getKeyStates().get(itemKey); if (state == null) { DatumCollector collector = new DatumCollector(stateOutput) { @Override public void collect(Object elem) { super.collect(WindowedPair.of(getAssignLabel(), itemKey, elem)); } }; collector.assignWindowing(w.getGroup(), w.getLabel()); state = (State) stateFactory.apply(collector); wStore.getKeyStates().put(itemKey, state); } return Pair.of(wStore.getWindow(), state); }', 'ground_truth': 'public Pair<Window, State> getWindowStateForUpdate(Window w, Object itemKey) { WindowStorage wStore = getOrCreateWindowStorage(w, false); if (wStore == null) { return null; } Map<Object, State> keyStates = wStore.getKeyStates(); State state = keyStates.remove(itemKey); if (state == null) { DatumCollector collector = new DatumCollector(stateOutput) { @Override public void collect(Object elem) { super.collect(WindowedPair.of(getAssignLabel(), itemKey, elem)); } }; collector.assignWindowing(w.getGroup(), w.getLabel()); state = (State) stateFactory.apply(collector); keyStates.put(itemKey, state); LOG.debug("wStore({}).keyStates.size: {}", w, wStore.getKeyStates().size()); } else { keyStates.put(itemKey, state); } return Pair.of(wStore.getWindow(), state); }', 'output': 'public Pair<Window, State> apply(Window w, Object itemKey) { WindowStorage wStore = getOrCreateWindowStorage(w, false); if (wStore == null) { return null; } State state = wStore.getKeyStates().get(itemKey); if (state == null) { DatumCollector collector = new DatumCollector(stateOutput) { @Override public void collect(Object elem) { super.collect(WindowedPair.of(getAssignLabel(), itemKey, elem)); } }; collector.assignWindowing(w.getGroup(), w.getLabel()); state = (State) stateFactory.apply(collector); wStore.getKeyStates().put(itemKey, state); } return Pair.of(wStore.getWindow(), state); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(String id) { try { userConfigSolrDao.XXX(id); } catch (SolrException | SolrServerException | IOException e) { throw RESTErrorUtil.createRESTException(MessageEnums.SOLR_ERROR.getMessage().getMessage(), MessageEnums.ERROR_SYSTEM); } }', 'ground_truth': 'public void deleteEvent(String id) { try { eventHistorySolrDao.deleteEventHistoryData(id); } catch (SolrException | SolrServerException | IOException e) { throw RESTErrorUtil.createRESTException(MessageEnums.SOLR_ERROR.getMessage().getMessage(), MessageEnums.ERROR_SYSTEM); } }', 'output': 'public void updateUserConfig(String id) { try { userConfigSolrDao.updateUserConfig(id); } catch (SolrException | SolrServerException | IOException e) { throw RESTErrorUtil.createRESTException(MessageEnums.SOLR_ERROR.getMessage().getMessage(), MessageEnums.ERROR_SYSTEM); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public long XXX() { return EMPTY_SIZE + columnName.XXX(); }', 'ground_truth': 'public long unsharedHeapSizeExcludingData() { return EMPTY_SIZE + columnName.unsharedHeapSizeExcludingData(); }', 'output': 'public long size() { return EMPTY_SIZE + columnName.size(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Throwable { createTable("CREATE TABLE %s (" + "k int PRIMARY KEY, " + "c int, " + "val int) WITH default_time_to_live = 60"); execute("USE " + keyspace()); executeNet("USE " + keyspace()); createView("mv_ttl2", "CREATE MATERIALIZED VIEW %s AS SELECT * FROM %%s WHERE k IS NOT NULL AND c IS NOT NULL PRIMARY KEY (k,c)"); try { executeNet("ALTER MATERIALIZED VIEW " + keyspace()+ ".mv_ttl2 WITH default_time_to_live = 30"); Assert.fail("Should fail if TTL is provided while altering materialized view"); } catch (Exception e) { assertEquals("Forbidden default_time_to_live detected for a materialized view. Data in a materialized view always expire at the same time than the corresponding " + "data in the parent table. default_time_to_live must be set to zero, see CASSANDRA-12868 for more information", e.getMessage()); } }', 'ground_truth': 'public void testAlterMvWithNoZeroTTL() throws Throwable { createTable("CREATE TABLE %s (" + "k int PRIMARY KEY, " + "c int, " + "val int) WITH default_time_to_live = 60"); createView("mv_ttl2", "CREATE MATERIALIZED VIEW " + keyspace() + ".%s AS SELECT * FROM %%s WHERE k IS NOT NULL AND c IS NOT NULL PRIMARY KEY (k,c)"); try { executeNet("ALTER MATERIALIZED VIEW " + keyspace() + ".mv_ttl2 WITH default_time_to_live = 30"); fail("Should fail if TTL is provided while altering materialized view"); } catch (Exception e) { assertEquals("Forbidden default_time_to_live detected for a materialized view. Data in a materialized view always expire at the same time than the corresponding " + "data in the parent table. default_time_to_live must be set to zero, see CASSANDRA-12868 for more information", e.getMessage()); } }', 'output': 'public void testAlterMaterializedViewWithTTL() throws Throwable { createTable("CREATE TABLE %s (" + "k int PRIMARY KEY, " + "c int, " + "val int) WITH default_time_to_live = 60"); execute("USE " + keyspace()); executeNet("USE " + keyspace()); createView("mv_ttl2", "CREATE MATERIALIZED VIEW %s AS SELECT * FROM %%s WHERE k IS NOT NULL AND c IS NOT NULL PRIMARY KEY (k,c)"); try { executeNet("ALTER MATERIALIZED VIEW " + keyspace()+ ".mv_ttl2 WITH default_time_to_live = 30"); Assert.fail("Should fail if TTL is provided while altering materialized view"); } catch (Exception e) { assertEquals("Forbidden default_time_to_live detected for a materialized view. Data in a materialized view always expire at the same time than the corresponding " + "data in the parent table. default_time_to_live must be set to zero, see CASSANDRA-12868 for more information", e.getMessage()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static Expression XXX(ClassNode staticImportType, String fieldName) { if (staticImportType.isPrimaryClassNode() || staticImportType.isResolved()) { FieldNode field = getField(staticImportType, fieldName); if (field != null && field.isStatic()) return newStaticPropertyX(staticImportType, fieldName); } return null; }', 'ground_truth': 'private Expression findStaticPropertyOrField(ClassNode staticImportType, String variableName) { Expression expression = findStaticPropertyAccessor(staticImportType, variableName); if (expression == null) { if (staticImportType.isPrimaryClassNode() || staticImportType.isResolved()) { FieldNode field = getField(staticImportType, variableName); if (field != null && field.isStatic()) expression = newStaticPropertyX(staticImportType, variableName); } } return expression; }', 'output': 'private static Expression resolveStaticImport(ClassNode staticImportType, String fieldName) { if (staticImportType.isPrimaryClassNode() || staticImportType.isResolved()) { FieldNode field = getField(staticImportType, fieldName); if (field != null && field.isStatic()) return newStaticPropertyX(staticImportType, fieldName); } return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static <T> List<T> XXX(BoundedSource.BoundedReader<T> reader) throws IOException { List<T> res = new ArrayList<>(); while (reader.advance()) { res.add(reader.getCurrent()); } return res; }', 'ground_truth': 'private static <T> List<T> readNItemsFromReader(Source.Reader<T> reader, int n, boolean started) throws IOException { List<T> res = new ArrayList<>(); for (int i = 0; i < n; i++) { boolean shouldStart = (i == 0 && !started); boolean more = shouldStart ? reader.start() : reader.advance(); if (n != Integer.MAX_VALUE) { assertTrue(more); } if (!more) { break; } res.add(reader.getCurrent()); } return res; }', 'output': 'public static <T> List<T> boundedReader(BoundedSource.BoundedReader<T> reader) throws IOException { List<T> res = new ArrayList<>(); while (reader.advance()) { res.add(reader.getCurrent()); } return res; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { assertOutput("<@mCAO 1 null null 4 />", "o=[1, 4]"); addToDataModel("args", Arrays.asList(1, null, null, 4)); assertOutput("<@mCAO?spreadArgs(args) />", "o=[1, 4]"); assertOutput("<@mCAO?spreadArgs(args) null 5 6 />", "o=[1, 4, 5, 6]"); }', 'ground_truth': 'public void testNullsWithMacroWithPositionalWithArgs() throws Exception { assertOutput("<@mCAO 1 null null 4 />", "o=[1, 4]"); addToDataModel("args", Arrays.asList(1, null, null, 4)); assertOutput("<@mCAO?withArgs(args) />", "o=[1, 4]"); assertOutput("<@mCAO?withArgs(args) null 5 6 />", "o=[1, 4, 5, 6]"); }', 'output': 'public void testSpreadArguments() throws Exception { assertOutput("<@mCAO 1 null null 4 />", "o=[1, 4]"); addToDataModel("args", Arrays.asList(1, null, null, 4)); assertOutput("<@mCAO?spreadArgs(args) />", "o=[1, 4]"); assertOutput("<@mCAO?spreadArgs(args) null 5 6 />", "o=[1, 4, 5, 6]"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private FileSystem XXX() { String user = IgfsUserContext.currentUser(); if (F.isEmpty(user)) user = dfltUserName; assert !F.isEmpty(user); if (F.eq(user, dfltUserName)) return dfltFs; return fileSysLazyMap.getOrCreate(user); }', 'ground_truth': 'private FileSystem fileSystemForUser() { String user = IgfsUserContext.currentUser(); if (F.isEmpty(user)) user = IgfsUtils.fixUserName(dfltUsrName); assert !F.isEmpty(user); try { return fsFactory.get(user); } catch (IOException ioe) { throw new IgniteException(ioe); } }', 'output': 'private FileSystem getFileSystem() { String user = IgfsUserContext.currentUser(); if (F.isEmpty(user)) user = dfltUserName; assert !F.isEmpty(user); if (F.eq(user, dfltUserName)) return dfltFs; return fileSysLazyMap.getOrCreate(user); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { ModsCreator c = new ModsCreator(); createTestJAR(data, jars, "client-module-depend-host", "client-module"); c.loadModules(); Module client = c.mgr.create(new File(jars, "client-module-depend-host.jar"), null, false, false, false); c.mgr.enable(client); c.checkHostAndOtherFragmentsLoaded(c.host); }', 'ground_truth': 'public void testAutoloadHostEnablesEagerFragments() throws Exception { ModsCreator c = new ModsCreator(); createTestJAR(data, jars, "client-module-depend-host", "client-module"); c.loadModules(); Module client = c.mgr.create(new File(jars, "client-module-depend-host.jar"), null, false, false, false); c.mgr.enable(client); c.checkHostAndOtherFragmentsLoaded(); }', 'output': 'public void testClientModuleDependencyOnHost() throws Exception { ModsCreator c = new ModsCreator(); createTestJAR(data, jars, "client-module-depend-host", "client-module"); c.loadModules(); Module client = c.mgr.create(new File(jars, "client-module-depend-host.jar"), null, false, false, false); c.mgr.enable(client); c.checkHostAndOtherFragmentsLoaded(c.host); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Throwable { QueryProcessor.executeInternal(String.format("CREATE TABLE \\"%s\\".legacy_static_rt_rt_dc (pk int, ck1 text, v int, s set<text>, primary key (pk, ck1))", KEYSPACE)); Keyspace keyspace = Keyspace.open(KEYSPACE); CFMetaData table = keyspace.getColumnFamilyStore("legacy_static_rt_rt_dc").metadata; ColumnDefinition v = table.getColumnDefinition(new ColumnIdentifier("v", false)); ColumnDefinition bug = table.getColumnDefinition(new ColumnIdentifier("s", false)); Row.Builder builder; builder = BTreeRow.unsortedBuilder(0); builder.newRow(new Clustering(UTF8Serializer.instance.serialize("a"))); builder.addCell(BufferCell.live(table, v, 0L, Int32Serializer.instance.serialize(1), null)); builder.addComplexDeletion(bug, new DeletionTime(1L, 1)); Row row = builder.build(); DecoratedKey pk = table.decorateKey(ByteBufferUtil.bytes(1)); PartitionUpdate upd = PartitionUpdate.singleRowUpdate(table, pk, row); try (RowIterator before = FilteredRows.filter(upd.unfilteredIterator(), FBUtilities.nowInSeconds()); DataOutputBuffer serialized21 = new DataOutputBuffer()) { LegacyLayout.serializeAsLegacyPartition(null, upd.unfilteredIterator(), serialized21, MessagingService.VERSION_21); QueryProcessor.executeInternal(String.format("ALTER TABLE \\"%s\\".legacy_static_rt_rt_dc DROP s", KEYSPACE)); try (DataInputBuffer in = new DataInputBuffer(serialized21.buffer(), false)) { try (UnfilteredRowIterator deser21 = LegacyLayout.deserializeLegacyPartition(in, MessagingService.VERSION_21, SerializationHelper.Flag.LOCAL, upd.partitionKey().getKey()); RowIterator after = FilteredRows.filter(deser21, FBUtilities.nowInSeconds());) { while (before.hasNext() || after.hasNext()) assertEquals(before.hasNext() ? before.next() : null, after.hasNext() ? after.next() : null); } } } }', 'ground_truth': 'public void testCollectionDeletionRoundTripForDroppedColumn() throws Throwable { QueryProcessor.executeInternal(String.format("CREATE TABLE \\"%s\\".legacy_rt_rt_dc (pk int, ck1 text, v int, s set<text>, primary key (pk, ck1))", KEYSPACE)); Keyspace keyspace = Keyspace.open(KEYSPACE); CFMetaData table = keyspace.getColumnFamilyStore("legacy_rt_rt_dc").metadata; ColumnDefinition v = table.getColumnDefinition(new ColumnIdentifier("v", false)); ColumnDefinition bug = table.getColumnDefinition(new ColumnIdentifier("s", false)); Row.Builder builder; builder = BTreeRow.unsortedBuilder(0); builder.newRow(new Clustering(UTF8Serializer.instance.serialize("a"))); builder.addCell(BufferCell.live(table, v, 0L, Int32Serializer.instance.serialize(1), null)); builder.addComplexDeletion(bug, new DeletionTime(1L, 1)); Row row = builder.build(); DecoratedKey pk = table.decorateKey(ByteBufferUtil.bytes(1)); PartitionUpdate upd = PartitionUpdate.singleRowUpdate(table, pk, row); try (RowIterator before = FilteredRows.filter(upd.unfilteredIterator(), FBUtilities.nowInSeconds()); DataOutputBuffer serialized21 = new DataOutputBuffer()) { LegacyLayout.serializeAsLegacyPartition(null, upd.unfilteredIterator(), serialized21, MessagingService.VERSION_21); QueryProcessor.executeInternal(String.format("ALTER TABLE \\"%s\\".legacy_rt_rt_dc DROP s", KEYSPACE)); try (DataInputBuffer in = new DataInputBuffer(serialized21.buffer(), false)) { try (UnfilteredRowIterator deser21 = LegacyLayout.deserializeLegacyPartition(in, MessagingService.VERSION_21, SerializationHelper.Flag.LOCAL, upd.partitionKey().getKey()); RowIterator after = FilteredRows.filter(deser21, FBUtilities.nowInSeconds());) { while (before.hasNext() || after.hasNext()) assertEquals(before.hasNext() ? before.next() : null, after.hasNext() ? after.next() : null); } } } }', 'output': 'public void testDroppingStaticColumns() throws Throwable { QueryProcessor.executeInternal(String.format("CREATE TABLE \\"%s\\".legacy_static_rt_rt_dc (pk int, ck1 text, v int, s set<text>, primary key (pk, ck1))", KEYSPACE)); Keyspace keyspace = Keyspace.open(KEYSPACE); CFMetaData table = keyspace.getColumnFamilyStore("legacy_static_rt_rt_dc").metadata; ColumnDefinition v = table.getColumnDefinition(new ColumnIdentifier("v", false)); ColumnDefinition bug = table.getColumnDefinition(new ColumnIdentifier("s", false)); Row.Builder builder; builder = BTreeRow.unsortedBuilder(0); builder.newRow(new Clustering(UTF8Serializer.instance.serialize("a"))); builder.addCell(BufferCell.live(table, v, 0L, Int32Serializer.instance.serialize(1), null)); builder.addComplexDeletion(bug, new DeletionTime(1L, 1)); Row row = builder.build(); DecoratedKey pk = table.decorateKey(ByteBufferUtil.bytes(1)); PartitionUpdate upd = PartitionUpdate.singleRowUpdate(table, pk, row); try (RowIterator before = FilteredRows.filter(upd.unfilteredIterator(), FBUtilities.nowInSeconds()); DataOutputBuffer serialized21 = new DataOutputBuffer()) { LegacyLayout.serializeAsLegacyPartition(null, upd.unfilteredIterator(), serialized21, MessagingService.VERSION_21); QueryProcessor.executeInternal(String.format("ALTER TABLE \\"%s\\".legacy_static_rt_rt_dc DROP s", KEYSPACE)); try (DataInputBuffer in = new DataInputBuffer(serialized21.buffer(), false)) { try (UnfilteredRowIterator deser21 = LegacyLayout.deserializeLegacyPartition(in, MessagingService.VERSION_21, SerializationHelper.Flag.LOCAL, upd.partitionKey().getKey()); RowIterator after = FilteredRows.filter(deser21, FBUtilities.nowInSeconds());) { while (before.hasNext() || after'}
{'instruction': 'Please refactor the following code:', 'input': 'public static BeamSQLRow XXX( CSVFormat csvFormat, String line, BeamSQLRecordType beamSqlRecordType) { BeamSQLRow row = new BeamSQLRow(beamSqlRecordType); try (StringReader reader = new StringReader(line)) { CSVParser parser = csvFormat.parse(reader); CSVRecord rawRecord = parser.getRecords().get(0); if (rawRecord.size() != beamSqlRecordType.size()) { throw new IllegalArgumentException(String.format( "Expect %d fields, but actually %d", line, beamSqlRecordType.size(), rawRecord.size() )); } else { for (int idx = 0; idx < beamSqlRecordType.size(); idx++) { String raw = rawRecord.get(idx); addFieldWithAutoTypeCasting(row, idx, raw); } } } catch (IOException e) { throw new IllegalArgumentException("decodeRecord failed!", e); } return row; }', 'ground_truth': 'public static BeamSqlRow csvLine2BeamSqlRow( CSVFormat csvFormat, String line, BeamSqlRecordType beamSqlRecordType) { BeamSqlRow row = new BeamSqlRow(beamSqlRecordType); try (StringReader reader = new StringReader(line)) { CSVParser parser = csvFormat.parse(reader); CSVRecord rawRecord = parser.getRecords().get(0); if (rawRecord.size() != beamSqlRecordType.size()) { throw new IllegalArgumentException(String.format( "Expect %d fields, but actually %d", line, beamSqlRecordType.size(), rawRecord.size() )); } else { for (int idx = 0; idx < beamSqlRecordType.size(); idx++) { String raw = rawRecord.get(idx); addFieldWithAutoTypeCasting(row, idx, raw); } } } catch (IOException e) { throw new IllegalArgumentException("decodeRecord failed!", e); } return row; }', 'output': 'public static BeamSQLRow decodeRecord( CSVFormat csvFormat, String line, BeamSQLRecordType beamSqlRecordType) { BeamSQLRow row = new BeamSQLRow(beamSqlRecordType); try (StringReader reader = new StringReader(line)) { CSVParser parser = csvFormat.parse(reader); CSVRecord rawRecord = parser.getRecords().get(0); if (rawRecord.size() != beamSqlRecordType.size()) { throw new IllegalArgumentException(String.format( "Expect %d fields, but actually %d", line, beamSqlRecordType.size(), rawRecord.size() )); } else { for (int idx = 0; idx < beamSqlRecordType.size(); idx++) { String raw = rawRecord.get(idx); addFieldWithAutoTypeCasting(row, idx, raw); } } } catch (IOException e) { throw new IllegalArgumentException("decodeRecord failed!", e); } return row; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Map<Range, List<String>> XXX(String keyspace) { if (keyspace == null) keyspace = DatabaseDescriptor.getNonSystemTables().get(0); /* All the ranges for the tokens */ Map<Range, List<String>> map = new HashMap<Range, List<String>>(); for (Map.Entry<Range,List<InetAddress>> entry : getRangeToAddressMap(keyspace).entrySet()) { map.put(entry.getKey(), stringify(entry.getValue())); } return map; }', 'ground_truth': 'public Map<Range, List<String>> getRangeToRpcaddressMap(String keyspace) { if (keyspace == null) keyspace = DatabaseDescriptor.getNonSystemTables().get(0); /* All the ranges for the tokens */ Map<Range, List<String>> map = new HashMap<Range, List<String>>(); for (Map.Entry<Range,List<InetAddress>> entry : getRangeToAddressMap(keyspace).entrySet()) { List<String> rpcaddrs = new ArrayList<String>(); for (InetAddress endpoint: entry.getValue()) { if (endpoint.equals(FBUtilities.getLocalAddress())) rpcaddrs.add(DatabaseDescriptor.getRpcAddress().toString()); else rpcaddrs.add(Gossiper.instance.getEndpointStateForEndpoint(endpoint).getApplicationState(ApplicationState.RPC_ADDRESS).value); } map.put(entry.getKey(), rpcaddrs); } return map; }', 'output': 'public Map<Range, List<String>> getRangesForKeyspace(String keyspace) { if (keyspace == null) keyspace = DatabaseDescriptor.getNonSystemTables().get(0); /* All the ranges for the tokens */ Map<Range, List<String>> map = new HashMap<Range, List<String>>(); for (Map.Entry<Range,List<InetAddress>> entry : getRangeToAddressMap(keyspace).entrySet()) { map.put(entry.getKey(), stringify(entry.getValue())); } return map; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected static boolean XXX(Cluster cluster) { boolean XXX = false; if (cluster != null) { Config rangerKnoxPluginProperties = cluster.getDesiredConfigByType(CONFIGURATION_TYPE_RANGER_KNOX_PLUGIN_PROPERTIES); if (rangerKnoxPluginProperties != null) { String rangerKnoxPluginEnabled = rangerKnoxPluginProperties.getProperties().get(PROPERTY_RANGER_KNOX_PLUGIN_ENABLED); if (StringUtils.isNotEmpty(rangerKnoxPluginEnabled)) { XXX = rangerKnoxPluginEnabled.toLowerCase().equals("yes"); } } } return XXX; }', 'ground_truth': 'protected static boolean isRangerKnoxPluginEnabled(Cluster cluster) { boolean isRangerKnoxPluginEnabled = false; if (cluster != null) { Config rangerKnoxPluginProperties = cluster.getDesiredConfigByType(CONFIGURATION_TYPE_RANGER_KNOX_PLUGIN_PROPERTIES); if (rangerKnoxPluginProperties != null) { String rangerKnoxPluginEnabled = rangerKnoxPluginProperties.getProperties().get(PROPERTY_RANGER_KNOX_PLUGIN_ENABLED); if (StringUtils.isNotEmpty(rangerKnoxPluginEnabled)) { isRangerKnoxPluginEnabled =  "yes".equalsIgnoreCase(rangerKnoxPluginEnabled); } } } return isRangerKnoxPluginEnabled; }', 'output': 'protected static boolean isRangerKnoxPluginEnabled(Cluster cluster) { boolean isRangerKnoxPluginEnabled = false; if (cluster != null) { Config rangerKnoxPluginProperties = cluster.getDesiredConfigByType(CONFIGURATION_TYPE_RANGER_KNOX_PLUGIN_PROPERTIES); if (rangerKnoxPluginProperties != null) { String rangerKnoxPluginEnabled = rangerKnoxPluginProperties.getProperties().get(PROPERTY_RANGER_KNOX_PLUGIN_ENABLED); if (StringUtils.isNotEmpty(rangerKnoxPluginEnabled)) { isRangerKnoxPluginEnabled = rangerKnoxPluginEnabled.toLowerCase().equals("yes"); } } } return isRangerKnoxPluginEnabled; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void XXX(Properties info, ConnectionProperty key, String value) { if (info.getProperty(key.camelName()) == null) { info.setProperty(key.camelName(), value); } }', 'ground_truth': 'private static void setIfNull(Properties info, ConnectionProperty key, String value) { if (info.getProperty(key.camelName()) == null) { info.setProperty(key.camelName(), value); } }', 'output': 'private static void setConnectionProperty(Properties info, ConnectionProperty key, String value) { if (info.getProperty(key.camelName()) == null) { info.setProperty(key.camelName(), value); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws IOException, TException { pig.setBatchOn(); pig.registerQuery("rows = LOAD \'cassandra: pig.registerQuery("STORE rows INTO \'cassandra: pig.executeBatch(); Assert.assertEquals("User Qux", getColumnValue("thrift_ks", "copy_of_some_app", "name", "qux", "UTF8Type")); Assert.assertEquals("dislike", getColumnValue("thrift_ks", "copy_of_some_app", "vote_type", "qux", "UTF8Type")); Assert.assertEquals("64.7", getColumnValue("thrift_ks", "copy_of_some_app", "percent", "qux", "FloatType")); }', 'ground_truth': 'public void testCqlNativeStorageFullCopy() throws IOException { pig.setBatchOn(); pig.registerQuery("rows = LOAD \'cql: pig.registerQuery("records = FOREACH rows GENERATE TOTUPLE(TOTUPLE(\'key\', key)),TOTUPLE(atomic_weight, created, name, percent, rating, score, vote_type);"); pig.registerQuery("STORE records INTO \'cql: pig.executeBatch(); Assert.assertEquals("User Qux", getColumnValue("thrift_ks", "copy_of_some_app", "name", "qux", "UTF8Type")); Assert.assertEquals("dislike", getColumnValue("thrift_ks", "copy_of_some_app", "vote_type", "qux", "UTF8Type")); Assert.assertEquals("64.7", getColumnValue("thrift_ks", "copy_of_some_app", "percent", "qux", "FloatType")); }', 'output': 'public void testCopyOfSomeApp() throws IOException, TException { pig.setBatchOn(); pig.registerQuery("rows = LOAD \'cassandra: pig.registerQuery("STORE rows INTO \'cassandra: pig.executeBatch(); Assert.assertEquals("User Qux", getColumnValue("thrift_ks", "copy_of_some_app", "name", "qux", "UTF8Type")); Assert.assertEquals("dislike", getColumnValue("thrift_ks", "copy_of_some_app", "vote_type", "qux", "UTF8Type")); Assert.assertEquals("64.7", getColumnValue("thrift_ks", "copy_of_some_app", "percent", "qux", "FloatType")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private String XXX(TemplateLoader tl, String name) throws IOException { Object tSrc = tl.findTemplateSource(name); if (tSrc == null) { return null; } return IOUtils.toString(tl.getReader(tSrc, "UTF-8")); }', 'ground_truth': 'private String getTemplateContent(TemplateLoader tl, String name) throws IOException { Object tSrc = tl.findTemplateSource(name); if (tSrc == null) { return null; } return IOUtils.toString(tl.getReader(tSrc, "UTF-8")); }', 'output': 'private String getTemplateText(TemplateLoader tl, String name) throws IOException { Object tSrc = tl.findTemplateSource(name); if (tSrc == null) { return null; } return IOUtils.toString(tl.getReader(tSrc, "UTF-8")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(final ClassNode lhsType, final Expression rhsExpression) { if (rhsExpression instanceof ClosureExpression) { inferParameterAndReturnTypesOfClosureOnRHS(lhsType, (ClosureExpression) rhsExpression); } else if (rhsExpression instanceof MethodReferenceExpression) { LambdaExpression lambdaExpression = constructLambdaExpressionForMethodReference(lhsType); inferParameterAndReturnTypesOfClosureOnRHS(lhsType, lambdaExpression); rhsExpression.putNodeMetaData(CONSTRUCTED_LAMBDA_EXPRESSION, lambdaExpression); rhsExpression.putNodeMetaData(CLOSURE_ARGUMENTS, Arrays.stream(lambdaExpression.getParameters()).map(Parameter::getType).toArray(ClassNode[]::new)); } }', 'ground_truth': 'private void applyTargetType(final ClassNode target, final Expression source) { if (isFunctionalInterface(target)) { if (source instanceof ClosureExpression) { inferParameterAndReturnTypesOfClosureOnRHS(target, (ClosureExpression) source); } else if (source instanceof MethodReferenceExpression) { LambdaExpression lambdaExpression = constructLambdaExpressionForMethodReference(target); inferParameterAndReturnTypesOfClosureOnRHS(target, lambdaExpression); source.putNodeMetaData(CONSTRUCTED_LAMBDA_EXPRESSION, lambdaExpression); source.putNodeMetaData(CLOSURE_ARGUMENTS, Arrays.stream(lambdaExpression.getParameters()).map(Parameter::getType).toArray(ClassNode[]::new)); } } else if (isClosureWithType(target)) { if (source instanceof ClosureExpression) { GenericsType returnType = target.getGenericsTypes()[0]; storeInferredReturnType(source, getCombinedBoundType(returnType)); } } }', 'output': 'private void inferParameterAndReturnTypesOfClosureOnRHS(final ClassNode lhsType, final Expression rhsExpression) { if (rhsExpression instanceof ClosureExpression) { inferParameterAndReturnTypesOfClosureOnRHS(lhsType, (ClosureExpression) rhsExpression); } else if (rhsExpression instanceof MethodReferenceExpression) { LambdaExpression lambdaExpression = constructLambdaExpressionForMethodReference(lhsType); inferParameterAndReturnTypesOfClosureOnRHS(lhsType, lambdaExpression); rhsExpression.putNodeMetaData(CONSTRUCTED_LAMBDA_EXPRESSION, lambdaExpression); rhsExpression.putNodeMetaData(CLOSURE_ARGUMENTS, Arrays.stream(lambdaExpression.getParameters()).map(Parameter::getType).toArray(ClassNode[]::new)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { IgniteCache<Organization, Address> cache = jcache(0, OBJ_CACHE_NAME); T2<Organization, Address> val = new T2<>(new Organization("123456"), new Address("2")); checkPutAll(shouldFail, cache, new T2<>(new Organization("1"), new Address("1")), val); checkPutOps(shouldFail, cache, val); }', 'ground_truth': 'public void testPutTooLongStringKeyFieldFail() throws Exception { IgniteCache<Organization, Address> cache = jcache(0, OBJ_CACHE_NAME); T2<Organization, Address> val = new T2<>(new Organization("123456"), new Address("2")); checkPutAll(shouldFail, cache, new T2<>(new Organization("1"), new Address("1")), val); checkPutOps(shouldFail, cache, val); }', 'output': 'public void testPutAllWithInvalidKey() throws Exception { IgniteCache<Organization, Address> cache = jcache(0, OBJ_CACHE_NAME); T2<Organization, Address> val = new T2<>(new Organization("123456"), new Address("2")); checkPutAll(shouldFail, cache, new T2<>(new Organization("1"), new Address("1")), val); checkPutOps(shouldFail, cache, val); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String XXX(String propertyName) { if (propertyName.equals("cluster name")) { return DatabaseDescriptor.getClusterName(); } else if (propertyName.equals("config file")) { String filename = DatabaseDescriptor.getConfigFileName(); try { StringBuilder fileData = new StringBuilder(8192); BufferedInputStream stream = new BufferedInputStream(new FileInputStream(filename)); byte[] buf = new byte[1024]; int numRead; while( (numRead = stream.read(buf)) != -1) { String str = new String(buf, 0, numRead); fileData.append(str); } stream.close(); return fileData.toString(); } catch (IOException e) { return "file not found!"; } } else if (propertyName.equals("version")) { return "0.3.0"; } else { return "?"; } }', 'ground_truth': 'public String get_string_property(String propertyName) { if (propertyName.equals("cluster name")) { return DatabaseDescriptor.getClusterName(); } else if (propertyName.equals("config file")) { String filename = DatabaseDescriptor.getConfigFileName(); try { StringBuilder fileData = new StringBuilder(8192); BufferedInputStream stream = new BufferedInputStream(new FileInputStream(filename)); byte[] buf = new byte[1024]; int numRead; while( (numRead = stream.read(buf)) != -1) { String str = new String(buf, 0, numRead); fileData.append(str); } stream.close(); return fileData.toString(); } catch (IOException e) { return "file not found!"; } } else if (propertyName.equals("version")) { return "0.3.0"; } else { return "?"; } }', 'output': 'public String getProperty(String propertyName) { if (propertyName.equals("cluster name")) { return DatabaseDescriptor.getClusterName(); } else if (propertyName.equals("config file")) { String filename = DatabaseDescriptor.getConfigFileName(); try { StringBuilder fileData = new StringBuilder(8192); BufferedInputStream stream = new BufferedInputStream(new FileInputStream(filename)); byte[] buf = new byte[1024]; int numRead; while( (numRead = stream.read(buf)) != -1) { String str = new String(buf, 0, numRead); fileData.append(str); } stream.close(); return fileData.toString(); } catch (IOException e) { return "file not found!"; } } else if (propertyName.equals("version")) { return "0.3.0"; } else { return "?"; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected static int XXX(String filename) { /* * File name is of the form <table>-<column family>-<index>-Data.db. * This tokenizer will strip the .db portion. */ StringTokenizer st = new StringTokenizer(filename, "-"); /* * Now I want to get the index portion of the filename. We accumulate * the indices and then sort them to get the max index. */ int count = st.countTokens(); int i = 0; String index = null; while (st.hasMoreElements()) { index = (String) st.nextElement(); if (i == (count - 2)) { break; } ++i; } return Integer.parseInt(index); }', 'ground_truth': 'protected static int getGenerationFromFileName(String filename) { /* * File name is of the form <table>-<column family>-<index>-Data.db. * This tokenizer will strip the .db portion. */ StringTokenizer st = new StringTokenizer(filename, "-"); /* * Now I want to get the index portion of the filename. We accumulate * the indices and then sort them to get the max index. */ int count = st.countTokens(); int i = 0; String index = null; while (st.hasMoreElements()) { index = (String) st.nextElement(); if (i == (count - 2)) { break; } ++i; } return Integer.parseInt(index); }', 'output': 'protected static int getIndex(String filename) { StringTokenizer st = new StringTokenizer(filename, "-"); int count = st.countTokens(); int i = 0; String index = null; while (st.hasMoreElements()) { index = (String) st.nextElement(); if (i == (count - 2)) { break; }++i; } return Integer.parseInt(index); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { ImpulseSourceFunction source = new ImpulseSourceFunction(false); source.run(sourceContext); verify(sourceContext).collect(argThat(elementMatcher)); }', 'ground_truth': 'public void testImpulseInitial() throws Exception { ImpulseSourceFunction source = new ImpulseSourceFunction(false); ListState<Object> mockListState = getMockListState(Collections.emptyList()); source.initializeState(getInitializationContext(mockListState)); source.run(sourceContext); verify(sourceContext).getCheckpointLock(); verify(sourceContext).collect(argThat(elementMatcher)); verifyNoMoreInteractions(sourceContext); verify(mockListState).get(); verify(mockListState).add(true); verifyNoMoreInteractions(mockListState); }', 'output': 'public void test_impulse_source_with_no_data() throws Exception { ImpulseSourceFunction source = new ImpulseSourceFunction(false); source.run(sourceContext); verify(sourceContext).collect(argThat(elementMatcher)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected UnfilteredPartitionIterator XXX(UnfilteredPartitionIterator iterator, ColumnFamilyStore cfs) { return new TombstonePurgingPartitionIterator(iterator, cfs.gcBefore(nowInSec())) { protected long getMaxPurgeableTimestamp() { return Long.MAX_VALUE; } }; }', 'ground_truth': 'protected UnfilteredPartitionIterator withoutPurgeableTombstones(UnfilteredPartitionIterator iterator, ColumnFamilyStore cfs) { return new PurgingPartitionIterator(iterator, cfs.gcBefore(nowInSec())) { protected long getMaxPurgeableTimestamp() { return Long.MAX_VALUE; } }; }', 'output': 'protected UnfilteredPartitionIterator purgeTombstones(UnfilteredPartitionIterator iterator, ColumnFamilyStore cfs) { return new TombstonePurgingPartitionIterator(iterator, cfs.gcBefore(nowInSec())) { protected long getMaxPurgeableTimestamp() { return Long.MAX_VALUE; } }; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Map<K, V> XXX(ByteBuffer bytes) { try { ByteBuffer input = bytes.duplicate(); int n = ByteBufferUtil.readShortLength(input); Map<K, V> m = new LinkedHashMap<K, V>(n); for (int i = 0; i < n; i++) { ByteBuffer kbb = ByteBufferUtil.readBytesWithShortLength(input); keys.validate(kbb); ByteBuffer vbb = ByteBufferUtil.readBytesWithShortLength(input); values.validate(vbb); m.put(keys.XXX(kbb), values.XXX(vbb)); } return m; } catch (BufferUnderflowException e) { throw new MarshalException("Not enough bytes to read a map"); } }', 'ground_truth': 'public Map<K, V> deserializeForNativeProtocol(ByteBuffer bytes, int version) { try { ByteBuffer input = bytes.duplicate(); int n = readCollectionSize(input, version); Map<K, V> m = new LinkedHashMap<K, V>(n); for (int i = 0; i < n; i++) { ByteBuffer kbb = readValue(input, version); keys.validate(kbb); ByteBuffer vbb = readValue(input, version); values.validate(vbb); m.put(keys.deserialize(kbb), values.deserialize(vbb)); } return m; } catch (BufferUnderflowException e) { throw new MarshalException("Not enough bytes to read a map"); } }', 'output': 'public Map<K, V> deserialize(ByteBuffer bytes) { try { ByteBuffer input = bytes.duplicate(); int n = ByteBufferUtil.readShortLength(input); Map<K, V> m = new LinkedHashMap<K, V>(n); for (int i = 0; i < n; i++) { ByteBuffer kbb = ByteBufferUtil.readBytesWithShortLength(input); keys.validate(kbb); ByteBuffer vbb = ByteBufferUtil.readBytesWithShortLength(input); m.put(keys.deserialize(kbb), values.deserialize(vbb)); } return m; } catch (BufferUnderflowException e) { throw new MarshalException("Not enough bytes to read a map"); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { final IgniteCache<Integer, DbValue> cache = cache(null); int cnt = KEYS_COUNT; Map<Integer, DbValue> map = new HashMap<>(cnt); long seed = 1460943282308L; X.println(" seed---> " + seed); Random rnd = new GridRandom(seed); for (int i = 0; i < 1000_000; i++) { if (i % 5000 == 0) X.println(" --> " + i); int key = rnd.nextInt(cnt); DbValue v0 = new DbValue(key, "test-value-" + rnd.nextInt(200), rnd.nextInt(500)); switch (rnd.nextInt(3)) { case 0: X.println("Put: " + key + " = " + v0); assertEquals(map.put(key, v0), cache.getAndPut(key, v0)); case 1: X.println("Get: " + key); assertEquals(map.get(key), cache.get(key)); break; case 2: X.println("Rmv: " + key); assertEquals(map.remove(key), cache.getAndRemove(key)); assertNull(cache.get(key)); } } for (Integer key : map.keySet()) assertEquals(map.get(key), cache.get(key)); }', 'ground_truth': 'public void testRandomPutGetRemove() throws Exception { final IgniteCache<Integer, DbValue> cache = cache(DEFAULT_CACHE_NAME); int cnt = KEYS_COUNT; Map<Integer, DbValue> map = new HashMap<>(cnt); long seed = System.currentTimeMillis(); int iterations = SF.apply(300_000); X.println("Seed: " + seed); X.println("Iterations total: " + iterations); Random rnd = new GridRandom(seed); for (int i = 0; i < iterations; i++) { if (i % 5000 == 0) X.println("Iteration #" + i); int key = rnd.nextInt(cnt); DbValue v0 = new DbValue(key, "test-value-" + rnd.nextInt(200), rnd.nextInt(500)); switch (rnd.nextInt(3)) { case 0: assertEquals(map.put(key, v0), cache.getAndPut(key, v0)); case 1: assertEquals(map.get(key), cache.get(key)); break; case 2: assertEquals(map.remove(key), cache.getAndRemove(key)); assertNull(cache.get(key)); } } assertEquals(map.size(), cache.size()); for (Integer key : map.keySet()) assertEquals(map.get(key), cache.get(key)); }', 'output': 'public void testCachePutGetRemove() throws Exception { final IgniteCache<Integer, DbValue> cache = cache(null); int cnt = KEYS_COUNT; Map<Integer, DbValue> map = new HashMap<>(cnt); long seed = 1460943282308L; X.println(" seed---> " + seed); Random rnd = new GridRandom(seed); for (int i = 0; i < 1000_000; i++) { if (i % 5000 == 0) X.println(" --> " + i); int key = rnd.nextInt(cnt); DbValue v0 = new DbValue(key, "test-value-" + rnd.nextInt(200), rnd.nextInt(500)); switch (rnd.nextInt(3)) { case 0: X.println("Put: " + key + " = " + v0); assertEquals(cache.getAndPut(key, v0), v0); break; case 1: X.println("Get: " + key); assertEquals(cache.get(key), v0); break; case 2: X.println("Rmv: " + key); assertEquals(cache.getAndRemove(key), v0); assertNull(cache.get(key)); break; } } for (Integer key : map.keySet()) assertEquals(map.get(key), cache.get(key)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { GridBinaryTestClasses.TestObjectContainer container = new GridBinaryTestClasses.TestObjectContainer(); BinaryObjectBuilderImpl wrapper = wrap(container); wrapper.setField("foo", toPortable(new GridBinaryTestClasses.TestObjectArrayList())); GridBinaryTestClasses.TestObjectContainer deserialized = wrapper.build().deserialize(); assertTrue(deserialized.foo instanceof GridBinaryTestClasses.TestObjectArrayList); }', 'ground_truth': 'public void testAssignBinaryObject() { GridBinaryTestClasses.TestObjectContainer container = new GridBinaryTestClasses.TestObjectContainer(); BinaryObjectBuilderImpl wrapper = wrap(container); wrapper.setField("foo", toBinary(new GridBinaryTestClasses.TestObjectArrayList())); GridBinaryTestClasses.TestObjectContainer deserialized = wrapper.build().deserialize(); assertTrue(deserialized.foo instanceof GridBinaryTestClasses.TestObjectArrayList); }', 'output': 'public void testWrapAndDeserializeList() { GridBinaryTestClasses.TestObjectContainer container = new GridBinaryTestClasses.TestObjectContainer(); BinaryObjectBuilderImpl wrapper = wrap(container); wrapper.setField("foo", toPortable(new GridBinaryTestClasses.TestObjectArrayList())); GridBinaryTestClasses.TestObjectContainer deserialized = wrapper.build().deserialize(); assertTrue(deserialized.foo instanceof GridBinaryTestClasses.TestObjectArrayList); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<? extends UnboundedSource<T, TestCheckpointMark>> XXX( int desiredNumSplits, PipelineOptions options) throws Exception { return ImmutableList.of(this); }', 'ground_truth': 'public List<? extends UnboundedSource<T, TestCheckpointMark>> split( int desiredNumSplits, PipelineOptions options) throws Exception { return ImmutableList.of(this); }', 'output': 'public List<? extends UnboundedSource<T, TestCheckpointMark>> split( int desiredNumSplits, PipelineOptions options) throws Exception { return ImmutableList.of(this); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public IModel<String> XXX() { return LambdaModel.<String>of(()->{ val memento = mementoModel().getObject(); if(memento == null) { return null; } val adapter = getCommonContext().reconstructObject(memento); return adapter != null ? adapter.titleString() : null; }); } public IModel<String> XXX2() { return LambdaModel.<String>of(()->{ final ObjectMemento inlinePromptMemento = this.memento(); return inlinePromptMemento != null ? inlinePromptMemento.asString(): null; }); }', 'ground_truth': 'public IModel<String> obtainOutputFormatModel() { return LambdaModel.<String>of(()->{ val memento = memento(); if(memento == null) { return null; } val adapter = getCommonContext().reconstructObject(memento); return adapter != null ? adapter.titleString() : null; }); }', 'output': 'public IModel<String> getTitle() { return LambdaModel.<String>of(()->{ val memento = mementoModel().getObject(); if(memento == null) { return null; } val adapter = getCommonContext().reconstructObject(memento); return adapter != null ? adapter.titleString() : null; }); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Map<String, CPPVariable> XXX() { Map<String, CPPVariable> vars = children; if (vars == null) { synchronized (this) { vars = children; if (vars == null) { children = vars = CPPFrame.retrieveVariables(frame, this); } } } return vars; }', 'ground_truth': 'public Map<String, NIVariable> getChildrenByNames() { Map<String, NIVariable> vars = children; if (vars == null) { synchronized (this) { vars = children; if (vars == null) { children = vars = CPPFrame.retrieveVariables(frame, this); } } } return vars; }', 'output': 'public Map<String, CPPVariable> getVariables() { Map<String, CPPVariable> vars = children; if (vars == null) { synchronized (this) { vars = children; if (vars == null) { children = vars = CPPFrame.retrieveVariables(frame, this); } } } return vars; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private DestroyObjectCommand XXX(final ManagedObject adapter) { state.ensureOpened(); val pojo = adapter.getPojo(); log.debug("destroy object - creating command for: {}", adapter); if (!getEntityState(pojo).isAttached()) { throw new IllegalArgumentException("Adapter is not persistent; adapter: " + adapter); } return new DataNucleusDeleteObjectCommand(adapter, persistenceManager); }', 'ground_truth': 'private DeleteObjectCommand newDeleteObjectCommand(final ManagedObject adapter) { state.ensureOpened(); val pojo = adapter.getPojo(); log.debug("destroy object - creating command for: {}", adapter); if (!DnEntityStateProvider.entityState(pojo).isAttached()) { throw new IllegalArgumentException("Adapter is not persistent; adapter: " + adapter); } return new DeleteObjectCommand(getPersistenceManager(), adapter); }', 'output': 'private DestroyObjectCommand destroyObject(final ManagedObject adapter) { state.ensureOpened(); val pojo = adapter.getPojo(); log.debug("destroy object - creating command for: {}", adapter); if (!getEntityState(pojo).isAttached()) { throw new IllegalArgumentException("Adapter is not persistent; adapter: " + adapter); } return new DataNucleusDeleteObjectCommand(adapter, persistenceManager); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Token XXX(String key) { return partitioner_.getTokenForKey(key); }', 'ground_truth': 'public static BigInteger hash(String key) { return partitioner_.hash(key); }', 'output': 'public static Token getToken(String key) { return partitioner_.getTokenForKey(key); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Backlog XXX() { if (lastAttemptedOffset == null) { return Backlog.of(BigDecimal.valueOf(range.getTo() - range.getFrom())); } return Backlog.of(BigDecimal.valueOf(Math.max(range.getTo() - lastAttemptedOffset, 0))); }', 'ground_truth': 'public double getSize() { if (lastAttemptedOffset == null) { return range.getTo() - range.getFrom(); } return Math.max(range.getTo() - lastAttemptedOffset, 0); }', 'output': 'public Backlog getBacklog() { if (lastAttemptedOffset == null) { return Backlog.of(BigDecimal.valueOf(range.getTo() - range.getFrom())); } return Backlog.of(BigDecimal.valueOf(Math.max(range.getTo() - lastAttemptedOffset, 0))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<InetAddressAndPort> XXX(String keyspaceName, RingPosition pos) { return Keyspace.open(keyspaceName).getReplicationStrategy().XXX(pos); }', 'ground_truth': 'public static EndpointsForToken getNaturalReplicasForToken(String keyspaceName, RingPosition pos) { return Keyspace.open(keyspaceName).getReplicationStrategy().getNaturalReplicasForToken(pos); }', 'output': 'public List<InetAddressAndPort> getRingMembers(String keyspaceName, RingPosition pos) { return Keyspace.open(keyspaceName).getReplicationStrategy().getRingMembers(pos); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(@NonNull ValidationFailures validationFailures) { validateAll(); summarize(); super.XXX(validationFailures); }', 'ground_truth': 'public void validate() { validateAll(); summarize(); }', 'output': 'public void validateAndSummarize(@NonNull ValidationFailures validationFailures) { validateAll(); summarize(); super.validateAndSummarize(validationFailures); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { GridHadoopClientProtocolProvider provider = provider(); ClientProtocol proto = provider.create(config(GridHadoopAbstractSelfTest.REST_PORT)); try { proto.getJobCounters(new JobID(UUID.randomUUID().toString(), -1)); fail("exception must be thrown"); } catch (Exception e) { assert e instanceof IOException : "wrong error has been thrown"; } }', 'ground_truth': 'private void tstUnknownJobCounters() throws Exception { GridHadoopClientProtocolProvider provider = provider(); ClientProtocol proto = provider.create(config(GridHadoopAbstractSelfTest.REST_PORT)); try { proto.getJobCounters(new JobID(UUID.randomUUID().toString(), -1)); fail("exception must be thrown"); } catch (Exception e) { assert e instanceof IOException : "wrong error has been thrown"; } }', 'output': 'public void testGetJobCounters() throws Exception { GridHadoopClientProtocolProvider provider = provider(); ClientProtocol proto = provider.create(config(GridHadoopAbstractSelfTest.REST_PORT)); try { proto.getJobCounters(new JobID(UUID.randomUUID().toString(), -1)); fail("exception must be thrown"); } catch (Exception e) { assert e instanceof IOException : "wrong error has been thrown"; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(String tableName) throws TException { oprot_.writeMessageBegin(new TMessage("describeTable", TMessageType.CALL, seqid_)); describeTable_args args = new describeTable_args(); args.tableName = tableName; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'ground_truth': 'public void send_describe_table(String table) throws TException { oprot_.writeMessageBegin(new TMessage("describe_table", TMessageType.CALL, seqid_)); describe_table_args args = new describe_table_args(); args.table = table; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'output': 'public void describeTable(String tableName) throws TException { oprot_.writeMessageBegin(new TMessage("describeTable", TMessageType.CALL, seqid_)); describeTable_args args = new describeTable_args(); args.tableName = tableName; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { String definitionName = ALERT_DEFINITION + "1"; String serviceName = "HDFS"; String componentName = "NAMENODE"; String text = serviceName + " " + componentName + " is OK"; Alert alert = new Alert(definitionName, null, serviceName, componentName, HOST1, AlertState.OK); alert.setCluster(m_cluster.getClusterName()); alert.setLabel(ALERT_LABEL); alert.setText(text); alert.setTimestamp(1L); AlertReceivedListener listener = m_injector.getInstance(AlertReceivedListener.class); AlertReceivedEvent event = new AlertReceivedEvent(m_cluster.getClusterId(), alert); listener.onAlertEvent(event); List<AlertCurrentEntity> allCurrent = m_dao.findCurrent(); assertEquals(1, allCurrent.size()); assertEquals(1L, (long) allCurrent.get(0).getOriginalTimestamp()); assertEquals(1L, (long) allCurrent.get(0).getLatestTimestamp()); alert.setState(AlertState.SKIPPED); alert.setTimestamp(2L); alert.setText("INVALID"); listener.onAlertEvent(event); allCurrent = m_dao.findCurrent(); assertEquals(1L, (long) allCurrent.get(0).getOriginalTimestamp()); assertEquals(2L, (long) allCurrent.get(0).getLatestTimestamp()); assertEquals(text, allCurrent.get(0).getLatestText()); }', 'ground_truth': 'public void testSkippedAlertUpdatesTimestampAndText() { String definitionName = ALERT_DEFINITION + "1"; String serviceName = "HDFS"; String componentName = "NAMENODE"; String text = serviceName + " " + componentName + " is OK"; Alert alert = new Alert(definitionName, null, serviceName, componentName, HOST1, AlertState.OK); alert.setCluster(m_cluster.getClusterName()); alert.setLabel(ALERT_LABEL); alert.setText(text); alert.setTimestamp(1L); AlertReceivedListener listener = m_injector.getInstance(AlertReceivedListener.class); AlertReceivedEvent event = new AlertReceivedEvent(m_cluster.getClusterId(), alert); listener.onAlertEvent(event); List<AlertCurrentEntity> allCurrent = m_dao.findCurrent(); assertEquals(1, allCurrent.size()); assertEquals(1L, (long) allCurrent.get(0).getOriginalTimestamp()); assertEquals(1L, (long) allCurrent.get(0).getLatestTimestamp()); alert.setState(AlertState.SKIPPED); alert.setTimestamp(2L); text = text + " Updated"; alert.setText(text); listener.onAlertEvent(event); allCurrent = m_dao.findCurrent(); assertEquals(1L, (long) allCurrent.get(0).getOriginalTimestamp()); assertEquals(2L, (long) allCurrent.get(0).getLatestTimestamp()); assertEquals(text, allCurrent.get(0).getLatestText()); alert.setText(""); alert.setTimestamp(3L); listener.onAlertEvent(event); allCurrent = m_dao.findCurrent(); assertEquals(1L, (long) allCurrent.get(0).getOriginalTimestamp()); assertEquals(3L, (long) allCurrent.get(0).getLatestTimestamp()); assertEquals(text, allCurrent.get(0).getLatestText()); }', 'output': 'public void testAlertTimestamps() { String definitionName = ALERT_DEFINITION + "1"; String serviceName = "HDFS"; String componentName = "NAMENODE"; String text = serviceName + " " + componentName + " is OK"; Alert alert = new Alert(definitionName, null, serviceName, componentName, HOST1, AlertState.OK); alert.setCluster(m_cluster.getClusterName()); alert.setLabel(ALERT_LABEL); alert.setText(text); alert.setTimestamp(1L); AlertReceivedListener listener = m_injector.getInstance(AlertReceivedListener.class); AlertReceivedEvent event = new AlertReceivedEvent(m_cluster.getClusterId(), alert); listener.onAlertEvent(event); List<AlertCurrentEntity> allCurrent = m_dao.findCurrent(); assertEquals(1, allCurrent.size()); assertEquals(1L, (long) allCurrent.get(0).getOriginalTimestamp()); assertEquals(1L, (long) allCurrent.get(0).getLatestTimestamp()); alert.setState(AlertState.SKIPPED); alert.setTimestamp(2L); alert.setText("INVALID"); listener.onAlertEvent(event); allCurrent = m_dao.findCurrent(); assertEquals(1L, (long) allCurrent.get(0).getOriginalTimestamp()); assertEquals(2L, (long) allCurrent.get(0).getLatestTimestamp()); assertEquals(text, allCurrent.get(0).getLatestText()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(ReuseBag bag) throws IgniteCheckedException { assert bag != null; for (int i = BPlusTree.randomInt(trees.length);;) { long pageId = bag.pollFreePage(); if (pageId == 0) break; trees[i].put(pageId, bag); if (++i == trees.length) i = 0; } }', 'ground_truth': '@Override public void addForRecycle(ReuseBag bag) throws IgniteCheckedException { assert bag != null; for (int i = BPlusTree.randomInt(trees.length);;) { long pageId = bag.pollFreePage(); if (pageId == 0) break; trees[i].put(pageId, bag); if (++i == trees.length) i = 0; } }', 'output': 'public void reusePages(ReuseBag bag) throws IgniteCheckedException { assert bag != null; for (int i = BPlusTree.randomInt(trees.length);;) { long pageId = bag.pollFreePage(); if (pageId == 0) break; trees[i].put(pageId, bag); if (++i == trees.length) i = 0; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public T XXX(double subsampleSize) { this.subsampleSize = subsampleSize; return instance(); }', 'ground_truth': 'public T withSubSampleSize(double subSampleSize) { this.subSampleSize = subSampleSize; return instance(); }', 'output': 'public T withSubsampleSize(double subsampleSize) { this.subsampleSize = subsampleSize; return instance(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ColumnDefinition XXX(ByteBuffer columnName) { if (!isSuper() && (comparator instanceof CompositeType)) { CompositeType composite = (CompositeType)comparator; ByteBuffer[] components = composite.split(columnName); for (ColumnDefinition def : column_metadata.values()) { ByteBuffer toCompare; if (def.componentIndex == null) { toCompare = columnName; } else { if (def.componentIndex >= components.length) break; toCompare = components[def.componentIndex]; } if (def.name.equals(toCompare)) return def; } return null; } else { return column_metadata.get(columnName); } }', 'ground_truth': 'public ColumnDefinition getColumnDefinitionFromCellName(ByteBuffer cellName) { if (!isSuper() && hasCompositeComparator()) { CompositeType composite = (CompositeType)comparator; ByteBuffer[] components = composite.split(cellName); for (ColumnDefinition def : allColumns()) { ByteBuffer toCompare; if (def.isOnAllComponents()) { toCompare = cellName; } else { if (def.position() >= components.length) break; toCompare = components[def.position()]; } if (def.name.equals(toCompare)) return def; } return null; } else { return columnMetadata.get(cellName); } }', 'output': 'public ColumnDefinition getColumn(ByteBuffer columnName) { if (!isSuper() && (comparator instanceof CompositeType)) { CompositeType composite = (CompositeType)comparator; ByteBuffer[] components = composite.split(columnName); for (ColumnDefinition def : column_metadata.values()) { ByteBuffer toCompare; if (def.componentIndex == null) { toCompare = columnName; } else { if (def.componentIndex >= components.length) break; toCompare = components[def.componentIndex]; } if (def.name.equals(toCompare)) return def; } return null; } else { return column_metadata.get(columnName); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { Ignite ig = testedGrid(); CacheConfiguration<Object, Object> cc = variationConfig("eviction"); cc.setEvictionPolicy(new FifoEvictionPolicy(1)); if (cc.getMemoryMode() == CacheMemoryMode.OFFHEAP_TIERED) cc.setOffHeapMaxMemory(2 * 1024); final IgniteCache<Object, Object> cache = ig.createCache(cc); try { for (int i = 0; i < KEYS; i++) cache.put(key(i), value(i)); assertTrue(GridTestUtils.waitForCondition(new PA() { @Override public boolean apply() { int size = cache.size(CachePeekMode.ONHEAP); int offheapSize = cache.size(CachePeekMode.OFFHEAP); System.out.println("Cache [onHeap=" + size + ", offHeap=" + offheapSize + \']\'); return size <= testsCfg.gridCount() && offheapSize < KEYS; } }, getTestTimeout())); for (int i = 0; i < KEYS; i++) assertEquals(value(i), cache.get(key(i))); } finally { destroyCacheSafe(ig, cc.getName()); } }', 'ground_truth': 'public void _testReadThroughEvictionPolicy() throws Exception { Ignite ig = testedGrid(); CacheConfiguration<Object, Object> cc = variationConfig("eviction"); cc.setEvictionPolicy(new FifoEvictionPolicy(1)); if (cc.getMemoryMode() == CacheMemoryMode.OFFHEAP_TIERED) cc.setOffHeapMaxMemory(2 * 1024); final IgniteCache<Object, Object> cache = ig.createCache(cc); try { for (int i = 0; i < KEYS; i++) cache.put(key(i), value(i)); assertTrue(GridTestUtils.waitForCondition(new PA() { @Override public boolean apply() { int size = cache.size(CachePeekMode.ONHEAP); int offheapSize = cache.size(CachePeekMode.OFFHEAP); System.out.println("Cache [onHeap=" + size + ", offHeap=" + offheapSize + \']\'); return size <= testsCfg.gridCount() && offheapSize < KEYS; } }, 30_000)); for (int i = 0; i < KEYS; i++) assertEquals(value(i), cache.get(key(i))); } finally { destroyCacheSafe(ig, cc.getName()); } }', 'output': 'public void testFifoEvictionPolicy() throws Exception { Ignite ig = testedGrid(); CacheConfiguration<Object, Object> cc = variationConfig("eviction"); cc.setEvictionPolicy(new FifoEvictionPolicy(1)); if (cc.getMemoryMode() == CacheMemoryMode.OFFHEAP_TIERED) cc.setOffHeapMaxMemory(2 * 1024); final IgniteCache<Object, Object> cache = ig.createCache(cc); try { for (int i = 0; i < KEYS; i++) cache.put(key(i), value(i)); assertTrue(GridTestUtils.waitForCondition(new PA() { @Override public boolean apply() { int size = cache.size(CachePeekMode.ONHEAP); int offheapSize = cache.size(CachePeekMode.OFFHEAP); System.out.println("Cache [onHeap=" + size + ", offHeap=" + offheapSize + \']\'); return size <= testsCfg.gridCount() && offheapSize < KEYS; } }, getTestTimeout())); for (int i = 0; i < KEYS; i++) assertEquals(value(i), cache.get(key(i))); } finally { destroyCacheSafe(ig, cc.getName()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Throwable { createTable("create table %s (k int primary key, v set<text>)"); execute("insert into %s (k, v) VALUES (0, {\'f\'})"); flush(); execute("alter table %s drop v"); execute("alter table %s add v int"); }', 'ground_truth': 'public void testDropAddSameType() throws Throwable { createTable("create table %s (k int primary key, v1 timeuuid, v2 set<uuid>, v3 frozen<list<text>>)"); execute("alter table %s drop v1"); execute("alter table %s add v1 timeuuid"); execute("alter table %s drop v2"); execute("alter table %s add v2 set<uuid>"); execute("alter table %s drop v3"); execute("alter table %s add v3 frozen<list<text>>"); }', 'output': 'public void testAddAndDropSet() throws Throwable { createTable("create table %s (k int primary key, v set<text>)"); execute("insert into %s (k, v) VALUES (0, {\'f\'})"); flush(); execute("alter table %s drop v"); execute("alter table %s add v int"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void XXX() { cluster.close(); }', 'ground_truth': 'public static void tearDown() { if (cluster != null) cluster.close(); if (cassandra != null) cassandra.stop(); }', 'output': 'public static void close() { cluster.close(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean XXX() { return wrapped.XXX(); }', 'ground_truth': 'public boolean invokeOnFailure() { return wrapped.invokeOnFailure(); }', 'output': 'public boolean xxx() { return wrapped.xxx(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { ldr = new URLClassLoader(URLS, getClass().getClassLoader()); clientMode = false; setExternalLoader = true; final Ignite ignite0 = startGrid(1); executeContinuousQuery(ignite0.cache("simple")); ListeningTestLogger listeningLogger = new ListeningTestLogger(); log = listeningLogger; LogListener lsnr = LogListener.matches(logStr -> logStr.contains("class org.apache.ignite.IgniteCheckedException: " + "Failed to find class with given class loader for unmarshalling") || logStr.contains("Failed to unmarshal continuous routine handler" )).build(); listeningLogger.registerListener(lsnr); setExternalLoader = false; GridTestUtils.assertThrows(log, () -> startGrid(2), IgniteCheckedException.class, "Failed to start"); assertTrue(lsnr.check()); }', 'ground_truth': 'public void testServerJoinsMissingClassException() throws Exception { clientMode = false; setExternalLoader = true; Ignite ignite0 = startGrid(1); executeContinuousQuery(ignite0.cache(DEFAULT_CACHE_NAME)); ListeningTestLogger listeningLogger = new ListeningTestLogger(); log = listeningLogger; LogListener lsnr = LogListener.matches(logStr -> logStr.contains("class org.apache.ignite.IgniteCheckedException: " + "Failed to find class with given class loader for unmarshalling") || logStr.contains("Failed to unmarshal continuous routine handler" )).build(); listeningLogger.registerListener(lsnr); setExternalLoader = false; GridTestUtils.assertThrows(log, () -> startGrid(2), IgniteCheckedException.class, "Failed to start"); assertTrue(lsnr.check()); }', 'output': 'public void testUnmarshallingWithExternalLoader() throws Exception { ldr = new URLClassLoader(URLS, getClass().getClassLoader()); clientMode = false; setExternalLoader = true; final Ignite ignite0 = startGrid(1); executeContinuousQuery(ignite0.cache("simple")); ListeningTestLogger listeningLogger = new ListeningTestLogger(); log = listeningLogger; LogListener lsnr = LogListener.matches(logStr -> logStr.contains("class org.apache.ignite.IgniteCheckedException: " + "Failed to find class with given class loader for unmarshalling") || logStr.contains("Failed to unmarshal continuous routine handler" )).build(); listeningLogger.registerListener(lsnr); setExternalLoader = false; GridTestUtils.assertThrows(log, () -> startGrid(2), IgniteCheckedException.class, "Failed to start"); assertTrue(lsnr.check()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws IOException { StorageService.instance.takeColumnFamilySnapshot(SchemaKeyspace.NAME, SchemaKeyspace.KEYSPACES, "cf_snapshot"); }', 'ground_truth': 'public void testTableSnapshot() throws IOException { StorageService.instance.takeTableSnapshot(SchemaKeyspace.NAME, SchemaKeyspace.KEYSPACES, "cf_snapshot"); }', 'output': 'public void takeColumnFamilySnapshot() throws IOException { StorageService.instance.takeColumnFamilySnapshot(SchemaKeyspace.NAME, SchemaKeyspace.KEYSPACES, "cf_snapshot"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private long XXX(AtomicLong[] arr) { long min = Long.MAX_VALUE; for (AtomicLong l : arr) { final long i = l.get(); if (i < min) { min = i; } } return min; }', 'ground_truth': 'private long min(AtomicLong[] arr) { long min = Long.MAX_VALUE; for (AtomicLong l : arr) { final long i = l.get(); if (i < min) { min = i; } } return min; }', 'output': 'private long getMin(AtomicLong[] arr) { long min = Long.MAX_VALUE; for (AtomicLong l : arr) { final long i = l.get(); if (i < min) { min = i; } } return min; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public KSMetaData XXX(Tables tables, Types types, Functions functions) { return new KSMetaData(name, strategyClass, strategyOptions, durableWrites, tables, types, functions); } public KSMetaData XXX(Tables tables)\n    { { return new KSMetaData(name, strategyClass, strategyOptions, durableWrites, tables, types, functions); }', 'ground_truth': 'public static KSMetaData create(String name, KeyspaceParams params, Tables tables) { return new KSMetaData(name, params, tables, Types.none(), Functions.none()); }', 'output': 'public KSMetaData createKSMetaData(Tables tables, Types types, Functions functions) { return new KSMetaData(name, strategyClass, strategyOptions, durableWrites, tables, types, functions); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { String[] args = new String[] {"--googleApiTrace=Dataflow.V1b3#TraceDestination"}; DataflowPipelineOptions options = PipelineOptionsFactory.fromArgs(args).as(DataflowPipelineOptions.class); options.setGcpCredential(new TestCredential()); assertNotNull(options.getGoogleApiTrace()); Get getRequest = options.getDataflowClient().v1b3().projects().jobs().get("testProjectId", "testJobId"); assertEquals("TraceDestination", getRequest.get("$trace")); Create createRequest = options.getDataflowClient().v1b3().projects().jobs().create("testProjectId", null); assertEquals("TraceDestination", createRequest.get("$trace")); }', 'ground_truth': 'public void testMatchingAllDataflowCalls() throws Exception { String[] args = new String[] {"--googleApiTrace=Dataflow#TraceDestination"}; DataflowPipelineOptions options = PipelineOptionsFactory.fromArgs(args).as(DataflowPipelineOptions.class); options.setGcpCredential(new TestCredential()); assertNotNull(options.getGoogleApiTrace()); Get getRequest = options.getDataflowClient().projects().jobs().get("testProjectId", "testJobId"); assertEquals("TraceDestination", getRequest.get("$trace")); Create createRequest = options.getDataflowClient().projects().jobs().create("testProjectId", null); assertEquals("TraceDestination", createRequest.get("$trace")); }', 'output': 'public void testGoogleApiTrace() throws Exception { String[] args = new String[] {"--googleApiTrace=Dataflow.V1b3#TraceDestination"}; DataflowPipelineOptions options = PipelineOptionsFactory.fromArgs(args).as(DataflowPipelineOptions.class); options.setGcpCredential(new TestCredential()); assertNotNull(options.getGoogleApiTrace()); Get getRequest = options.getDataflowClient().v1b3().projects().jobs().get("testProjectId", "testJobId"); assertEquals("TraceDestination", getRequest.get("$trace")); Create createRequest = options.getDataflowClient().v1b3().projects().jobs().create("testProjectId", null); assertEquals("TraceDestination", createRequest.get("$trace")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { runner.invokeClassAsTool("org.apache.cassandra.tools.SSTableExport",findOneSSTable("legacy_sstables", "legacy_ma_simple")) .waitAndAssertOnCleanExit(); assertNoUnexpectedThreadsStarted(null, OPTIONAL_THREADS_WITH_SCHEMA); assertSchemaNotLoaded(); assertCLSMNotLoaded(); assertSystemKSNotLoaded(); assertServerNotLoaded(); }', 'ground_truth': 'private void assertPostTestEnv() { assertNoUnexpectedThreadsStarted(null, OPTIONAL_THREADS_WITH_SCHEMA); assertCLSMNotLoaded(); assertSystemKSNotLoaded(); assertServerNotLoaded(); }', 'output': 'public void testSSTableExport() throws Exception { runner.invokeClassAsTool("org.apache.cassandra.tools.SSTableExport",findOneSSTable("legacy_sstables", "legacy_ma_simple")) .waitAndAssertOnCleanExit(); assertNoUnexpectedThreadsStarted(null, OPTIONAL_THREADS_WITH_SCHEMA); assertSchemaNotLoaded(); assertCLSMNotLoaded(); assertSystemKSNotLoaded(); assertServerNotLoaded(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { ldr = new URLClassLoader(URLS, getClass().getClassLoader()); clientMode = false; setExternalLoader = true; final Ignite ignite0 = startGrid(1); executeContinuousQuery(ignite0.cache("simple")); log = new GridStringLogger(); clientMode = true; setExternalLoader = false; startGrid(2); String logStr = log.toString(); assertTrue(logStr.contains("Failed to unmarshal continuous query remote filter on client node. " + "Can be ignored.") || logStr.contains("Failed to unmarshal continuous routine handler")); }', 'ground_truth': 'public void testClientJoinsMissingClassWarning() throws Exception { clientMode = false; setExternalLoader = true; Ignite ignite0 = startGrid(1); executeContinuousQuery(ignite0.cache(DEFAULT_CACHE_NAME)); log = new GridStringLogger(); clientMode = true; setExternalLoader = false; startGrid(2); String logStr = log.toString(); assertTrue(logStr.contains("Failed to unmarshal continuous query remote filter on client node. " + "Can be ignored.") || logStr.contains("Failed to unmarshal continuous routine handler")); }', 'output': 'public void testContinuousQueryOnClient() throws Exception { ldr = new URLClassLoader(URLS, getClass().getClassLoader()); clientMode = false; setExternalLoader = true; final Ignite ignite0 = startGrid(1); executeContinuousQuery(ignite0.cache("simple")); log = new GridStringLogger(); clientMode = true; setExternalLoader = false; startGrid(2); String logStr = log.toString(); assertTrue(logStr.contains("Failed to unmarshal continuous query remote filter on client node. " + "Can be ignored.") || logStr.contains("Failed to unmarshal continuous routine handler")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public TrackActionResponse XXX(HostRequest request) throws AmbariException { return null; } public TrackActionResponse XXXComponent(ServiceComponentHostRequest request) throws AmbariException { return null; }', 'ground_truth': 'public TrackActionResponse deleteServices(Set<ServiceRequest> request) throws AmbariException { return null; }', 'output': 'public TrackActionResponse trackAction(HostRequest request) throws AmbariException { return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public CompletableFuture<Object> XXX(NbCodeLanguageClient client, String command, List<Object> arguments) { if (arguments.size() > 2) { String uri = gson.fromJson(gson.toJson(arguments.get(0)), String.class); int offset = gson.fromJson(gson.toJson(arguments.get(1)), Integer.class); List<QuickPickItem> fields = Arrays.asList(gson.fromJson(gson.toJson(arguments.get(2)), QuickPickItem[].class)); if (fields.isEmpty()) { generate(client, uri, offset, fields); } else { client.showQuickPick(new ShowQuickPickParams(Bundle.DN_SelectToString(), true, fields)).thenAccept(selected -> { if (selected != null) { generate(client, uri, offset, selected); } }); } } else { client.logMessage(new MessageParams(MessageType.Error, String.format("Illegal number of arguments received for command: %s", command))); } return CompletableFuture.completedFuture(true); }', 'ground_truth': 'public CompletableFuture<CodeAction> resolve(NbCodeLanguageClient client, CodeAction codeAction, Object data) { CompletableFuture<CodeAction> future = new CompletableFuture<>(); try { String uri = ((JsonObject) data).getAsJsonPrimitive(URI).getAsString(); int offset = ((JsonObject) data).getAsJsonPrimitive(OFFSET).getAsInt(); List<QuickPickItem> fields = Arrays.asList(gson.fromJson(((JsonObject) data).get(FIELDS), QuickPickItem[].class)); if (fields.isEmpty()) { WorkspaceEdit edit = generate(uri, offset, fields); if (edit != null) { codeAction.setEdit(edit); } future.complete(codeAction); } else { client.showQuickPick(new ShowQuickPickParams(Bundle.DN_SelectToString(), true, fields)).thenAccept(selected -> { try { if (selected != null) { WorkspaceEdit edit = generate(uri, offset, fields); if (edit != null) { codeAction.setEdit(edit); } } future.complete(codeAction); } catch (IOException | IllegalArgumentException ex) { future.completeExceptionally(ex); } }); } } catch (JsonSyntaxException | IOException | IllegalArgumentException ex) { future.completeExceptionally(ex); } return future; }', 'output': 'public CompletableFuture<Object> showQuickPick(NbCodeLanguageClient client, String command, List<Object> arguments) { if (arguments.size() > 2) { String uri = gson.fromJson(gson.toJson(arguments.get(0)), String.class); int offset = gson.fromJson(gson.toJson(arguments.get(1)), Integer.class); List<QuickPickItem> fields = Arrays.asList(gson.fromJson(gson.toJson(arguments.get(2)), QuickPickItem[].class)); if (fields.isEmpty()) { generate(client, uri, offset, fields); } else { client.showQuickPick(new ShowQuickPickParams(Bundle.DN_SelectToString(), true, fields)).thenAccept(selected -> { if (selected != null) { generate(client, uri, offset, selected); } }); } } else { client.logMessage(new MessageParams(MessageType.Error, String.format("Illegal number of arguments received for command: %s", command))); } return CompletableFuture.completedFuture(true); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ColumnFamily XXX() { return emptyColumnFamily; }', 'ground_truth': 'public ColumnFamily getColumnFamily() throws IOException { return reader == null ? null : reader.getColumnFamily(); }', 'output': 'public ColumnFamily getColumnFamily() { return emptyColumnFamily; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public synchronized void XXX(Iterable<? extends WindowedValue<?>> finishedElements) { for (WindowedValue<?> finishedElement : finishedElements) { pendingElements.remove(finishedElement); } }', 'ground_truth': 'private synchronized void removePendingElements( Iterable<? extends WindowedValue<?>> finishedElements) { for (WindowedValue<?> finishedElement : finishedElements) { pendingElements.remove(finishedElement); } }', 'output': 'public synchronized void removeFinishedElements(Iterable<? extends WindowedValue<?>> finishedElements) { for (WindowedValue<?> finishedElement : finishedElements) { pendingElements.remove(finishedElement); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean XXX(Expression exp) { if (exp instanceof PropertyExpression) { PropertyExpression pexp = (PropertyExpression) exp; Expression objectExpression = pexp.getObjectExpression(); if (objectExpression instanceof ClassExpression) { ClassNode type = objectExpression.getType(); if (Traits.isTrait(type) && "super".equals(pexp.getPropertyAsString())) { return true; } } } return false; }', 'ground_truth': 'private ClassNode getTraitSuperTarget(final Expression exp) { if (exp instanceof PropertyExpression) { PropertyExpression pexp = (PropertyExpression) exp; Expression objExp = pexp.getObjectExpression(); if (objExp instanceof ClassExpression) { ClassNode type = objExp.getType(); if (Traits.isTrait(type) && "super".equals(pexp.getPropertyAsString())) { return type; } } } return null; }', 'output': 'private boolean isSuperProperty(Expression exp) { if (exp instanceof PropertyExpression) { PropertyExpression pexp = (PropertyExpression) exp; Expression objectExpression = pexp.getObjectExpression(); if (objectExpression instanceof ClassExpression) { ClassNode type = objectExpression.getType(); if (Traits.isTrait(type) && "super".equals(pexp.getPropertyAsString())) { return true; } } } return false; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public int XXX(String table) { int total = 0; for (int repFactor : datacenters.get(table).values()) total += repFactor; return total; }', 'ground_truth': 'public int getReplicationFactor(String table) { int total = 0; for (int repFactor : datacenters.get(table).values()) total += repFactor; return total; }', 'output': 'public int getTotalReplicationFactor(String table) { int total = 0; for (int repFactor : datacenters.get(table).values()) total += repFactor; return total; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public int XXX(ByteBuffer o1, ByteBuffer o2) { if (!o1.hasRemaining() || !o2.hasRemaining()) return o1.hasRemaining() ? 1 : o2.hasRemaining() ? -1 : 0; byte b1 = o1.get(o1.position()); byte b2 = o2.get(o2.position()); if (b1 == 0) return b2 == 0 ? 0 : -1; return b2 == 0 ? 1 : 0; }', 'ground_truth': 'public int compareCustom(ByteBuffer o1, ByteBuffer o2) { if (!o1.hasRemaining() || !o2.hasRemaining()) return o1.hasRemaining() ? 1 : o2.hasRemaining() ? -1 : 0; byte b1 = o1.get(o1.position()); byte b2 = o2.get(o2.position()); if (b1 == 0) return b2 == 0 ? 0 : -1; return b2 == 0 ? 1 : 0; }', 'output': 'public int compare(ByteBuffer o1, ByteBuffer o2) { if (!o1.hasRemaining() || !o2.hasRemaining()) return o1.hasRemaining() ? 1 : o2.hasRemaining() ? -1 : 0; byte b1 = o1.get(o1.position()); byte b2 = o2.get(o2.position()); if (b1 == 0) return b2 == 0 ? 0 : -1; return b2 == 0 ? 1 : 0; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void XXX() { super.XXX(); entityLink.setEnabled(true); syncWithInput(); }', 'ground_truth': 'protected void onInitializeNotEditable() { super.onInitializeNotEditable(); entityLink.setEnabled(false); syncWithInput(); }', 'output': 'protected void enableEntityLink() { super.enableEntityLink(); entityLink.setEnabled(true); syncWithInput(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private <T> T XXX(int colIdx, Class<T> cls) throws SQLException { ensureNotClosed(); ensureHasCurrentRow(); try { Object val = curr.get(colIdx - 1); wasNull = val == null; if (val == null) return null; else if (cls == String.class) return (T)String.valueOf(val); else return cls.cast(val); } catch (IndexOutOfBoundsException ignored) { throw new SQLException("Invalid column index: " + colIdx); } catch (ClassCastException ignored) { throw new SQLException("Cannot convert to " + cls.getSimpleName().toLowerCase(), SqlStateCode.CONVERSION_FAILED); } }', 'ground_truth': 'private Object getValue(int colIdx) throws SQLException { ensureNotClosed(); ensureHasCurrentRow(); try { Object val = curr.get(colIdx - 1); wasNull = val == null; return val; } catch (IndexOutOfBoundsException e) { throw new SQLException("Invalid column index: " + colIdx, e); } }', 'output': 'private <T> T getValue(int colIdx, Class<T> cls) throws SQLException { ensureNotClosed(); ensureHasCurrentRow(); try { Object val = curr.get(colIdx - 1); wasNull = val == null; if (val == null) return null; else if (cls == String.class) return (T)String.valueOf(val); else return cls.cast(val); } catch (IndexOutOfBoundsException ignored) { throw new SQLException("Invalid column index: " + colIdx); } catch (ClassCastException ignored) { throw new SQLException("Cannot convert to " + cls.getSimpleName().toLowerCase(), SqlStateCode.CONVERSION_FAILED); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<? extends BoundedSource<String>> XXX( long desiredBundleSizeBytes, PipelineOptions options) throws Exception { return null; }', 'ground_truth': 'public List<? extends BoundedSource<String>> split( long desiredBundleSizeBytes, PipelineOptions options) throws Exception { return null; }', 'output': 'public List<? extends BoundedSource<String>> getSources( long desiredBundleSizeBytes, PipelineOptions options) throws Exception { return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private IgfsMode XXX(String[] ss, int ix) { if (ss.length <= ix) return null; else { String s = ss[ix]; return s.isEmpty() ? null : IgfsMode.valueOf(s); } }', 'ground_truth': 'private IgfsMode parseIgfsMode(String[] ss, int ix) { if (ss.length <= ix) return null; else { String s = ss[ix]; return s.isEmpty() ? null : IgfsMode.valueOf(s); } }', 'output': 'private IgfsMode igfsMode(String[] ss, int ix) { if (ss.length <= ix) return null; else { String s = ss[ix]; return s.isEmpty() ? null : IgfsMode.valueOf(s); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { startGrids(3); awaitPartitionMapExchange(); initClient(getClientConfiguration(0, 1, 2), 0, 1, 2); testAffinityAwareness(true); stopAllGrids(); for (int i = 0; i < channels.length; i++) channels[i] = null; startGrids(2); awaitPartitionMapExchange(); try { client.cache(REPL_CACHE_NAME).put(0, 0); } catch (Exception expected) { } initDefaultChannel(); awaitChannelsInit(0, 1); testAffinityAwareness(true); }', 'ground_truth': 'public void testPartitionAwarenessOnClusterRestart() throws Exception { startGrids(3); awaitPartitionMapExchange(); initClient(getClientConfiguration(0, 1, 2), 0, 1, 2); testPartitionAwareness(true); stopAllGrids(); for (int i = 0; i < channels.length; i++) channels[i] = null; startGrids(2); awaitPartitionMapExchange(); try { client.cache(REPL_CACHE_NAME).put(0, 0); } catch (Exception expected) { } initDefaultChannel(); awaitChannelsInit(0, 1); testPartitionAwareness(true); }', 'output': 'public void testAffinityAwareness() throws Exception { startGrids(3); awaitPartitionMapExchange(); initClient(getClientConfiguration(0, 1, 2), 0, 1, 2); testAffinityAwareness(true); stopAllGrids(); for (int i = 0; i < channels.length; i++) channels[i] = null; startGrids(2); awaitPartitionMapExchange(); try { client.cache(REPL_CACHE_NAME).put(0, 0); } catch (Exception expected) { } initDefaultChannel(); awaitChannelsInit(0, 1); testAffinityAwareness(true); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public LinkAndLabel XXX( IsisAppCommonContext commonContext, String named, ManagedAction managedAction) { val serviceModel = EntityModel.ofAdapter(commonContext, managedAction.getOwner()); val actionLinkFactory = new MenuActionLinkFactory( PageAbstract.ID_MENU_LINK, serviceModel); return LinkAndLabel.of( model->actionLinkFactory.XXXLink(model.getObjectAction(), named).getUiComponent(), named, serviceModel, managedAction.getAction()); }', 'ground_truth': 'public LinkAndLabel newActionLink( String named, ManagedAction managedAction) { val serviceModel = EntityModel.ofAdapter(commonContext, managedAction.getOwner()); val actionLinkFactory = new MenuActionLinkFactory( PageAbstract.ID_MENU_LINK, serviceModel); return LinkAndLabel.of( model->actionLinkFactory.newActionLink(model.getObjectAction(), named).getUiComponent(), named, serviceModel, managedAction.getAction()); }', 'output': 'public LinkAndLabel getMenuLink( IsisAppCommonContext commonContext, String named, ManagedAction managedAction) { val serviceModel = EntityModel.ofAdapter(commonContext, managedAction.getOwner()); val actionLinkFactory = new MenuActionLinkFactory( PageAbstract.ID_MENU_LINK, serviceModel); return LinkAndLabel.of( actionLinkFactory.getMenuLink(managedAction.getObjectAction(), named).getUiComponent(), named, serviceModel, managedAction.getAction()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { VariableExpression intExpression = new VariableExpression("foo", ClassHelper.DYNAMIC_TYPE); assertTrue(intExpression.isDynamicTyped()); }', 'ground_truth': 'public void testIsDynamicTyped_DYNAMIC_TYPE() { VariableExpression intExpression = new VariableExpression("foo", ClassHelper.DYNAMIC_TYPE); assertTrue(intExpression.isDynamicTyped()); }', 'output': 'public void testIsDynamicTyped() { VariableExpression intExpression = new VariableExpression("foo", ClassHelper.INT_TYPE); assertTrue(intExpression.isDynamicTyped()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(final InetAddressAndPort address, List<InetAddressAndPort> addresses) { if (addresses.size() < 2) return; subsnitch.sortByProximity(address, addresses); HashMap<InetAddressAndPort, Double> scores = this.scores; ArrayList<Double> subsnitchOrderedScores = new ArrayList<>(addresses.size()); for (InetAddressAndPort inet : addresses) { Double score = scores.get(inet); if (score == null) score = 0.0; subsnitchOrderedScores.add(score); } ArrayList<Double> sortedScores = new ArrayList<>(subsnitchOrderedScores); Collections.sort(sortedScores); Iterator<Double> sortedScoreIterator = sortedScores.iterator(); for (Double subsnitchScore : subsnitchOrderedScores) { if (subsnitchScore > (sortedScoreIterator.next() * (1.0 + dynamicBadnessThreshold))) { sortByProximityWithScore(address, addresses); return; } } }', 'ground_truth': 'private <C extends ReplicaCollection<? extends C>> C sortedByProximityWithBadness(final InetAddressAndPort address, C replicas) { if (replicas.size() < 2) return replicas; replicas = subsnitch.sortedByProximity(address, replicas); HashMap<InetAddressAndPort, Double> scores = this.scores; ArrayList<Double> subsnitchOrderedScores = new ArrayList<>(replicas.size()); for (Replica replica : replicas) { Double score = scores.get(replica.endpoint()); if (score == null) score = 0.0; subsnitchOrderedScores.add(score); } ArrayList<Double> sortedScores = new ArrayList<>(subsnitchOrderedScores); Collections.sort(sortedScores); Iterator<Double> sortedScoreIterator = sortedScores.iterator(); for (Double subsnitchScore : subsnitchOrderedScores) { if (subsnitchScore > (sortedScoreIterator.next() * (1.0 + dynamicBadnessThreshold))) { return sortedByProximityWithScore(address, replicas); } } return replicas; }', 'output': 'private void sortByProximity(final InetAddressAndPort address, List<InetAddressAndPort> addresses) { if (addresses.size() < 2) return; subsnitch.sortByProximity(address, addresses); HashMap<InetAddressAndPort, Double> scores = this.scores; ArrayList<Double> subsnitchOrderedScores = new ArrayList<>(addresses.size()); for (InetAddressAndPort inet : addresses) { Double score = scores.get(inet); if (score == null) score = 0.0; subsnitchOrderedScores.add(score); } ArrayList<Double> sortedScores = new ArrayList<>(subsnitchOrderedScores); Collections.sort(sortedScores); Iterator<Double> sortedScoreIterator = sortedScores.iterator(); for (Double subsnitchScore : subsnitchOrderedScores) { if (subsnitchScore > (sortedScoreIterator.next() * (1.0 + dynamicBadnessThreshold))) { sortByProximityWithScore(address, addresses); return; } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { PCollection<EmptyClass> input = p.apply(Create.of(1, 2, 3)).apply(ParDo.of(new EmptyClassDoFn())); thrown.expect(IllegalStateException.class); thrown.expectMessage(not(containsString("erasure"))); thrown.expectMessage(not(containsString("see TupleTag Javadoc"))); thrown.expectMessage(containsString("Building a Coder using a registered CoderFactory failed")); thrown.expectMessage( containsString("Building a Coder from the @DefaultCoder annotation failed")); thrown.expectMessage(containsString("Building a Coder from the fallback CoderProvider failed")); input.getCoder(); }', 'ground_truth': 'public void testParDoWithNoOutputsErrorDoesNotMentionTupleTag() { PCollection<EmptyClass> input = p.apply(Create.of(1, 2, 3)).apply(ParDo.of(new EmptyClassDoFn())); thrown.expect(IllegalStateException.class); thrown.expectMessage(not(containsString("erasure"))); thrown.expectMessage(not(containsString("see TupleTag Javadoc"))); thrown.expectMessage(containsString("Building a Coder using a registered CoderFactory failed")); thrown.expectMessage( containsString("Building a Coder from the @DefaultCoder annotation failed")); thrown.expectMessage(containsString("Building a Coder from the fallback CoderProvider failed")); input.getCoder(); }', 'output': 'public void testUnregisteredCoder() { PCollection<EmptyClass> input = p.apply(Create.of(1, 2, 3)).apply(ParDo.of(new EmptyClassDoFn())); thrown.expect(IllegalStateException.class); thrown.expectMessage(not(containsString("erasure"))); thrown.expectMessage(not(containsString("see TupleTag Javadoc"))); thrown.expectMessage(containsString("Building a Coder using a registered CoderFactory failed")); thrown.expectMessage( containsString("Building a Coder from the @DefaultCoder annotation failed")); thrown.expectMessage(containsString("Building a Coder from the fallback CoderProvider failed")); input.getCoder(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(int size) { bucketOffsets = new long[size]; long last = 1; bucketOffsets[0] = last; for (int i = 1; i < size; i++) { long next = Math.round(last * 1.2); if (next == last) next++; bucketOffsets[i] = next; last = next; } }', 'ground_truth': 'private static long[] newOffsets(int size) { long[] result = new long[size]; long last = 1; result[0] = last; for (int i = 1; i < size; i++) { long next = Math.round(last * 1.2); if (next == last) next++; result[i] = next; last = next; } return result; }', 'output': 'private void setBucketOffsets(int size) { bucketOffsets = new long[size]; long last = 1; bucketOffsets[0] = last; for (int i = 1; i < size; i++) { long next = Math.round(last * 1.2); if (next == last) next++; bucketOffsets[i] = next; last = next; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Result XXX(ProjectArtifactsQuery.Filter query) { ProjectActionContext ctx = query.getBuildContext(); if (ctx != null) { if (ctx.getProjectAction() != null) { switch (ctx.getProjectAction()) { case ActionProvider.COMMAND_BUILD: case ActionProvider.COMMAND_REBUILD: case ActionProvider.COMMAND_RUN: case ActionProvider.COMMAND_DEBUG: case ActionProvider.COMMAND_RUN_SINGLE: case ActionProvider.COMMAND_DEBUG_SINGLE: case ActionProvider.COMMAND_TEST: case ActionProvider.COMMAND_TEST_SINGLE: case ActionProvider.COMMAND_DEBUG_STEP_INTO: break; default: return null; } } if (ctx.getProfiles() != null && !ctx.getProfiles().isEmpty()) { LOG.log(Level.WARNING, "Custom action profiles are not supported yet by Maven projects"); } if (ctx.getConfiguration() != null) { LOG.log(Level.WARNING, "Custom action configurations are not supported yet by Maven projects"); } if (ctx.getProperties() != null && !ctx.getProperties().isEmpty()) { LOG.log(Level.WARNING, "Custom action properties are not supported yet by Maven projects"); } } return new Res(project, query); }', 'ground_truth': 'public Res evaluate(ProjectArtifactsQuery.Filter query) { ProjectActionContext ctx = query.getBuildContext(); if (ctx != null) { if (ctx.getProjectAction() != null) { switch (ctx.getProjectAction()) { case ActionProvider.COMMAND_BUILD: case ActionProvider.COMMAND_REBUILD: case ActionProvider.COMMAND_RUN: case ActionProvider.COMMAND_DEBUG: case ActionProvider.COMMAND_RUN_SINGLE: case ActionProvider.COMMAND_DEBUG_SINGLE: case ActionProvider.COMMAND_TEST: case ActionProvider.COMMAND_TEST_SINGLE: case ActionProvider.COMMAND_DEBUG_STEP_INTO: break; default: return null; } } if (ctx.getProfiles() != null && !ctx.getProfiles().isEmpty()) { LOG.log(Level.WARNING, "Custom action profiles are not supported yet by Maven projects"); } if (ctx.getConfiguration() != null) { LOG.log(Level.WARNING, "Custom action configurations are not supported yet by Maven projects"); } if (ctx.getProperties() != null && !ctx.getProperties().isEmpty()) { LOG.log(Level.WARNING, "Custom action properties are not supported yet by Maven projects"); } } return new Res(project, query); }', 'output': 'public Result apply(ProjectArtifactsQuery.Filter query) { ProjectActionContext ctx = query.getBuildContext(); if (ctx != null) { if (ctx.getProjectAction() != null) { switch (ctx.getProjectAction()) { case ActionProvider.COMMAND_BUILD: case ActionProvider.COMMAND_REBUILD: case ActionProvider.COMMAND_RUN: case ActionProvider.COMMAND_DEBUG: case ActionProvider.COMMAND_RUN_SINGLE: case ActionProvider.COMMAND_DEBUG_SINGLE: case ActionProvider.COMMAND_TEST: case ActionProvider.COMMAND_TEST_SINGLE: case ActionProvider.COMMAND_DEBUG_STEP_INTO: break; default: return null; } } if (ctx.getProfiles() != null && !ctx.getProfiles().isEmpty()) { LOG.log(Level.WARNING, "Custom action profiles are not supported yet by Maven projects"); } if (ctx.getConfiguration() != null) { LOG.log(Level.WARNING, "Custom action configurations are not supported yet by Maven projects"); } if (ctx.getProperties() != null && !ctx.getProperties().isEmpty()) { LOG.log(Level.WARNING, "Custom action properties are not supported yet by Maven projects"); } } return new Res(project, query); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static BinaryExpression XXX( ExpressionType binaryType, Expression expression0, Expression expression1, boolean liftToNull, Method method) { throw Extensions.todo(); } public static BinaryExpression XXX(\n        ExpressionType binaryType,\n        Expression expression0,\n        Expression expression1,\n        boolean liftToNull,\n        Method method,\n        LambdaExpression lambdaExpression)\n    { ExpressionType binaryType, Expression expression0, Expression expression1, boolean liftToNull, Method method, LambdaExpression lambdaExpression) { throw Extensions.todo(); }', 'ground_truth': 'public static BinaryExpression power( Expression left, Expression right, Method method) { throw Extensions.todo(); }', 'output': 'public static BinaryExpression binaryExpression( ExpressionType binaryType, Expression expression0, Expression expression1, boolean liftToNull, Method method) { throw Extensions.todo(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void XXX(Writer writer) { if (writer != null) { try { writer.close(); } catch (IOException e) { LOG.warning("Caught exception closing Writer: " + e); } } }', 'ground_truth': 'private static void closeWithWarning(Closeable c) { if (c != null) { try { c.close(); } catch (IOException e) { LOG.warning("Caught exception during close(): " + e); } } }', 'output': 'private static void closeWriter(Writer writer) { if (writer != null) { try { writer.close(); } catch (IOException e) { LOG.warning("Caught exception closing Writer: " + e); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static KSMetaData XXX() { return KSMetaData.create(NAME, KeyspaceParams.simple(2), Tables.of(Sessions, Events)); }', 'ground_truth': 'public static KeyspaceMetadata metadata() { return KeyspaceMetadata.create(NAME, KeyspaceParams.simple(2), Tables.of(Sessions, Events)); }', 'output': 'public static KSMetaData create() { return KSMetaData.create(NAME, KeyspaceParams.simple(2), Tables.of(Sessions, Events)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public TestCase XXX(Semver from) { return upgrades(from, CURRENT); }', 'ground_truth': 'public TestCase upgradesToCurrentFrom(Semver from) { return upgradesTo(from, CURRENT); }', 'output': 'public TestCase upgrade(Semver from) { return upgrades(from, CURRENT); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected Component XXX(final String id) { final MarkupContainer scalarIfCompact = new WebMarkupContainer(id); createDownloadLink(ID_SCALAR_IF_COMPACT_DOWNLOAD, scalarIfCompact); return scalarIfCompact; }', 'ground_truth': 'protected Component createComponentForOutput(final String id) { final MarkupContainer scalarIfCompact = new WebMarkupContainer(id); createDownloadLink(ID_SCALAR_IF_COMPACT_DOWNLOAD, scalarIfCompact); return scalarIfCompact; }', 'output': 'protected Component getScalarIfCompactComponent(final String id) { final MarkupContainer scalarIfCompact = new WebMarkupContainer(id); createDownloadLink(ID_SCALAR_IF_COMPACT_DOWNLOAD, scalarIfCompact); return scalarIfCompact; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { for (int k = 0; k < 10; k++) { log.info("Iteration: " + k); clientFlagGlobal = false; final int START_NODES = 5; final int JOIN_NODES = 5; startGrids(START_NODES); ignite(0).createCache(new CacheConfiguration<>(DEFAULT_CACHE_NAME)); final AtomicInteger startIdx = new AtomicInteger(START_NODES); final CyclicBarrier barrier = new CyclicBarrier(JOIN_NODES + 1); clientFlagGlobal = true; IgniteInternalFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() { @Override public Object call() throws Exception { int idx = startIdx.getAndIncrement(); Thread.currentThread().setName("start-thread-" + idx); barrier.await(); Ignite ignite = startGrid(idx); assertTrue(ignite.configuration().isClientMode()); log.info("Started node: " + ignite.name()); IgniteCache<Object, Object> cache = ignite.getOrCreateCache(DEFAULT_CACHE_NAME); for (int i = 0; i < 10; i++) { ContinuousQuery<Object, Object> qry = new ContinuousQuery<>(); qry.setLocalListener(new CacheEntryUpdatedListener<Object, Object>() { @Override public void onUpdated(Iterable<CacheEntryEvent<?, ?>> evts) { } }); cache.query(qry); } return null; } }, JOIN_NODES, "start-thread"); barrier.await(); U.sleep(ThreadLocalRandom.current().nextInt(100, 500)); for (int i = 0; i < START_NODES - 1; i++) { GridTestUtils.invoke(ignite(i).configuration().getDiscoverySpi(), "simulateNodeFailure"); stopGrid(i); } fut.get(); stopAllGrids(); } }', 'ground_truth': 'public void testClientContinuousQueryCoordinatorStop() throws Exception { fail("https: for (int k = 0; k < 10; k++) { log.info("Iteration: " + k); clientFlagGlobal = false; final int START_NODES = 5; final int JOIN_NODES = 5; startGrids(START_NODES); ignite(0).createCache(new CacheConfiguration<>(DEFAULT_CACHE_NAME)); final AtomicInteger startIdx = new AtomicInteger(START_NODES); final CyclicBarrier barrier = new CyclicBarrier(JOIN_NODES + 1); clientFlagGlobal = true; IgniteInternalFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() { @Override public Object call() throws Exception { int idx = startIdx.getAndIncrement(); Thread.currentThread().setName("start-thread-" + idx); barrier.await(); Ignite ignite = startGrid(idx); assertTrue(ignite.configuration().isClientMode()); log.info("Started node: " + ignite.name()); IgniteCache<Object, Object> cache = ignite.getOrCreateCache(DEFAULT_CACHE_NAME); for (int i = 0; i < 10; i++) { ContinuousQuery<Object, Object> qry = new ContinuousQuery<>(); qry.setLocalListener(new CacheEntryUpdatedListener<Object, Object>() { @Override public void onUpdated(Iterable<CacheEntryEvent<?, ?>> evts) { } }); cache.query(qry); } return null; } }, JOIN_NODES, "start-thread"); barrier.await(); U.sleep(ThreadLocalRandom.current().nextInt(100, 500)); for (int i = 0; i < START_NODES - 1; i++) { GridTestUtils.invoke(ignite(i).configuration().getDiscoverySpi(), "simulateNodeFailure"); stopGrid(i); } fut.get(); stopAllGrids(); } }', 'output': 'public void testContinuousQueryOnStartup() throws Exception { for (int k = 0; k < 10; k++) { log.info("Iteration: " + k); clientFlagGlobal = false; final int START_NODES = 5; final int JOIN_NODES = 5; startGrids(START_NODES); ignite(0).createCache(new CacheConfiguration<>(DEFAULT_CACHE_NAME)); final AtomicInteger startIdx = new AtomicInteger(START_NODES); final CyclicBarrier barrier = new CyclicBarrier(JOIN_NODES + 1); clientFlagGlobal = true; IgniteInternalFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() { @Override public Object call() throws Exception { int idx = startIdx.getAndIncrement(); Thread.currentThread().setName("start-thread-" + idx); barrier.await(); Ignite ignite = startGrid(idx); assertTrue(ignite.configuration().isClientMode()); log.info("Started node: " + ignite.name()); IgniteCache<Object, Object> cache = ignite.getOrCreateCache(DEFAULT_CACHE_NAME); for (int i = 0; i < 10; i++) { ContinuousQuery<Object, Object> qry = new ContinuousQuery<>(); qry.setLocalListener(new CacheEntryUpdatedListener<Object, Object>() { @Override public void onUpdated(Iterable<CacheEntryEvent<?, ?>> evts) { } }); cache.query(qry); } return null; } }, JOIN_NODES, "start-thread"); barrier.await(); U.sleep(ThreadLocalRandom.current().nextInt(100, 500)); for (int i = 0; i < START_NODES - 1; i++) { GridTestUtils.invoke(ignite(i).configuration().getDiscoverySpi(), "simulateNodeFailure"); stopGrid(i); } fut.get(); stopAllGrids(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX() { if (null == passiveMap) { String entity = hostStateEntity.getPassiveState(); if (null == entity) { passiveMap = new HashMap<Long, PassiveState>(); } else { try { passiveMap = gson.fromJson(entity, passiveMapType); } catch (Exception e) { passiveMap = new HashMap<Long, PassiveState>(); } } } }', 'ground_truth': 'private void ensureMaintMap() { if (null == maintMap) { String entity = hostStateEntity.getMaintenanceState(); if (null == entity) { maintMap = new HashMap<Long, MaintenanceState>(); } else { try { maintMap = gson.fromJson(entity, maintMapType); } catch (Exception e) { maintMap = new HashMap<Long, MaintenanceState>(); } } } }', 'output': 'private void updatePassiveMap() { if (null == passiveMap) { String entity = hostStateEntity.getPassiveState(); if (null == entity) { passiveMap = new HashMap<Long, PassiveState>(); } else { try { passiveMap = gson.fromJson(entity, passiveMapType); } catch (Exception e) { passiveMap = new HashMap<Long, PassiveState>(); } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String XXX() { return metadata.name; }', 'ground_truth': 'public String table() { return metadata.name; }', 'output': 'public String getName() { return metadata.name; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected CacheConfiguration[] XXX(String gridName) { CacheConfiguration cacheCfg = defaultCacheConfiguration(); cacheCfg.setName("partitioned"); cacheCfg.setCacheMode(PARTITIONED); cacheCfg.setNearConfiguration(null); cacheCfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC); cacheCfg.setAffinityMapper(new IgfsGroupDataBlocksKeyMapper(GRP_SIZE)); cacheCfg.setBackups(0); cacheCfg.setAtomicityMode(TRANSACTIONAL); CacheConfiguration metaCacheCfg = defaultCacheConfiguration(); metaCacheCfg.setName("replicated"); metaCacheCfg.setCacheMode(REPLICATED); metaCacheCfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC); metaCacheCfg.setAtomicityMode(TRANSACTIONAL); return new CacheConfiguration[] {metaCacheCfg, cacheCfg}; }', 'ground_truth': 'protected CacheConfiguration dataCacheConfiguration(String gridName) { CacheConfiguration cacheCfg = defaultCacheConfiguration(); cacheCfg.setCacheMode(PARTITIONED); cacheCfg.setNearConfiguration(null); cacheCfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC); cacheCfg.setAffinityMapper(new IgfsGroupDataBlocksKeyMapper(GRP_SIZE)); cacheCfg.setBackups(0); cacheCfg.setAtomicityMode(TRANSACTIONAL); return cacheCfg; }', 'output': 'protected CacheConfiguration[] getCacheConfigurations(String gridName) { CacheConfiguration cacheCfg = defaultCacheConfiguration(); cacheCfg.setName("partitioned"); cacheCfg.setCacheMode(PARTITIONED); cacheCfg.setNearConfiguration(null); cacheCfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC); cacheCfg.setAffinityMapper(new IgfsGroupDataBlocksKeyMapper(GRP_SIZE)); cacheCfg.setBackups(0); cacheCfg.setAtomicityMode(TRANSACTIONAL); CacheConfiguration metaCacheCfg = defaultCacheConfiguration(); metaCacheCfg.setName("replicated"); metaCacheCfg.setCacheMode(REPLICATED); metaCacheCfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC); metaCacheCfg.setAtomicityMode(TRANSACTIONAL); return new CacheConfiguration[] {metaCacheCfg, cacheCfg}; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Set<K> XXX(int n) { return map.XXX(n); }', 'ground_truth': 'public Iterator<K> hotKeyIterator(int n) { return map.hotKeyIterator(n); }', 'output': 'public Set<K> partition(int n) { return map.partition(n); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public <K> long XXX(@Nullable String cacheName, K key, @Nullable Long init, long incr) throws IgniteCheckedException { assert key != null; return makeRequest(Command.INCREMENT, cacheName, key, null, incr, init).<Long>getObject(); }', 'ground_truth': 'public <K> long increment(K key, @Nullable Long init, long incr) throws IgniteCheckedException { assert key != null; return makeRequest(Command.INCREMENT, null, key, null, incr, init).<Long>getObject(); }', 'output': 'public <K> long increment(@Nullable String cacheName, K key, @Nullable Long init, long incr) throws IgniteCheckedException { assert key != null; return makeRequest(Command.INCREMENT, cacheName, key, null, incr, init).<Long>getObject(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public org.apache.cassandra.db.migration.avro.KsDef XXX() { org.apache.cassandra.db.migration.avro.KsDef ks = new org.apache.cassandra.db.migration.avro.KsDef(); ks.name = new Utf8(name); ks.strategy_class = new Utf8(strategyClass.getName()); if (strategyOptions != null) { ks.strategy_options = new HashMap<CharSequence, CharSequence>(); for (Map.Entry<String, String> e : strategyOptions.entrySet()) { ks.strategy_options.put(new Utf8(e.getKey()), new Utf8(e.getValue())); } } ks.cf_defs = SerDeUtils.createArray(cfMetaData.size(), org.apache.cassandra.db.migration.avro.CfDef.SCHEMA$); for (CFMetaData cfm : cfMetaData.values()) ks.cf_defs.add(cfm.XXX()); ks.durable_writes = durable_writes; return ks; }', 'ground_truth': 'public org.apache.cassandra.db.migration.avro.KsDef toAvro() { org.apache.cassandra.db.migration.avro.KsDef ks = new org.apache.cassandra.db.migration.avro.KsDef(); ks.name = new Utf8(name); ks.strategy_class = new Utf8(strategyClass.getName()); if (strategyOptions != null) { ks.strategy_options = new HashMap<CharSequence, CharSequence>(); for (Map.Entry<String, String> e : strategyOptions.entrySet()) { ks.strategy_options.put(new Utf8(e.getKey()), new Utf8(e.getValue())); } } ks.cf_defs = SerDeUtils.createArray(cfMetaData.size(), org.apache.cassandra.db.migration.avro.CfDef.SCHEMA$); for (CFMetaData cfm : cfMetaData.values()) ks.cf_defs.add(cfm.toAvro()); ks.durable_writes = durable_writes; return ks; }', 'output': 'public org.apache.cassandra.db.migration.avro.KsDef getKsDef() { org.apache.cassandra.db.migration.avro.KsDef ks = new org.apache.cassandra.db.migration.avro.KsDef(); ks.name = new Utf8(name); ks.strategy_class = new Utf8(strategyClass.getName()); if (strategyOptions != null) { ks.strategy_options = new HashMap<CharSequence, CharSequence>(); for (Map.Entry<String, String> e : strategyOptions.entrySet()) { ks.strategy_options.put(new Utf8(e.getKey()), new Utf8(e.getValue())); } } ks.cf_defs = SerDeUtils.createArray(cfMetaData.size(), org.apache.cassandra.db.migration.avro.CfDef.SCHEMA$); for (CFMetaData cfm : cfMetaData.values()) ks.cf_defs.add(cfm.getKsDef()); ks.durable_writes = durable_writes; return ks; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Message XXX() throws IOException { return XXX(StorageService.Verb.MUTATION); } public Message XXX(StorageService.Verb verb) throws IOException\n    { { return new Message(FBUtilities.getLocalAddress(), verb, getSerializedBuffer()); }', 'ground_truth': 'public Message getMessage(int version) throws IOException { return makeRowMutationMessage(StorageService.Verb.MUTATION, version); }', 'output': 'public Message buildMessage() throws IOException { return buildMessage(StorageService.Verb.MUTATION); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public final List<? extends FileBasedSource<T>> XXX(long desiredShardSizeBytes, PipelineOptions options) throws Exception { if (mode == Mode.FILEPATTERN) { long startTime = System.currentTimeMillis(); List<FileBasedSource<T>> splitResults = new ArrayList<>(); for (String file : expandFilePattern()) { splitResults.addAll(createForSubrangeOfFile(file, 0, Long.MAX_VALUE).XXX( desiredShardSizeBytes, options)); } LOG.debug("Splitting the source based on file pattern " + fileOrPatternSpec + " took " + (System.currentTimeMillis() - startTime) + " ms"); return splitResults; } else { IOChannelFactory factory = IOChannelUtils.getFactory(fileOrPatternSpec); if (factory.isReadSeekEfficient(fileOrPatternSpec)) { List<FileBasedSource<T>> splitResults = new ArrayList<>(); for (ByteOffsetBasedSource<T> split : super.XXX(desiredShardSizeBytes, options)) { splitResults.add((FileBasedSource<T>) split); } return splitResults; } else { LOG.debug("The source for file " + fileOrPatternSpec + " is not split into sub-range based sources since the file is not seekable"); return ImmutableList.of(this); } } }', 'ground_truth': 'public final List<? extends FileBasedSource<T>> splitIntoBundles(long desiredBundleSizeBytes, PipelineOptions options) throws Exception { if (mode == Mode.FILEPATTERN) { long startTime = System.currentTimeMillis(); List<FileBasedSource<T>> splitResults = new ArrayList<>(); for (String file : expandFilePattern()) { splitResults.addAll(createForSubrangeOfFile(file, 0, Long.MAX_VALUE).splitIntoBundles( desiredBundleSizeBytes, options)); } LOG.debug("Splitting the source based on file pattern " + fileOrPatternSpec + " took " + (System.currentTimeMillis() - startTime) + " ms"); return splitResults; } else { IOChannelFactory factory = IOChannelUtils.getFactory(fileOrPatternSpec); if (factory.isReadSeekEfficient(fileOrPatternSpec)) { List<FileBasedSource<T>> splitResults = new ArrayList<>(); for (ByteOffsetBasedSource<T> split : super.splitIntoBundles(desiredBundleSizeBytes, options)) { splitResults.add((FileBasedSource<T>) split); } return splitResults; } else { LOG.debug("The source for file " + fileOrPatternSpec + " is not split into sub-range based sources since the file is not seekable"); return ImmutableList.of(this); } } }', 'output': 'public final List<? extends FileBasedSource<T>> split(long desiredShardSizeBytes, PipelineOptions options) throws Exception { if (mode == Mode.FILEPATTERN) { long startTime = System.currentTimeMillis(); List<FileBasedSource<T>> splitResults = new ArrayList<>(); for (String file : expandFilePattern()) { splitResults.addAll(createForSubrangeOfFile(file, 0, Long.MAX_VALUE).split(desiredShardSizeBytes, options)); } LOG.debug("Splitting the source based on file pattern " + fileOrPatternSpec + " took " + (System.currentTimeMillis() - startTime) + " ms"); return splitResults; } else { IOChannelFactory factory = IOChannelUtils.getFactory(fileOrPatternSpec); if (factory.isReadSeekEfficient(fileOrPatternSpec)) { List<FileBasedSource<T>> splitResults = new ArrayList<>(); for (ByteOffsetBasedSource<T> split : super.split(desiredShardSizeBytes, options)) { splitResults.add((FileBasedSource<T>) split); } return splitResults; } else { LOG.debug("The source for file " + fileOrPatternSpec + " is not split into sub-range based sources since the file is not seekable"); return ImmutableList.of(this); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private UUID XXX(UntypedResultSet page, RateLimiter rateLimiter) { UUID id = null; ArrayList<Batch> batches = new ArrayList<>(page.size()); for (UntypedResultSet.Row row : page) { id = row.getUUID("id"); long writtenAt = row.getLong("written_at"); long timeout = getBatchlogTimeout(); if (System.currentTimeMillis() < writtenAt + timeout) continue; int version = row.has("version") ? row.getInt("version") : MessagingService.VERSION_12; Batch batch = new Batch(id, writtenAt, row.getBytes("data"), version); try { if (batch.replay(rateLimiter) > 0) { batches.add(batch); } else { deleteBatch(id); totalBatchesReplayed.incrementAndGet(); } } catch (IOException e) { logger.warn("Skipped batch replay of {} due to {}", id, e); deleteBatch(id); } } for (Batch batch : batches) { batch.finish(); deleteBatch(batch.id); } totalBatchesReplayed.addAndGet(batches.size()); return id; }', 'ground_truth': 'private void processBatchlogEntries(UntypedResultSet batches, int pageSize, RateLimiter rateLimiter) { int positionInPage = 0; ArrayList<Batch> unfinishedBatches = new ArrayList<>(pageSize); for (UntypedResultSet.Row row : batches) { UUID id = row.getUUID("id"); int version = row.getInt("version"); Batch batch = new Batch(id, row.getBytes("data"), version); try { if (batch.replay(rateLimiter) > 0) { unfinishedBatches.add(batch); } else { deleteBatch(id); ++totalBatchesReplayed; } } catch (IOException e) { logger.warn("Skipped batch replay of {} due to {}", id, e); deleteBatch(id); } if (++positionInPage == pageSize) { finishAndClearBatches(unfinishedBatches); positionInPage = 0; } } finishAndClearBatches(unfinishedBatches); }', 'output': 'private UUID nextId(UntypedResultSet page, RateLimiter rateLimiter) { UUID id = null; ArrayList<Batch> batches = new ArrayList<>(page.size()); for (UntypedResultSet.Row row : page) { id = row.getUUID("id"); long writtenAt = row.getLong("written_at"); long timeout = getBatchlogTimeout(); if (System.currentTimeMillis() < writtenAt + timeout) continue; int version = row.has("version") ? row.getInt("version") : MessagingService.VERSION_12; Batch batch = new Batch(id, writtenAt, row.getBytes("data"), version); try { if (batch.replay(rateLimiter) > 0) { batches.add(batch); } else { deleteBatch(id); totalBatchesReplayed.incrementAndGet(); } } catch (IOException e) { logger.warn("Skipped batch replay of {} due to {}", id, e); deleteBatch(id); } } for (Batch batch : batches) { batch.finish(); deleteBatch(batch.id); } totalBatchesReplayed.addAndGet(batches.size()); return id; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Map<Integer, Set<Integer>> XXX() { Map<Integer, Set<Integer>> part4CheckpointHistSearch = new HashMap<>(); for (CacheGroupContext grp : cctx.cache().cacheGroups()) { if (grp.isLocal()) continue; for (GridDhtLocalPartition part : grp.topology().currentLocalPartitions()) { if (part.state() != GridDhtPartitionState.OWNING || part.dataStore().fullSize() <= walRebalanceThreshold) continue; Set<Integer> parts = part4CheckpointHistSearch.get(grp.groupId()); if (parts == null) part4CheckpointHistSearch.put(grp.groupId(), parts = new HashSet<>()); parts.add(part.id()); } } return part4CheckpointHistSearch; }', 'ground_truth': 'private Map<Integer, Set<Integer>> partitionsApplicableForWalRebalance() { Map<Integer, Set<Integer>> res = new HashMap<>(); for (CacheGroupContext grp : cctx.cache().cacheGroups()) { if (grp.isLocal()) continue; for (GridDhtLocalPartition locPart : grp.topology().currentLocalPartitions()) { if (locPart.state() == GridDhtPartitionState.OWNING && locPart.fullSize() > walRebalanceThreshold) res.computeIfAbsent(grp.groupId(), k -> new HashSet<>()).add(locPart.id()); } } return res; }', 'output': 'private Map<Integer, Collection<Integer>> part4CheckpointHistSearch = new HashMap<>(); for (CacheGroupContext grp : cctx.cache().cacheGroups()) { if (grp.isLocal()) continue; for (GridDhtLocalPartition part : grp.topology().currentLocalPartitions()) { if (part.state() != GridDhtPartitionState.OWNING || part.dataStore().fullSize() <= walRebalanceThreshold) continue; Collection<Integer> parts = part4CheckpointHistSearch.get(grp.groupId()); if (parts == null) part4CheckpointHistSearch.put(grp.groupId(), parts = new HashSet<>()); parts.add(part.id()); } } return part4CheckpointHistSearch;</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX() { if (lastTimeStatusMsgSent < locNode.lastUpdateTime()) lastTimeStatusMsgSent = locNode.lastUpdateTime(); long updateTime = Math.max(lastTimeStatusMsgSent, lastRingMsgTime); long elapsed = (updateTime + hbCheckFreq) - U.currentTimeMillis(); if (elapsed > 0) return; msgWorker.addMessage(new TcpDiscoveryStatusCheckMessage(locNode, null)); lastTimeStatusMsgSent = U.currentTimeMillis(); }', 'ground_truth': 'private void checkMetricsReceiving() { if (lastTimeStatusMsgSent < locNode.lastUpdateTime()) lastTimeStatusMsgSent = locNode.lastUpdateTime(); long updateTime = Math.max(lastTimeStatusMsgSent, lastRingMsgTime); long elapsed = (updateTime + metricsCheckFreq) - U.currentTimeMillis(); if (elapsed > 0) return; msgWorker.addMessage(new TcpDiscoveryStatusCheckMessage(locNode, null)); lastTimeStatusMsgSent = U.currentTimeMillis(); }', 'output': 'private void checkStatus() { if (lastTimeStatusMsgSent < locNode.lastUpdateTime()) lastTimeStatusMsgSent = locNode.lastUpdateTime(); long updateTime = Math.max(lastTimeStatusMsgSent, lastRingMsgTime); long elapsed = (updateTime + hbCheckFreq) - U.currentTimeMillis(); if (elapsed > 0) return; msgWorker.addMessage(new TcpDiscoveryStatusCheckMessage(locNode, null)); lastTimeStatusMsgSent = U.currentTimeMillis(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public boolean XXX(Ignite ignite, FailureContext failureCtx) { new Thread( new Runnable() { @Override public void run() { U.error(ignite.log(), "Stopping local node on Ignite failure: [failureCtx=" + failureCtx + \']\'); IgnitionEx.stop(ignite.name(), true, true); } }, "node-stopper" ).start(); return true; } /** {@inheritDoc} */ @Override public String toString() { return S.toString(StopNodeFailureHandler.class, this); } } }', 'ground_truth': '@Override public boolean handle(Ignite ignite, FailureContext failureCtx) { new Thread( new Runnable() { @Override public void run() { U.error(ignite.log(), "Stopping local node on Ignite failure: [failureCtx=" + failureCtx + \']\'); IgnitionEx.stop(ignite.name(), true, true); } }, "node-stopper" ).start(); return true; }', 'output': '@Override public boolean stopNodeOnFailure(Ignite ignite, FailureContext failureCtx) { new Thread( new Runnable() { @Override public void run() { U.error(ignite.log(), "Stopping local node on Ignite failure: [failureCtx=" + failureCtx + \']\'); IgnitionEx.stop(ignite.name(), true, true); } }, "node-stopper" ).start(); return true; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private List<List<ClusterNode>> XXX(AffinityTopologyVersion topVer) { Map<Object, ClusterNode> alives = new HashMap<>(); for (ClusterNode node : ctx.discovery().nodes(topVer)) { if (!node.isClient() && !node.isDaemon()) alives.put(node.consistentId(), node); } List<List<ClusterNode>> result = new ArrayList<>(baselineAssignment.size()); for (int p = 0; p < baselineAssignment.size(); p++) { List<ClusterNode> baselineMapping = baselineAssignment.get(p); List<ClusterNode> currentMapping = null; for (ClusterNode node : baselineMapping) { ClusterNode aliveNode = alives.get(node.consistentId()); if (aliveNode != null) { if (currentMapping == null) currentMapping = new ArrayList<>(); currentMapping.add(aliveNode); } } result.add(p, currentMapping != null ? currentMapping : Collections.<ClusterNode>emptyList()); } return result; }', 'ground_truth': 'private List<List<ClusterNode>> baselineAssignmentWithoutOfflineNodes(AffinityTopologyVersion topVer) { Map<Object, ClusterNode> alives = new HashMap<>(); for (ClusterNode node : ctx.discovery().nodes(topVer)) { if (!node.isClient() && !node.isDaemon()) alives.put(node.consistentId(), node); } List<List<ClusterNode>> assignment = baselineAssignment.assignment(); List<List<ClusterNode>> result = new ArrayList<>(assignment.size()); for (int p = 0; p < assignment.size(); p++) { List<ClusterNode> baselineMapping = assignment.get(p); List<ClusterNode> currentMapping = null; for (ClusterNode node : baselineMapping) { ClusterNode aliveNode = alives.get(node.consistentId()); if (aliveNode != null) { if (currentMapping == null) currentMapping = new ArrayList<>(); currentMapping.add(aliveNode); } } result.add(p, currentMapping != null ? currentMapping : Collections.<ClusterNode>emptyList()); } return result; }', 'output': 'private List<List<ClusterNode>> getBaselineAssignment(AffinityTopologyVersion topVer) { Map<Object, ClusterNode> alives = new HashMap<>(); for (ClusterNode node : ctx.discovery().nodes(topVer)) { if (!node.isClient() && !node.isDaemon()) alives.put(node.consistentId(), node); } List<List<ClusterNode>> result = new ArrayList<>(baselineAssignment.size()); for (int p = 0; p < baselineAssignment.size(); p++) { List<ClusterNode> baselineMapping = baselineAssignment.get(p); List<ClusterNode> currentMapping = null; for (ClusterNode node : baselineMapping) { ClusterNode aliveNode = alives.get(node.consistentId()); if (aliveNode != null) { if (currentMapping == null) currentMapping = new ArrayList<>(); currentMapping.add(aliveNode); } } result.add(p, currentMapping != null ? currentMapping : Collections.<ClusterNode>emptyList()); } return result; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { List<List<?>> res = execute(ignite, "SELECT REPLACE(name, \'io.dataregion.default.\'), value, description FROM SYS.METRICS"); Set<String> names = new HashSet<>(); for (List<?> row : res) { names.add((String)row.get(0)); assertNotNull(row.get(1)); } for (String attr : EXPECTED_ATTRIBUTES) assertTrue(attr + " should be exporterd via SQL view", names.contains(attr)); }', 'ground_truth': 'public void testDataRegionMetrics() throws Exception { List<List<?>> res = execute(ignite0, "SELECT REPLACE(name, \'io.dataregion.default.\'), value, description FROM SYS.METRICS"); Set<String> names = new HashSet<>(); for (List<?> row : res) { names.add((String)row.get(0)); assertNotNull(row.get(1)); } for (String attr : EXPECTED_ATTRIBUTES) assertTrue(attr + " should be exporterd via SQL view", names.contains(attr)); }', 'output': 'public void testMetricsSqlView() throws Exception { List<List<?>> res = execute(ignite, "SELECT REPLACE(name, \'io.dataregion.default.\'), value, description FROM SYS.METRICS"); Set<String> names = new HashSet<>(); for (List<?> row : res) { names.add((String)row.get(0)); assertNotNull(row.get(1)); } for (String attr : EXPECTED_ATTRIBUTES) assertTrue(attr + " should be exporterd via SQL view", names.contains(attr)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<? extends BoundedSource<T>> XXX( long desiredBundleSizeBytes, PipelineOptions options) throws Exception { return ImmutableList.of(this); }', 'ground_truth': 'public List<? extends BoundedSource<T>> split( long desiredBundleSizeBytes, PipelineOptions options) throws Exception { return ImmutableList.of(this); }', 'output': 'public List<? extends BoundedSource<T>> getSources( long desiredBundleSizeBytes, PipelineOptions options) throws Exception { return ImmutableList.of(this); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX( final Page page, final @Nullable CacheEntryFragmentContext fctx, final CacheDataRow row, final int entrySize ) throws IgniteCheckedException { if (fctx != null) writePage(page.id(), page, writeFragmentRow, fctx, entrySize); else writePage(page.id(), page, writeRow, row, entrySize); }', 'ground_truth': 'private void writeExistingPage( final Page page, final @Nullable CacheEntryFragmentContext fctx, final CacheDataRow row, final int entrySize ) throws IgniteCheckedException { if (fctx != null) writePage(page.id(), page, writeFragmentRow, fctx, entrySize); else writePage(page.id(), page, writeRow, row, entrySize); }', 'output': 'private void writeToPage(final Page page, final @Nullable CacheEntryFragmentContext fctx, final CacheDataRow row, final int entrySize ) throws IgniteCheckedException { if (fctx != null) writePage(page.id(), page, writeFragmentRow, fctx, entrySize); else writePage(page.id(), page, writeRow, row, entrySize); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { List<String> expected; List<String> actual; SimpleSink sink = new SimpleSink(getBaseOutputFilename(), "test", ".SS.of.NN"); FilenamePolicy policy = sink.getFileNamePolicy(); expected = Arrays.asList(appendToTempFolder("output.00.of.03.test"), appendToTempFolder("output.01.of.03.test"), appendToTempFolder("output.02.of.03.test")); actual = generateDestinationFilenames(policy, 3); assertEquals(expected, actual); expected = Arrays.asList(appendToTempFolder("output.00.of.01.test")); actual = generateDestinationFilenames(policy, 1); assertEquals(expected, actual); expected = new ArrayList<>(); actual = generateDestinationFilenames(policy, 0); assertEquals(expected, actual); sink = new SimpleSink(getBaseOutputFilename(), ".test", ".SS.of.NN"); expected = Arrays.asList(appendToTempFolder("output.00.of.03.test"), appendToTempFolder("output.01.of.03.test"), appendToTempFolder("output.02.of.03.test")); actual = generateDestinationFilenames(policy, 3); assertEquals(expected, actual); expected = Arrays.asList(appendToTempFolder("output.00.of.01.test")); actual = generateDestinationFilenames(policy, 1); assertEquals(expected, actual); expected = new ArrayList<>(); actual = generateDestinationFilenames(policy, 0); assertEquals(expected, actual); }', 'ground_truth': 'public void testGenerateOutputFilenames() { List<ResourceId> expected; List<ResourceId> actual; ResourceId root = getBaseOutputDirectory(); SimpleSink sink = new SimpleSink(root, "file", ".SSSSS.of.NNNNN", ".test"); FilenamePolicy policy = sink.getFilenamePolicy(); expected = Arrays.asList( root.resolve("file.00000.of.00003.test", StandardResolveOptions.RESOLVE_FILE), root.resolve("file.00001.of.00003.test", StandardResolveOptions.RESOLVE_FILE), root.resolve("file.00002.of.00003.test", StandardResolveOptions.RESOLVE_FILE) ); actual = generateDestinationFilenames(root, policy, 3); assertEquals(expected, actual); expected = Collections.singletonList( root.resolve("file.00000.of.00001.test", StandardResolveOptions.RESOLVE_FILE) ); actual = generateDestinationFilenames(root, policy, 1); assertEquals(expected, actual); expected = new ArrayList<>(); actual = generateDestinationFilenames(root, policy, 0); assertEquals(expected, actual); }', 'output': 'public void testGeneratedFilenames() { List<String> expected; List<String> actual; SimpleSink sink = new SimpleSink(getBaseOutputFilename(), "test", ".SS.of.NN"); FilenamePolicy policy = sink.getFileNamePolicy(); expected = Arrays.asList(appendToTempFolder("output.00.of.03.test"), appendToTempFolder("output.01.of.03.test"), appendToTempFolder("output.02.of.03.test")); actual = generateDestinationFilenames(policy, 3); assertEquals(expected, actual); expected = Arrays.asList(appendToTempFolder("output.00.of.01.test")); actual = generateDestinationFilenames(policy, 1); assertEquals(expected, actual); expected = new ArrayList<>(); actual = generateDestinationFilenames(policy, 0); assertEquals(expected, actual); sink = new SimpleSink(getBaseOutputFilename(), ".test", ".SS.of.NN"); expected = Arrays.asList(appendToTempFolder("output.00.of.03.test"), appendToTempFolder("output.01.of.03.test"), appendToTempFolder("output.02.of.03.test")); actual = generateDestinationFilenames(policy, 3); assertEquals(expected, actual); expected = Arrays.asList(appendToTempFolder("output.00.of.01.test")); actual = generateDestinationFilenames(policy, 1); assertEquals(expected, actual); expected = new ArrayList<>(); actual = generateDestinationFilenames(policy, 0); assertEquals(expected, actual); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<Column> XXX(Collection<IColumn> columns) { return XXX(columns, false); } public List<Column> XXX(Collection<IColumn> columns, boolean reverseOrder)\n    { { if (columns == null || columns.isEmpty()) { return EMPTY_COLUMNS; } ArrayList<Column> thriftColumns = new ArrayList<Column>(columns.size()); for (IColumn column : columns) { if (column.isMarkedForDelete()) { continue; } Column thrift_column = new Column(column.name(), column.value(), column.timestamp()); thriftColumns.add(thrift_column); } if (reverseOrder) Collections.reverse(thriftColumns); return thriftColumns; }', 'ground_truth': 'public List<Column> thriftifySubColumns(Collection<IColumn> columns) { if (columns == null || columns.isEmpty()) { return EMPTY_SUBCOLUMNS; } ArrayList<Column> thriftColumns = new ArrayList<Column>(columns.size()); for (IColumn column : columns) { if (column.isMarkedForDelete()) { continue; } Column thrift_column = new Column(column.name(), column.value(), column.timestamp()); thriftColumns.add(thrift_column); } return thriftColumns; }', 'output': 'public List<Column> getColumns(Collection<IColumn> columns) { return getColumns(columns, false); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static KV<ByteString, Iterable<Mutation>> XXX(String key, String value) { ByteString rowKey = ByteString.copyFromUtf8(key); Iterable<Mutation> mutations = ImmutableList.of( Mutation.newBuilder() .setSetCell(SetCell.newBuilder().setValue(ByteString.copyFromUtf8(value))) .build()); return KV.of(rowKey, mutations); }', 'ground_truth': 'private static KV<ByteString, Iterable<Mutation>> makeWrite(String key, String value) { ByteString rowKey = ByteString.copyFromUtf8(key); Iterable<Mutation> mutations = ImmutableList.of( Mutation.newBuilder() .setSetCell(SetCell.newBuilder().setValue(ByteString.copyFromUtf8(value))) .build()); return KV.of(rowKey, mutations); }', 'output': 'private static KV<ByteString, Iterable<Mutation>> apply(String key, String value) { ByteString rowKey = ByteString.copyFromUtf8(key); Iterable<Mutation> mutations = ImmutableList.of( Mutation.newBuilder() .setSetCell(SetCell.newBuilder().setValue(ByteString.copyFromUtf8(value))) .build()); return KV.of(rowKey, mutations); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected Component XXX() { this.isOutputFormat = true; final ScalarModel scalarModel = getModel(); final String name = scalarModel.getFriendlyName(); this.entityLinkOutputFormat = (EntityLinkSimplePanel) getComponentFactoryRegistry() .createComponent(ComponentType.ENTITY_LINK, scalarModel); entityLinkOutputFormat.setOutputMarkupId(true); entityLinkOutputFormat.setLabel(Model.of(name)); final WebMarkupContainer labelIfCompact = new WebMarkupContainer(ID_SCALAR_IF_COMPACT); labelIfCompact.add(entityLinkOutputFormat); return labelIfCompact; }', 'ground_truth': 'protected Component createComponentForOutput() { this.isOutputFormat = true; final ScalarModel scalarModel = getModel(); final String name = scalarModel.getFriendlyName(); this.entityLinkOutputFormat = (EntityLinkSimplePanel) getComponentFactoryRegistry() .createComponent(ComponentType.ENTITY_LINK, scalarModel); entityLinkOutputFormat.setOutputMarkupId(true); entityLinkOutputFormat.setLabel(Model.of(name)); val labelIfOutput = FragmentContainer.SCALAR_IF_OUTPUT .createComponent(WebMarkupContainer::new); labelIfOutput.add(entityLinkOutputFormat); return labelIfOutput; }', 'output': 'protected Component outputFormat() { this.isOutputFormat = true; final ScalarModel scalarModel = getModel(); final String name = scalarModel.getFriendlyName(); this.entityLinkOutputFormat = (EntityLinkSimplePanel) getComponentFactoryRegistry() .createComponent(ComponentType.ENTITY_LINK, scalarModel); entityLinkOutputFormat.setOutputMarkupId(true); entityLinkOutputFormat.setLabel(Model.of(name)); final WebMarkupContainer labelIfCompact = new WebMarkupContainer(ID_SCALAR_IF_COMPACT); labelIfCompact.add(entityLinkOutputFormat); return labelIfCompact; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static long XXX() { return conf.client_large_read_abort_threshold_kb; }', 'ground_truth': 'public static long getCoordinatorReadSizeWarnThresholdKB() { return conf.track_warnings.coordinator_read_size.getWarnThresholdKb(); }', 'output': 'public static long getClientLargeReadAbortThresholdKB() { return conf.client_large_read_abort_threshold_kb; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static String XXX(BeamSQLRow row, CSVFormat csvFormat) { StringWriter writer = new StringWriter(); try (CSVPrinter printer = csvFormat.print(writer)) { for (int i = 0; i < row.size(); i++) { printer.print(row.getFieldValue(i).toString()); } printer.println(); } catch (IOException e) { throw new IllegalArgumentException("encodeRecord failed!", e); } return writer.toString(); }', 'ground_truth': 'public static String beamSqlRow2CsvLine(BeamSqlRow row, CSVFormat csvFormat) { StringWriter writer = new StringWriter(); try (CSVPrinter printer = csvFormat.print(writer)) { for (int i = 0; i < row.size(); i++) { printer.print(row.getFieldValue(i).toString()); } printer.println(); } catch (IOException e) { throw new IllegalArgumentException("encodeRecord failed!", e); } return writer.toString(); }', 'output': 'public static String encodeRecord(BeamSQLRow row, CSVFormat csvFormat) { StringWriter writer = new StringWriter(); try (CSVPrinter printer = csvFormat.print(writer)) { for (int i = 0; i < row.size(); i++) { printer.print(row.getFieldValue(i).toString()); } printer.println(); } catch (IOException e) { throw new IllegalArgumentException("encodeRecord failed!", e); } return writer.toString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<byte[]> XXX(Collection<String> metricNames, List<String> hostnames, String appId, String instanceId) { Collection<String> sanitizedMetricNames = new HashSet<>(); List<byte[]> uuids = new ArrayList<>(); for (String metricName : metricNames) { if (metricName.contains("%")) { String metricRegEx = getJavaRegexFromSqlRegex(metricName); for (TimelineMetricMetadataKey key : METADATA_CACHE.keySet()) { String metricNameFromMetadata = key.getMetricName(); if (metricNameFromMetadata.matches(metricRegEx)) { sanitizedMetricNames.add(metricNameFromMetadata); } } } else { sanitizedMetricNames.add(metricName); } } if(sanitizedMetricNames.isEmpty()) { return uuids; } Set<String> sanitizedHostNames = new HashSet<>(); if (CollectionUtils.isNotEmpty(hostnames)) { for (String hostname : hostnames) { if (hostname.contains("%")) { String hostRegEx; hostRegEx = hostname.replace("%", ".*"); for (String host : HOSTED_APPS_MAP.keySet()) { if (host.matches(hostRegEx)) { sanitizedHostNames.add(host); } } } else { sanitizedHostNames.add(hostname); } } } if ( StringUtils.isNotEmpty(appId) && !(appId.equals("HOST") || appId.equals("FLUME_HANDLER"))) { appId = appId.toLowerCase(); } if (CollectionUtils.isNotEmpty(sanitizedHostNames)) { if (CollectionUtils.isNotEmpty(sanitizedMetricNames)) { for (String metricName : sanitizedMetricNames) { TimelineMetric metric = new TimelineMetric(); metric.setMetricName(metricName); metric.setAppId(appId); metric.setInstanceId(instanceId); for (String hostname : sanitizedHostNames) { metric.setHostName(hostname); byte[] uuid = getUuid(metric); if (uuid != null) { uuids.add(uuid); } } } } else { for (String hostname : sanitizedHostNames) { byte[] uuid = getUuidForHostname(hostname); if (uuid != null) { uuids.add(uuid); } } } } else { for (String metricName : sanitizedMetricNames) { TimelineClusterMetric metric = new TimelineClusterMetric(metricName, appId, instanceId, -1l); byte[] uuid = getUuid(metric); if (uuid != null) { uuids.add(uuid); } } } return uuids; }', 'ground_truth': 'public List<byte[]> getUuidsForGetMetricQuery(Collection<String> metricNames, List<String> hostnames, String appId, String instanceId, List<String> transientMetricNames) { Collection<String> sanitizedMetricNames = new HashSet<>(); List<byte[]> uuids = new ArrayList<>(); for (String metricName : metricNames) { if (metricName.contains("%")) { String metricRegEx = getJavaRegexFromSqlRegex(metricName); for (TimelineMetricMetadataKey key : METADATA_CACHE.keySet()) { String metricNameFromMetadata = key.getMetricName(); if (metricNameFromMetadata.matches(metricRegEx)) { sanitizedMetricNames.add(metricNameFromMetadata); } } } else { sanitizedMetricNames.add(metricName); } } if(sanitizedMetricNames.isEmpty()) { return uuids; } Set<String> sanitizedHostNames = getSanitizedHostnames(hostnames); if ( StringUtils.isNotEmpty(appId) && !(appId.equals("HOST") || appId.equals("FLUME_HANDLER"))) { appId = appId.toLowerCase(); } if (CollectionUtils.isNotEmpty(sanitizedHostNames)) { if (CollectionUtils.isNotEmpty(sanitizedMetricNames)) { for (String metricName : sanitizedMetricNames) { if (isTransientMetric(metricName, appId)) { transientMetricNames.add(metricName); continue; } TimelineMetric metric = new TimelineMetric(); metric.setMetricName(metricName); metric.setAppId(appId); metric.setInstanceId(instanceId); for (String hostname : sanitizedHostNames) { metric.setHostName(hostname); byte[] uuid = getUuid(metric, false); if (uuid != null) { uuids.add(uuid); } } } } else { for (String hostname : sanitizedHostNames) { byte[] uuid = getUuidForHostname(hostname, false); if (uuid != null) { uuids.add(uuid); } } } } else { for (String metricName : sanitizedMetricNames) { if (isTransientMetric(metricName, appId)) { transientMetricNames.add(metricName); continue; } TimelineClusterMetric metric = new TimelineClusterMetric(metricName, appId, instanceId, -1l); byte[] uuid = getUuid(metric, false); if (uuid != null) { uuids.add(uuid); } } } return uuids; }', 'output': 'public List<byte[]> getUuids(Collection<String> metricNames, List<String> hostnames, String appId, String instanceId) { Collection<String> sanitizedMetricNames = new HashSet<>(); List<byte[]> uuids = new ArrayList<>(); for (String metricName : metricNames) { if (metricName.contains("%")) { String metricRegEx = getJavaRegexFromSqlRegex(metricName); for (TimelineMetricMetadataKey key : METADATA_CACHE.keySet()) { String metricNameFromMetadata = key.getMetricName(); if (metricNameFromMetadata.matches(metricRegEx)) { sanitizedMetricNames.add(metricNameFromMetadata); } } } else { sanitizedMetricNames.add(metricName); } } if(sanitizedMetricNames.isEmpty()) { return uuids; } Set<String> sanitizedHostNames = new HashSet<>(); if (CollectionUtils.isNotEmpty(hostnames)) { for (String hostname : hostnames) { if (hostname.contains("%")) { String hostRegEx; hostRegEx = hostname.replace("%", ".*"); for (String host : HOSTED_APPS_MAP.keySet()) { if (host.matches(hostRegEx)) { sanitizedHostNames.add(host); } } } else { sanitizedHostNames.add(hostname); } } } if ( StringUtils.isNotEmpty(appId) && !(appId.equals("HOST") || appId.equals("FLUME_HANDLER"))) { appId = appId.toLowerCase(); } if (CollectionUtils.isNotEmpty(sanitizedHostNames)) { if (CollectionUtils.isNotEmpty(sanitizedMetricNames)) { for (String metricName : sanitizedMetricNames) { TimelineMetric metric = new TimelineMetric(); metric.setMetricName(metricName); metric.setAppId(appId); metric.setInstanceId(instanceId); for (String hostname : sanitizedHostNames) { metric.setHostName(hostname); byte[] uuid = getUuid(metric); if (uuid != null) { uuids.add(uuid); } } } } else { for (String host'}
{'instruction': 'Please refactor the following code:', 'input': 'public final void XXX(List<IndexExpression> expressions, SecondaryIndexManager indexManager, QueryOptions options) { throw new UnsupportedOperationException("Index expression cannot be created for token restriction"); }', 'ground_truth': 'public void addRowFilterTo(RowFilter filter, SecondaryIndexManager indexManager, QueryOptions options) { throw new UnsupportedOperationException("Index expression cannot be created for token restriction"); }', 'output': 'public final void addTokenRestrictions(List<IndexExpression> expressions, SecondaryIndexManager indexManager, QueryOptions options) { throw new UnsupportedOperationException("Token restrictions cannot be added to secondary indexes"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws IOException { for (Map.Entry<ByteBuffer, SecondaryIndex> entry : indexesByColumn.entrySet()) entry.getValue().removeIndex(entry.getKey()); }', 'ground_truth': 'public void invalidate() { for (Map.Entry<ByteBuffer, SecondaryIndex> entry : indexesByColumn.entrySet()) entry.getValue().invalidate(); }', 'output': 'public void removeAllIndexes() throws IOException { for (Map.Entry<ByteBuffer, SecondaryIndex> entry : indexesByColumn.entrySet()) entry.getValue().removeIndex(entry.getKey()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { init(user(UserType.PAM)); assertEquals(UserType.PAM, userDAO.findSingleUserByName(SERVICEOP_USER_NAME).getUserType()); }', 'ground_truth': 'public void testUserByName() { init(user()); assertEquals(SERVICEOP_USER_NAME, userDAO.findUserByName(SERVICEOP_USER_NAME).getUserName()); }', 'output': 'public void testServiceOpUser() { init(user(UserType.PAM)); assertEquals(UserType.PAM, userDAO.findSingleUserByName(SERVICEOP_USER_NAME).getUserType()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public double XXX() { return readStats_.mean(); }', 'ground_truth': 'public long getTotalReadLatency() { return readStats_.getTotalLatency(); }', 'output': 'public double getMean() { return readStats_.mean(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(String tag, String... columnFamilyList) throws IOException { Map<Keyspace, List<String>> keyspaceColumnfamily = new HashMap<Keyspace, List<String>>(); for (String columnFamily : columnFamilyList) { String splittedString[] = columnFamily.split("\\\\."); if (splittedString.length == 2) { String keyspaceName = splittedString[0]; String columnFamilyName = splittedString[1]; if (keyspaceName == null) throw new IOException("You must supply a keyspace name"); if (operationMode.equals(Mode.JOINING)) throw new IOException("Cannot snapshot until bootstrap completes"); if (columnFamilyName == null) throw new IOException("You must supply a column family name"); if (tag == null || tag.equals("")) throw new IOException("You must supply a snapshot name."); Keyspace keyspace = getValidKeyspace(keyspaceName); ColumnFamilyStore columnFamilyStore = keyspace.getColumnFamilyStore(columnFamilyName); if (columnFamilyStore.snapshotExists(tag)) throw new IOException("Snapshot " + tag + " already exists."); if (!keyspaceColumnfamily.containsKey(keyspace)) { keyspaceColumnfamily.put(keyspace, new ArrayList<String>()); } keyspaceColumnfamily.get(keyspace).add(columnFamilyName); } else { throw new IllegalArgumentException( "Cannot take a snapshot on secondary index or invalid column family name. You must supply a column family name in the form of keyspace.columnfamily"); } } for (Entry<Keyspace, List<String>> entry : keyspaceColumnfamily.entrySet()) { for (String columnFamily : entry.getValue()) entry.getKey().snapshot(tag, columnFamily); } }', 'ground_truth': 'public void takeMultipleTableSnapshot(String tag, String... tableList) throws IOException { Map<Keyspace, List<String>> keyspaceColumnfamily = new HashMap<Keyspace, List<String>>(); for (String table : tableList) { String splittedString[] = table.split("\\\\."); if (splittedString.length == 2) { String keyspaceName = splittedString[0]; String tableName = splittedString[1]; if (keyspaceName == null) throw new IOException("You must supply a keyspace name"); if (operationMode.equals(Mode.JOINING)) throw new IOException("Cannot snapshot until bootstrap completes"); if (tableName == null) throw new IOException("You must supply a table name"); if (tag == null || tag.equals("")) throw new IOException("You must supply a snapshot name."); Keyspace keyspace = getValidKeyspace(keyspaceName); ColumnFamilyStore columnFamilyStore = keyspace.getColumnFamilyStore(tableName); if (columnFamilyStore.snapshotExists(tag)) throw new IOException("Snapshot " + tag + " already exists."); if (!keyspaceColumnfamily.containsKey(keyspace)) { keyspaceColumnfamily.put(keyspace, new ArrayList<String>()); } keyspaceColumnfamily.get(keyspace).add(tableName); } else { throw new IllegalArgumentException( "Cannot take a snapshot on secondary index or invalid column family name. You must supply a column family name in the form of keyspace.columnfamily"); } } for (Entry<Keyspace, List<String>> entry : keyspaceColumnfamily.entrySet()) { for (String table : entry.getValue()) entry.getKey().snapshot(tag, table); } }', 'output': 'public void snapshot(String tag, String... columnFamilyList) throws IOException { Map<Keyspace, List<String>> keyspaceColumnfamily = new HashMap<Keyspace, List<String>>(); for (String columnFamily : columnFamilyList) { String splittedString[] = columnFamily.split("\\\\."); if (splittedString.length == 2) { String keyspaceName = splittedString[0]; String columnFamilyName = splittedString[1]; if (keyspaceName == null) throw new IOException("You must supply a keyspace name"); if (operationMode.equals(Mode.JOINING)) throw new IOException("Cannot snapshot until bootstrap completes"); if (columnFamilyName == null) throw new IOException("You must supply a column family name"); if (tag == null || tag.equals("")) throw new IOException("You must supply a snapshot name."); Keyspace keyspace = getValidKeyspace(keyspaceName); ColumnFamilyStore columnFamilyStore = keyspace.getColumnFamilyStore(columnFamilyName); if (columnFamilyStore.snapshotExists(tag)) throw new IOException("Snapshot " + tag + " already exists."); if (!keyspaceColumnfamily.containsKey(keyspace)) { keyspaceColumnfamily.put(keyspace, new ArrayList<String>()); } keyspaceColumnfamily.get(keyspace).add(columnFamilyName); } else { throw new IllegalArgumentException( "Cannot take a snapshot on secondary index or invalid column family name. You must supply a column family name in the form of keyspace.columnfamily"); } } for (Entry<Keyspace, List<String>> entry : keyspaceColumnfamily.entrySet()) { for (String columnFamily : entry.getValue()) entry.getKey().snapshot(tag, columnFamily); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { int targetCount = m_dao.findAllTargets().size(); AlertTargetEntity target = m_helper.createAlertTarget(); Set<AlertTargetEntity> targets = new HashSet<AlertTargetEntity>(); targets.add(target); AlertGroupEntity group = m_helper.createAlertGroup( m_cluster.getClusterId(), targets); AlertTargetEntity actual = m_dao.findTargetById(target.getTargetId()); assertNotNull(actual); assertEquals(target.getTargetName(), actual.getTargetName()); assertEquals(target.getDescription(), actual.getDescription()); assertEquals(target.getNotificationType(), actual.getNotificationType()); assertEquals(target.getProperties(), actual.getProperties()); assertEquals(false, actual.isGlobal()); assertNotNull(actual.getAlertGroups()); Iterator<AlertGroupEntity> iterator = actual.getAlertGroups().iterator(); AlertGroupEntity actualGroup = iterator.next(); assertEquals(group, actualGroup); assertEquals(targetCount + 1, m_dao.findAllTargets().size()); }', 'ground_truth': 'public void testCreateAndRemoveTarget() throws Exception { int targetCount = m_dao.findAllTargets().size(); AlertTargetEntity target = m_helper.createAlertTarget(); Set<AlertTargetEntity> targets = new HashSet<AlertTargetEntity>(); targets.add(target); AlertGroupEntity group = m_helper.createAlertGroup( m_cluster.getClusterId(), targets); AlertTargetEntity actual = m_dao.findTargetById(target.getTargetId()); assertNotNull(actual); assertEquals(target.getTargetName(), actual.getTargetName()); assertEquals(target.getDescription(), actual.getDescription()); assertEquals(target.getNotificationType(), actual.getNotificationType()); assertEquals(target.getProperties(), actual.getProperties()); assertEquals(false, actual.isGlobal()); assertNotNull(actual.getAlertGroups()); Iterator<AlertGroupEntity> iterator = actual.getAlertGroups().iterator(); AlertGroupEntity actualGroup = iterator.next(); assertEquals(group, actualGroup); assertEquals(targetCount + 1, m_dao.findAllTargets().size()); m_dao.remove(target); target = m_dao.findTargetById(target.getTargetId()); assertNull(target); }', 'output': 'public void testFindTargetById() throws Exception { int targetCount = m_dao.findAllTargets().size(); AlertTargetEntity target = m_helper.createAlertTarget(); Set<AlertTargetEntity> targets = new HashSet<AlertTargetEntity>(); targets.add(target); AlertGroupEntity group = m_helper.createAlertGroup( m_cluster.getClusterId(), targets); AlertTargetEntity actual = m_dao.findTargetById(target.getTargetId()); assertNotNull(actual); assertEquals(target.getTargetName(), actual.getTargetName()); assertEquals(target.getDescription(), actual.getDescription()); assertEquals(target.getNotificationType(), actual.getNotificationType()); assertEquals(target.getProperties(), actual.getProperties()); assertEquals(false, actual.isGlobal()); assertNotNull(actual.getAlertGroups()); Iterator<AlertGroupEntity> iterator = actual.getAlertGroups().iterator(); AlertGroupEntity actualGroup = iterator.next(); assertEquals(group, actualGroup); assertEquals(targetCount + 1, m_dao.findAllTargets().size()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public PCollection<TimestampedValue<KnownSize>> XXX(PCollection<Event> events) { if (configuration.debug) { events = events .XXX(eventMonitor.getTransform()) .XXX(NexmarkUtils.snoop(name)); } if (configuration.cpuDelayMs > 0) { events = events.XXX(NexmarkUtils.<Event>cpuDelay(name, configuration.cpuDelayMs)); } if (configuration.diskBusyBytes > 0) { events = events.XXX(NexmarkUtils.<Event>diskBusy(name, configuration.diskBusyBytes)); } PCollection<KnownSize> queryResults = XXXPrim(events); if (configuration.debug) { queryResults = queryResults.XXX(resultMonitor.getTransform()); } return queryResults.XXX(NexmarkUtils.<KnownSize>stamp(name)); }', 'ground_truth': 'public PCollection<TimestampedValue<KnownSize>> expand(PCollection<Event> events) { if (configuration.debug) { events = events .apply(name + ".Monitor", eventMonitor.getTransform()) .apply(name + ".Snoop", NexmarkUtils.snoop(name)); } if (configuration.cpuDelayMs > 0) { events = events.apply(name + ".CpuDelay", NexmarkUtils.<Event>cpuDelay(name, configuration.cpuDelayMs)); } if (configuration.diskBusyBytes > 0) { events = events.apply(name + ".DiskBusy", NexmarkUtils.<Event>diskBusy(name, configuration.diskBusyBytes)); } PCollection<KnownSize> queryResults = applyPrim(events); if (configuration.debug) { queryResults = queryResults.apply(name + ".Debug", resultMonitor.getTransform()); } return queryResults.apply(name + ".Stamp", NexmarkUtils.<KnownSize>stamp(name)); }', 'output': 'public PCollection<TimestampedValue<KnownSize>> apply(PCollection<Event> events) { if (configuration.debug) { events = events .apply(eventMonitor.getTransform()) .apply(NexmarkUtils.snoop(name)); } if (configuration.cpuDelayMs > 0) { events = events.apply(NexmarkUtils.<Event>cpuDelay(name, configuration.cpuDelayMs)); } if (configuration.diskBusyBytes > 0) { events = events.apply(NexmarkUtils.<Event>diskBusy(name, configuration.diskBusyBytes)); } PCollection<KnownSize> queryResults = applyPrim(events); if (configuration.debug) { queryResults = queryResults.apply(resultMonitor.getTransform()); } return queryResults.apply(NexmarkUtils.<KnownSize>stamp(name)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private MarkupContainer XXX(final String downloadId, final MarkupContainer parent) { val resourceLink = createResourceLink(downloadId); if(resourceLink != null) { parent.addOrReplace(resourceLink); Tooltips.addTooltip(resourceLink, "download"); } else { Components.permanentlyHide(parent, downloadId); } return resourceLink; }', 'ground_truth': 'private MarkupContainer createDownloadLink(final String id, final MarkupContainer parent) { return getBlobOrClobFromModel() .map(this::newResource) .map(resource->Wkt.downloadLinkNoCache(id, resource)) .map(peek(downloadLink->{ parent.addOrReplace(downloadLink); Tooltips.addTooltip(downloadLink, "download"); })) .orElseGet(()->{ Components.permanentlyHide(parent, id); return null; }); }', 'output': 'private MarkupContainer addDownloadLink(final String downloadId, final MarkupContainer parent) { val resourceLink = createResourceLink(downloadId); if(resourceLink != null) { parent.addOrReplace(resourceLink); Tooltips.addTooltip(resourceLink, "download"); } else { Components.permanentlyHide(parent, downloadId); } return resourceLink; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean XXX(Indexed item) { Owned object = (Owned) item; return object.getOwner().compareTo(username) == 0; }', 'ground_truth': 'public boolean isConform(Indexed item) { Owned object = (Owned) item; return object.getOwner().compareTo(username) == 0; }', 'output': 'public boolean equals(Indexed item) { Owned object = (Owned) item; return object.getOwner().equals(username); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void XXX(long newSize) { throw new BufferOverflowException(); }', 'ground_truth': 'protected void expandToFit(long newSize) { throw new BufferOverflowException(); }', 'output': 'protected void ensureCapacity(long newSize) { throw new BufferOverflowException(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean XXX(String keyspace, ColumnSpecification receiver) { return true; }', 'ground_truth': 'public AssignmentTestable.TestResult testAssignment(String keyspace, ColumnSpecification receiver) { return AssignmentTestable.TestResult.WEAKLY_ASSIGNABLE; }', 'output': 'public boolean isCompatible(String keyspace, ColumnSpecification receiver) { return true; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(ColumnFamily returnCF, ReducingIterator<IColumn> reducedColumns, int gcBefore) { int liveColumns = 0; AbstractType comparator = returnCF.getComparator(); for (IColumn column : reducedColumns) { if (liveColumns >= count) break; if (finish.length > 0 && ((isAscending && comparator.compare(column.name(), finish) > 0)) || (!isAscending && comparator.compare(column.name(), finish) < 0)) break; if (!column.isMarkedForDelete()) liveColumns++; if (!column.isMarkedForDelete() || column.getLocalDeletionTime() > gcBefore) returnCF.addColumn(column); } }', 'ground_truth': 'public void collectReducedColumns(ColumnFamily returnCF, Iterator<IColumn> reducedColumns, int gcBefore) { int liveColumns = 0; AbstractType comparator = returnCF.getComparator(); while (reducedColumns.hasNext()) { IColumn column = reducedColumns.next(); if (liveColumns >= count) break; if (finish.length > 0 && ((isAscending && comparator.compare(column.name(), finish) > 0)) || (!isAscending && comparator.compare(column.name(), finish) < 0)) break; if (!column.isMarkedForDelete()) liveColumns++; if (!column.isMarkedForDelete() || column.getLocalDeletionTime() > gcBefore) returnCF.addColumn(column); } }', 'output': 'public void addColumns(ColumnFamily returnCF, ReducingIterator<IColumn> reducedColumns, int gcBefore) { int liveColumns = 0; AbstractType comparator = returnCF.getComparator(); for (IColumn column : reducedColumns) { if (liveColumns >= count) break; if (finish.length > 0 && ((isAscending && comparator.compare(column.name(), finish) > 0) || (!isAscending && comparator.compare(column.name(), finish) < 0))) break; if (!column.isMarkedForDelete()) liveColumns++; if (!column.isMarkedForDelete() || column.getLocalDeletionTime() > gcBefore) returnCF.addColumn(column); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Throwable { perfTest(new SizeCaps(10, 20, 5, 10), new SizeCaps(10, 20, 5, 10), Cluster.builder().addContactPoint(nativeAddr.getHostAddress()) .allowBetaProtocolVersion() .withCompression(ProtocolOptions.Compression.LZ4) .withPort(nativePort)); }', 'ground_truth': 'public void measureSmallV6WithCompression() throws Throwable { perfTest(new SizeCaps(10, 20, 5, 10), new SizeCaps(10, 20, 5, 10), Cluster.builder().addContactPoint(nativeAddr.getHostAddress()) .allowBetaProtocolVersion() .withCompression(ProtocolOptions.Compression.LZ4) .withPort(nativePort), ProtocolVersion.V6); }', 'output': 'public void testSizeCaps() throws Throwable { perfTest(new SizeCaps(10, 20, 5, 10), new SizeCaps(10, 20, 5, 10), Cluster.builder().addContactPoint(nativeAddr.getHostAddress()) .allowBetaProtocolVersion() .withCompression(ProtocolOptions.Compression.LZ4) .withPort(nativePort)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Class<?> XXX(final @Nullable String objectType) { return specificationLoader.specForLogicalTypeName(objectType) .map(ObjectSpecification::getCorrespondingClass) .orElse(null); }', 'ground_truth': 'public Optional<LogicalType> lookupLogicalTypeByName(final @Nullable String objectType) { return specificationLoader.specForLogicalTypeName(objectType) .map(ObjectSpecification::getLogicalType); }', 'output': 'public Class<?> getTargetClass(final @Nullable String objectType) { return specificationLoader.specForLogicalTypeName(objectType) .map(ObjectSpecification::getCorrespondingClass) .orElse(null); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static ByteString XXX(ByteString s, int length) { if (s.length() >= length) { return s; } return new ByteString(Arrays.copyOf(s.getBytes(), length)); }', 'ground_truth': 'private static ByteString padRight(ByteString s, int length) { if (s.length() >= length) { return s; } return new ByteString(Arrays.copyOf(s.getBytes(), length)); }', 'output': 'private static ByteString copyOf(ByteString s, int length) { if (s.length() >= length) { return s; } return new ByteString(Arrays.copyOf(s.getBytes(), length)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public JobExecutionResult XXX() throws Exception { if (options.isStreaming()) { System.out.println("Plan: " + this.flinkStreamEnv.getExecutionPlan()); if (this.flinkStreamEnv == null) { throw new RuntimeException("JobExecutionEnvironment not initialized."); } if (this.flinkPipelineTranslator == null) { throw new RuntimeException("JobGraphTranslator not initialized."); } return this.flinkStreamEnv.execute(); } else { if (this.flinkBatchEnv == null) { throw new RuntimeException("JobExecutionEnvironment not initialized."); } if (this.flinkPipelineTranslator == null) { throw new RuntimeException("JobGraphTranslator not initialized."); } return this.flinkBatchEnv.execute(); } }', 'ground_truth': 'public JobExecutionResult executePipeline() throws Exception { if (options.isStreaming()) { if (this.flinkStreamEnv == null) { throw new RuntimeException("FlinkPipelineExecutionEnvironment not initialized."); } if (this.flinkPipelineTranslator == null) { throw new RuntimeException("FlinkPipelineTranslator not initialized."); } return this.flinkStreamEnv.execute(); } else { if (this.flinkBatchEnv == null) { throw new RuntimeException("FlinkPipelineExecutionEnvironment not initialized."); } if (this.flinkPipelineTranslator == null) { throw new RuntimeException("FlinkPipelineTranslator not initialized."); } return this.flinkBatchEnv.execute(); } }', 'output': 'public JobExecutionResult execute() throws Exception { if (options.isStreaming()) { System.out.println("Plan: " + this.flinkStreamEnv.getExecutionPlan()); if (this.flinkStreamEnv == null) { throw new RuntimeException("JobExecutionEnvironment not initialized."); } if (this.flinkPipelineTranslator == null) { throw new RuntimeException("JobGraphTranslator not initialized."); } return this.flinkStreamEnv.execute(); } else { if (this.flinkBatchEnv == null) { throw new RuntimeException("JobExecutionEnvironment not initialized."); } if (this.flinkPipelineTranslator == null) { throw new RuntimeException("JobGraphTranslator not initialized."); } return this.flinkBatchEnv.execute(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private <T> void XXX(TupleTag<T> tag, WindowedValue<T> windowedElem) { if (!outputTags.contains(tag)) { if (outputTags.size() >= MAX_SIDE_OUTPUTS) { throw new IllegalArgumentException( "the number of side outputs has exceeded a limit of " + MAX_SIDE_OUTPUTS); } outputTags.add(tag); } outputManager.output(tag, windowedElem); if (stepContext != null) { stepContext.noteSideOutput(tag, windowedElem); } }', 'ground_truth': 'private <T> void outputWindowedValue(TupleTag<T> tag, WindowedValue<T> windowedElem) { if (!outputTags.contains(tag)) { if (outputTags.size() >= MAX_SIDE_OUTPUTS) { throw new IllegalArgumentException( "the number of outputs has exceeded a limit of " + MAX_SIDE_OUTPUTS); } outputTags.add(tag); } outputManager.output(tag, windowedElem); if (stepContext != null) { stepContext.noteOutput(tag, windowedElem); } }', 'output': 'private <T> void output(TupleTag<T> tag, WindowedValue<T> windowedElem) { if (!outputTags.contains(tag)) { if (outputTags.size() >= MAX_SIDE_OUTPUTS) { throw new IllegalArgumentException( "the number of side outputs has exceeded a limit of " + MAX_SIDE_OUTPUTS); } outputTags.add(tag); } outputManager.output(tag, windowedElem); if (stepContext != null) { stepContext.noteSideOutput(tag, windowedElem); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(MessageIn<IMutation> msg) { super.XXX(msg); viewWriteMetrics.viewReplicasSuccess.inc(); }', 'ground_truth': 'public void onResponse(Message<IMutation> msg) { super.onResponse(msg); viewWriteMetrics.viewReplicasSuccess.inc(); }', 'output': 'public void onMessage(MessageIn<IMutation> msg) { super.onMessage(msg); viewWriteMetrics.viewReplicasSuccess.inc(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(ColumnFamily returnCF, ReducingIterator<IColumn> reducedColumns, int gcBefore) { for (IColumn column : reducedColumns) { if (!column.isMarkedForDelete() || column.getLocalDeletionTime() > gcBefore) returnCF.addColumn(column); } }', 'ground_truth': 'public void collectReducedColumns(ColumnFamily returnCF, Iterator<IColumn> reducedColumns, int gcBefore) { while (reducedColumns.hasNext()) { IColumn column = reducedColumns.next(); if (!column.isMarkedForDelete() || column.getLocalDeletionTime() > gcBefore) returnCF.addColumn(column); } }', 'output': 'public void addColumns(ColumnFamily returnCF, ReducingIterator<IColumn> reducedColumns, int gcBefore) { for (IColumn column : reducedColumns) { if (!column.isMarkedForDelete() || column.getLocalDeletionTime() > gcBefore) returnCF.addColumn(column); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { assertEquals(V_2_3_21, new PropertyAssignments(V_2_3_21).getIncompatibleImprovements()); assertEquals(new Version(2, 3, 0), new PropertyAssignments(new Version(2, 3, 20)).getIncompatibleImprovements()); try { new PropertyAssignments(new Version(2, 3, 22)); fail(); } catch (IllegalArgumentException e) { assertTrue(e.getMessage().contains("upgrade")); } PropertyAssignments pa1; PropertyAssignments pa2; pa1 = new PropertyAssignments(V_2_3_21); pa2 = new PropertyAssignments(V_2_3_21); assertEquals(pa1, pa2); pa1.setSimpleMapWrapper(true); assertNotEquals(pa1, pa2); pa2.setSimpleMapWrapper(true); assertEquals(pa1, pa2); pa1.setExposeFields(true); assertNotEquals(pa1, pa2); pa2.setExposeFields(true); assertEquals(pa1, pa2); pa1.setExposureLevel(0); assertNotEquals(pa1, pa2); pa2.setExposureLevel(0); assertEquals(pa1, pa2); pa1.setExposureLevel(1); assertNotEquals(pa1, pa2); pa2.setExposureLevel(1); assertEquals(pa1, pa2); pa1.setDefaultDateType(TemplateDateModel.DATE); assertNotEquals(pa1, pa2); pa2.setDefaultDateType(TemplateDateModel.DATE); assertEquals(pa1, pa2); pa1.setStrict(true); assertNotEquals(pa1, pa2); pa2.setStrict(true); assertEquals(pa1, pa2); pa1.setUseModelCache(true); assertNotEquals(pa1, pa2); pa2.setUseModelCache(true); assertEquals(pa1, pa2); AlphabeticalMethodSorter ms = new AlphabeticalMethodSorter(true); pa1.setMethodSorter(ms); assertNotEquals(pa1, pa2); pa2.setMethodSorter(ms); assertEquals(pa1, pa2); MethodAppearanceFineTuner maft = new MethodAppearanceFineTuner() { public void process(MethodAppearanceDecisionInput in, MethodAppearanceDecision out) { } }; pa1.setMethodAppearanceFineTuner(maft); assertNotEquals(pa1, pa2); pa2.setMethodAppearanceFineTuner(maft); assertEquals(pa1, pa2); }', 'ground_truth': 'public void testBeansWrapperFactoryEquals() throws Exception { assertEquals(V_2_3_21, new BeansWrapperBuilder(V_2_3_21).getIncompatibleImprovements()); assertEquals(new Version(2, 3, 0), new BeansWrapperBuilder(new Version(2, 3, 20)).getIncompatibleImprovements()); try { new BeansWrapperBuilder(new Version(2, 3, 22)); fail(); } catch (IllegalArgumentException e) { assertTrue(e.getMessage().contains("upgrade")); } BeansWrapperBuilder pa1; BeansWrapperBuilder pa2; pa1 = new BeansWrapperBuilder(V_2_3_21); pa2 = new BeansWrapperBuilder(V_2_3_21); assertEquals(pa1, pa2); pa1.setSimpleMapWrapper(true); assertNotEquals(pa1, pa2); assertFalse(pa1.hashCode() == pa2.hashCode()); pa2.setSimpleMapWrapper(true); assertEquals(pa1, pa2); assertTrue(pa1.hashCode() == pa2.hashCode()); pa1.setExposeFields(true); assertNotEquals(pa1, pa2); assertFalse(pa1.hashCode() == pa2.hashCode()); pa2.setExposeFields(true); assertEquals(pa1, pa2); assertTrue(pa1.hashCode() == pa2.hashCode()); pa1.setExposureLevel(0); assertNotEquals(pa1, pa2); assertFalse(pa1.hashCode() == pa2.hashCode()); pa2.setExposureLevel(0); assertEquals(pa1, pa2); assertTrue(pa1.hashCode() == pa2.hashCode()); pa1.setExposureLevel(1); assertNotEquals(pa1, pa2); assertFalse(pa1.hashCode() == pa2.hashCode()); pa2.setExposureLevel(1); assertEquals(pa1, pa2); assertTrue(pa1.hashCode() == pa2.hashCode()); pa1.setDefaultDateType(TemplateDateModel.DATE); assertNotEquals(pa1, pa2); pa2.setDefaultDateType(TemplateDateModel.DATE); assertEquals(pa1, pa2); assertTrue(pa1.hashCode() == pa2.hashCode()); pa1.setStrict(true); assertNotEquals(pa1, pa2); assertFalse(pa1.hashCode() == pa2.hashCode()); pa2.setStrict(true); assertEquals(pa1, pa2); assertTrue(pa1.hashCode() == pa2.hashCode()); pa1.setUseModelCache(true); assertNotEquals(pa1, pa2); assertFalse(pa1.hashCode() == pa2.hashCode()); pa2.setUseModelCache(true); assertEquals(pa1, pa2); assertTrue(pa1.hashCode() == pa2.hashCode()); AlphabeticalMethodSorter ms = new AlphabeticalMethodSorter(true); pa1.setMethodSorter(ms); assertNotEquals(pa1, pa2); pa2.setMethodSorter(ms); assertEquals(pa1, pa2); assertTrue(pa1.hashCode() == pa2.hashCode()); MethodAppearanceFineTuner maft = new MethodAppearanceFineTuner() { public void process(MethodAppearanceDecisionInput in, MethodAppearanceDecision out) { } }; pa1.setMethodAppearanceFineTuner(maft); assertNotEquals(pa1, pa2); pa2.setMethodAppearanceFineTuner(maft); assertEquals(pa1, pa2); assertTrue(pa1.hashCode() == pa2.hashCode()); }', 'output': 'public void testGetIncompatibleImprovements() throws Exception { assertEquals(V_2_3_21, new PropertyAssignments(V_2_3_21).getIncompatibleImprovements()); assertEquals(new Version(2, 3, 0), new PropertyAssignments(new Version(2, 3, 20)).getIncompatibleImprovements()); try { new PropertyAssignments(new Version(2, 3, 22)); fail(); } catch (IllegalArgumentException e) { assertTrue(e.getMessage().contains("upgrade")); } PropertyAssignments pa1; PropertyAssignments pa2; pa1 = new PropertyAssignments(V_2_3_21); pa2 = new PropertyAssignments(V_2_3_21); assertEquals(pa1, pa2); pa1.setSimpleMapWrapper(true); assertNotEquals(pa1, pa2); pa2.setSimpleMapWrapper(true); assertEquals(pa1, pa2); pa1.setExposeFields(true); assertNotEquals(pa1, pa2); pa2.setExposeFields(true); assertEquals(pa1, pa2); pa1.setExposureLevel(0); assertNotEquals(pa1, pa2); pa2.setExposureLevel(0); assertEquals(pa1, pa2); pa1.setExposureLevel(1); assertNotEquals(pa1, pa2); pa2.setExposureLevel(1); assertEquals(pa1, pa2); pa1.setDefaultDateType(TemplateDateModel.DATE); assertNotEquals(pa1, pa2); pa2.setDefaultDateType(TemplateDateModel.DATE); assertEquals(pa1, pa2); pa1.setStrict(true); assertNotEquals(pa1, pa2); pa2.setStrict(true); assertEquals(pa1, pa2); pa1.setUseModelCache(true); assertNotEquals(pa1, pa2); pa2.setUseModelCache(true); assertEquals(pa1, pa2); AlphabeticalMethodSorter ms = new AlphabeticalMethodSorter(true); pa1.setMethodSorter(ms); assertNotEquals(pa1, pa2); pa'}
{'instruction': 'Please refactor the following code:', 'input': 'public PassiveState XXX() { clusterGlobalLock.readLock().lock(); try { readLock.lock(); try { return desiredStateEntity.XXX(); } finally { readLock.unlock(); } } finally { clusterGlobalLock.readLock().unlock(); } }', 'ground_truth': 'public MaintenanceState getMaintenanceState() { clusterGlobalLock.readLock().lock(); try { readLock.lock(); try { return desiredStateEntity.getMaintenanceState(); } finally { readLock.unlock(); } } finally { clusterGlobalLock.readLock().unlock(); } }', 'output': 'public PassiveState getPassiveState() { clusterGlobalLock.readLock().lock(); try { readLock.lock(); try { return desiredStateEntity.getPassiveState(); } finally { readLock.unlock(); } } finally { clusterGlobalLock.readLock().unlock(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { AmbariMetaInfo metaInfo = injector.getInstance(AmbariMetaInfo.class); metaInfo.init(); clusters.addCluster("c1"); Cluster c1 = clusters.getCluster("c1"); Assert.assertEquals("c1", c1.getClusterName()); Assert.assertEquals(1, c1.getClusterId()); clusters.addHost("h1"); Host host = clusters.getHost("h1"); host.setIPv4("ipv4"); host.setIPv6("ipv6"); host.setOsType("centos5"); host.persist(); c1.setDesiredStackVersion(new StackId("HDP-0.1")); clusters.mapHostToCluster("h1", "c1"); HostEntity entity = hostDAO.findByName("h1"); HostStateEntity stateEntity = entity.getHostStateEntity(); Assert.assertNull(stateEntity.getPassiveState()); Assert.assertEquals(PassiveState.ACTIVE, host.getPassiveState(c1.getClusterId())); host.setPassiveState(c1.getClusterId(), PassiveState.PASSIVE); entity = hostDAO.findByName("h1"); stateEntity = entity.getHostStateEntity(); Assert.assertNotNull(stateEntity.getPassiveState()); Assert.assertEquals(PassiveState.PASSIVE, host.getPassiveState(c1.getClusterId())); }', 'ground_truth': 'public void testHostMaintenance() throws Exception { AmbariMetaInfo metaInfo = injector.getInstance(AmbariMetaInfo.class); metaInfo.init(); clusters.addCluster("c1"); Cluster c1 = clusters.getCluster("c1"); Assert.assertEquals("c1", c1.getClusterName()); Assert.assertEquals(1, c1.getClusterId()); clusters.addHost("h1"); Host host = clusters.getHost("h1"); host.setIPv4("ipv4"); host.setIPv6("ipv6"); host.setOsType("centos5"); host.persist(); c1.setDesiredStackVersion(new StackId("HDP-0.1")); clusters.mapHostToCluster("h1", "c1"); HostEntity entity = hostDAO.findByName("h1"); HostStateEntity stateEntity = entity.getHostStateEntity(); Assert.assertNull(stateEntity.getMaintenanceState()); Assert.assertEquals(MaintenanceState.OFF, host.getMaintenanceState(c1.getClusterId())); host.setMaintenanceState(c1.getClusterId(), MaintenanceState.ON); entity = hostDAO.findByName("h1"); stateEntity = entity.getHostStateEntity(); Assert.assertNotNull(stateEntity.getMaintenanceState()); Assert.assertEquals(MaintenanceState.ON, host.getMaintenanceState(c1.getClusterId())); }', 'output': 'public void testMapHostToCluster() throws Exception { AmbariMetaInfo metaInfo = injector.getInstance(AmbariMetaInfo.class); metaInfo.init(); clusters.addCluster("c1"); Cluster c1 = clusters.getCluster("c1"); Assert.assertEquals("c1", c1.getClusterName()); Assert.assertEquals(1, c1.getClusterId()); clusters.addHost("h1"); Host host = clusters.getHost("h1"); host.setIPv4("ipv4"); host.setIPv6("ipv6"); host.setOsType("centos5"); host.persist(); c1.setDesiredStackVersion(new StackId("HDP-0.1")); clusters.mapHostToCluster("h1", "c1"); HostEntity entity = hostDAO.findByName("h1"); HostStateEntity stateEntity = entity.getHostStateEntity(); Assert.assertNull(stateEntity.getPassiveState()); Assert.assertEquals(PassiveState.ACTIVE, host.getPassiveState(c1.getClusterId())); host.setPassiveState(c1.getClusterId(), PassiveState.PASSIVE); entity = hostDAO.findByName("h1"); stateEntity = entity.getHostStateEntity(); Assert.assertNotNull(stateEntity.getPassiveState()); Assert.assertEquals(PassiveState.PASSIVE, host.getPassiveState(c1.getClusterId())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { List<AlertDefinitionEntity> definitions = dao.findAll(); Assert.assertNotNull(definitions); AlertDefinitionEntity definition = definitions.get(2); AlertDefinitionEntity retrieved = dao.XXX(definition.getDefinitionId()); Assert.assertEquals(definition, retrieved); }', 'ground_truth': 'public void testFindById() { List<AlertDefinitionEntity> definitions = dao.findAll(); assertNotNull(definitions); AlertDefinitionEntity definition = definitions.get(2); AlertDefinitionEntity retrieved = dao.findById(definition.getDefinitionId()); assertEquals(definition, retrieved); }', 'output': 'public void testFindById() { List<AlertDefinitionEntity> definitions = dao.findAll(); Assert.assertNotNull(definitions); AlertDefinitionEntity definition = definitions.get(2); AlertDefinitionEntity retrieved = dao.findById(definition.getDefinitionId()); Assert.assertEquals(definition, retrieved); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Multimap<InetAddressAndPort, Range<Token>> XXX(TokenMetadata metadata) { Multimap<InetAddressAndPort, Range<Token>> map = HashMultimap.create(); for (Token token : metadata.sortedTokens()) { Range<Token> range = metadata.getPrimaryRangeFor(token); for (InetAddressAndPort ep : calculateNaturalEndpoints(token, metadata)) { map.put(ep, range); } } return map; } public Multimap<InetAddressAndPort, Range<Token>> XXX()\n    { { return XXX(tokenMetadata.cloneOnlyTokenMap()); }', 'ground_truth': 'public RangesByEndpoint getAddressReplicas() { return getAddressReplicas(tokenMetadata.cloneOnlyTokenMap()); }', 'output': 'public Multimap<InetAddressAndPort, Range<Token>> getRanges(TokenMetadata metadata) { Multimap<InetAddressAndPort, Range<Token>> map = HashMultimap.create(); for (Token token : metadata.sortedTokens()) { Range<Token> range = metadata.getPrimaryRangeFor(token); for (InetAddressAndPort ep : calculateNaturalEndpoints(token, metadata)) { map.put(ep, range); } } return map; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(ByteBuffer buf, int cnt) { buf.putShort(LIVE_CNT_OFF, (short)cnt); assert cnt == getLiveCount(buf); }', 'ground_truth': 'private void setIndirectCount(ByteBuffer buf, int cnt) { assert check(cnt): cnt; buf.put(INDIRECT_CNT_OFF, (byte)cnt); }', 'output': 'public void setLiveCount(ByteBuffer buf, int cnt) { buf.putShort(LIVE_CNT_OFF, (short)cnt); assert cnt == getLiveCount(buf); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static long XXX() { return counterCacheSizeInMB; }', 'ground_truth': 'public static long getCounterCacheSizeInMiB() { return counterCacheSizeInMiB; }', 'output': 'public static long getCounterCacheSizeInMB() { return counterCacheSizeInMB; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected static boolean XXX(Cluster cluster) { boolean XXX = false; if (cluster != null) { Config rangerKnoxPluginProperties = cluster.getDesiredConfigByType(CONFIGURATION_TYPE_RANGER_KNOX_PLUGIN_PROPERTIES); if (rangerKnoxPluginProperties != null) { String rangerKnoxPluginEnabled = rangerKnoxPluginProperties.getProperties().get(PROPERTY_RANGER_KNOX_PLUGIN_ENABLED); if (StringUtils.isNotEmpty(rangerKnoxPluginEnabled)) { XXX =  "yes".equalsIgnoreCase(rangerKnoxPluginEnabled); } } } return XXX; }', 'ground_truth': 'protected static boolean isConfigEnabled(Cluster cluster, String configType, String propertyName) { boolean isRangerPluginEnabled = false; if (cluster != null) { Config rangerPluginProperties = cluster.getDesiredConfigByType(configType); if (rangerPluginProperties != null) { String rangerPluginEnabled = rangerPluginProperties.getProperties().get(propertyName); if (StringUtils.isNotEmpty(rangerPluginEnabled)) { isRangerPluginEnabled =  "yes".equalsIgnoreCase(rangerPluginEnabled); } } } return isRangerPluginEnabled; }', 'output': 'protected static boolean isRangerKnoxPluginEnabled(Cluster cluster) { boolean isRangerKnoxPluginEnabled = false; if (cluster != null) { Config rangerKnoxPluginProperties = cluster.getDesiredConfigByType(CONFIGURATION_TYPE_RANGER_KNOX_PLUGIN_PROPERTIES); if (rangerKnoxPluginProperties != null) { String rangerKnoxPluginEnabled = rangerKnoxPluginProperties.getProperties().get(PROPERTY_RANGER_KNOX_PLUGIN_ENABLED); if (StringUtils.isNotEmpty(rangerKnoxPluginEnabled)) { isRangerKnoxPluginEnabled =  "yes".equalsIgnoreCase(rangerKnoxPluginEnabled); } } } return isRangerKnoxPluginEnabled; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { SslContextFactory socketFactory = (SslContextFactory)GridTestUtils.sslTrustedFactory("node01", "trustone"); SSLContext sslCtx = socketFactory.create(); sslSrvSockFactory = sslCtx.getServerSocketFactory(); sslSockFactory = sslCtx.getSocketFactory(); fakeTcpDiscoverySpi = new TcpDiscoverySpi(); fakeTcpDiscoverySpi.setSoLinger(1); }', 'ground_truth': '@Override protected void beforeTest() throws Exception { SslContextFactory socketFactory = (SslContextFactory)GridTestUtils.sslTrustedFactory("node01", "trustone"); SSLContext sslCtx = socketFactory.create(); sslSrvSockFactory = sslCtx.getServerSocketFactory(); sslSockFactory = sslCtx.getSocketFactory(); fakeTcpDiscoverySpi = new TcpDiscoverySpi(); fakeTcpDiscoverySpi.setSoLinger(1); }', 'output': 'public void setUp() { SslContextFactory socketFactory = (SslContextFactory)GridTestUtils.sslTrustedFactory("node01", "trustone"); SSLContext sslCtx = socketFactory.create(); sslSrvSockFactory = sslCtx.getServerSocketFactory(); sslSockFactory = sslCtx.getSocketFactory(); fakeTcpDiscoverySpi = new TcpDiscoverySpi(); fakeTcpDiscoverySpi.setSoLinger(1); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(DecoratedKey prevKey, DecoratedKey key, UnfilteredRowIterator iterator) { outputHandler.warn(String.format("Out of order row detected (%s found after %s)", key, prevKey)); outOfOrder.add(ImmutableBTreePartition.create(iterator)); }', 'ground_truth': 'private void saveOutOfOrderPartition(DecoratedKey prevKey, DecoratedKey key, UnfilteredRowIterator iterator) { outputHandler.warn(String.format("Out of order partition detected (%s found after %s)", keyString(key), keyString(prevKey))); outOfOrder.add(ImmutableBTreePartition.create(iterator)); }', 'output': 'private void addOutOfOrder(DecoratedKey prevKey, DecoratedKey key, UnfilteredRowIterator iterator) { outputHandler.warn(String.format("Out of order row detected (%s found after %s)", key, prevKey)); outOfOrder.add(ImmutableBTreePartition.create(iterator)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public CheckpointEntry XXX(Integer grpId) { synchronized (earliestCp) { CheckpointEntry lastCp = lastCheckpoint(); earliestCp.keySet().removeIf(grpPart -> grpId.equals(grpPart.getGroupId())); return lastCp; } }', 'ground_truth': 'public CheckpointEntry removeFromEarliestCheckpoints(Integer grpId) { synchronized (earliestCp) { CheckpointEntry lastCp = lastCheckpoint(); earliestCp.keySet().removeIf(grpPart -> grpId.equals(grpPart.getGroupId())); return lastCp; } }', 'output': 'public CheckpointEntry getLastCheckpoint(Integer grpId) { synchronized (earliestCp) { CheckpointEntry lastCp = lastCheckpoint(); earliestCp.keySet().removeIf(grpPart -> grpId.equals(grpPart.getGroupId())); return lastCp; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX() throws StorageException { Collection<File> tmpFiles = new HashSet<>(); for (File walDir : F.asList(walWorkDir, walArchiveDir)) { tmpFiles.addAll(F.asList(walDir.listFiles(WAL_SEGMENT_TEMP_FILE_FILTER))); tmpFiles.addAll(F.asList(walDir.listFiles(WAL_SEGMENT_TEMP_FILE_COMPACTED_FILTER))); } for (File tmpFile : tmpFiles) { if (tmpFile.exists() && !tmpFile.delete()) { throw new StorageException("Failed to delete previously created temp file " + "(make sure Ignite process has enough rights): " + tmpFile.getAbsolutePath()); } } File[] allFiles = walWorkDir.listFiles(WAL_SEGMENT_FILE_FILTER); if (isArchiverEnabled() && !F.isEmpty(allFiles) && allFiles.length > dsCfg.getWalSegments()) { throw new StorageException("Failed to initialize wal (work directory contains incorrect " + "number of segments) [cur=" + allFiles.length + ", expected=" + dsCfg.getWalSegments() + \']\'); } if (F.isEmpty(allFiles)) { File first = new File(walWorkDir, fileName(0)); createFile(first); } else if (isArchiverEnabled()) checkFiles(0, false, null, null); }', 'ground_truth': 'private void prepareAndCheckWalFiles() throws StorageException { Collection<File> tmpFiles = new HashSet<>(); for (File walDir : F.asList(walWorkDir, walArchiveDir)) { tmpFiles.addAll(F.asList(walDir.listFiles(WAL_SEGMENT_TEMP_FILE_FILTER))); tmpFiles.addAll(F.asList(walDir.listFiles(WAL_SEGMENT_TEMP_FILE_COMPACTED_FILTER))); } for (File tmpFile : tmpFiles) { if (tmpFile.exists() && !tmpFile.delete()) { throw new StorageException("Failed to delete previously created temp file " + "(make sure Ignite process has enough rights): " + tmpFile.getAbsolutePath()); } } if (F.isEmpty(walWorkDir.listFiles(WAL_SEGMENT_FILE_FILTER))) createFile(new File(walWorkDir, fileName(0))); if (isArchiverEnabled()) { moveSegmentsToArchive(); renameLastSegment(); formatWorkSegments(); checkFiles(0, false, null, null); } }', 'output': 'private void initWal() throws StorageException { Collection<File> tmpFiles = new HashSet<>(); for (File walDir : F.asList(walWorkDir, walArchiveDir)) { tmpFiles.addAll(F.asList(walDir.listFiles(WAL_SEGMENT_TEMP_FILE_FILTER))); tmpFiles.addAll(F.asList(walDir.listFiles(WAL_SEGMENT_TEMP_FILE_COMPACTED_FILTER))); } for (File tmpFile : tmpFiles) { if (tmpFile.exists() && !tmpFile.delete()) { throw new StorageException("Failed to delete previously created temp file " + "(make sure Ignite process has enough rights): " + tmpFile.getAbsolutePath()); } } File[] allFiles = walWorkDir.listFiles(WAL_SEGMENT_FILE_FILTER); if (isArchiverEnabled() && !F.isEmpty(allFiles) && allFiles.length > dsCfg.getWalSegments()) { throw new StorageException("Failed to initialize wal (work directory contains incorrect " + "number of segments) [cur=" + allFiles.length + ", expected=" + dsCfg.getWalSegments() + \']\'); } if (F.isEmpty(allFiles)) { File first = new File(walWorkDir, fileName(0)); createFile(first); } else if (isArchiverEnabled()) checkFiles(0, false, null, null); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected SSTableWriter XXX() { return SSTableWriter.create(createDescriptor(directory, metadata.ksName, metadata.cfName, formatType), 0, ActiveRepairService.UNREPAIRED_SSTABLE); }', 'ground_truth': 'protected SSTableWriter createWriter() { return SSTableWriter.create(createDescriptor(directory, metadata.ksName, metadata.cfName, formatType), 0, ActiveRepairService.UNREPAIRED_SSTABLE, new SerializationHeader(metadata, columns, RowStats.NO_STATS)); }', 'output': 'protected SSTableWriter writer() { return SSTableWriter.create(createDescriptor(directory, metadata.ksName, metadata.cfName, formatType), 0, ActiveRepairService.UNREPAIRED_SSTABLE); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(Keyspace keyspace, RingPosition pos, List<InetAddressAndPort> liveEps) { List<InetAddressAndPort> endpoints = keyspace.getReplicationStrategy().getNaturalEndpoints(pos); for (InetAddressAndPort endpoint : endpoints) { if (FailureDetector.instance.isAlive(endpoint)) liveEps.add(endpoint); } }', 'ground_truth': 'public EndpointsForRange getLiveNaturalReplicas(Keyspace keyspace, RingPosition pos) { EndpointsForRange replicas = keyspace.getReplicationStrategy().getNaturalReplicas(pos); return replicas.filter(r -> FailureDetector.instance.isAlive(r.endpoint())); }', 'output': 'public void addLiveEndpoints(Keyspace keyspace, RingPosition pos, List<InetAddressAndPort> liveEps) { List<InetAddressAndPort> endpoints = keyspace.getReplicationStrategy().getNaturalEndpoints(pos); for (InetAddressAndPort endpoint : endpoints) { if (FailureDetector.instance.isAlive(endpoint)) liveEps.add(endpoint); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(boolean value) { if (!value) { this.columnFamily = null; } } public void XXX(boolean value) { if (!value) { this.columnFamily = null; } } public void XXX(boolean value) { if (!value) { this.columnFamily = null; } } public void XXX(boolean value) { if (!value) { this.columnFamily = null; } }', 'ground_truth': 'public void setColumnParentIsSet(boolean value) { if (!value) { this.columnParent = null; } }', 'output': 'public void setColumnFamily(Boolean value) { if (!value) { this.columnFamily = null; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected CfDef XXX(Cassandra.Client client) throws InvalidRequestException, UnavailableException, TimedOutException, SchemaDisagreementException, TException, CharacterCodingException, NotFoundException, org.apache.cassandra.exceptions.InvalidRequestException, ConfigurationException { String query = "SELECT type," + "       comparator," + "       subcomparator," + "       default_validator," + "       key_validator," + "       key_aliases " + "FROM system.schema_columnfamilies " + "WHERE keyspace_name = \'%s\' " + "  AND columnfamily_name = \'%s\' "; CqlResult result = client.execute_cql3_query( ByteBufferUtil.bytes(String.format(query, keyspace, column_family)), Compression.NONE, ConsistencyLevel.ONE); if (result == null || result.rows == null || result.rows.isEmpty()) return null; Iterator<CqlRow> iteraRow = result.rows.iterator(); CfDef cfDef = new CfDef(); cfDef.keyspace = keyspace; cfDef.name = column_family; boolean cql3Table = false; if (iteraRow.hasNext()) { CqlRow cqlRow = iteraRow.next(); cfDef.column_type = ByteBufferUtil.string(cqlRow.columns.get(0).value); cfDef.comparator_type = ByteBufferUtil.string(cqlRow.columns.get(1).value); ByteBuffer subComparator = cqlRow.columns.get(2).value; if (subComparator != null) cfDef.subcomparator_type = ByteBufferUtil.string(subComparator); cfDef.default_validation_class = ByteBufferUtil.string(cqlRow.columns.get(3).value); cfDef.key_validation_class = ByteBufferUtil.string(cqlRow.columns.get(4).value); String keyAliases = ByteBufferUtil.string(cqlRow.columns.get(5).value); List<String> keys = FBUtilities.fromJsonList(keyAliases); if (keys.size() == 0) { CFDefinition cfDefinition = XXXinition(keyspace, column_family, client); for (ColumnIdentifier column : cfDefinition.keys.keySet()) { String key = column.toString(); String type = cfDefinition.keys.get(column).type.toString(); logger.debug("name: {}, type: {} ", key, type); keys.add(key); } } else cql3Table = true; } cfDef.column_metadata = getColumnMetadata(client, cql3Table); return cfDef; } protected CFDefinition XXXinition(String ks, String cf, Cassandra.Client client)\n            throws NotFoundException,\n            InvalidRequestException,\n            TException,\n            org.apache.cassandra.exceptions.InvalidRequestException,\n            ConfigurationException\n    { throws NotFoundException, InvalidRequestException, TException, org.apache.cassandra.exceptions.InvalidRequestException, ConfigurationException { KsDef ksDef = client.describe_keyspace(ks); for (CfDef cfDef : ksDef.cf_defs) { if (cfDef.name.equalsIgnoreCase(cf)) return new CFDefinition(CFMetaData.fromThrift(cfDef)); } return null; }', 'ground_truth': 'protected CfInfo getCfInfo(Cassandra.Client client) throws InvalidRequestException, UnavailableException, TimedOutException, SchemaDisagreementException, TException, NotFoundException, org.apache.cassandra.exceptions.InvalidRequestException, ConfigurationException, IOException { String query = "SELECT type," + "       comparator," + "       subcomparator," + "       default_validator," + "       key_validator," + "       key_aliases " + "FROM system.schema_columnfamilies " + "WHERE keyspace_name = \'%s\' " + "  AND columnfamily_name = \'%s\' "; CqlResult result = client.execute_cql3_query( ByteBufferUtil.bytes(String.format(query, keyspace, column_family)), Compression.NONE, ConsistencyLevel.ONE); if (result == null || result.rows == null || result.rows.isEmpty()) return null; Iterator<CqlRow> iteraRow = result.rows.iterator(); CfDef cfDef = new CfDef(); cfDef.keyspace = keyspace; cfDef.name = column_family; boolean cql3Table = false; if (iteraRow.hasNext()) { CqlRow cqlRow = iteraRow.next(); cfDef.column_type = ByteBufferUtil.string(cqlRow.columns.get(0).value); cfDef.comparator_type = ByteBufferUtil.string(cqlRow.columns.get(1).value); ByteBuffer subComparator = cqlRow.columns.get(2).value; if (subComparator != null) cfDef.subcomparator_type = ByteBufferUtil.string(subComparator); cfDef.default_validation_class = ByteBufferUtil.string(cqlRow.columns.get(3).value); cfDef.key_validation_class = ByteBufferUtil.string(cqlRow.columns.get(4).value); String keyAliases = ByteBufferUtil.string(cqlRow.columns.get(5).value); List<String> keys = FBUtilities.fromJsonList(keyAliases); if (keys.size() == 0) { CFDefinition cfDefinition = getCfDefinition(keyspace, column_family, client); for (ColumnIdentifier column : cfDefinition.keys.keySet()) { String key = column.toString(); String type = cfDefinition.keys.get(column).type.toString(); logger.debug("name: {}, type: {} ", key, type); keys.add(key); } } else cql3Table = true; } cfDef.column_metadata = getColumnMetadata(client); CfInfo cfInfo = new CfInfo(); cfInfo.cfDef = cfDef; if (cql3Table && !(parseType(cfDef.comparator_type) instanceof AbstractCompositeType)) cfInfo.compactCqlTable = true; if (cql3Table) cfInfo.cql3Table = true; return cfInfo; }', 'output': 'protected CfDef XXX(Cassandra.Client client) throws InvalidRequestException, UnavailableException, TimedOutException, SchemaDisagreementException, TException, CharacterCodingException, NotFoundException, org.apache.cassandra.exceptions.InvalidRequestException, ConfigurationException { String query = "SELECT type," + "       comparator," + "       subcomparator," + "       default_validator," + "       key_validator," + "       key_aliases " + "FROM system.schema_columnfamilies " + "WHERE keyspace_name = \'%s\' " + "  AND columnfamily_name = \'%s\' "; CqlResult result = client.execute_cql3_query( ByteBufferUtil.bytes(String.format(query, keyspace, column_family)), Compression.NONE, ConsistencyLevel.ONE); if (result == null || result.rows == null || result.rows.isEmpty()) return null; Iterator<CqlRow> iteraRow = result.rows.iterator(); CfDef cfDef = new CfDef(); cfDef.keyspace = keyspace; cfDef.name = column_family; boolean cql3Table = false; if (iteraRow.hasNext()) { CqlRow cqlRow = iteraRow.next(); cfDef.column_type = ByteBufferUtil.string(cqlRow.columns.get(0).value); cfDef.comparator_type = ByteBufferUtil.string(cqlRow.columns.get(1).value); ByteBuffer subComparator = cqlRow.columns.get(2).value; if (subComparator != null) cfDef.subcomparator_type = ByteBufferUtil.string(subComparator); cfDef.default_validation_class = ByteBufferUtil.string(cqlRow.columns.get(3).value); cfDef.key_validation_class = ByteBufferUtil.string(cqlRow.columns.get(4).value); String keyAliases = ByteBufferUtil.string(cqlRow.columns.get(5).value); List<String> keys = FBUtilities.fromJsonList(keyAliases); if (keys.size() == 0) { CFDefinition cfDefinition = definition(ks,'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(GridClient client, String baselineArgs) throws Throwable { try { VisorBaselineTaskResult res = executeTask(client, VisorBaselineTask.class, arg(ADD, baselineArgs)); baselinePrint0(res); } catch (Throwable e) { log("Failed to add nodes to baseline."); throw e; } }', 'ground_truth': 'private void baseline(GridClient client, BaselineArguments baselineArgs) throws Throwable { try { VisorBaselineTaskResult res = executeTask(client, VisorBaselineTask.class, toVisorArguments(baselineArgs)); baselinePrint0(res); } catch (Throwable e) { log("Failed to execute baseline command=\'" + baselineArgs.getCmd().text() + "\'", e); throw e; } }', 'output': 'private void addNodesToBaseline(GridClient client, String baselineArgs) throws Throwable { try { VisorBaselineTaskResult res = executeTask(client, VisorBaselineTask.class, arg(ADD, baselineArgs)); baselinePrint0(res); } catch (Throwable e) { log("Failed to add nodes to baseline."); throw e; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { mode = TopologyModificationMode.ADD; AffinityFunction aff0 = new RendezvousAffinityFunction(true, 1024); AffinityFunction aff1 = new RendezvousAffinityFunctionOld(true, 1024); GridTestUtils.setFieldValue(aff1, "ignite", ignite); affinityCompatibility(aff0, aff1); }', 'ground_truth': 'public void testAffinityCompatibility() { AffinityFunction aff0 = new RendezvousAffinityFunction(true, 1024); AffinityFunction aff1 = new RendezvousAffinityFunctionOld(true, 1024); GridTestUtils.setFieldValue(aff1, "ignite", ignite); structuralCompatibility(aff0, aff1); }', 'output': 'public void testAffinityCompatibility() { mode = TopologyModificationMode.ADD; AffinityFunction aff0 = new RendezvousAffinityFunction(true, 1024); AffinityFunction aff1 = new RendezvousAffinityFunctionOld(true, 1024); GridTestUtils.setFieldValue(aff1, "ignite", ignite); affinityCompatibility(aff0, aff1); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { log.debug("iaEnded"); kvStoreForTesting.incrementCounter(InteractionBoundaryProbe.class, "iaEnded"); }', 'ground_truth': 'public void afterLeavingTransactionalBoundary(InteractionSession interactionSession) { log.debug("iaEnded"); kvStoreForTesting.incrementCounter(InteractionBoundaryProbe.class, "iaEnded"); }', 'output': 'public void onIaEnded() { log.debug("iaEnded"); kvStoreForTesting.incrementCounter(InteractionBoundaryProbe.class, "iaEnded"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static boolean XXX(String keyspaceName) { return SYSTEM_KEYSPACE_NAMES.contains(keyspaceName.toLowerCase()); }', 'ground_truth': 'public static boolean isLocalSystemKeyspace(String keyspaceName) { return LOCAL_SYSTEM_KEYSPACE_NAMES.contains(keyspaceName.toLowerCase()); }', 'output': 'public static boolean isSystemKeyspace(String keyspaceName) { return SYSTEM_KEYSPACES.contains(keyspaceName.toLowerCase()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean XXX(final @NonNull ManagedObject adapter) { return ManagedObjects.bookmark(adapter) .map(changeKindByEnlistedAdapter::containsKey) .orElse(false); }', 'ground_truth': 'private boolean isEnlistedWrtChangeKind(final @NonNull ManagedObject adapter) { return ManagedObjects.bookmark(adapter) .map(changeKindByEnlistedAdapter::containsKey) .orElse(false); }', 'output': 'private boolean isBookmarked(final @NonNull ManagedObject adapter) { return ManagedObjects.bookmark(adapter) .map(changeKindByEnlistedAdapter::containsKey) .orElse(false); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private TemplateTestCase XXX(Element testCaseElem, Pattern nameFilter, Version testedIcI, boolean firstTestedIcI) throws Exception { final String caseName = StringUtil.emptyToNull(testCaseElem.getAttribute("name")); if (caseName == null) throw new Exception("Invalid XML: the \\"name\\" attribute is mandatory."); if (nameFilter != null && !nameFilter.matcher(caseName).matches()) return null; final String templateName; final String expectedFileName; { final String baseName; final String namePostfix; { int nameBaseSep = caseName.indexOf(BASE_NAME_SEPARATOR); baseName = nameBaseSep == -1 ? caseName : caseName.substring(0, nameBaseSep); namePostfix = nameBaseSep == -1 ? "" : caseName.substring(nameBaseSep + 1); } { String s = StringUtil.emptyToNull(testCaseElem.getAttribute(ATTR_TEMPLATE)); templateName = s != null ? s : baseName + ".ftl"; } { String s = StringUtil.emptyToNull(testCaseElem.getAttribute(ATTR_EXPECTED)); expectedFileName = s != null ? s : baseName + namePostfix + ".txt"; } } final boolean noOutput; { String s = StringUtil.emptyToNull(testCaseElem.getAttribute(ATTR_NO_OUTPUT)); noOutput = s == null ? false : StringUtil.getYesNo(s); } final Map<String, String> testCaseSettings = getCaseFMSettings(testCaseElem); final boolean caseSpecifiesIcI = testCaseSettings.containsKey(Configuration.INCOMPATIBLE_IMPROVEMENTS) || testCaseSettings.containsKey(Configuration.INCOMPATIBLE_ENHANCEMENTS); if (caseSpecifiesIcI && !firstTestedIcI) { return null; } TemplateTestCase result = new TemplateTestCase( caseSpecifiesIcI ? caseName : caseName + "(ici=" + testedIcI + ")", templateName, expectedFileName, noOutput, testedIcI); for (Map.Entry<String, String> setting : testSuiteSettings.entrySet()) { result.setSetting(setting.getKey(), setting.getValue()); } for (Map.Entry<String, String> setting : testCaseSettings.entrySet()) { result.setSetting(setting.getKey(), setting.getValue()); } return result; }', 'ground_truth': 'private List<TemplateTestCase> createTestCasesFromElement(Element testCaseElem) throws Exception { final String caseName = StringUtil.emptyToNull(testCaseElem.getAttribute("name")); if (caseName == null) throw new Exception("Invalid XML: the \\"name\\" attribute is mandatory."); if (testCaseNameFilter != null && !testCaseNameFilter.matcher(caseName).matches()) { return Collections.emptyList(); } final String templateName; final String expectedFileName; { final String beforeEndTN; final String afterEndTN; { int tBNameSep = caseName.indexOf(END_TEMPLATE_NAME_MARK); beforeEndTN = tBNameSep == -1 ? caseName : caseName.substring(0, tBNameSep); afterEndTN = tBNameSep == -1 ? "" : caseName.substring(tBNameSep + END_TEMPLATE_NAME_MARK.length()); } { String s = StringUtil.emptyToNull(testCaseElem.getAttribute(ATTR_TEMPLATE)); templateName = s != null ? s : beforeEndTN + ".ftl"; } { String s = StringUtil.emptyToNull(testCaseElem.getAttribute(ATTR_EXPECTED)); expectedFileName = s != null ? s : beforeEndTN + afterEndTN + ".txt"; } } final boolean noOutput; { String s = StringUtil.emptyToNull(testCaseElem.getAttribute(ATTR_NO_OUTPUT)); noOutput = s == null ? false : StringUtil.getYesNo(s); } final Map<String, String> testCaseSettings = getCaseFMSettings(testCaseElem); final List<Version> icisToTest; { final String testCaseIcis = testCaseSettings.get(Configuration.INCOMPATIBLE_IMPROVEMENTS) != null ? testCaseSettings.get(Configuration.INCOMPATIBLE_IMPROVEMENTS) : testCaseSettings.get(Configuration.INCOMPATIBLE_ENHANCEMENTS); icisToTest = testCaseIcis != null ? parseVersionList(testCaseIcis) : testSuiteIcis; if (icisToTest.isEmpty()) { throw new Exception("The incompatible_improvement list was empty"); } } List<TemplateTestCase> result = new ArrayList<TemplateTestCase>(); for (Version iciToTest : icisToTest) { TemplateTestCase testCase = new TemplateTestCase( caseName + "(ici=" + iciToTest + ")", caseName, templateName, expectedFileName, noOutput, iciToTest); for (Map.Entry<String, String> setting : testSuiteSettings.entrySet()) { testCase.setSetting(setting.getKey(), setting.getValue()); } for (Map.Entry<String, String> setting : testCaseSettings.entrySet()) { testCase.setSetting(setting.getKey(), setting.getValue()); } result.add(testCase); } return result; }', 'output': 'private TemplateTestCase createTemplateTestCase(Element testCaseElem, Pattern nameFilter, Version testedIcI, boolean firstTestedIcI) throws Exception { final String caseName = StringUtil.emptyToNull(testCaseElem.getAttribute("name")); if (caseName == null) throw new Exception("Invalid XML: the \\"name\\" attribute is mandatory."); if (nameFilter != null && !nameFilter.matcher(caseName).matches()) return null; final String templateName; final String expectedFileName; { final String baseName; final String namePostfix; { int nameBaseSep = caseName.indexOf(BASE_NAME_SEPARATOR); baseName = nameBaseSep == -1 ? caseName : caseName.substring(0, nameBaseSep); namePostfix = nameBaseSep == -1 ? "" : caseName.substring(nameBaseSep + 1); } { String s = StringUtil.emptyToNull(testCaseElem.getAttribute(ATTR_TEMPLATE)); templateName = s != null ? s : baseName + ".ftl"; } { String s = StringUtil.emptyToNull(testCaseElem.getAttribute(ATTR_EXPECTED)); expectedFileName = s != null ? s : baseName + namePostfix + ".txt"; } } final boolean noOutput; { String s = StringUtil.emptyToNull(testCaseElem.getAttribute(ATTR_NO_OUTPUT)); noOutput = s == null ? false : StringUtil.getYesNo(s); } final Map<String, String> testCaseSettings = getCaseFMSettings(testCaseElem); final boolean caseSpecifiesIcI = testCaseSettings.containsKey(Configuration.INCOMPATIBLE_IMPROVEMENTS) || testCaseSettings.containsKey(Configuration.INCOMPATIBLE_ENHANCEMENTS); if (caseSpecifiesIcI && !firstTestedIcI) { return null; } TemplateTestCase result = new TemplateTestCase( caseSpecifiesIcI ? caseName : caseName + "(ici=" + testedIcI + ")", templateName, expectedFileName, noOutput, testedIcI); for (Map.Entry<String, String> setting : testSuiteSettings.entrySet()) { result.set'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { List<ParallelInstruction> instructions = Arrays.asList( makeSourceInstruction(StringUtf8Coder.of()), makeSinkInstruction(StringUtf8Coder.of(), 0)); FakeWindmillServer server = new FakeWindmillServer(errorCollector); server.setIsReady(false); StreamingConfigTask streamingConfig = new StreamingConfigTask(); streamingConfig.setStreamingComputationConfigs( ImmutableList.of(makeDefaultStreamingComputationConfig(instructions))); streamingConfig.setWindmillServiceEndpoint("foo"); WorkItem workItem = new WorkItem(); workItem.setStreamingConfigTask(streamingConfig); when(mockWorkUnitClient.getGlobalStreamingConfigWorkItem()).thenReturn(Optional.of(workItem)); StreamingDataflowWorkerOptions options = createTestingPipelineOptions(server, "--experiments=enable_streaming_engine"); StreamingDataflowWorker worker = makeWorker(instructions, options, true /* publishCounters */); worker.start(); final int numIters = 2000; for (int i = 0; i < numIters; ++i) { server.addWorkToOffer(makeInput(i, TimeUnit.MILLISECONDS.toMicros(i))); } Map<Long, Windmill.WorkItemCommitRequest> result = server.waitForAndGetCommits(numIters); worker.stop(); for (int i = 0; i < numIters; ++i) { assertTrue(result.containsKey((long) i)); assertEquals( makeExpectedOutput(i, TimeUnit.MILLISECONDS.toMicros(i)).build(), result.get((long) i)); } }', 'ground_truth': 'public void testBasic() throws Exception { List<ParallelInstruction> instructions = Arrays.asList( makeSourceInstruction(StringUtf8Coder.of()), makeSinkInstruction(StringUtf8Coder.of(), 0)); FakeWindmillServer server = new FakeWindmillServer(errorCollector); server.setIsReady(false); StreamingConfigTask streamingConfig = new StreamingConfigTask(); streamingConfig.setStreamingComputationConfigs( ImmutableList.of(makeDefaultStreamingComputationConfig(instructions))); streamingConfig.setWindmillServiceEndpoint("foo"); WorkItem workItem = new WorkItem(); workItem.setStreamingConfigTask(streamingConfig); when(mockWorkUnitClient.getGlobalStreamingConfigWorkItem()).thenReturn(Optional.of(workItem)); StreamingDataflowWorkerOptions options = createTestingPipelineOptions(server); StreamingDataflowWorker worker = makeWorker(instructions, options, true /* publishCounters */); worker.start(); final int numIters = 2000; for (int i = 0; i < numIters; ++i) { server.addWorkToOffer(makeInput(i, TimeUnit.MILLISECONDS.toMicros(i))); } Map<Long, Windmill.WorkItemCommitRequest> result = server.waitForAndGetCommits(numIters); worker.stop(); for (int i = 0; i < numIters; ++i) { assertTrue(result.containsKey((long) i)); assertEquals( makeExpectedOutput(i, TimeUnit.MILLISECONDS.toMicros(i)).build(), result.get((long) i)); } }', 'output': 'public void testStreamingEngine() throws Exception { List<ParallelInstruction> instructions = Arrays.asList( makeSourceInstruction(StringUtf8Coder.of()), makeSinkInstruction(StringUtf8Coder.of(), 0)); FakeWindmillServer server = new FakeWindmillServer(errorCollector); server.setIsReady(false); StreamingConfigTask streamingConfig = new StreamingConfigTask(); streamingConfig.setStreamingComputationConfigs( ImmutableList.of(makeDefaultStreamingComputationConfig(instructions))); streamingConfig.setWindmillServiceEndpoint("foo"); WorkItem workItem = new WorkItem(); workItem.setStreamingConfigTask(streamingConfig); when(mockWorkUnitClient.getGlobalStreamingConfigWorkItem()).thenReturn(Optional.of(workItem)); StreamingDataflowWorkerOptions options = createTestingPipelineOptions(server, "--experiments=enable_streaming_engine"); StreamingDataflowWorker worker = makeWorker(instructions, options, true /* publishCounters */); worker.start(); final int numIters = 2000; for (int i = 0; i < numIters; ++i) { server.addWorkToOffer(makeInput(i, TimeUnit.MILLISECONDS.toMicros(i))); } Map<Long, Windmill.WorkItemCommitRequest> result = server.waitForAndGetCommits(numIters); worker.stop(); for (int i = 0; i < numIters; ++i) { assertTrue(result.containsKey((long) i)); assertEquals( makeExpectedOutput(i, TimeUnit.MILLISECONDS.toMicros(i)).build(), result.get((long) i)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(ChangeGlobalStateMessage msg) { if (msg.state() == ClusterState.INACTIVE) { forbidStartingNewTasks = true; awaitForWorkersStop(asyncDurableBackgroundTaskWorkers, true, log); } }', 'ground_truth': 'public void onStateChangeStarted(ChangeGlobalStateMessage msg) { if (msg.state() == ClusterState.INACTIVE) cancelTasks(); }', 'output': 'public void onChangeGlobalState(ChangeGlobalStateMessage msg) { if (msg.state() == ClusterState.INACTIVE) { forbidStartingNewTasks = true; awaitForWorkersStop(asyncDurableBackgroundTaskWorkers, true, log); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(ByteBuffer buf, long nextSnapshotTag, long lastSuccessfulSnapshotTag, int pageSize) { assert nextSnapshotTag != lastSuccessfulSnapshotTag : "nextSnapshotTag = " + nextSnapshotTag + ", lastSuccessfulSnapshotId = " + lastSuccessfulSnapshotTag; long last = getLastSnapshotTag(buf); assert last <= nextSnapshotTag : "last = " + last + ", nextSnapshotTag = " + nextSnapshotTag; if (nextSnapshotTag == last) return; int cntOfPage = countOfPageToTrack(pageSize); if (last <= lastSuccessfulSnapshotTag) { buf.putLong(LAST_SNAPSHOT_TAG_OFFSET, nextSnapshotTag); PageHandler.zeroMemory(buf, SIZE_FIELD_OFFSET, buf.capacity() - SIZE_FIELD_OFFSET); } else { int len = cntOfPage >> 3; int sizeOff = useLeftHalf(nextSnapshotTag) ? SIZE_FIELD_OFFSET : BITMAP_OFFSET + len; int sizeOff2 = !useLeftHalf(nextSnapshotTag) ? SIZE_FIELD_OFFSET : BITMAP_OFFSET + len; if (last - lastSuccessfulSnapshotTag == 1) { if ((nextSnapshotTag - last) % 2 == 0) PageHandler.copyMemory(buf, sizeOff, buf, sizeOff2, len + SIZE_FIELD_SIZE); } else { int newSize = 0; int i = 0; for (; i < len - 8; i += 8) { long newVal = buf.getLong(sizeOff + SIZE_FIELD_SIZE + i) | buf.getLong(sizeOff2 + SIZE_FIELD_SIZE + i); newSize += Long.bitCount(newVal); buf.putLong(sizeOff2 + SIZE_FIELD_SIZE + i, newVal); } for (; i < len; i ++) { byte newVal = (byte) (buf.get(sizeOff + SIZE_FIELD_SIZE + i) | buf.get(sizeOff2 + SIZE_FIELD_SIZE + i)); newSize += Integer.bitCount(newVal & 0xFF); buf.put(sizeOff2 + SIZE_FIELD_SIZE + i, newVal); } buf.putShort(sizeOff2, (short)newSize); } buf.putLong(LAST_SNAPSHOT_TAG_OFFSET, nextSnapshotTag); PageHandler.zeroMemory(buf, sizeOff, len + SIZE_FIELD_SIZE); } }', 'ground_truth': 'private long validateSnapshotTag(ByteBuffer buf, long nextSnapshotTag, long lastSuccessfulSnapshotTag, int pageSize) { assert nextSnapshotTag != lastSuccessfulSnapshotTag : "nextSnapshotTag = " + nextSnapshotTag + ", lastSuccessfulSnapshotId = " + lastSuccessfulSnapshotTag; long last = getLastSnapshotTag(buf); if(last > nextSnapshotTag) { PageHandler.zeroMemory(buf, LAST_SNAPSHOT_TAG_OFFSET, buf.capacity() - LAST_SNAPSHOT_TAG_OFFSET); setLastSnasphotTag(buf, nextSnapshotTag | CORRUPT_FLAG_MASK); return last; } if (nextSnapshotTag == last) return -1; int cntOfPage = countOfPageToTrack(pageSize); if (last <= lastSuccessfulSnapshotTag) { setLastSnasphotTag(buf, nextSnapshotTag); PageHandler.zeroMemory(buf, SIZE_FIELD_OFFSET, buf.capacity() - SIZE_FIELD_OFFSET); } else { int len = cntOfPage >> 3; int sizeOff = useLeftHalf(nextSnapshotTag) ? SIZE_FIELD_OFFSET : BITMAP_OFFSET + len; int sizeOff2 = !useLeftHalf(nextSnapshotTag) ? SIZE_FIELD_OFFSET : BITMAP_OFFSET + len; if (last - lastSuccessfulSnapshotTag == 1) { if ((nextSnapshotTag - last) % 2 == 0) PageHandler.copyMemory(buf, sizeOff, buf, sizeOff2, len + SIZE_FIELD_SIZE); } else { int newSize = 0; int i = 0; for (; i < len - 8; i += 8) { long newVal = buf.getLong(sizeOff + SIZE_FIELD_SIZE + i) | buf.getLong(sizeOff2 + SIZE_FIELD_SIZE + i); newSize += Long.bitCount(newVal); buf.putLong(sizeOff2 + SIZE_FIELD_SIZE + i, newVal); } for (; i < len; i ++) { byte newVal = (byte) (buf.get(sizeOff + SIZE_FIELD_SIZE + i) | buf.get(sizeOff2 + SIZE_FIELD_SIZE + i)); newSize += Integer.bitCount(newVal & 0xFF); buf.put(sizeOff2 + SIZE_FIELD_SIZE + i, newVal); } buf.putShort(sizeOff2, (short)newSize); } setLastSnasphotTag(buf, nextSnapshotTag); PageHandler.zeroMemory(buf, sizeOff, len + SIZE_FIELD_SIZE); } return -1; }', 'output': 'private void updateSnapshots(ByteBuffer buf, long nextSnapshotTag, long lastSuccessfulSnapshotTag, int pageSize) { assert nextSnapshotTag != lastSuccessfulSnapshotTag : "nextSnapshotTag = " + nextSnapshotTag + ", lastSuccessfulSnapshotTag = " + lastSuccessfulSnapshotTag; long last = getLastSnapshotTag(buf); assert last <= nextSnapshotTag : "last = " + last + ", nextSnapshotTag = " + nextSnapshotTag; if (nextSnapshotTag == last) return; int cntOfPage = countOfPageToTrack(pageSize); if (last <= lastSuccessfulSnapshotTag) { buf.putLong(LAST_SNAPSHOT_TAG_OFFSET, nextSnapshotTag); PageHandler.zeroMemory(buf, SIZE_FIELD_OFFSET, buf.capacity() - SIZE_FIELD_OFFSET); } else { int len = cntOfPage >> 3; int sizeOff = useLeftHalf(nextSnapshotTag) ? SIZE_FIELD_OFFSET : BITMAP_OFFSET + len; int sizeOff2 = !useLeftHalf(nextSnapshotTag) ? SIZE_FIELD_OFFSET : BITMAP_OFFSET + len; if (last - lastSuccessfulSnapshotTag == 1) { if ((nextSnapshotTag - last) % 2 == 0) PageHandler.copyMemory(buf, sizeOff, buf, sizeOff2, len + SIZE_FIELD_SIZE); } else { int newSize = 0; int i = 0; for (; i < len - 8; i += 8) { long newVal = buf.getLong(sizeOff + SIZE_FIELD_SIZE + i) | buf.getLong(sizeOff2 + SIZE_FIELD_SIZE + i); newSize += Long.bitCount(newVal); buf.putLong(sizeOff2 + SIZE_FIELD_SIZE + i, newVal); } for (; i < len; i ++) { byte newVal = (byte) (buf.get(sizeOff + SIZE_FIELD_SIZE + i) | buf.get(sizeOff2 + SIZE_FIELD_SIZE + i)); newSize += Integer.bitCount(newVal & 0xFF); buf.put(sizeOff'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { assertNull(cache().put("kk1", 100500)); assertEquals(100500, map.get("kk1")); GridCacheProjection<String, Integer> c = cache().flagsOn(GridCacheFlag.SKIP_STORE); assertNull(c.put("noStore", 123)); assertEquals(123, (Object) c.get("noStore")); assertNull(map.get("noStore")); assertTrue(c.remove("kk1", 100500)); assertEquals(100500, map.get("kk1")); assertNull(c.get("kk1")); assertEquals(100500, (Object) cache().get("kk1")); } public void XXXMultinode() throws Exception { final int nGrids = 3; for (int i = 1; i < nGrids; i++) startGrid(i); try { XXX(); } finally { for (int i = 1; i < nGrids; i++) stopGrid(i); } }', 'ground_truth': 'public void testSkipStoreFlag() throws Exception { assertNull(cache().put("kk1", 100500)); assertEquals(100500, map.get("kk1")); GridCacheProjection<String, Integer> c = cache().flagsOn(GridCacheFlag.SKIP_STORE); assertNull(c.put("noStore", 123)); assertEquals(123, (Object) c.get("noStore")); assertNull(map.get("noStore")); assertTrue(c.remove("kk1", 100500)); assertEquals(100500, map.get("kk1")); assertNull(c.get("kk1")); assertEquals(100500, (Object) cache().get("kk1")); }', 'output': 'public void testMultiNode() throws Exception { final int nGrids = 3; for (int i = 1; i < nGrids; i++) startGrid(i); try { assertNull(cache().put("kk1", 100500)); assertEquals(100500, map.get("kk1")); GridCacheProjection<String, Integer> c = cache().flagsOn(GridCacheFlag.SKIP_STORE); assertNull(c.put("noStore", 123)); assertEquals(123, (Object) c.get("noStore")); assertNull(map.get("noStore")); assertTrue(c.remove("kk1", 100500)); assertEquals(100500, map.get("kk1")); assertNull(c.get("kk1")); assertEquals(100500, (Object) cache().get("kk1")); } finally { for (int i = 1; i < nGrids; i++) stopGrid(i); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public double XXX() { return writeStats.mean(); }', 'ground_truth': 'public double getRecentWriteLatency() { return writeStats.getRecentLatency(); }', 'output': 'public double getMean() { return writeStats.mean(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static TransactionalProcessor XXX( MetaModelContext mmc, HasPersistenceManager pmProvider, FetchResultHandler fetchResultHandler) { val txMan = new _TxManagerInternal(mmc, pmProvider, fetchResultHandler); val isisInteractionTracker = mmc.getServiceRegistry() .lookupServiceElseFail(InteractionTracker.class); isisInteractionTracker.currentInteractionSession() .map(interaction->interaction.putAttribute(_TxManagerInternal.class, txMan)); return txMan; }', 'ground_truth': 'public static TransactionalProcessor newTransactionalProcessor( MetaModelContext mmc, HasPersistenceManager pmProvider) { val txMan = new _TxManagerInternal(mmc, pmProvider); val isisInteractionTracker = mmc.getServiceRegistry() .lookupServiceElseFail(InteractionTracker.class); isisInteractionTracker.currentInteractionSession() .map(interaction->interaction.putAttribute(_TxManagerInternal.class, txMan)); return txMan; }', 'output': 'public static TransactionalProcessor getTransactionalProcessor( MetaModelContext mmc, HasPersistenceManager pmProvider, FetchResultHandler fetchResultHandler) { val txMan = new _TxManagerInternal(mmc, pmProvider, fetchResultHandler); val isisInteractionTracker = mmc.getServiceRegistry() .lookupServiceElseFail(InteractionTracker.class); isisInteractionTracker.currentInteractionSession() .map(interaction->interaction.putAttribute(_TxManagerInternal.class, txMan)); return txMan; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static Coder XXX(String coderId) { Coder.Builder lengthPrefixed = Coder.newBuilder().addComponentCoderIds(coderId); lengthPrefixed .getSpecBuilder() .getSpecBuilder() .setUrn(ModelCoders.LENGTH_PREFIX_CODER_URN) .build(); return lengthPrefixed.build(); }', 'ground_truth': 'private static String addWrappedWithLengthPrefixCoder( String coderId, RunnerApi.Components.Builder components) { Coder.Builder lengthPrefixed = Coder.newBuilder().addComponentCoderIds(coderId); lengthPrefixed .getSpecBuilder() .getSpecBuilder() .setUrn(ModelCoders.LENGTH_PREFIX_CODER_URN) .build(); return addCoder(lengthPrefixed.build(), components, coderId + "-length_prefix"); }', 'output': 'private static Coder createLengthPrefixedCoder(String coderId) { Coder.Builder lengthPrefixed = Coder.newBuilder().addComponentCoders(coderId); lengthPrefixed .getSpecBuilder() .getSpecBuilder() .setUrn(ModelCoders.LENGTH_PREFIX_CODER_URN) .build(); return lengthPrefixed.build(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void XXX(String ksname, String viewName, int generationNumber) { executeInternal(String.format("INSERT INTO system.%s (keyspace_name, view_name, generation_number) VALUES (?, ?, ?)", MATERIALIZED_VIEWS_BUILDS_IN_PROGRESS), ksname, viewName, generationNumber); }', 'ground_truth': 'public static void beginViewBuild(String ksname, String viewName, int generationNumber) { executeInternal(String.format("INSERT INTO system.%s (keyspace_name, view_name, generation_number) VALUES (?, ?, ?)", VIEWS_BUILDS_IN_PROGRESS), ksname, viewName, generationNumber); }', 'output': 'public static void insertMaterializedViewBuildInProgress(String ksname, String viewName, int generationNumber) { executeInternal(String.format("INSERT INTO system.%s (keyspace_name, view_name, generation_number) VALUES (?, ?, ?)", MATERIALIZED_VIEWS_BUILDS_IN_PROGRESS), ksname, viewName, generationNumber); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { Configuration cfg = createConfiguration(); assertNull(cfg.getSQLDateAndTimeTimeZone()); cfg.setTimeZone(TimeZone.getTimeZone("GMT+01:00")); assertOutput(FTL, OUTPUT_BEFORE_SETTZ_GMT1_SQL_SAME + OUTPUT_AFTER_SETTZ_SQL_SAME, cfg); }', 'ground_truth': 'public void testWithGMT1AndNullSQL() throws Exception { Configuration cfg = createConfiguration(); assertNull(cfg.getSQLDateAndTimeTimeZone()); cfg.setTimeZone(TimeZone.getTimeZone("GMT+01:00")); assertOutput(FTL, OUTPUT_BEFORE_SETTING_GMT_CFG_GMT1_SQL_SAME + OUTPUT_AFTER_SETTING_GMT_CFG_SQL_SAME, cfg); }', 'output': 'public void testSetTimeZone() throws Exception { Configuration cfg = createConfiguration(); assertNull(cfg.getSQLDateAndTimeTimeZone()); cfg.setTimeZone(TimeZone.getTimeZone("GMT+01:00")); assertOutput(FTL, OUTPUT_BEFORE_SETTZ_GMT1_SQL_SAME + OUTPUT_AFTER_SETTZ_SQL_SAME, cfg); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(boolean canGetRow) throws IgniteCheckedException { TestTree tree = createTestTree(canGetRow); long cnt = 10; for (long x = PUT_INC > 0 ? 0 : cnt - 1; x >= 0 && x < cnt; x += PUT_INC) { assertNull(tree.findOne(x)); tree.put(x); assertEquals(x, tree.findOne(x).longValue()); } X.println(tree.printTree()); assertNull(tree.findOne(-1L)); for (long x = 0; x < cnt; x++) assertEquals(x, tree.findOne(x).longValue()); assertNull(tree.findOne(cnt)); for (long x = RMV_INC > 0 ? 0 : cnt - 1; x >= 0 && x < cnt; x += RMV_INC) { X.println(" -- " + x); assertEquals(x, tree.remove(x).longValue()); assertNull(tree.findOne(x)); X.println(tree.printTree()); } assertFalse(tree.find(null, null).next()); }', 'ground_truth': 'private void doTestPutRemove(boolean canGetRow) throws IgniteCheckedException { TestTree tree = createTestTree(canGetRow); long cnt = 10; for (long x = PUT_INC > 0 ? 0 : cnt - 1; x >= 0 && x < cnt; x += PUT_INC) { assertNull(tree.findOne(x)); tree.put(x); assertEquals(x, tree.findOne(x).longValue()); } X.println(tree.printTree()); assertNull(tree.findOne(-1L)); for (long x = 0; x < cnt; x++) assertEquals(x, tree.findOne(x).longValue()); assertNull(tree.findOne(cnt)); for (long x = RMV_INC > 0 ? 0 : cnt - 1; x >= 0 && x < cnt; x += RMV_INC) { X.println(" -- " + x); assertEquals(x, tree.remove(x).longValue()); X.println(tree.printTree()); assertNull(tree.findOne(x)); } assertFalse(tree.find(null, null).next()); }', 'output': 'private void testPutAndFind(boolean canGetRow) throws IgniteCheckedException { TestTree tree = createTestTree(canGetRow); long cnt = 10; for (long x = PUT_INC > 0 ? 0 : cnt - 1; x >= 0 && x < cnt; x += PUT_INC) { assertNull(tree.findOne(x)); tree.put(x); assertEquals(x, tree.findOne(x).longValue()); } X.println(tree.printTree()); assertNull(tree.findOne(-1L)); for (long x = 0; x < cnt; x++) assertEquals(x, tree.findOne(x).longValue()); assertNull(tree.findOne(cnt)); for (long x = RMV_INC > 0 ? 0 : cnt - 1; x >= 0 && x < cnt; x += RMV_INC) { X.println(" -- " + x); assertEquals(x, tree.remove(x).longValue()); assertNull(tree.findOne(x)); X.println(tree.printTree()); } assertFalse(tree.find(null, null).next()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Set<T> XXX(ByteBuffer bytes) { try { ByteBuffer input = bytes.duplicate(); int n = getUnsignedShort(input); Set<T> l = new LinkedHashSet<T>(n); for (int i = 0; i < n; i++) { int s = getUnsignedShort(input); byte[] data = new byte[s]; input.get(data); ByteBuffer databb = ByteBuffer.wrap(data); elements.validate(databb); l.add(elements.XXX(databb)); } return l; } catch (BufferUnderflowException e) { throw new MarshalException("Not enough bytes to read a list"); } }', 'ground_truth': 'public Set<T> deserialize(ByteBuffer bytes) { try { ByteBuffer input = bytes.duplicate(); int n = getUnsignedShort(input); Set<T> l = new LinkedHashSet<T>(n); for (int i = 0; i < n; i++) { int s = getUnsignedShort(input); byte[] data = new byte[s]; input.get(data); ByteBuffer databb = ByteBuffer.wrap(data); elements.validate(databb); l.add(elements.deserialize(databb)); } return l; } catch (BufferUnderflowException e) { throw new MarshalException("Not enough bytes to read a list"); } }', 'output': 'public Set<T> deserialize(ByteBuffer bytes) { try { ByteBuffer input = bytes.duplicate(); int n = getUnsignedShort(input); Set<T> l = new LinkedHashSet<T>(n); for (int i = 0; i < n; i++) { int s = getUnsignedShort(input); byte[] data = new byte[s]; input.get(data); ByteBuffer databb = ByteBuffer.wrap(data); elements.validate(databb); l.add(elements.deserialize(databb)); } return l; } catch (BufferUnderflowException e) { throw new MarshalException("Not enough bytes to read a list"); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean XXX() { return endpoint.equals(FBUtilities.getBroadcastAddressAndPort()); }', 'ground_truth': 'public boolean isSelf() { return endpoint.equals(FBUtilities.getBroadcastAddressAndPort()); }', 'output': 'public boolean isBroadcastAddress() { return endpoint.equals(FBUtilities.getBroadcastAddressAndPort()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static CacheTypeMetadata XXX() { CacheTypeMetadata organizationTypeMeta = new CacheTypeMetadata(); organizationTypeMeta.setValueType(Organization.class); organizationTypeMeta.setKeyType(Integer.class); Map<String, Class<?>> ascFields = new HashMap<>(); ascFields.put("name", String.class); Map<String, Class<?>> queryFields = new HashMap<>(); queryFields.put("address.street", String.class); organizationTypeMeta.setAscendingFields(ascFields); organizationTypeMeta.setQueryFields(queryFields); return organizationTypeMeta; }', 'ground_truth': 'private static QueryEntity createOrganizationQueryEntity() { QueryEntity organizationEntity = new QueryEntity(); organizationEntity.setValueType(Organization.class.getName()); organizationEntity.setKeyType(Integer.class.getName()); LinkedHashMap<String, String> fields = new LinkedHashMap<>(); fields.put("name", String.class.getName()); fields.put("address.street", String.class.getName()); organizationEntity.setFields(fields); organizationEntity.setIndexes(Arrays.asList( new QueryIndex("name") )); return organizationEntity; }', 'output': 'private static CacheTypeMetadata getOrganizationTypeMetadata() { CacheTypeMetadata organizationTypeMeta = new CacheTypeMetadata(); organizationTypeMeta.setValueType(Organization.class); organizationTypeMeta.setKeyType(Integer.class); Map<String, Class<?>> ascFields = new HashMap<>(); ascFields.put("name", String.class); Map<String, Class<?>> queryFields = new HashMap<>(); queryFields.put("address.street", String.class); organizationTypeMeta.setAscendingFields(ascFields); organizationTypeMeta.setQueryFields(queryFields); return organizationTypeMeta; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void XXX() { select2.setEnabled(true); clearTitleAttribute(); }', 'ground_truth': 'protected void onInitializeEditable() { select2.setEnabled(true); clearTitleAttribute(); }', 'output': 'protected void enableSelect2() { select2.setEnabled(true); clearTitleAttribute(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(CompositeStringExpression expression) { int size = expression.getValues().size(); pushConstant(size); cv.visitTypeInsn(ANEWARRAY, "java/lang/Object"); for (int i = 0; i < size; i++) { cv.visitInsn(DUP); pushConstant(i); expression.getValue(i).visit(this); cv.visitInsn(AASTORE); } int paramIdx = defineVariable(createArgumentsName(), "java.lang.Object", false).getIndex(); cv.visitVarInsn(ASTORE, paramIdx); ClassNode innerClass = createCompositeStringClass(expression); innerClasses.add(innerClass); String innerClassinternalName = getClassInternalName(innerClass.getName()); cv.visitTypeInsn(NEW, innerClassinternalName); cv.visitInsn(DUP); cv.visitVarInsn(ALOAD, paramIdx); cv.visitMethodInsn(INVOKESPECIAL, innerClassinternalName, "<init>", "([Ljava/lang/Object;)V"); }', 'ground_truth': 'public void visitGStringExpression(GStringExpression expression) { int size = expression.getValues().size(); pushConstant(size); cv.visitTypeInsn(ANEWARRAY, "java/lang/Object"); for (int i = 0; i < size; i++) { cv.visitInsn(DUP); pushConstant(i); expression.getValue(i).visit(this); cv.visitInsn(AASTORE); } int paramIdx = defineVariable(createArgumentsName(), "java.lang.Object", false).getIndex(); cv.visitVarInsn(ASTORE, paramIdx); ClassNode innerClass = createGStringClass(expression); innerClasses.add(innerClass); String innerClassinternalName = getClassInternalName(innerClass.getName()); cv.visitTypeInsn(NEW, innerClassinternalName); cv.visitInsn(DUP); cv.visitVarInsn(ALOAD, paramIdx); cv.visitMethodInsn(INVOKESPECIAL, innerClassinternalName, "<init>", "([Ljava/lang/Object;)V"); }', 'output': 'public void visitCompositeStringExpression(CompositeStringExpression expression) { int size = expression.getValues().size(); pushConstant(size); cv.visitTypeInsn(ANEWARRAY, "java/lang/Object"); for (int i = 0; i < size; i++) { cv.visitInsn(DUP); pushConstant(i); expression.getValue(i).visit(this); cv.visitInsn(AASTORE); } int paramIdx = defineVariable(createArgumentsName(), "java.lang.Object", false).getIndex(); cv.visitVarInsn(ASTORE, paramIdx); ClassNode innerClass = createCompositeStringClass(expression); innerClasses.add(innerClass); String innerClassinternalName = getClassInternalName(innerClass.getName()); cv.visitTypeInsn(NEW, innerClassinternalName); cv.visitInsn(DUP); cv.visitVarInsn(ALOAD, paramIdx); cv.visitMethodInsn(INVOKESPECIAL, innerClassinternalName, "<init>", "([Ljava/lang/Object;)V"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(String tableName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException { ssProxy.XXX(tableName, columnFamilies); }', 'ground_truth': 'public void forceKeyspaceCompaction(String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException { ssProxy.forceKeyspaceCompaction(keyspaceName, columnFamilies); }', 'output': 'public void createTable(String tableName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException { ssProxy.createTable(tableName, columnFamilies); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String XXX(final ValueSemanticsProvider.Context context, final T value) { val delegateValue = value!=null ? toDelegateValue(value) : null; return delegateRenderer().XXX(context, delegateValue); }', 'ground_truth': 'public String titlePresentation(final ValueSemanticsProvider.Context context, final T value) { val delegateValue = value!=null ? toDelegateValue(value) : null; return delegateRenderer().titlePresentation(context, delegateValue); }', 'output': 'public String render(final ValueSemanticsProvider.Context context, final T value) { val delegateValue = value!=null ? toDelegateValue(value) : null; return delegateRenderer().render(context, delegateValue); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { BinaryMarshaller marsh = binaryMarshaller(Arrays.asList( new BinaryTypeConfiguration(SimpleObject.class.getName()) )); SimpleObject obj = simpleObject(); BinaryObject po = marshal(obj, marsh); BinaryObject copy = copy(po, F.<String, Object>asMap("sArr", new short[]{1, 2, 3})); assertArrayEquals(new short[] {1, 2, 3}, copy.<short[]>field("sArr")); SimpleObject obj0 = copy.deserialize(); assertArrayEquals(new short[] {1, 2, 3}, obj0.sArr); }', 'ground_truth': 'public void testBinaryCopyShortArray() throws Exception { BinaryMarshaller marsh = binaryMarshaller(Arrays.asList( new BinaryTypeConfiguration(SimpleObject.class.getName()) )); SimpleObject obj = simpleObject(); BinaryObject po = marshal(obj, marsh); BinaryObject copy = copy(po, F.<String, Object>asMap("sArr", new short[]{1, 2, 3})); assertArrayEquals(new short[] {1, 2, 3}, copy.<short[]>field("sArr")); SimpleObject obj0 = copy.deserialize(); assertArrayEquals(new short[] {1, 2, 3}, obj0.sArr); }', 'output': 'public void testBinaryMarshalling() throws Exception { BinaryMarshaller marsh = binaryMarshaller(Arrays.asList( new BinaryTypeConfiguration(SimpleObject.class.getName()) )); SimpleObject obj = simpleObject(); BinaryObject po = marshal(obj, marsh); BinaryObject copy = copy(po, F.<String, Object>asMap("sArr", new short[]{1, 2, 3})); assertArrayEquals(new short[] {1, 2, 3}, copy.<short[]>field("sArr")); SimpleObject obj0 = copy.deserialize(); assertArrayEquals(new short[] {1, 2, 3}, obj0.sArr); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void XXX(byte[] endpointAddress, String tableName, byte[] key, long timestamp) throws IOException { RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, tableName.getBytes(UTF8)); rm.delete(new QueryPath(HINTS_CF, key, endpointAddress), new TimestampClock(timestamp)); rm.apply(); }', 'ground_truth': 'private static void deleteHintKey(byte[] endpointAddress, byte[] key, byte[] tableCF) throws IOException { RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, endpointAddress); rm.delete(new QueryPath(HINTS_CF, key, tableCF), new TimestampClock(System.currentTimeMillis())); rm.apply(); }', 'output': 'private static void deleteHint(byte[] endpointAddress, String tableName, byte[] key, long timestamp) throws IOException { RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, tableName.getBytes(UTF8)); rm.delete(new QueryPath(HINTS_CF, key, endpointAddress), new TimestampClock(timestamp)); rm.apply(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Collection XXX() { if (value == null) { return Collections.EMPTY_LIST; } else if (value instanceof Collection) { return (Collection) value; } else { return Collections.singletonList(value); } }', 'ground_truth': 'public List children() { if (value == null) { return Collections.EMPTY_LIST; } else if (value instanceof List) { return (List) value; } else { return Collections.singletonList(value); } }', 'output': 'public Collection<T> toList() { if (value == null) { return Collections.emptyList(); } else if (value instanceof Collection) { return (Collection<T>) value; } else { return Collections.singletonList(value); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX() { DataflowExecutionState currentState = (DataflowExecutionState) executionContext.getExecutionStateTracker().getCurrentState(); if (currentState == null || currentState.getStepName().originalName() == null || Objects.equals(latestConsumingStepName, currentState.getStepName())) { return; } if (!byteCounters.containsKey(currentState.getStepName())) { byteCounters.put( currentState.getStepName(), executionContext .getCounterFactory() .longSum( CounterName.named("read-sideinput-byte-count") .withOriginalName(declaringOperationContext.nameContext()) .withOrigin("SYSTEM") .withOriginalRequestingStepName(currentState.getStepName().originalName()) .withInputIndex(sideInputIndex))); executionStates.put( currentState.getStepName(), executionContext .getExecutionStateRegistry() .getIOState( declaringOperationContext.nameContext(), "read-sideinput", currentState.getStepName().originalName(), sideInputIndex, currentState.getMetricsContainer(), currentState.getProfileScope())); } currentCounter = byteCounters.get(currentState.getStepName()); currentExecutionState = executionStates.get(currentState.getStepName()); latestConsumingStepName = currentState.getStepName(); }', 'ground_truth': 'private void updateCurrentStateIfOutdated() { DataflowExecutionState currentState = (DataflowExecutionState) executionContext.getExecutionStateTracker().getCurrentState(); if (currentState == null || currentState.getStepName().originalName() == null || Objects.equals(latestConsumingStepName, currentState.getStepName())) { return; } if (!byteCounters.containsKey(currentState.getStepName())) { byteCounters.put( currentState.getStepName(), executionContext .getCounterFactory() .longSum( CounterName.named("read-sideinput-byte-count") .withOriginalName(declaringOperationContext.nameContext()) .withOrigin("SYSTEM") .withOriginalRequestingStepName(currentState.getStepName().originalName()) .withInputIndex(sideInputIndex))); executionStates.put( currentState.getStepName(), executionContext .getExecutionStateRegistry() .getIOState( declaringOperationContext.nameContext(), "read-sideinput", currentState.getStepName().originalName(), sideInputIndex, currentState.getMetricsContainer(), currentState.getProfileScope())); } currentCounter = byteCounters.get(currentState.getStepName()); currentExecutionState = executionStates.get(currentState.getStepName()); latestConsumingStepName = currentState.getStepName(); }', 'output': 'private void updateByteCounters() { DataflowExecutionState currentState = (DataflowExecutionState) executionContext.getExecutionStateTracker().getCurrentState(); if (currentState == null || currentState.getStepName().originalName() == null || Objects.equals(latestConsumingStepName, currentState.getStepName())) { return; } if (!byteCounters.containsKey(currentState.getStepName())) { byteCounters.put( currentState.getStepName(), executionContext .getCounterFactory() .longSum( CounterName.named("read-sideinput-byte-count") .withOriginalName(declaringOperationContext.nameContext()) .withOrigin("SYSTEM") .withOriginalRequestingStepName(currentState.getStepName().originalName()) .withInputIndex(sideInputIndex))); executionStates.put( currentState.getStepName(), executionContext .getExecutionStateRegistry() .getIOState( declaringOperationContext.nameContext(), "read-sideinput", currentState.getStepName().originalName(), sideInputIndex, currentState.getMetricsContainer(), currentState.getProfileScope())); } currentCounter = byteCounters.get(currentState.getStepName()); currentExecutionState = executionStates.get(currentState.getStepName()); latestConsumingStepName = currentState.getStepName(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'XXXpXXXuXXXbXXXlXXXiXXXcXXX XXXvXXXoXXXiXXXdXXX XXXtXXXeXXXsXXXtXXXGXXXrXXXoXXXoXXXvXXXyXXXcXXXTXXXeXXXsXXXtXXX1XXX_XXXJXXXoXXXiXXXnXXXtXXX_XXXFXXXoXXXrXXXkXXX_XXXIXXXnXXXhXXXeXXXrXXXiXXXtXXX XXX(XXX XXX)XXX XXX{XXX XXXeXXXnXXXsXXXuXXXrXXXeXXXNXXXoXXXtXXXPXXXrXXXeXXXsXXXeXXXnXXXtXXX XXX(XXX XXX"XXXGXXXrXXXoXXXoXXXvXXXyXXXcXXXTXXXeXXXsXXXtXXX1XXX"XXX XXX)XXX XXX;XXX XXXpXXXrXXXoXXXjXXXeXXXcXXXtXXX.XXXeXXXxXXXeXXXcXXXuXXXtXXXeXXXTXXXaXXXrXXXgXXXeXXXtXXX XXX(XXX XXX"XXXGXXXrXXXoXXXoXXXvXXXyXXXcXXXTXXXeXXXsXXXtXXX1XXX_XXXJXXXoXXXiXXXnXXXtXXX_XXXFXXXoXXXrXXXkXXX_XXXIXXXnXXXhXXXeXXXrXXXiXXXtXXX"XXX XXX)XXX XXX;XXX XXXeXXXnXXXsXXXuXXXrXXXeXXXRXXXeXXXsXXXuXXXlXXXtXXXOXXXKXXX XXX(XXX XXX"XXXGXXXrXXXoXXXoXXXvXXXyXXXcXXXTXXXeXXXsXXXtXXX1XXX"XXX XXX)XXX XXX;XXX XXX}XXX', 'ground_truth': 'public void testGroovycTest1_Joint_NoFork_WithJavaClasspath ( ) { ensureNotPresent ( "GroovycTest1" ) ; project.executeTarget ( "GroovycTest1_Joint_NoFork_WithJavaClasspath" ) ; ensureResultOK ( "GroovycTest1" ) ; }', 'output': 'private static void main(String[] args) {\n    System.out.println("Hello, world!");\n}</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Long XXX(ByteBuffer bytes) { return ByteBufferUtil.toLong(bytes); }', 'ground_truth': 'public Long deserialize(ByteBuffer bytes) { return bytes.remaining() == 0 ? null : ByteBufferUtil.toLong(bytes); }', 'output': 'public Long toLong(ByteBuffer bytes) { return ByteBufferUtil.toLong(bytes); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(RelOptPlanWriter pw) { pw.XXX( this, new String[] { "child", "table", "operation", "updateColumnList", "flattened" }, new Object[] { Arrays.asList(table.getQualifiedName()), getOperation(), (updateColumnList == null) ? Collections.EMPTY_LIST : updateColumnList, flattened }); }', 'ground_truth': 'public RelOptPlanWriter explainTerms(RelOptPlanWriter pw) { return super.explainTerms(pw) .item("table", Arrays.asList(table.getQualifiedName())) .item("operation", getOperation()) .item( "updateColumnList", (updateColumnList == null) ? Collections.EMPTY_LIST : updateColumnList) .item("flattened", flattened); }', 'output': 'public void writeTo(RelOptPlanWriter pw) { pw.write(this, new String[] { "child", "table", "operation", "updateColumnList", "flattened" }, new Object[] { Arrays.asList(table.getQualifiedName()), getOperation(), (updateColumnList == null) ? Collections.EMPTY_LIST : updateColumnList, flattened }); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public CompletableFuture<Object> XXX(NbCodeLanguageClient client, String command, List<Object> arguments) { if (arguments.size() > 2) { String uri = gson.fromJson(gson.toJson(arguments.get(0)), String.class); int offset = gson.fromJson(gson.toJson(arguments.get(1)), Integer.class); List<QuickPickItem> fields = Arrays.asList(gson.fromJson(gson.toJson(arguments.get(2)), QuickPickItem[].class)); String text; boolean generateEquals = !GENERATE_HASH_CODE.equals(command); boolean generateHashCode = !GENERATE_EQUALS.equals(command); switch (command) { case GENERATE_EQUALS: text = Bundle.DN_SelectEquals(); break; case GENERATE_HASH_CODE: text = Bundle.DN_SelectHashCode(); break; default: text = Bundle.DN_SelectEqualsHashCode(); break; } client.showQuickPick(new ShowQuickPickParams(text, true, fields)).thenAccept(selected -> { if (selected != null) { try { FileObject file = Utils.fromUri(uri); JavaSource js = JavaSource.forFileObject(file); if (js == null) { throw new IOException("Cannot get JavaSource for: " + uri); } List<TextEdit> edits = TextDocumentServiceImpl.modify2TextEdits(js, wc -> { wc.toPhase(JavaSource.Phase.RESOLVED); TreePath tp = wc.getTreeUtilities().pathFor(offset); tp = wc.getTreeUtilities().getPathElementOfKind(Tree.Kind.CLASS, tp); if (tp != null) { List<VariableElement> selectedFields = selected.stream().map(item -> { ElementData data = gson.fromJson(gson.toJson(item.getUserData()), ElementData.class); return (VariableElement)data.resolve(wc); }).collect(Collectors.toList()); org.netbeans.modules.java.editor.codegen.EqualsHashCodeGenerator.generateEqualsAndHashCode(wc, tp, generateEquals ? selectedFields : null, generateHashCode ? selectedFields : null, -1); } }); client.applyEdit(new ApplyWorkspaceEditParams(new WorkspaceEdit(Collections.singletonMap(uri, edits)))); } catch (IOException | IllegalArgumentException ex) { client.logMessage(new MessageParams(MessageType.Error, ex.getLocalizedMessage())); } } }); } else { client.logMessage(new MessageParams(MessageType.Error, String.format("Illegal number of arguments received for command: %s", command))); } return CompletableFuture.completedFuture(true); }', 'ground_truth': 'public CompletableFuture<CodeAction> resolve(NbCodeLanguageClient client, CodeAction codeAction, Object data) { CompletableFuture<CodeAction> future = new CompletableFuture<>(); try { int kind = ((JsonObject) data).getAsJsonPrimitive(KIND).getAsInt(); String uri = ((JsonObject) data).getAsJsonPrimitive(URI).getAsString(); int offset = ((JsonObject) data).getAsJsonPrimitive(OFFSET).getAsInt(); List<QuickPickItem> fields = Arrays.asList(gson.fromJson(((JsonObject) data).get(FIELDS), QuickPickItem[].class)); String text; boolean generateEquals = HASH_CODE_ONLY != kind; boolean generateHashCode = EQUALS_ONLY != kind; switch (kind) { case EQUALS_ONLY: text = Bundle.DN_SelectEquals(); break; case HASH_CODE_ONLY: text = Bundle.DN_SelectHashCode(); break; default: text = Bundle.DN_SelectEqualsHashCode(); break; } client.showQuickPick(new ShowQuickPickParams(text, true, fields)).thenAccept(selected -> { try { if (selected != null) { FileObject file = Utils.fromUri(uri); JavaSource js = JavaSource.forFileObject(file); if (js == null) { throw new IOException("Cannot get JavaSource for: " + uri); } List<TextEdit> edits = TextDocumentServiceImpl.modify2TextEdits(js, wc -> { wc.toPhase(JavaSource.Phase.RESOLVED); TreePath tp = wc.getTreeUtilities().pathFor(offset); tp = wc.getTreeUtilities().getPathElementOfKind(Tree.Kind.CLASS, tp); if (tp != null) { List<VariableElement> selectedFields = selected.stream().map(item -> { ElementData userData = gson.fromJson(gson.toJson(item.getUserData()), ElementData.class); return (VariableElement) userData.resolve(wc); }).collect(Collectors.toList()); org.netbeans.modules.java.editor.codegen.EqualsHashCodeGenerator.generateEqualsAndHashCode(wc, tp, generateEquals ? selectedFields : null, generateHashCode ? selectedFields : null, -1); } }); if (!edits.isEmpty()) { codeAction.setEdit(new WorkspaceEdit(Collections.singletonMap(uri, edits))); } } future.complete(codeAction); } catch (IOException | IllegalArgumentException ex) { future.completeExceptionally(ex); } }); } catch(JsonSyntaxException ex) { future.completeExceptionally(ex); } return future; }', 'output': 'public CompletableFuture<Object> showQuickPick(NbCodeLanguageClient client, String command, List<Object> arguments) { if (arguments.size() > 2) { String uri = gson.fromJson(gson.toJson(arguments.get(0)), String.class); int offset = gson.fromJson(gson.toJson(arguments.get(1)), Integer.class); List<QuickPickItem> fields = Arrays.asList(gson.fromJson(gson.toJson(arguments.get(2)), QuickPickItem[].class)); String text; boolean generateEquals = !GENERATE_HASH_CODE.equals(command); boolean generateHashCode = !GENERATE_EQUALS.equals(command); switch (command) { case GENERATE_EQUALS: text = Bundle.DN_SelectEquals(); break; case GENERATE_HASH_CODE: text = Bundle.DN_SelectHashCode(); break; default: text = Bundle.DN_SelectEqualsHashCode(); break; } client.showQuickPick(new ShowQuickPickParams(text, true, fields)).thenAccept(selected -> { if (selected != null) { try { FileObject file = Utils.fromUri(uri); JavaSource js = JavaSource.forFileObject(file); if (js == null) { throw new IOException("Cannot get JavaSource for: " + uri); } List<TextEdit> edits = TextDocumentServiceImpl.modify2TextEdits(js, wc -> { wc.toPhase(JavaSource.Phase.RESOLVED); TreePath tp = wc.getTreeUtilities().pathFor(offset); tp = wc.getTreeUtilities().getPathElementOfKind(Tree.Kind.CLASS, tp); if (tp != null) { List<VariableElement> selectedFields = selected.stream().map(item -> { ElementData data = gson.fromJson(gson.toJson(item.getUserData()), ElementData.class); return (VariableElement)data.resolve(wc); }).collect(Collectors.toList()); org.netbeans.modules.java.editor.codegen.EqualsHashCodeGenerator.generateEqualsAndHashCode(wc, tp, generate'}
{'instruction': 'Please refactor the following code:', 'input': 'public Set<InetAddress> XXX() { return streamProxy.XXX(); }', 'ground_truth': 'public Set<StreamState> getStreamStatus() { return streamProxy.getCurrentStreams(); }', 'output': 'public Set<InetAddress> getStreamProxies() { return streamProxy.getStreamProxies(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean XXX() { return this == MANAGED_BEAN_CONTRIBUTING; } public boolean isManagedBeanNotContributing() { return this == MANAGED_BEAN_NOT_CONTRIBUTING; } public boolean isManagedBean() { return this == MANAGED_BEAN_CONTRIBUTING || this == MANAGED_BEAN_NOT_CONTRIBUTING; }', 'ground_truth': 'public boolean isManagedBeanAny() { return this == MANAGED_BEAN_CONTRIBUTING || this == MANAGED_BEAN_NOT_CONTRIBUTING; }', 'output': 'public boolean isManageable() { return this == MANAGED_BEAN_CONTRIBUTING || this == MANAGED_BEAN_NOT_CONTRIBUTING; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static TestDataflowPipelineOptions XXX() { try { TestDataflowPipelineOptions options = PipelineOptionsFactory.fromArgs( MAPPER.readValue(System.getProperty(PROPERTY_DATAFLOW_OPTIONS), String[].class)) .as(TestDataflowPipelineOptions.class); options.setAppName(getAppName()); return options; } catch (IOException e) { throw new RuntimeException("Unable to instantiate test options from system property " + PROPERTY_DATAFLOW_OPTIONS + ":" + System.getProperty(PROPERTY_DATAFLOW_OPTIONS), e); } }', 'ground_truth': 'public static PipelineOptions testingPipelineOptions() { try { @Nullable String systemDataflowOptions = System.getProperty(PROPERTY_DATAFLOW_OPTIONS); PipelineOptions options = systemDataflowOptions == null ? PipelineOptionsFactory.create() : PipelineOptionsFactory.fromArgs( MAPPER.readValue( System.getProperty(PROPERTY_DATAFLOW_OPTIONS), String[].class)) .as(PipelineOptions.class); options.as(ApplicationNameOptions.class).setAppName(getAppName()); if (isIntegrationTest()) { options.setRunner(TestDataflowPipelineRunner.class); } else { options.as(GcpOptions.class).setGcpCredential(new TestCredential()); } options.setStableUniqueNames(CheckEnabled.ERROR); return options; } catch (IOException e) { throw new RuntimeException("Unable to instantiate test options from system property " + PROPERTY_DATAFLOW_OPTIONS + ":" + System.getProperty(PROPERTY_DATAFLOW_OPTIONS), e); } }', 'output': 'public static TestDataflowPipelineOptions getTestOptions() { try { TestDataflowPipelineOptions options = PipelineOptionsFactory.fromArgs( MAPPER.readValue(System.getProperty(PROPERTY_DATAFLOW_OPTIONS), String[].class)) .as(TestDataflowPipelineOptions.class); options.setAppName(getAppName()); return options; } catch (IOException e) { throw new RuntimeException("Unable to instantiate test options from system property " + PROPERTY_DATAFLOW_OPTIONS + ":" + System.getProperty(PROPERTY_DATAFLOW_OPTIONS), e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws IOException { GcsOptions pipelineOptions = gcsOptionsWithTestCredential(); GcsUtil gcsUtil = pipelineOptions.getGcsUtil(); Storage mockStorage = Mockito.mock(Storage.class); gcsUtil.setStorageClient(mockStorage); Storage.Objects mockStorageObjects = Mockito.mock(Storage.Objects.class); Storage.Objects.List mockStorageList = Mockito.mock(Storage.Objects.List.class); Objects modelObjects = new Objects(); List<StorageObject> items = new ArrayList<>(); items.add(new StorageObject().setBucket("testbucket").setName("testdirectory/")); modelObjects.setItems(items); when(mockStorage.objects()).thenReturn(mockStorageObjects); when(mockStorageObjects.list("testbucket")).thenReturn(mockStorageList); when(mockStorageList.execute()).thenReturn(modelObjects); { GcsPath pattern = GcsPath.fromUri("gs: List<GcsPath> expectedFiles = ImmutableList.of(GcsPath.fromUri("gs: assertThat(expectedFiles, contains(gcsUtil.expand(pattern).toArray())); } { GcsPath pattern = GcsPath.fromUri("gs: List<GcsPath> expectedFiles = ImmutableList.of(GcsPath.fromUri("gs: assertThat(expectedFiles, contains(gcsUtil.expand(pattern).toArray())); } }', 'ground_truth': 'public void testAccessDeniedObjectThrowsIOException() throws IOException { GcsOptions pipelineOptions = gcsOptionsWithTestCredential(); GcsUtil gcsUtil = pipelineOptions.getGcsUtil(); Storage mockStorage = Mockito.mock(Storage.class); gcsUtil.setStorageClient(mockStorage); Storage.Objects mockStorageObjects = Mockito.mock(Storage.Objects.class); Storage.Objects.Get mockStorageGet = Mockito.mock(Storage.Objects.Get.class); GcsPath pattern = GcsPath.fromUri("gs: GoogleJsonResponseException expectedException = googleJsonResponseException(HttpStatusCodes.STATUS_CODE_FORBIDDEN, "Waves hand mysteriously", "These aren\'t the buckets your looking for"); when(mockStorage.objects()).thenReturn(mockStorageObjects); when(mockStorageObjects.get(pattern.getBucket(), pattern.getObject())).thenReturn( mockStorageGet); when(mockStorageGet.execute()).thenThrow(expectedException); thrown.expect(IOException.class); thrown.expectMessage("Unable to match files for pattern"); gcsUtil.expand(pattern); }', 'output': 'public void testExpansion() throws IOException { GcsOptions pipelineOptions = gcsOptionsWithTestCredential(); GcsUtil gcsUtil = pipelineOptions.getGcsUtil(); Storage mockStorage = Mockito.mock(Storage.class); gcsUtil.setStorageClient(mockStorage); Storage.Objects mockStorageObjects = Mockito.mock(Storage.Objects.class); Storage.Objects.List mockStorageList = Mockito.mock(Storage.Objects.List.class); Objects modelObjects = new Objects(); List<StorageObject> items = new ArrayList<>(); items.add(new StorageObject().setBucket("testbucket").setName("testdirectory/")); modelObjects.setItems(items); when(mockStorage.objects()).thenReturn(mockStorageObjects); when(mockStorageObjects.list("testbucket")).thenReturn(mockStorageList); when(mockStorageList.execute()).thenReturn(modelObjects); { GcsPath pattern = GcsPath.fromUri("gs: List<GcsPath> expectedFiles = ImmutableList.of(GcsPath.fromUri("gs: assertThat(expectedFiles, contains(gcsUtil.expand(pattern).toArray())); } { GcsPath pattern = GcsPath.fromUri("gs: List<GcsPath> expectedFiles = ImmutableList.of(GcsPath.fromUri("gs: assertThat(expectedFiles, contains(gcsUtil.expand(pattern).toArray())); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected static Message XXX(StreamRequestMessage streamRequestMessage) { ByteArrayOutputStream bos = new ByteArrayOutputStream(); DataOutputStream dos = new DataOutputStream(bos); try { StreamRequestMessage.serializer().serialize(streamRequestMessage, dos); } catch (IOException e) { throw new IOError(e); } return new Message(FBUtilities.getLocalAddress(), StageManager.STREAM_STAGE, StorageService.Verb.STREAM_REQUEST, bos.toByteArray() ); }', 'ground_truth': 'Message makeMessage() { ByteArrayOutputStream bos = new ByteArrayOutputStream(); DataOutputStream dos = new DataOutputStream(bos); try { StreamRequestMessage.serializer().serialize(this, dos); } catch (IOException e) { throw new IOError(e); } return new Message(FBUtilities.getLocalAddress(), StageManager.STREAM_STAGE, StorageService.Verb.STREAM_REQUEST, bos.toByteArray() ); }', 'output': 'protected static Message serializeStreamRequest(StreamRequestMessage streamRequestMessage) { ByteArrayOutputStream bos = new ByteArrayOutputStream(); DataOutputStream dos = new DataOutputStream(bos); try { StreamRequestMessage.serializer().serialize(streamRequestMessage, dos); } catch (IOException e) { throw new IOError(e); } return new Message(FBUtilities.getLocalAddress(), StageManager.STREAM_STAGE, StorageService.Verb.STREAM_REQUEST, bos.toByteArray() ); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private String XXX(NIVariable lengthVariable, int itemSize) { int length = Integer.parseInt(lengthVariable.getValue()); String expressionPath = getExpressionPath(lengthVariable); if (expressionPath != null && !expressionPath.isEmpty()) { String addressExpr = "&" + expressionPath; return debugger.readMemory(addressExpr, 4, length * itemSize); } return null; } private String XXX(NIVariable lengthVariable, int offset, int itemSize) { int length = Integer.parseInt(lengthVariable.getValue()); String expressionPath = getExpressionPath(lengthVariable); if (expressionPath != null && !expressionPath.isEmpty()) { String addressExpr = "&" + expressionPath; return debugger.readMemory(addressExpr, 4 + offset, length * itemSize); } return null; }', 'ground_truth': 'private String readArrayEE(NIVariable lengthVariable, int itemSize) { int length = Integer.parseInt(lengthVariable.getValue()); String expressionPath = getExpressionPath(lengthVariable); if (expressionPath != null && !expressionPath.isEmpty()) { String addressExpr = "&" + expressionPath; return debugger.readMemory(addressExpr, 4, length * itemSize); } return null; }', 'output': 'private String readMemory(NIVariable lengthVariable, int offset, int itemSize) { int length = Integer.parseInt(lengthVariable.getValue()); String expressionPath = getExpressionPath(lengthVariable); if (expressionPath != null && !expressionPath.isEmpty()) { String addressExpr = "&" + expressionPath; return debugger.readMemory(addressExpr, 4 + offset, length * itemSize); } return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { TestPager pager = new TestPager(); List<Row> rows = Arrays.asList(createRow("r1", 1), createRow("r2", 3), createRow("r3", 2)); assertEquals(3, rows.size()); assertRow(rows.get(0), "r1", 0); assertRow(rows.get(1), "r2", 0, 1, 2); assertRow(rows.get(2), "r3", 0, 1); rows = pager.discardFirst(rows, 1); assertEquals(2, rows.size()); assertRow(rows.get(0), "r2", 0, 1, 2); assertRow(rows.get(1), "r3", 0, 1); rows = pager.discardFirst(rows, 1); assertEquals(2, rows.size()); assertRow(rows.get(0), "r2", 1, 2); assertRow(rows.get(1), "r3", 0, 1); rows = pager.discardFirst(rows, 3); assertEquals(1, rows.size()); assertRow(rows.get(0), "r3", 1); rows = pager.discardFirst(rows, 1); assertTrue(rows.isEmpty()); }', 'ground_truth': 'public void discardFirstTest() { TestPager pager = new TestPager(); List<Row> rows = Arrays.asList(createRow("r1", 1), createRow("r2", 3), createRow("r3", 2)); assertEquals(3, rows.size()); assertRow(rows.get(0), "r1", 0); assertRow(rows.get(1), "r2", 0, 1, 2); assertRow(rows.get(2), "r3", 0, 1); rows = pager.discardFirst(rows, 1); assertEquals(2, rows.size()); assertRow(rows.get(0), "r2", 0, 1, 2); assertRow(rows.get(1), "r3", 0, 1); rows = pager.discardFirst(rows, 1); assertEquals(2, rows.size()); assertRow(rows.get(0), "r2", 1, 2); assertRow(rows.get(1), "r3", 0, 1); rows = pager.discardFirst(rows, 3); assertEquals(1, rows.size()); assertRow(rows.get(0), "r3", 1); rows = pager.discardFirst(rows, 1); assertTrue(rows.isEmpty()); }', 'output': 'public void testDiscardFirst() { TestPager pager = new TestPager(); List<Row> rows = Arrays.asList(createRow("r1", 1), createRow("r2", 3), createRow("r3", 2)); assertEquals(3, rows.size()); assertRow(rows.get(0), "r1", 0); assertRow(rows.get(1), "r2", 0, 1, 2); assertRow(rows.get(2), "r3", 0, 1); rows = pager.discardFirst(rows, 1); assertEquals(2, rows.size()); assertRow(rows.get(0), "r2", 0, 1, 2); assertRow(rows.get(1), "r3", 0, 1); rows = pager.discardFirst(rows, 1); assertEquals(2, rows.size()); assertRow(rows.get(0), "r2", 1, 2); assertRow(rows.get(1), "r3", 0, 1); rows = pager.discardFirst(rows, 3); assertEquals(1, rows.size()); assertRow(rows.get(0), "r3", 1); rows = pager.discardFirst(rows, 1); assertTrue(rows.isEmpty()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Throwable { String functionName = createSimpleFunction(true); String cql = String.format("SELECT * FROM %s WHERE token(k) < token(%s)", KEYSPACE + "." + currentTable(), functionCall(functionName)); assertPermissionsOnFunction(cql, functionName); }', 'ground_truth': 'public void functionInSelectTokenSliceRestriction() throws Throwable { String functionName = createSimpleFunction(); String cql = String.format("SELECT * FROM %s WHERE token(k) < token(%s)", KEYSPACE + "." + currentTable(), functionCall(functionName)); assertPermissionsOnFunction(cql, functionName); }', 'output': 'public void testTokenLessThan() throws Throwable { String functionName = createSimpleFunction(true); String cql = String.format("SELECT * FROM %s WHERE token(k) < token(%s)", KEYSPACE + "." + currentTable(), functionCall(functionName)); assertPermissionsOnFunction(cql, functionName); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected static void XXX(Runnable r) { if (r instanceof LocalSessionWrapper) { LocalSessionWrapper tsw = (LocalSessionWrapper) r; tsw.reset(); } }', 'ground_truth': 'protected static void maybeResetLocalSessionWrapper(Runnable r) { if (r instanceof LocalSessionWrapper) { LocalSessionWrapper tsw = (LocalSessionWrapper) r; tsw.reset(); } }', 'output': 'protected static void reset(Runnable r) { if (r instanceof LocalSessionWrapper) { LocalSessionWrapper tsw = (LocalSessionWrapper) r; tsw.reset(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { portables().toBinary(new TestObject2()); BinaryType meta = portables().type(TestObject2.class); assertNotNull(meta); assertEquals("TestObject2", meta.typeName()); Collection<String> fields = meta.fieldNames(); assertEquals(7, fields.size()); assertTrue(fields.contains("boolVal")); assertTrue(fields.contains("dateVal")); assertTrue(fields.contains("uuidArrVal")); assertTrue(fields.contains("objVal")); assertTrue(fields.contains("mapVal")); assertTrue(fields.contains("decVal")); assertTrue(fields.contains("decArrVal")); assertEquals("boolean", meta.fieldTypeName("boolVal")); assertEquals("Date", meta.fieldTypeName("dateVal")); assertEquals("UUID[]", meta.fieldTypeName("uuidArrVal")); assertEquals("Object", meta.fieldTypeName("objVal")); assertEquals("Map", meta.fieldTypeName("mapVal")); assertEquals("decimal", meta.fieldTypeName("decVal")); assertEquals("decimal[]", meta.fieldTypeName("decArrVal")); }', 'ground_truth': 'public void testBinaryMarshalAware() throws Exception { binaries().toBinary(new TestObject2()); BinaryType meta = binaries().type(TestObject2.class); assertNotNull(meta); assertEquals("TestObject2", meta.typeName()); Collection<String> fields = meta.fieldNames(); assertEquals(7, fields.size()); assertTrue(fields.contains("boolVal")); assertTrue(fields.contains("dateVal")); assertTrue(fields.contains("uuidArrVal")); assertTrue(fields.contains("objVal")); assertTrue(fields.contains("mapVal")); assertTrue(fields.contains("decVal")); assertTrue(fields.contains("decArrVal")); assertEquals("boolean", meta.fieldTypeName("boolVal")); assertEquals("Date", meta.fieldTypeName("dateVal")); assertEquals("UUID[]", meta.fieldTypeName("uuidArrVal")); assertEquals("Object", meta.fieldTypeName("objVal")); assertEquals("Map", meta.fieldTypeName("mapVal")); assertEquals("decimal", meta.fieldTypeName("decVal")); assertEquals("decimal[]", meta.fieldTypeName("decArrVal")); }', 'output': 'public void testToBinary() throws Exception { BinaryType meta = portables().type(TestObject2.class); assertNotNull(meta); assertEquals("TestObject2", meta.typeName()); Collection<String> fields = meta.fieldNames(); assertEquals(7, fields.size()); assertTrue(fields.contains("boolVal")); assertTrue(fields.contains("dateVal")); assertTrue(fields.contains("uuidArrVal")); assertTrue(fields.contains("objVal")); assertTrue(fields.contains("mapVal")); assertTrue(fields.contains("decVal")); assertTrue(fields.contains("decArrVal")); assertEquals("boolean", meta.fieldTypeName("boolVal")); assertEquals("Date", meta.fieldTypeName("dateVal")); assertEquals("UUID[]", meta.fieldTypeName("uuidArrVal")); assertEquals("Object", meta.fieldTypeName("objVal")); assertEquals("Map", meta.fieldTypeName("mapVal")); assertEquals("decimal", meta.fieldTypeName("decVal")); assertEquals("decimal[]", meta.fieldTypeName("decArrVal")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public long XXX() { return maxSSTableSizeInMB * 1024L * 1024L; }', 'ground_truth': 'public long getMaxSSTableBytes() { return maxSSTableSizeInMB * 1024L * 1024L; }', 'output': 'public long getMaxSSTableSizeInMB() { return maxSSTableSizeInMB * 1024L * 1024L; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<? extends UnboundedSource<String, CheckpointMark>> XXX( int desiredNumSplits, PipelineOptions options) throws Exception { return Collections.<UnboundedSource<String, CheckpointMark>>singletonList(this); }', 'ground_truth': 'public List<? extends UnboundedSource<String, CheckpointMark>> split( int desiredNumSplits, PipelineOptions options) throws Exception { return Collections.<UnboundedSource<String, CheckpointMark>>singletonList(this); }', 'output': 'public List<? extends UnboundedSource<String, CheckpointMark>> getSources( int desiredNumSplits, PipelineOptions options) throws Exception { return Collections.<UnboundedSource<String, CheckpointMark>>singletonList(this); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { rowIndex = parent.nextRowIndex(); for (Iterator iter = cells.iterator(); iter.hasNext(); ) { TableLayoutCell cell = (TableLayoutCell) iter.next(); GridBagConstraints c = cell.getConstraints(); c.gridy = rowIndex; parent.addCell(cell); } }', 'ground_truth': 'public void addComponentsForRow() { rowIndex = parent.nextRowIndex(); for (Iterator iter = cells.iterator(); iter.hasNext(); ) { TableLayoutCell cell = (TableLayoutCell) iter.next(); GridBagConstraints c = cell.getConstraints(); c.gridy = rowIndex; parent.addCell(cell); } }', 'output': 'public void moveToNextRow() { rowIndex = parent.nextRowIndex(); for (Iterator iter = cells.iterator(); iter.hasNext(); ) { TableLayoutCell cell = (TableLayoutCell) iter.next(); GridBagConstraints c = cell.getConstraints(); c.gridy = rowIndex; parent.addCell(cell); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Pair<ArrayTable.Representation, Object> XXX( int ordinal, Primitive p, long min, long max) { int bitCountMax = log2(nextPowerOf2(abs2(max))); int bitCountMin = log2(nextPowerOf2(abs2(min))); int bitCount = Math.max(bitCountMin, bitCountMax) + 1; if (bitCount > 21 && bitCount < 32) { bitCount = 32; } if (bitCount >= 33 && bitCount < 64) { bitCount = 64; } switch (bitCount) { case 8: return pair( new ArrayTable.PrimitiveArray( ordinal, Primitive.BYTE, p)); case 16: return pair( new ArrayTable.PrimitiveArray( ordinal, Primitive.SHORT, p)); case 32: return pair( new ArrayTable.PrimitiveArray( ordinal, Primitive.INT, p)); case 64: return pair( new ArrayTable.PrimitiveArray( ordinal, Primitive.LONG, p)); default: return pair( new ArrayTable.BitSlicedPrimitiveArray( ordinal, bitCount, p)); } }', 'ground_truth': 'private ArrayTable.Representation chooseFixedRep( int ordinal, Primitive p, long min, long max) { int bitCountMax = log2(nextPowerOf2(abs2(max))); int bitCountMin = log2(nextPowerOf2(abs2(min))); int bitCount = Math.max(bitCountMin, bitCountMax) + 1; if (bitCount > 21 && bitCount < 32) { bitCount = 32; } if (bitCount >= 33 && bitCount < 64) { bitCount = 64; } switch (bitCount) { case 8: return new ArrayTable.PrimitiveArray( ordinal, Primitive.BYTE, p); case 16: return new ArrayTable.PrimitiveArray( ordinal, Primitive.SHORT, p); case 32: return new ArrayTable.PrimitiveArray( ordinal, Primitive.INT, p); case 64: return new ArrayTable.PrimitiveArray( ordinal, Primitive.LONG, p); default: return new ArrayTable.BitSlicedPrimitiveArray( ordinal, bitCount, p); } }', 'output': 'private Pair<ArrayTable.Representation, Object> createPrimitiveArray( int ordinal, Primitive p, long min, long max) { int bitCountMax = log2(nextPowerOf2(abs2(max))); int bitCountMin = log2(nextPowerOf2(abs2(min))); int bitCount = Math.max(bitCountMin, bitCountMax) + 1; if (bitCount > 21 && bitCount < 32) { bitCount = 32; } if (bitCount >= 33 && bitCount < 64) { bitCount = 64; } switch (bitCount) { case 8: return pair( new ArrayTable.PrimitiveArray( ordinal, Primitive.BYTE, p)); case 16: return pair( new ArrayTable.PrimitiveArray( ordinal, Primitive.SHORT, p)); case 32: return pair( new ArrayTable.PrimitiveArray( ordinal, Primitive.INT, p)); case 64: return pair( new ArrayTable.PrimitiveArray( ordinal, Primitive.LONG, p)); default: return pair( new ArrayTable.BitSlicedPrimitiveArray( ordinal, bitCount, p)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Throwable { try (Cluster cluster = init(Cluster.create(1))) { assertEquals(0, cluster.get(1).nodetool("help")); assertEquals(0, cluster.get(1).nodetool("flush")); assertEquals(1, cluster.get(1).nodetool("not_a_legal_command")); } }', 'ground_truth': 'public void testCommands() throws Throwable { try (Cluster cluster = init(Cluster.create(1))) { assertEquals(0, cluster.get(1).nodetool("help")); assertEquals(0, cluster.get(1).nodetool("flush")); assertEquals(1, cluster.get(1).nodetool("not_a_legal_command")); } }', 'output': 'public void testNodetoolCommands() throws Throwable { try (Cluster cluster = init(Cluster.create(1))) { assertEquals(0, cluster.get(1).nodetool("help")); assertEquals(0, cluster.get(1).nodetool("flush")); assertEquals(1, cluster.get(1).nodetool("not_a_legal_command")); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean XXX() { BeamRelNode leftRelNode = BeamSqlRelUtils.getBeamRelInput(left); BeamRelNode rightRelNode = BeamSqlRelUtils.getBeamRelInput(right); return !seekable(leftRelNode) && seekable(rightRelNode); }', 'ground_truth': 'private Optional<Integer> nonSeekableInputIndex() { BeamRelNode leftRelNode = BeamSqlRelUtils.getBeamRelInput(left); BeamRelNode rightRelNode = BeamSqlRelUtils.getBeamRelInput(right); return !seekable(leftRelNode) ? Optional.of(0) : !seekable(rightRelNode) ? Optional.of(1) : Optional.absent(); }', 'output': 'private boolean isSeekable() { BeamRelNode leftRelNode = BeamSqlRelUtils.getBeamRelInput(left); BeamRelNode rightRelNode = BeamSqlRelUtils.getBeamRelInput(right); return !seekable(leftRelNode) && seekable(rightRelNode); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public IgniteInternalFuture<Boolean> XXX(AffinityTopologyVersion topVer) { GridCompoundFuture<IgniteInternalTx, Boolean> res = new CacheObjectsReleaseFuture<>( "Tx", topVer, new IgniteReducer<IgniteInternalTx, Boolean>() { @Override public boolean collect(IgniteInternalTx e) { return true; } @Override public Boolean reduce() { return true; } }); for (IgniteInternalTx tx : txs()) { if (needWaitTransaction(tx, topVer)) res.add(tx.finishFuture()); } res.markInitialized(); return res; }', 'ground_truth': 'public IgniteInternalFuture<Boolean> finishLocalTxs(AffinityTopologyVersion topVer) { GridCompoundFuture<IgniteInternalTx, Boolean> res = new CacheObjectsReleaseFuture<>( "LocalTx", topVer, new IgniteReducer<IgniteInternalTx, Boolean>() { @Override public boolean collect(IgniteInternalTx e) { return true; } @Override public Boolean reduce() { return true; } }); for (IgniteInternalTx tx : txs()) { if (needWaitTransaction(tx, topVer)) { res.add(tx.finishFuture()); } } res.markInitialized(); return res; }', 'output': 'public IgniteInternalFuture<Boolean> release(AffinityTopologyVersion topVer) { GridCompoundFuture<IgniteInternalTx, Boolean> res = new CacheObjectsReleaseFuture<>( "Tx", topVer, new IgniteReducer<IgniteInternalTx, Boolean>() { @Override public boolean collect(IgniteInternalTx e) { return true; } @Override public Boolean reduce() { return true; } }); for (IgniteInternalTx tx : txs()) { if (needWaitTransaction(tx, topVer)) res.add(tx.finishFuture()); } res.markInitialized(); return res; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private String XXX(Collection<Integer> cacheIds) { StringBuilder names = new StringBuilder(); for (Integer cacheId : cacheIds) { String name = registeredCaches.get(cacheId).cacheConfiguration().getName(); if (names.length() != 0) names.append(", "); names.append(name); } return names.toString(); }', 'ground_truth': 'private String groupNames(Collection<Integer> grpIds) { StringBuilder names = new StringBuilder(); for (Integer grpId : grpIds) { String name = registeredGrps.get(grpId).cacheOrGroupName(); if (names.length() != 0) names.append(", "); names.append(name); } return names.toString(); }', 'output': 'private String getCacheNames(Collection<Integer> cacheIds) { StringBuilder names = new StringBuilder(); for (Integer cacheId : cacheIds) { String name = registeredCaches.get(cacheId).cacheConfiguration().getName(); if (names.length() != 0) names.append(", "); names.append(name); } return names.toString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static CompletionStage<Pair<DataFolder, String>> XXX(CompletionStage<DataObject> findTemplate, NbCodeLanguageClient client, ExecuteCommandParams params) { final DataObject[] templateObject = new DataObject[1]; final CompletionStage<DataFolder> findTarget = findTemplate.thenCompose(any -> { templateObject[0] = any; return client.workspaceFolders(); }).thenCompose(folders -> { boolean[] suggestionIsExact = { true }; DataFolder suggestion = findTargetFolder(params, templateObject[0], folders, suggestionIsExact); if (suggestionIsExact[0]) { return CompletableFuture.completedFuture(suggestion); } class VerifyPath implements Function<String, CompletionStage<DataFolder>> { @Override public CompletionStage<DataFolder> apply(String path) { if (path == null) { throw raise(RuntimeException.class, new UserCancelException(path)); } FileObject fo = FileUtil.toFileObject(new File(path)); if (fo == null || !fo.isFolder()) { client.showMessage(new MessageParams(MessageType.Error, Bundle.ERR_InvalidPath(path))); return client.showInputBox(new ShowInputBoxParams(Bundle.CTL_TemplateUI_SelectTarget(), suggestion.getPrimaryFile().getPath())).thenCompose(this); } return CompletableFuture.completedFuture(DataFolder.findFolder(fo)); } } return client.showInputBox(new ShowInputBoxParams(Bundle.CTL_TemplateUI_SelectTarget(), suggestion.getPrimaryFile().getPath())).thenCompose(new VerifyPath()); }); CompletionStage<String> findTargetName = findTarget.thenCombine(findTemplate, (target, source) -> source).thenCompose((source) -> { return client.showInputBox(new ShowInputBoxParams(Bundle.CTL_TemplateUI_SelectName(), source.getName())); }).thenCombine(findTemplate, (nameWithExtension, source) -> { String templateExtension = source.getPrimaryFile().getExt(); return removeExtensionFromFileName(nameWithExtension, templateExtension); }); return findTarget.thenCombine(findTargetName, (t, u) -> { return Pair.of(t, u); }); }', 'ground_truth': 'private static CompletionStage<DataFolder> findTargetForTemplate(CompletionStage<DataObject> findTemplate, NbCodeLanguageClient client, ExecuteCommandParams params) { final DataObject[] templateObject = new DataObject[1]; return findTemplate.thenCompose(any -> { templateObject[0] = any; return client.workspaceFolders(); }).thenCompose(folders -> { boolean[] suggestionIsExact = { true }; DataFolder suggestion = findTargetFolder(params, templateObject[0], folders, suggestionIsExact); if (suggestionIsExact[0]) { return CompletableFuture.completedFuture(suggestion); } class VerifyPath implements Function<String, CompletionStage<DataFolder>> { @Override public CompletionStage<DataFolder> apply(String path) { if (path == null) { throw raise(RuntimeException.class, new UserCancelException(path)); } FileObject fo = FileUtil.toFileObject(new File(path)); if (fo == null || !fo.isFolder()) { client.showMessage(new MessageParams(MessageType.Error, Bundle.ERR_InvalidPath(path))); return client.showInputBox(new ShowInputBoxParams(Bundle.CTL_TemplateUI_SelectTarget(), suggestion.getPrimaryFile().getPath())).thenCompose(this); } return CompletableFuture.completedFuture(DataFolder.findFolder(fo)); } } return client.showInputBox(new ShowInputBoxParams(Bundle.CTL_TemplateUI_SelectTarget(), suggestion.getPrimaryFile().getPath())).thenCompose(new VerifyPath()); }); }', 'output': 'private static CompletionStage<Pair<DataFolder, String>> findTemplate(CompletionStage<DataObject> findTemplate, NbCodeLanguageClient client, ExecuteCommandParams params) { final DataObject[] templateObject = new DataObject[1]; final CompletionStage<DataFolder> findTarget = findTemplate.thenCompose(any -> { templateObject[0] = any; return client.workspaceFolders(); }).thenCompose(folders -> { boolean[] suggestionIsExact = { true }; DataFolder suggestion = findTargetFolder(params, templateObject[0], folders, suggestionIsExact); if (suggestionIsExact[0]) { return CompletableFuture.completedFuture(suggestion); } class VerifyPath implements Function<String, CompletionStage<DataFolder>> { @Override public CompletionStage<DataFolder> apply(String path) { if (path == null) { throw raise(RuntimeException.class, new UserCancelException(path)); } FileObject fo = FileUtil.toFileObject(new File(path)); if (fo == null || !fo.isFolder()) { client.showMessage(new MessageParams(MessageType.Error, Bundle.ERR_InvalidPath(path))); return client.showInputBox(new ShowInputBoxParams(Bundle.CTL_TemplateUI_SelectTarget(), suggestion.getPrimaryFile().getPath())).thenCompose(this); } return CompletableFuture.completedFuture(DataFolder.findFolder(fo)); } } return client.showInputBox(new ShowInputBoxParams(Bundle.CTL_TemplateUI_SelectTarget(), suggestion.getPrimaryFile().getPath())).thenCompose(new VerifyPath()); }); CompletionStage<String> findTargetName = findTarget.thenCombine(findTemplate, (target, source) -> source).thenCompose((source) -> { return client.showInputBox(new ShowInputBoxParams(Bundle.CTL_TemplateUI_SelectName(), source.getName())); }).thenCombine(findTemplate, (nameWithExtension, source) -> { String templateExtension = source.getPrimaryFile().getExt(); return removeExtensionFromFileName(nameWithExtension, templateExtension); }); return findTarget.thenCombine(findTargetName, (t, u) -> { return Pair.of(t, u); }); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void XXX(Object newValue, GroovyObject object, String property) throws Throwable { try { object.setProperty(property, newValue); } catch (GroovyRuntimeException gre) { unwrap(gre); } }', 'ground_truth': 'public static void setPropertyOnSuper(Object messageArgument, Class senderClass, GroovyObject receiver, String messageName) throws Throwable{ try { InvokerHelper.setAttribute(receiver, messageName,messageArgument); } catch (GroovyRuntimeException gre) { unwrap(gre); } }', 'output': 'public static void setProperty(Object newValue, GroovyObject object, String property) throws Throwable { try { object.setProperty(property, newValue); } catch (GroovyRuntimeException gre) { unwrap(gre); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(CFDefinition cfDef) throws InvalidRequestException { UpdateStatement.processKeys(cfDef, whereClause, processedKeys); }', 'ground_truth': 'public ParsedStatement.Prepared prepare() throws InvalidRequestException { CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace(), columnFamily()); cfDef = metadata.getCfDef(); AbstractType[] types = new AbstractType[getBoundsTerms()]; UpdateStatement.processKeys(cfDef, whereClause, processedKeys, types); return new ParsedStatement.Prepared(this, Arrays.<AbstractType<?>>asList(types)); }', 'output': 'private void updateCf(CFDefinition cfDef) throws InvalidRequestException { UpdateStatement.processKeys(cfDef, whereClause, processedKeys); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Temporal XXX(final ValueSemanticsProvider.Context context, final Temporal t) { if(t instanceof ZonedDateTime) { return LocalDateTime.ofInstant(((ZonedDateTime) t).toInstant(), context.getInteractionContext().getTimeZone()); } if(t instanceof OffsetDateTime) { return LocalDateTime.ofInstant(((OffsetDateTime) t).toInstant(), context.getInteractionContext().getTimeZone()); } if(t instanceof OffsetTime) { return ((OffsetTime) t) .withOffsetSameInstant(context.getInteractionContext().getTimeZoneOffsetNow()) .XXX(); } return t; }', 'ground_truth': 'private Temporal translateToUserLocalTimeZone(final ValueSemanticsProvider.Context context, final Temporal t) { if(t instanceof ZonedDateTime) { return _Temporals.translateToTimeZone((ZonedDateTime) t, context.getInteractionContext().getTimeZone()); } if(t instanceof OffsetDateTime) { return _Temporals.translateToTimeZone((OffsetDateTime) t, context.getInteractionContext().getTimeZone()); } if(t instanceof OffsetTime) { return _Temporals.translateToTimeOffset((OffsetTime) t, context.getInteractionContext().getTimeZoneOffsetNow()); } return t; }', 'output': 'private Temporal toTemporal(final ValueSemanticsProvider.Context context, final Temporal t) { if(t instanceof ZonedDateTime) { return LocalDateTime.ofInstant(((ZonedDateTime) t).toInstant(), context.getInteractionContext().getTimeZone()); } if(t instanceof OffsetDateTime) { return LocalDateTime.ofInstant(((OffsetDateTime) t).toInstant(), context.getInteractionContext().getTimeZone()); } if(t instanceof OffsetTime) { return ((OffsetTime) t) .withOffsetSameInstant(context.getInteractionContext().getTimeZoneOffsetNow()) .toTemporal(); } return t; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws IOException { pig.registerQuery("rows = LOAD \'cassandra: Iterator<Tuple> it = pig.openIterator("rows"); if (it.hasNext()) { Tuple t = it.next(); String rowKey =  t.get(0).toString(); if ("qux".equals(rowKey)) { Tuple column = (Tuple) t.get(1); Assert.assertEquals(column.get(0), "atomic_weight"); Assert.assertEquals(column.get(1), 0.660161815846869d); column = (Tuple) t.get(3); Assert.assertEquals(column.get(0), "name"); Assert.assertEquals(column.get(1), "User Qux"); column = (Tuple) t.get(4); Assert.assertEquals(column.get(0), "percent"); Assert.assertEquals(column.get(1), 64.7f); column = (Tuple) t.get(5); Assert.assertEquals(column.get(0), "rating"); Assert.assertEquals(column.get(1), 2); column = (Tuple) t.get(6); Assert.assertEquals(column.get(0), "score"); Assert.assertEquals(column.get(1), 12000L); column = (Tuple) t.get(7); Assert.assertEquals(column.get(0), "vote_type"); Assert.assertEquals(column.get(1), "dislike"); } } }', 'ground_truth': 'public void testCqlNativeStorageSchema() throws IOException { pig.registerQuery("rows = LOAD \'cql: Iterator<Tuple> it = pig.openIterator("rows"); if (it.hasNext()) { Tuple t = it.next(); String rowKey =  t.get(0).toString(); if ("qux".equals(rowKey)) { Tuple column = (Tuple) t.get(1); Assert.assertEquals(column.get(0), "atomic_weight"); Assert.assertEquals(column.get(1), 0.660161815846869d); column = (Tuple) t.get(3); Assert.assertEquals(column.get(0), "name"); Assert.assertEquals(column.get(1), "User Qux"); column = (Tuple) t.get(4); Assert.assertEquals(column.get(0), "percent"); Assert.assertEquals(column.get(1), 64.7f); column = (Tuple) t.get(5); Assert.assertEquals(column.get(0), "rating"); Assert.assertEquals(column.get(1), 2); column = (Tuple) t.get(6); Assert.assertEquals(column.get(0), "score"); Assert.assertEquals(column.get(1), 12000L); column = (Tuple) t.get(7); Assert.assertEquals(column.get(0), "vote_type"); Assert.assertEquals(column.get(1), "dislike"); } } }', 'output': 'public void testLoadFromCassandra() throws IOException { pig.registerQuery("rows = LOAD \'cassandra: Iterator<Tuple> it = pig.openIterator("rows"); if (it.hasNext()) { Tuple t = it.next(); String rowKey =  t.get(0).toString(); if ("qux".equals(rowKey)) { Tuple column = (Tuple) t.get(1); Assert.assertEquals(column.get(0), "atomic_weight"); Assert.assertEquals(column.get(1), 0.660161815846869d); column = (Tuple) t.get(3); Assert.assertEquals(column.get(0), "name"); Assert.assertEquals(column.get(1), "User Qux"); column = (Tuple) t.get(4); Assert.assertEquals(column.get(0), "percent"); Assert.assertEquals(column.get(1), 64.7f); column = (Tuple) t.get(5); Assert.assertEquals(column.get(0), "rating"); Assert.assertEquals(column.get(1), 2); column = (Tuple) t.get(6); Assert.assertEquals(column.get(0), "score"); Assert.assertEquals(column.get(1), 12000L); column = (Tuple) t.get(7); Assert.assertEquals(column.get(0), "vote_type"); Assert.assertEquals(column.get(1), "dislike"); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(String keyspaceName, String columnFamilyName, String tag) throws IOException { if (keyspaceName == null) throw new IOException("You must supply a keyspace name"); if (operationMode == Mode.JOINING) throw new IOException("Cannot snapshot until bootstrap completes"); if (columnFamilyName == null) throw new IOException("You must supply a table name"); if (columnFamilyName.contains(".")) throw new IllegalArgumentException("Cannot take a snapshot of a secondary index by itself. Run snapshot on the table that owns the index."); if (tag == null || tag.equals("")) throw new IOException("You must supply a snapshot name."); Keyspace keyspace = getValidKeyspace(keyspaceName); ColumnFamilyStore columnFamilyStore = keyspace.getColumnFamilyStore(columnFamilyName); if (columnFamilyStore.snapshotExists(tag)) throw new IOException("Snapshot " + tag + " already exists."); columnFamilyStore.snapshot(tag); }', 'ground_truth': 'public void takeTableSnapshot(String keyspaceName, String tableName, String tag) throws IOException { if (keyspaceName == null) throw new IOException("You must supply a keyspace name"); if (operationMode == Mode.JOINING) throw new IOException("Cannot snapshot until bootstrap completes"); if (tableName == null) throw new IOException("You must supply a table name"); if (tableName.contains(".")) throw new IllegalArgumentException("Cannot take a snapshot of a secondary index by itself. Run snapshot on the table that owns the index."); if (tag == null || tag.equals("")) throw new IOException("You must supply a snapshot name."); Keyspace keyspace = getValidKeyspace(keyspaceName); ColumnFamilyStore columnFamilyStore = keyspace.getColumnFamilyStore(tableName); if (columnFamilyStore.snapshotExists(tag)) throw new IOException("Snapshot " + tag + " already exists."); columnFamilyStore.snapshot(tag); }', 'output': 'public void snapshot(String keyspaceName, String columnFamilyName, String tag) throws IOException { if (keyspaceName == null) throw new IOException("You must supply a keyspace name"); if (operationMode == Mode.JOINING) throw new IOException("Cannot snapshot until bootstrap completes"); if (columnFamilyName == null) throw new IOException("You must supply a table name"); if (columnFamilyName.contains(".")) throw new IllegalArgumentException("Cannot take a snapshot of a secondary index by itself. Run snapshot on the table that owns the index."); if (tag == null || tag.equals("")) throw new IOException("You must supply a snapshot name."); Keyspace keyspace = getValidKeyspace(keyspaceName); ColumnFamilyStore columnFamilyStore = keyspace.getColumnFamilyStore(columnFamilyName); if (columnFamilyStore.snapshotExists(tag)) throw new IOException("Snapshot " + tag + " already exists."); columnFamilyStore.snapshot(tag); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { BinaryMarshaller marsh = binaryMarshaller(Arrays.asList( new BinaryTypeConfiguration(SimpleObject.class.getName()) )); SimpleObject obj = simpleObject(); BinaryObject po = marshal(obj, marsh); UUID uuid = UUID.randomUUID(); BinaryObject copy = copy(po, F.<String, Object>asMap("uuid", uuid)); assertEquals(uuid, copy.<UUID>field("uuid")); SimpleObject obj0 = copy.deserialize(); assertEquals(uuid, obj0.uuid); }', 'ground_truth': 'public void testBinaryCopyUuid() throws Exception { BinaryMarshaller marsh = binaryMarshaller(Arrays.asList( new BinaryTypeConfiguration(SimpleObject.class.getName()) )); SimpleObject obj = simpleObject(); BinaryObject po = marshal(obj, marsh); UUID uuid = UUID.randomUUID(); BinaryObject copy = copy(po, F.<String, Object>asMap("uuid", uuid)); assertEquals(uuid, copy.<UUID>field("uuid")); SimpleObject obj0 = copy.deserialize(); assertEquals(uuid, obj0.uuid); }', 'output': 'public void testCopyBinaryObject() throws Exception { BinaryMarshaller marsh = binaryMarshaller(Arrays.asList( new BinaryTypeConfiguration(SimpleObject.class.getName()) )); SimpleObject obj = simpleObject(); BinaryObject po = marshal(obj, marsh); UUID uuid = UUID.randomUUID(); BinaryObject copy = copy(po, F.<String, Object>asMap("uuid", uuid)); assertEquals(uuid, copy.<UUID>field("uuid")); SimpleObject obj0 = copy.deserialize(); assertEquals(uuid, obj0.uuid); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(IsisInteraction isisInteraction) { currentSession(isisInteraction) .ifPresent(PersistenceSession::close); }', 'ground_truth': 'private void closeInteraction(IsisInteraction isisInteraction) { currentSession(isisInteraction) .ifPresent(PersistenceSession::close); }', 'output': 'private void closeCurrentSession(IsisInteraction isisInteraction) { currentSession(isisInteraction) .ifPresent(PersistenceSession::close); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static int XXX(ByteBuffer bytes, int offset) { if ( bytes.remaining() - offset < 4 ) { throw new IllegalArgumentException("An integer must be 4 bytes in size."); } int n = 0; for ( int i = 0; i < 4; ++i ) { n <<= 8; n |= bytes.array()[bytes.position()+bytes.arrayOffset()+ offset + i] & 0xFF; } return n; }', 'ground_truth': 'public static int byteBufferToInt(ByteBuffer bytes) { if (bytes.remaining() < 4 ) { throw new IllegalArgumentException("An integer must be 4 bytes in size."); } int n = 0; for ( int i = 0; i < 4; ++i ) { n <<= 8; n |= bytes.array()[bytes.position() + bytes.arrayOffset() + i] & 0xFF; } return n; }', 'output': 'public static int readInt(ByteBuffer bytes, int offset) { if ( bytes.remaining() - offset < 4 ) { throw new IllegalArgumentException("An integer must be 4 bytes in size."); } int n = 0; for ( int i = 0; i < 4; ++i ) { n <<= 8; n |= bytes.array()[bytes.position()+bytes.arrayOffset()+ offset + i] & 0xFF; } return n; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public PCollection<Row> XXX(PCollectionTuple inputPCollections) { PCollection<Row> leftRows = inputPCollections.apply( "left", BeamSqlRelUtils.getBeamRelInput(inputs.get(0)).toPTransform()); PCollection<Row> rightRows = inputPCollections.apply( "right", BeamSqlRelUtils.getBeamRelInput(inputs.get(1)).toPTransform()); WindowFn leftWindow = leftRows.getWindowingStrategy().getWindowFn(); WindowFn rightWindow = rightRows.getWindowingStrategy().getWindowFn(); if (!leftWindow.isCompatible(rightWindow)) { throw new IllegalArgumentException( "inputs of " + opType + " have different window strategy: " + leftWindow + " VS " + rightWindow); } final TupleTag<Row> leftTag = new TupleTag<>(); final TupleTag<Row> rightTag = new TupleTag<>(); String stageName = BeamSqlRelUtils.getStageName(beamRelNode); PCollection<KV<Row, CoGbkResult>> coGbkResultCollection = KeyedPCollectionTuple.of( leftTag, leftRows.apply( stageName + "_CreateLeftIndex", MapElements.via(new BeamSetOperatorsTransforms.BeamSqlRow2KvFn()))) .and( rightTag, rightRows.apply( stageName + "_CreateRightIndex", MapElements.via(new BeamSetOperatorsTransforms.BeamSqlRow2KvFn()))) .apply(CoGroupByKey.create()); PCollection<Row> ret = coGbkResultCollection.apply( ParDo.of( new BeamSetOperatorsTransforms.SetOperatorFilteringDoFn( leftTag, rightTag, opType, all))); return ret; }', 'ground_truth': 'public PCollection<Row> expand(PInput pinput) { PCollectionList<Row> inputs = (PCollectionList<Row>) pinput; assert inputs.size() == 2; PCollection<Row> leftRows = inputs.get(0); PCollection<Row> rightRows = inputs.get(1); WindowFn leftWindow = leftRows.getWindowingStrategy().getWindowFn(); WindowFn rightWindow = rightRows.getWindowingStrategy().getWindowFn(); if (!leftWindow.isCompatible(rightWindow)) { throw new IllegalArgumentException( "inputs of " + opType + " have different window strategy: " + leftWindow + " VS " + rightWindow); } final TupleTag<Row> leftTag = new TupleTag<>(); final TupleTag<Row> rightTag = new TupleTag<>(); String stageName = BeamSqlRelUtils.getStageName(beamRelNode); PCollection<KV<Row, CoGbkResult>> coGbkResultCollection = KeyedPCollectionTuple.of( leftTag, leftRows.apply( stageName + "_CreateLeftIndex", MapElements.via(new BeamSetOperatorsTransforms.BeamSqlRow2KvFn()))) .and( rightTag, rightRows.apply( stageName + "_CreateRightIndex", MapElements.via(new BeamSetOperatorsTransforms.BeamSqlRow2KvFn()))) .apply(CoGroupByKey.create()); PCollection<Row> ret = coGbkResultCollection.apply( ParDo.of( new BeamSetOperatorsTransforms.SetOperatorFilteringDoFn( leftTag, rightTag, opType, all))); return ret; }', 'output': 'public PCollection<Row> apply(PCollectionTuple inputPCollections) { PCollection<Row> leftRows = inputPCollections.apply( "left", BeamSqlRelUtils.getBeamRelInput(inputs.get(0)).toPTransform()); PCollection<Row> rightRows = inputPCollections.apply( "right", BeamSqlRelUtils.getBeamRelInput(inputs.get(1)).toPTransform()); WindowFn leftWindow = leftRows.getWindowingStrategy().getWindowFn(); WindowFn rightWindow = rightRows.getWindowingStrategy().getWindowFn(); if (!leftWindow.isCompatible(rightWindow)) { throw new IllegalArgumentException( "inputs of " + opType + " have different window strategy: " + leftWindow + " VS " + rightWindow); } final TupleTag<Row> leftTag = new TupleTag<>(); final TupleTag<Row> rightTag = new TupleTag<>(); String stageName = BeamSqlRelUtils.getStageName(beamRelNode); PCollection<KV<Row, CoGbkResult>> coGbkResultCollection = KeyedPCollectionTuple.of( leftTag, leftRows.apply( stageName + "_CreateLeftIndex", MapElements.via(new BeamSetOperatorsTransforms.BeamSqlRow2KvFn()))) .and( rightTag, rightRows.apply( stageName + "_CreateRightIndex", MapElements.via(new BeamSetOperatorsTransforms.BeamSqlRow2KvFn()))) .apply(CoGroupByKey.create()); PCollection<Row> ret = coGbkResultCollection.apply( ParDo.of( new BeamSetOperatorsTransforms.SetOperatorFilteringDoFn( leftTag, rightTag, opType, all))); return ret; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { Token[] tokens = new Token[] { Token.leftParenthesis( 1, 1 ), Token.rightParenthesis( 1, 2 ), Token.leftSquareBracket( 1, 3 ), Token.rightSquareBracket( 1, 4 ), Token.leftCurlyBrace( 1, 5 ), Token.rightCurlyBrace( 1, 6 ) }; MockTokenStream in = new MockTokenStream( tokens ); assertSame( tokens[0], in.la() ); assertSame( tokens[1], in.la( 2 ) ); assertSame( tokens[2], in.la( 3 ) ); assertSame( tokens[3], in.la( 4 ) ); assertSame( tokens[4], in.la( 5 ) ); try { in.la( 6 ); fail( "should have thrown LookAheadExhaustionException" ); } catch (LookAheadExhaustionException e) { assertEquals( 6, e.getLookAhead() ); } }', 'ground_truth': 'public void DISABLED_testExhaustLookAhead() throws Exception { Token[] tokens = new Token[] { Token.leftParenthesis( 1, 1 ), Token.rightParenthesis( 1, 2 ), Token.leftSquareBracket( 1, 3 ), Token.rightSquareBracket( 1, 4 ), Token.leftCurlyBrace( 1, 5 ), Token.rightCurlyBrace( 1, 6 ) }; MockTokenStream in = new MockTokenStream( tokens ); assertSame( tokens[0], in.la() ); assertSame( tokens[1], in.la( 2 ) ); assertSame( tokens[2], in.la( 3 ) ); assertSame( tokens[3], in.la( 4 ) ); assertSame( tokens[4], in.la( 5 ) ); try { in.la( 6 ); fail( "should have thrown LookAheadExhaustionException" ); } catch (LookAheadExhaustionException e) { assertEquals( 6, e.getLookAhead() ); } }', 'output': 'public void testMockTokenStream() throws Exception { Token[] tokens = new Token[] { Token.leftParenthesis( 1, 1 ), Token.rightParenthesis( 1, 2 ), Token.leftSquareBracket( 1, 3 ), Token.rightSquareBracket( 1, 4 ), Token.leftCurlyBrace( 1, 5 ), Token.rightCurlyBrace( 1, 6 ) }; MockTokenStream in = new MockTokenStream( tokens ); assertSame( tokens[0], in.la() ); assertSame( tokens[1], in.la( 2 ) ); assertSame( tokens[2], in.la( 3 ) ); assertSame( tokens[3], in.la( 4 ) ); assertSame( tokens[4], in.la( 5 ) ); try { in.la( 6 ); fail( "should have thrown LookAheadExhaustionException" ); } catch (LookAheadExhaustionException e) { assertEquals( 6, e.getLookAhead() ); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static Views XXX(RowIterator partition) { Views.Builder views = org.apache.cassandra.schema.Views.builder(); String query = String.format("SELECT * FROM %s.%s", NAME, VIEWS); for (UntypedResultSet.Row row : QueryProcessor.resultify(query, partition)) { ViewDefinition view = createViewFromViewRow(row); views.add(view); } return views.build(); }', 'ground_truth': 'private static Views fetchViews(String keyspaceName, Types types) { String query = format("SELECT view_name FROM %s.%s WHERE keyspace_name = ?", NAME, VIEWS); Views.Builder views = org.apache.cassandra.schema.Views.builder(); for (UntypedResultSet.Row row : query(query, keyspaceName)) views.add(fetchView(keyspaceName, row.getString("view_name"), types)); return views.build(); }', 'output': 'private static Views buildViews(RowIterator partition) { Views.Builder views = org.apache.cassandra.schema.Views.builder(); String query = String.format("SELECT * FROM %s.%s", NAME, VIEWS); for (UntypedResultSet.Row row : QueryProcessor.resultify(query, partition)) { ViewDefinition view = createViewFromViewRow(row); views.add(view); } return views.build(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private ByteBuffer XXX(ByteBuffer columnName, String columnFamilyName, String columnValue) { CfDef columnFamilyDef = getCfDef(columnFamilyName); for (ColumnDef columnDefinition : columnFamilyDef.getColumn_metadata()) { byte[] currentColumnName = columnDefinition.getName(); if (ByteBufferUtil.compare(currentColumnName,columnName)==0) { try { String validationClass = columnDefinition.getValidation_class(); return getBytesAccordingToType(columnValue, getFormatTypeForColumn(validationClass)); } catch (Exception e) { throw new RuntimeException(e.getMessage(), e); } } } return ByteBuffer.wrap(columnValue.getBytes()); }', 'ground_truth': 'private ByteBuffer columnValueAsBytes(ByteBuffer columnName, String columnFamilyName, String columnValue) { CfDef columnFamilyDef = getCfDef(columnFamilyName); for (ColumnDef columnDefinition : columnFamilyDef.getColumn_metadata()) { byte[] currentColumnName = columnDefinition.getName(); if (ByteBufferUtil.compare(currentColumnName,columnName)==0) { try { String validationClass = columnDefinition.getValidation_class(); return getBytesAccordingToType(columnValue, getFormatTypeForColumn(validationClass)); } catch (Exception e) { throw new RuntimeException(e.getMessage(), e); } } } return ByteBuffer.wrap(columnValue.getBytes()); }', 'output': 'private ByteBuffer getColumnValue(ByteBuffer columnName, String columnFamilyName, String columnValue) { CfDef columnFamilyDef = getCfDef(columnFamilyName); for (ColumnDef columnDefinition : columnFamilyDef.getColumn_metadata()) { byte[] currentColumnName = columnDefinition.getName(); if (ByteBufferUtil.compare(currentColumnName,columnName)==0) { try { String validationClass = columnDefinition.getValidation_class(); return getBytesAccordingToType(columnValue, getFormatTypeForColumn(validationClass)); } catch (Exception e) { throw new RuntimeException(e.getMessage(), e); } } } return ByteBuffer.wrap(columnValue.getBytes()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private BeamRecord XXX(BeamRecordType beamRowType, ImmutableList<RexLiteral> tuple) { return IntStream .range(0, tuple.size()) .mapToObj(i -> autoCastField(beamRowType.getFieldCoder(i), tuple.get(i).getValue())) .collect(toRecord(beamRowType)); }', 'ground_truth': 'private Row tupleToRow(RowType rowType, ImmutableList<RexLiteral> tuple) { return IntStream .range(0, tuple.size()) .mapToObj(i -> autoCastField(rowType.getFieldCoder(i), tuple.get(i).getValue())) .collect(toRow(rowType)); }', 'output': 'private BeamRecord record(BeamRecordType beamRowType, ImmutableList<RexLiteral> tuple) { return IntStream .range(0, tuple.size()) .mapToObj(i -> autoCastField(beamRowType.getFieldCoder(i), tuple.get(i).getValue())) .collect(toRecord(beamRowType)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Object XXX(float value) { /** @todo if (value >= 0 && value < 100) { return integerCache[i]; } */ return new Float(value); }', 'ground_truth': 'public static Object box(byte value) { /** @todo cache? */ return new Byte(value); }', 'output': 'public static Object apply(float value) { if (value >= 0 && value < 100) { return integerCache[i]; } return new Float(value); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean XXX(String keyspace, ColumnSpecification receiver) { AbstractType<?> returnType = Functions.getReturnType(functionName, receiver.ksName, receiver.cfName); return returnType == null || receiver.type.isValueCompatibleWith(returnType); }', 'ground_truth': 'public AssignmentTestable.TestResult testAssignment(String keyspace, ColumnSpecification receiver) { try { Function fun = Functions.get(keyspace, name, terms, receiver.ksName, receiver.cfName); if (fun != null && receiver.type.equals(fun.returnType())) return AssignmentTestable.TestResult.EXACT_MATCH; else if (fun == null || receiver.type.isValueCompatibleWith(fun.returnType())) return AssignmentTestable.TestResult.WEAKLY_ASSIGNABLE; else return AssignmentTestable.TestResult.NOT_ASSIGNABLE; } catch (InvalidRequestException e) { return AssignmentTestable.TestResult.WEAKLY_ASSIGNABLE; } }', 'output': 'public boolean isCompatible(String keyspace, ColumnSpecification receiver) { AbstractType<?> returnType = Functions.getReturnType(functionName, receiver.ksName, receiver.cfName); return returnType == null || receiver.type.isValueCompatibleWith(returnType); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Predicate XXX(Resource.Type type, Request request, Predicate predicate) throws AmbariException, UnsupportedPropertyException { Set<String> requestPropertyIds = request == null ? new HashSet<String>() : PropertyHelper.getAssociatedPropertyIds(request); if (predicate != null) { requestPropertyIds.addAll(PredicateHelper.getPropertyIds(predicate)); } if (requestPropertyIds.size() > 0) { ResourceProvider provider = ensureResourceProvider(type); requestPropertyIds.removeAll(provider.getPropertyIds()); List<PropertyProvider> propertyProviders = ensurePropertyProviders(type); for (PropertyProvider propertyProvider : propertyProviders) { if (requestPropertyIds.removeAll(propertyProvider.getPropertyIds())) { Set<String>  keyPropertyIds = new HashSet<String>(provider.getKeyPropertyIds().values()); Request          readRequest    = PropertyHelper.getReadRequest(keyPropertyIds); Iterable<Resource> resources = getResources(type, readRequest, predicate); PredicateBuilder pb = new PredicateBuilder(); PredicateBuilder.PredicateBuilderWithPredicate pbWithPredicate = null; for (Resource resource : resources) { if (pbWithPredicate != null) { pb = pbWithPredicate.or(); } pb              = pb.begin(); pbWithPredicate = null; for (String keyPropertyId : keyPropertyIds) { if (pbWithPredicate != null) { pb = pbWithPredicate.and(); } pbWithPredicate = pb.property(keyPropertyId).equals((Comparable) resource.getPropertyValue(keyPropertyId)); } if (pbWithPredicate != null) { pbWithPredicate = pbWithPredicate.end(); } } return pbWithPredicate == null ? null : pbWithPredicate.toPredicate(); } } } return predicate; }', 'ground_truth': 'private Predicate resolvePredicate(Resource.Type type, Predicate predicate) throws UnsupportedPropertyException, SystemException, NoSuchResourceException, NoSuchParentResourceException{ ResourceProvider provider = ensureResourceProvider(type); Set<String>  keyPropertyIds = new HashSet<String>(provider.getKeyPropertyIds().values()); Request      readRequest    = PropertyHelper.getReadRequest(keyPropertyIds); Iterable<Resource> resources = getResources(type, readRequest, predicate); PredicateBuilder pb = new PredicateBuilder(); PredicateBuilder.PredicateBuilderWithPredicate pbWithPredicate = null; for (Resource resource : resources) { if (pbWithPredicate != null) { pb = pbWithPredicate.or(); } pb              = pb.begin(); pbWithPredicate = null; for (String keyPropertyId : keyPropertyIds) { if (pbWithPredicate != null) { pb = pbWithPredicate.and(); } pbWithPredicate = pb.property(keyPropertyId).equals((Comparable) resource.getPropertyValue(keyPropertyId)); } if (pbWithPredicate != null) { pbWithPredicate = pbWithPredicate.end(); } } return pbWithPredicate == null ? null : pbWithPredicate.toPredicate(); }', 'output': 'private Predicate applyPredicate(Resource.Type type, Request request, Predicate predicate) throws AmbariException, UnsupportedPropertyException { Set<String> requestPropertyIds = request == null ? new HashSet<String>() : PropertyHelper.getAssociatedPropertyIds(request); if (predicate != null) { requestPropertyIds.addAll(PredicateHelper.getPropertyIds(predicate)); } if (requestPropertyIds.size() > 0) { ResourceProvider provider = ensureResourceProvider(type); requestPropertyIds.removeAll(provider.getPropertyIds()); List<PropertyProvider> propertyProviders = ensurePropertyProviders(type); for (PropertyProvider propertyProvider : propertyProviders) { if (requestPropertyIds.removeAll(propertyProvider.getPropertyIds())) { Set<String>  keyPropertyIds = new HashSet<String>(provider.getKeyPropertyIds().values()); Request          readRequest    = PropertyHelper.getReadRequest(keyPropertyIds); Iterable<Resource> resources = getResources(type, readRequest, predicate); PredicateBuilder pb = new PredicateBuilder(); PredicateBuilder.PredicateBuilderWithPredicate pbWithPredicate = null; for (Resource resource : resources) { if (pbWithPredicate != null) { pb = pbWithPredicate.or(); } pb              = pb.begin(); pbWithPredicate = pb.property(keyPropertyIds.iterator().next()).equals((Comparable) resource.getPropertyValue(keyPropertyIds.iterator().next())); } if (pbWithPredicate != null) { pbWithPredicate = pbWithPredicate.end(); } } return pbWithPredicate == null ? null : pbWithPredicate.toPredicate(); } } return predicate; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Response XXX(@Context HttpHeaders headers, @Context UriInfo ui, @PathParam("serviceName") String serviceName, @PathParam("componentName") String componentName) { return handleRequest(headers, null, ui, Request.Type.GET, createHostComponentResource(serviceName, null, componentName)); }', 'ground_truth': 'public Response getRootHostComponent(String body, @Context HttpHeaders headers, @Context UriInfo ui, @PathParam("serviceName") String serviceName, @PathParam("hostName") String hostName, @PathParam("hostComponent") String hostComponent) { return handleRequest(headers, body, ui, Request.Type.GET, createHostComponentResource(serviceName, hostName, hostComponent)); }', 'output': 'public Response getHostComponent(HttpHeaders headers, @Context UriInfo ui, @PathParam("serviceName") String serviceName, @PathParam("componentName") String componentName) { return handleRequest(headers, null, ui, Request.Type.GET, createHostComponentResource(serviceName, null, componentName)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Consent XXX( final InteractionHead head, final Can<ManagedObject> proposedArguments, final InteractionInitiatedBy interactionInitiatedBy) { final InteractionResultSet resultSet = new InteractionResultSet(); validateArgumentsIndividually(head, proposedArguments, interactionInitiatedBy, resultSet); return resultSet.createConsent(); }', 'ground_truth': 'public Consent isArgumentSetValidForParameters( final InteractionHead head, final Can<ManagedObject> proposedArguments, final InteractionInitiatedBy interactionInitiatedBy) { final InteractionResultSet resultSet = new InteractionResultSet(); validateArgumentsIndividually(head, proposedArguments, interactionInitiatedBy, resultSet); return resultSet.createConsent(); }', 'output': 'public Consent createConsent( final InteractionHead head, final Can<ManagedObject> proposedArguments, final InteractionInitiatedBy interactionInitiatedBy) { final InteractionResultSet resultSet = new InteractionResultSet(); validateArgumentsIndividually(head, proposedArguments, interactionInitiatedBy, resultSet); return resultSet.createConsent(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public StreamPlan XXX(InetAddressAndPort to, Collection<StreamSession.SSTableStreamingSections> sstableDetails) { coordinator.XXX(to, sstableDetails); return this; }', 'ground_truth': 'public StreamPlan transferStreams(InetAddressAndPort to, Collection<OutgoingStream> streams) { coordinator.transferStreams(to, streams); return this; }', 'output': 'public StreamPlan addSSTables(InetAddressAndPort to, Collection<StreamSession.SSTableStreamingSections> sstableDetails) { coordinator.addSSTables(to, sstableDetails); return this; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public <T> List<T> XXX(TupleTag<T> tag) { List<T> resultElems = new ArrayList<>(peekSideOutputElements(tag)); clearSideOutputElements(tag); return resultElems; }', 'ground_truth': 'public <T> List<T> takeOutputElements(TupleTag<T> tag) { List<T> resultElems = new ArrayList<>(peekOutputElements(tag)); clearOutputElements(tag); return resultElems; }', 'output': 'public <T> List<T> peekSideOutputs(TupleTag<T> tag) { List<T> resultElems = new ArrayList<>(peekSideOutputElements(tag)); clearSideOutputElements(tag); return resultElems; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void XXX(URI file, long start, long len, IgfsBlockLocation... locations) { assert locations != null && locations.length > 0; IgfsPath path = new IgfsPath(file); Block block = new Block(path, start, len); Collection<IgfsBlockLocation> locationsList = new ArrayList<>(); Collections.addAll(locationsList, locations); BLOCK_MAP.put(block, locationsList); }', 'ground_truth': 'private static void mapIgfsBlock(URI file, long start, long len, IgfsBlockLocation... locations) { assert locations != null && locations.length > 0; IgfsPath path = new IgfsPath(file); Block block = new Block(path, start, len); Collection<IgfsBlockLocation> locationsList = new ArrayList<>(); Collections.addAll(locationsList, locations); BLOCK_MAP.put(block, locationsList); }', 'output': 'private static void addLocations(IgfsFile file, long start, long len, IgfsBlockLocation... locations) { assert locations != null && locations.length > 0; IgfsPath path = new IgfsPath(file); Block block = new Block(path, start, len); Collection<IgfsBlockLocation> locationsList = new ArrayList<>(); Collections.addAll(locationsList, locations); BLOCK_MAP.put(block, locationsList); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public final void XXX(List<IndexExpression> expressions, SecondaryIndexManager indexManager, QueryOptions options) throws InvalidRequestException { for (Restriction restriction : restrictions.values()) restriction.XXX(expressions, indexManager, options); }', 'ground_truth': 'public final void addRowFilterTo(RowFilter filter, SecondaryIndexManager indexManager, QueryOptions options) throws InvalidRequestException { for (Restriction restriction : restrictions.values()) restriction.addRowFilterTo(filter, indexManager, options); }', 'output': 'public final void addRestrictions(List<IndexExpression> expressions, SecondaryIndexManager indexManager, QueryOptions options) throws InvalidRequestException { for (Restriction restriction : restrictions.values()) restriction.addRestrictions(expressions, indexManager, options); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Coder<Row> XXX(Schema schema, UUID coderId) { return generatedCoders.computeIfAbsent(coderId, h -> { TypeDescription.Generic coderType = TypeDescription.Generic.Builder.parameterizedType(Coder.class, Row.class).build(); DynamicType.Builder<Coder> builder = (DynamicType.Builder<Coder>) BYTE_BUDDY.subclass(coderType); builder = createComponentCoders(schema, builder); builder = implementMethods(builder, schema); try { return builder .make() .load( Coder.class.getClassLoader(), ClassLoadingStrategy.Default.INJECTION) .getLoaded() .getDeclaredConstructor().newInstance(); } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) { throw new RuntimeException("Unable to generate coder for schema " + schema); } }); }', 'ground_truth': 'public static Coder<Row> generate(Schema schema, UUID coderId) { return generatedCoders.computeIfAbsent( coderId, h -> { TypeDescription.Generic coderType = TypeDescription.Generic.Builder.parameterizedType(Coder.class, Row.class).build(); DynamicType.Builder<Coder> builder = (DynamicType.Builder<Coder>) BYTE_BUDDY.subclass(coderType); builder = createComponentCoders(schema, builder); builder = implementMethods(schema, builder); try { return builder .make() .load(Coder.class.getClassLoader(), ClassLoadingStrategy.Default.INJECTION) .getLoaded() .getDeclaredConstructor() .newInstance(); } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) { throw new RuntimeException("Unable to generate coder for schema " + schema); } }); }', 'output': 'public static Coder<Row> getCoder(Schema schema, UUID coderId) { return generatedCoders.computeIfAbsent(coderId, h -> { TypeDescription.Generic coderType = TypeDescription.Generic.Builder.parameterizedType(Coder.class, Row.class).build(); DynamicType.Builder<Coder> builder = (DynamicType.Builder<Coder>) BYTE_BUDDY.subclass(coderType); builder = createComponentCoders(schema, builder); builder = implementMethods(builder, schema); try { return builder .make() .load( Coder.class.getClassLoader(), ClassLoadingStrategy.Default.INJECTION) .getLoaded() .getDeclaredConstructor().newInstance(); } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) { throw new RuntimeException("Unable to generate coder for schema " + schema); } }); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<BoundedSource<Document>> XXX(long desiredBundleSizeBytes, PipelineOptions options) { MongoClient mongoClient = new MongoClient(new MongoClientURI(spec.uri())); MongoDatabase mongoDatabase = mongoClient.getDatabase(spec.database()); List<Document> splitKeys; if (spec.numSplits() > 0) { long estimatedSizeBytes = getEstimatedSizeBytes(options); desiredBundleSizeBytes = estimatedSizeBytes / spec.numSplits(); } if (desiredBundleSizeBytes < 1024 * 1024) { desiredBundleSizeBytes = 1 * 1024 * 1024; } BasicDBObject splitVectorCommand = new BasicDBObject(); splitVectorCommand.append("splitVector", spec.database() + "." + spec.collection()); splitVectorCommand.append("keyPattern", new BasicDBObject().append("_id", 1)); splitVectorCommand.append("force", false); LOG.debug("Splitting in chunk of {} MB", desiredBundleSizeBytes / 1024 / 1024); splitVectorCommand.append("maxChunkSize", desiredBundleSizeBytes / 1024 / 1024); Document splitVectorCommandResult = mongoDatabase.runCommand(splitVectorCommand); splitKeys = (List<Document>) splitVectorCommandResult.get("splitKeys"); List<BoundedSource<Document>> sources = new ArrayList<>(); if (splitKeys.size() < 1) { LOG.debug("Split keys is low, using an unique source"); sources.add(this); return sources; } LOG.debug("Number of splits is {}", splitKeys.size()); for (String shardFilter : splitKeysToFilters(splitKeys, spec.filter())) { sources.add(new BoundedMongoDbSource(spec.withFilter(shardFilter))); } return sources; }', 'ground_truth': 'public List<BoundedSource<Document>> split(long desiredBundleSizeBytes, PipelineOptions options) { MongoClient mongoClient = new MongoClient(new MongoClientURI(spec.uri())); MongoDatabase mongoDatabase = mongoClient.getDatabase(spec.database()); List<Document> splitKeys; if (spec.numSplits() > 0) { long estimatedSizeBytes = getEstimatedSizeBytes(options); desiredBundleSizeBytes = estimatedSizeBytes / spec.numSplits(); } if (desiredBundleSizeBytes < 1024 * 1024) { desiredBundleSizeBytes = 1 * 1024 * 1024; } BasicDBObject splitVectorCommand = new BasicDBObject(); splitVectorCommand.append("splitVector", spec.database() + "." + spec.collection()); splitVectorCommand.append("keyPattern", new BasicDBObject().append("_id", 1)); splitVectorCommand.append("force", false); LOG.debug("Splitting in chunk of {} MB", desiredBundleSizeBytes / 1024 / 1024); splitVectorCommand.append("maxChunkSize", desiredBundleSizeBytes / 1024 / 1024); Document splitVectorCommandResult = mongoDatabase.runCommand(splitVectorCommand); splitKeys = (List<Document>) splitVectorCommandResult.get("splitKeys"); List<BoundedSource<Document>> sources = new ArrayList<>(); if (splitKeys.size() < 1) { LOG.debug("Split keys is low, using an unique source"); sources.add(this); return sources; } LOG.debug("Number of splits is {}", splitKeys.size()); for (String shardFilter : splitKeysToFilters(splitKeys, spec.filter())) { sources.add(new BoundedMongoDbSource(spec.withFilter(shardFilter))); } return sources; }', 'output': 'public List<BoundedSource<Document>> split(long desiredBundleSizeBytes, PipelineOptions options) { MongoClient mongoClient = new MongoClient(new MongoClientURI(spec.uri())); MongoDatabase mongoDatabase = mongoClient.getDatabase(spec.database()); List<Document> splitKeys; if (spec.numSplits() > 0) { long estimatedSizeBytes = getEstimatedSizeBytes(options); desiredBundleSizeBytes = estimatedSizeBytes / spec.numSplits(); } if (desiredBundleSizeBytes < 1024 * 1024) { desiredBundleSizeBytes = 1 * 1024 * 1024; } BasicDBObject splitVectorCommand = new BasicDBObject(); splitVectorCommand.append("splitVector", spec.database() + "." + spec.collection()); splitVectorCommand.append("keyPattern", new BasicDBObject().append("_id", 1)); splitVectorCommand.append("force", false); LOG.debug("Splitting in chunk of {} MB", desiredBundleSizeBytes / 1024 / 1024); splitVectorCommand.append("maxChunkSize", desiredBundleSizeBytes / 1024 / 1024); Document splitVectorCommandResult = mongoDatabase.runCommand(splitVectorCommand); splitKeys = (List<Document>) splitVectorCommandResult.get("splitKeys"); List<BoundedSource<Document>> sources = new ArrayList<>(); if (splitKeys.size() < 1) { LOG.debug("Split keys is low, using an unique source"); sources.add(this); return sources; } LOG.debug("Number of splits is {}", splitKeys.size()); for (String shardFilter : splitKeysToFilters(splitKeys, spec.filter())) { sources.add(new BoundedMongoDbSource(spec.withFilter(shardFilter))); } return sources; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private int XXX(String attrName, String attrVal, boolean srv, boolean client) throws Exception { String[] signature = {"java.lang.String", "java.lang.String", "boolean", "boolean"}; Object[] params = {attrName, attrVal, srv, client}; return (int)mbeanSrv.invoke(mbean, "XXX", params, signature); }', 'ground_truth': 'public Set<UUID> nodeIdsForAttribute(String attrName, String attrVal, boolean includeSrvs, boolean includeClients) throws Exception { String[] signature = {"java.lang.String", "java.lang.String", "boolean", "boolean"}; Object[] params = {attrName, attrVal, includeSrvs, includeClients}; return (Set<UUID>)mbeanSrv.invoke(mbean, "nodeIdsForAttribute", params, signature); }', 'output': 'private int invokeMethod(String mbeanName, String methodName, String attrName, String attrVal, boolean srv, boolean client) throws Exception { String[] signature = {"java.lang.String", "java.lang.String", "java.lang.String", "boolean", "boolean"}; Object[] params = {mbeanName, methodName, attrName, attrVal, srv, client}; return (int)mbeanSrv.invoke(mbeanName, methodName, params, signature); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws IOException { for (String mode : Arrays.asList("r", "rw")) { for (int bufferSize : Arrays.asList(1, 2, 3, 5, 8, 64)) { final byte[] target = new byte[32]; for (final int offset : Arrays.asList(0, 8)) { final BufferedRandomAccessFile file = new BufferedRandomAccessFile(writeTemporaryFile(new byte[16]), mode, bufferSize); expectEOF(new Callable<Object>() { public Object call() throws IOException { file.readFully(target, offset, 17); return null; } }); } for (final int n : Arrays.asList(1, 2, 4, 8)) { final BufferedRandomAccessFile file = new BufferedRandomAccessFile(writeTemporaryFile(new byte[16]), mode, bufferSize); expectEOF(new Callable<Object>() { public Object call() throws IOException { while (true) file.readFully(target, 0, n); } }); } } } }', 'ground_truth': 'public void testEOF() throws IOException { for (String mode : Arrays.asList("r", "rw")) { for (int bufferSize : Arrays.asList(1, 2, 3, 5, 8, 64)) { final byte[] target = new byte[32]; for (final int offset : Arrays.asList(0, 8)) { final BufferedRandomAccessFile file = new BufferedRandomAccessFile(writeTemporaryFile(new byte[16]), mode, bufferSize); expectEOF(new Callable<Object>() { public Object call() throws IOException { file.readFully(target, offset, 17); return null; } }); } for (final int n : Arrays.asList(1, 2, 4, 8)) { final BufferedRandomAccessFile file = new BufferedRandomAccessFile(writeTemporaryFile(new byte[16]), mode, bufferSize); expectEOF(new Callable<Object>() { public Object call() throws IOException { while (true) file.readFully(target, 0, n); } }); } } } }', 'output': 'public void testBufferedRandomAccessFile() throws IOException { for (String mode : Arrays.asList("r", "rw")) { for (int bufferSize : Arrays.asList(1, 2, 3, 5, 8, 64)) { final byte[] target = new byte[32]; for (final int offset : Arrays.asList(0, 8)) { final BufferedRandomAccessFile file = new BufferedRandomAccessFile(writeTemporaryFile(new byte[16]), mode, bufferSize); expectEOF(new Callable<Object>() { public Object call() throws IOException { file.readFully(target, offset, 17); return null; } }); } for (final int n : Arrays.asList(1, 2, 4, 8)) { final BufferedRandomAccessFile file = new BufferedRandomAccessFile(writeTemporaryFile(new byte[16]), mode, bufferSize); expectEOF(new Callable<Object>() { public Object call() throws IOException { while (true) file.readFully(target, 0, n); } }); } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { String profileName = "inconsistent_quicklinks_profile.json"; QuickLinksProfileParser parser = new QuickLinksProfileParser(); QuickLinksProfile profile = parser.parse(Resources.getResource(profileName)); }', 'ground_truth': 'public void testParseInconsistentProfile_ambigousFilterDefinition() throws Exception { String profileName = "inconsistent_quicklinks_profile.json"; QuickLinksProfileParser parser = new QuickLinksProfileParser(); parser.parse(Resources.getResource(profileName)); }', 'output': 'public void testInconsistentQuicklinksProfile() throws Exception { String profileName = "inconsistent_quicklinks_profile.json"; QuickLinksProfileParser parser = new QuickLinksProfileParser(); QuickLinksProfile profile = parser.parse(Resources.getResource(profileName)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws SQLException { int updatesCnt = stmt.executeUpdate( "copy from \\"" + BULKLOAD_UTF_CSV_FILE + "\\" into " + TBL_NAME + " (_key, age, firstName, lastName)" + " format csv batch_size 1"); assertEquals(2, updatesCnt); checkUtfCacheContents(TBL_NAME, true, 2); }', 'ground_truth': 'public void testUtfPacketSize_1() throws SQLException { int updatesCnt = stmt.executeUpdate( "copy from \\"" + BULKLOAD_UTF_CSV_FILE + "\\" into " + TBL_NAME + " (_key, age, firstName, lastName)" + " format csv packet_size 1"); assertEquals(2, updatesCnt); checkUtfCacheContents(TBL_NAME, true, 2); }', 'output': 'public void testCopyFromCsvFile() throws SQLException { int updatesCnt = stmt.executeUpdate( "copy from \\"" + BULKLOAD_UTF_CSV_FILE + "\\" into " + TBL_NAME + " (_key, age, firstName, lastName)" + " format csv batch_size 1"); assertEquals(2, updatesCnt); checkUtfCacheContents(TBL_NAME, true, 2); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { Ignite ig1P = primary(0); Ignite ig2P = primary(1); Ignite ig3P = primary(2); Ignite ig1CP = primaryClient(0); Ignite ig2CP = primaryClient(1); Ignite ig3CP = primaryClient(2); Ignite ig1B = backUp(0); Ignite ig2B = backUp(1); Ignite ig3B = backUp(2); Ignite ig1CB = backUpClient(0); Ignite ig2CB = backUpClient(1); Ignite ig3CB = backUpClient(2); assertTrue(ig1P.active()); assertTrue(ig2P.active()); assertTrue(ig3P.active()); assertTrue(!ig1B.active()); assertTrue(!ig2B.active()); assertTrue(!ig3B.active()); assertTrue(!ig1CB.active()); assertTrue(!ig2CB.active()); assertTrue(!ig3CB.active()); stopPrimary(0); boolean exc = false; try { ig3CB.active(true); } catch (IgniteException e) { exc = true; log.error("stack trace from remote node", e); for (Throwable t : e.getSuppressed()) assertTrue(t.getMessage().contains("can\'t get lock during")); } if (!exc) fail(); assertTrue(!ig1B.active()); assertTrue(!ig2B.active()); assertTrue(!ig3B.active()); assertTrue(!ig1CB.active()); assertTrue(!ig2CB.active()); assertTrue(!ig3CB.active()); assertTrue(ig2P.active()); assertTrue(ig3P.active()); assertTrue(ig1CP.active()); assertTrue(ig2CP.active()); assertTrue(ig3CP.active()); stopAllPrimary(); ig2CB.active(true); assertTrue(ig1B.active()); assertTrue(ig2B.active()); assertTrue(ig3B.active()); assertTrue(ig1CB.active()); assertTrue(ig2CB.active()); assertTrue(ig3CB.active()); }', 'ground_truth': 'public void _testActivateAfterFailGetLock() throws Exception { Ignite ig1P = primary(0); Ignite ig2P = primary(1); Ignite ig3P = primary(2); Ignite ig1CP = primaryClient(0); Ignite ig2CP = primaryClient(1); Ignite ig3CP = primaryClient(2); Ignite ig1B = backUp(0); Ignite ig2B = backUp(1); Ignite ig3B = backUp(2); Ignite ig1CB = backUpClient(0); Ignite ig2CB = backUpClient(1); Ignite ig3CB = backUpClient(2); assertTrue(ig1P.active()); assertTrue(ig2P.active()); assertTrue(ig3P.active()); assertTrue(!ig1B.active()); assertTrue(!ig2B.active()); assertTrue(!ig3B.active()); assertTrue(!ig1CB.active()); assertTrue(!ig2CB.active()); assertTrue(!ig3CB.active()); stopPrimary(0); boolean exc = false; try { ig3CB.active(true); } catch (IgniteException e) { exc = true; log.error("stack trace from remote node", e); for (Throwable t : e.getSuppressed()) assertTrue(t.getMessage().contains("can\'t get lock during")); } if (!exc) fail(); assertTrue(!ig1B.active()); assertTrue(!ig2B.active()); assertTrue(!ig3B.active()); assertTrue(!ig1CB.active()); assertTrue(!ig2CB.active()); assertTrue(!ig3CB.active()); assertTrue(ig2P.active()); assertTrue(ig3P.active()); assertTrue(ig1CP.active()); assertTrue(ig2CP.active()); assertTrue(ig3CP.active()); stopAllPrimary(); ig2CB.active(true); assertTrue(ig1B.active()); assertTrue(ig2B.active()); assertTrue(ig3B.active()); assertTrue(ig1CB.active()); assertTrue(ig2CB.active()); assertTrue(ig3CB.active()); }', 'output': 'public void testActive() throws Exception { Ignite ig1P = primary(0); Ignite ig2P = primary(1); Ignite ig3P = primary(2); Ignite ig1CP = primaryClient(0); Ignite ig2CP = primaryClient(1); Ignite ig3CP = primaryClient(2); Ignite ig1B = backUp(0); Ignite ig2B = backUp(1); Ignite ig3B = backUp(2); Ignite ig1CB = backUpClient(0); Ignite ig2CB = backUpClient(1); Ignite ig3CB = backUpClient(2); assertTrue(ig1P.active()); assertTrue(ig2P.active()); assertTrue(ig3P.active()); assertTrue(!ig1B.active()); assertTrue(!ig2B.active()); assertTrue(!ig3B.active()); assertTrue(!ig1CB.active()); assertTrue(!ig2CB.active()); assertTrue(!ig3CB.active()); stopPrimary(0); boolean exc = false; try { ig3CB.active(true); } catch (IgniteException e) { exc = true; log.error("stack trace from remote node", e); for (Throwable t : e.getSuppressed()) assertTrue(t.getMessage().contains("can\'t get lock during")); } if (!exc) fail(); assertTrue(!ig1B.active()); assertTrue(!ig2B.active()); assertTrue(!ig3B.active()); assertTrue(!ig1CB.active()); assertTrue(!ig2CB.active()); assertTrue(!ig3CB.active()); assertTrue(ig2P.active()); assertTrue(ig3P.active()); assertTrue(ig1CP.active()); assertTrue(ig2CP.active()); assertTrue(ig3CP.active()); stopAllPrimary(); ig2CB.active(true); assertTrue(ig1B.active()); assertTrue(ig2B.active()); assertTrue(ig3B.active()); assertTrue(ig1CB.active()); assertTrue(ig2CB.active()); assertTrue(ig3CB.active()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Row XXX(Row existing, Row update) { Row.Builder builder = builder(existing.clustering()); colUpdateTimeDelta = Math.min(colUpdateTimeDelta, Rows.merge(existing, update, builder)); Row reconciled = builder.build(); indexer.onUpdated(existing, reconciled); dataSize += reconciled.dataSize() - existing.dataSize(); onAllocatedOnHeap(reconciled.unsharedHeapSizeExcludingData() - existing.unsharedHeapSizeExcludingData()); if (inserted == null) inserted = new ArrayList<>(); inserted.add(reconciled); return reconciled; }', 'ground_truth': 'public Row merge(Row existing, Row update) { Row reconciled = Rows.merge(existing, update, this); indexer.onUpdated(existing, reconciled); if (inserted == null) inserted = new ArrayList<>(); inserted.add(reconciled); return reconciled; }', 'output': 'public Row reconcile(Row existing, Row update) { Row.Builder builder = builder(existing.clustering()); colUpdateTimeDelta = Math.min(colUpdateTimeDelta, Rows.merge(existing, update, builder)); Row reconciled = builder.build(); indexer.onUpdated(existing, reconciled); dataSize += reconciled.dataSize() - existing.dataSize(); onAllocatedOnHeap(reconciled.unsharedHeapSizeExcludingData() - existing.unsharedHeapSizeExcludingData()); if (inserted == null) inserted = new ArrayList<>(); inserted.add(reconciled); return reconciled; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { GridCache<Object, Object> cache = grid(0).cache(null); cache.putx(1, 1); cache.putx(2, 2); assertEquals(cache.metrics().getAverageRemoveTime(), 0.0, 0.0); IgniteInternalFuture<Object> fut = cache.removeAsync(1); assertEquals(1, (int)fut.get()); assert cache.metrics().getAverageRemoveTime() > 0; fut = cache.removeAsync(2); assertEquals(2, (int)fut.get()); assert cache.metrics().getAverageRemoveTime() > 0; }', 'ground_truth': 'public void testGetAndRemoveAsyncAvgTime() throws Exception { IgniteCache<Object, Object> cache = grid(0).jcache(null); IgniteCache<Object, Object> cacheAsync = cache.withAsync(); cache.put(1, 1); cache.put(2, 2); assertEquals(cache.metrics().getAverageRemoveTime(), 0.0, 0.0); cacheAsync.getAndRemove(1); IgniteFuture<Object> fut = cacheAsync.future(); assertEquals(1, (int)fut.get()); assert cache.metrics().getAverageRemoveTime() > 0; cacheAsync.getAndRemove(2); fut = cacheAsync.future(); assertEquals(2, (int)fut.get()); assert cache.metrics().getAverageRemoveTime() > 0; }', 'output': 'public void testRemoveAsync() throws Exception { Cache<Object, Object> cache = grid(0).cache(null); cache.putx(1, 1); cache.putx(2, 2); assertEquals(cache.metrics().getAverageRemoveTime(), 0.0, 0.0); IgniteInternalFuture<Object> fut = cache.removeAsync(1); assertEquals(1, (int)fut.get()); assert cache.metrics().getAverageRemoveTime() > 0; fut = cache.removeAsync(2); assertEquals(2, (int)fut.get()); assert cache.metrics().getAverageRemoveTime() > 0; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { UnboundedSource<Long, ?> source = CountingSource.unboundedWithTimestampFn(new LongToInstantFn()); TestPipeline p = TestPipeline.create(); PCollection<Long> longs = p.apply(Read.from(source)); TransformEvaluatorFactory factory = new UnboundedReadEvaluatorFactory(); InProcessEvaluationContext context = mock(InProcessEvaluationContext.class); UncommittedBundle<Long> output = InProcessBundle.unkeyed(longs); when(context.createRootBundle(longs)).thenReturn(output); TransformEvaluator<?> evaluator = factory.forApplication(longs.getProducingTransformInternal(), null, context); InProcessTransformResult result = evaluator.finishBundle(); assertThat( result.getWatermarkHold(), Matchers.<ReadableInstant>lessThan(DateTime.now().toInstant())); assertThat( output.commit(Instant.now()).getElements(), containsInAnyOrder(tgw(1L), tgw(2L), tgw(4L), tgw(8L), tgw(9L), tgw(7L), tgw(6L), tgw(5L), tgw(3L), tgw(0L))); UncommittedBundle<Long> secondOutput = InProcessBundle.unkeyed(longs); when(context.createRootBundle(longs)).thenReturn(secondOutput); TransformEvaluator<?> secondEvaluator = factory.forApplication(longs.getProducingTransformInternal(), null, context); InProcessTransformResult secondResult = secondEvaluator.finishBundle(); assertThat( secondResult.getWatermarkHold(), Matchers.<ReadableInstant>lessThan(DateTime.now().toInstant())); assertThat( secondOutput.commit(Instant.now()).getElements(), containsInAnyOrder(tgw(11L), tgw(12L), tgw(14L), tgw(18L), tgw(19L), tgw(17L), tgw(16L), tgw(15L), tgw(13L), tgw(10L))); }', 'ground_truth': 'public void unboundedSourceInMemoryTransformEvaluatorMultipleSequentialCalls() throws Exception { TransformEvaluator<?> evaluator = factory.forApplication(longs.getProducingTransformInternal(), null, context); InProcessTransformResult result = evaluator.finishBundle(); assertThat( result.getWatermarkHold(), Matchers.<ReadableInstant>lessThan(DateTime.now().toInstant())); assertThat( output.commit(Instant.now()).getElements(), containsInAnyOrder( tgw(1L), tgw(2L), tgw(4L), tgw(8L), tgw(9L), tgw(7L), tgw(6L), tgw(5L), tgw(3L), tgw(0L))); UncommittedBundle<Long> secondOutput = InProcessBundle.unkeyed(longs); when(context.createRootBundle(longs)).thenReturn(secondOutput); TransformEvaluator<?> secondEvaluator = factory.forApplication(longs.getProducingTransformInternal(), null, context); InProcessTransformResult secondResult = secondEvaluator.finishBundle(); assertThat( secondResult.getWatermarkHold(), Matchers.<ReadableInstant>lessThan(DateTime.now().toInstant())); assertThat( secondOutput.commit(Instant.now()).getElements(), containsInAnyOrder(tgw(11L), tgw(12L), tgw(14L), tgw(18L), tgw(19L), tgw(17L), tgw(16L), tgw(15L), tgw(13L), tgw(10L))); }', 'output': 'public void testFinishBundle() throws Exception { UnboundedSource<Long, ?> source = CountingSource.unboundedWithTimestampFn(new LongToInstantFn()); TestPipeline p = TestPipeline.create(); PCollection<Long> longs = p.apply(Read.from(source)); TransformEvaluatorFactory factory = new UnboundedReadEvaluatorFactory(); InProcessEvaluationContext context = mock(InProcessEvaluationContext.class); UncommittedBundle<Long> output = InProcessBundle.unkeyed(longs); when(context.createRootBundle(longs)).thenReturn(output); TransformEvaluator<?> evaluator = factory.forApplication(longs.getProducingTransformInternal(), null, context); InProcessTransformResult result = evaluator.finishBundle(); assertThat( result.getWatermarkHold(), Matchers.<ReadableInstant>lessThan(DateTime.now().toInstant())); assertThat( output.commit(Instant.now()).getElements(), containsInAnyOrder(tgw(1L), tgw(2L), tgw(4L), tgw(8L), tgw(9L), tgw(7L), tgw(6L), tgw(5L), tgw(3L), tgw(0L))); UncommittedBundle<Long> secondOutput = InProcessBundle.unkeyed(longs); when(context.createRootBundle(longs)).thenReturn(secondOutput); TransformEvaluator<?> secondEvaluator = factory.forApplication(longs.getProducingTransformInternal(), null, context); InProcessTransformResult secondResult = secondEvaluator.finishBundle(); assertThat( secondResult.getWatermarkHold(), Matchers.<ReadableInstant>lessThan(DateTime.now().toInstant())); assertThat( secondOutput.commit(Instant.now()).getElements(), containsInAnyOrder(tgw(11L), tgw(12L), tgw(14L), tgw(18L), tgw(19L), tgw(17L), tgw(16L), tgw(15L), tgw(13L), tg'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws InvalidRequestException { CFMetaData cfMetaData = newCFMetaData(4); ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); ByteBuffer value4 = ByteBufferUtil.bytes(4); ByteBuffer value5 = ByteBufferUtil.bytes(5); Restriction multiEq = newMultiEq(cfMetaData, 0, value1, value2); Restriction multiSlice = newMultiSlice(cfMetaData, 2, Bound.START, false, value3, value4); PrimaryKeyRestrictions restrictions = new PrimaryKeyRestrictionSet(cfMetaData.comparator); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiSlice); List<Composite> bounds = restrictions.boundsAsComposites(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertComposite(bounds.get(0), value1, value2, value3, value4, EOC.END); bounds = restrictions.boundsAsComposites(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertComposite(bounds.get(0),  value1, value2, EOC.END); multiEq = newMultiEq(cfMetaData, 0, value1, value2); Restriction multiIN = newMultiIN(cfMetaData, 2, asList(value3, value4), asList(value4, value5)); restrictions = new PrimaryKeyRestrictionSet(cfMetaData.comparator); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiIN); bounds = restrictions.boundsAsComposites(Bound.START, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertComposite(bounds.get(0), value1, value2, value3, value4, EOC.START); assertComposite(bounds.get(1), value1, value2, value4, value5, EOC.START); bounds = restrictions.boundsAsComposites(Bound.END, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertComposite(bounds.get(0), value1, value2, value3, value4, EOC.END); assertComposite(bounds.get(1), value1, value2, value4, value5, EOC.END); multiEq = newMultiEq(cfMetaData, 0, value1, value2); Restriction multiEq2 = newMultiEq(cfMetaData, 2, value3, value4); restrictions = new PrimaryKeyRestrictionSet(cfMetaData.comparator); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiEq2); bounds = restrictions.boundsAsComposites(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertComposite(bounds.get(0), value1, value2, value3, value4, EOC.START); bounds = restrictions.boundsAsComposites(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertComposite(bounds.get(0), value1, value2, value3, value4, EOC.END); }', 'ground_truth': 'public void testboundsAsClusteringWithSeveralMultiColumnRestrictions() throws InvalidRequestException { CFMetaData cfMetaData = newCFMetaData(4); ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); ByteBuffer value4 = ByteBufferUtil.bytes(4); ByteBuffer value5 = ByteBufferUtil.bytes(5); Restriction multiEq = newMultiEq(cfMetaData, 0, value1, value2); Restriction multiSlice = newMultiSlice(cfMetaData, 2, Bound.START, false, value3, value4); PrimaryKeyRestrictions restrictions = new PrimaryKeyRestrictionSet(cfMetaData.comparator, false); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiSlice); SortedSet<Slice.Bound> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3, value4); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value2); multiEq = newMultiEq(cfMetaData, 0, value1, value2); Restriction multiIN = newMultiIN(cfMetaData, 2, asList(value3, value4), asList(value4, value5)); restrictions = new PrimaryKeyRestrictionSet(cfMetaData.comparator, false); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiIN); bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertStartBound(get(bounds, 0), true, value1, value2, value3, value4); assertStartBound(get(bounds, 1), true, value1, value2, value4, value5); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value2, value3, value4); assertEndBound(get(bounds, 1), true, value1, value2, value4, value5); multiEq = newMultiEq(cfMetaData, 0, value1, value2); Restriction multiEq2 = newMultiEq(cfMetaData, 2, value3, value4); restrictions = new PrimaryKeyRestrictionSet(cfMetaData.comparator, false); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiEq2); bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), true, value1, value2, value3, value4); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value2, value3, value4); }', 'output': 'public void testMergePrimaryKeyRestrictions() throws InvalidRequestException { CFMetaData cfMetaData = newCFMetaData(4); ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); ByteBuffer value4 = ByteBufferUtil.bytes(4); ByteBuffer value5 = ByteBufferUtil.bytes(5); Restriction multiEq = newMultiEq(cfMetaData, 0, value1, value2); Restriction multiSlice = newMultiSlice(cfMetaData, 2, Bound.START, false, value3, value4); PrimaryKeyRestrictions restrictions = new PrimaryKeyRestrictionSet(cfMetaData.comparator); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiSlice); List<Composite> bounds = restrictions.boundsAsComposites(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertComposite(bounds.get(0), value1, value2, value3, value4, EOC.END); bounds = restrictions.boundsAsComposites(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertComposite(bounds.get(0),  value1, value2, EOC.END); multiEq = newMultiEq(cfMetaData, 0, value1, value2); Restriction multiIN = newMultiIN(cfMetaData, 2, asList(value3, value4), asList(value4, value5)); restrictions = new PrimaryKeyRestrictionSet(cfMetaData.comparator); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiIN); bounds = restrictions.boundsAsComposites(Bound.START, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertComposite(bounds.get(0), value1, value2, value3, value4, EOC.START); assertComposite(bounds.get(1), value1, value2, value4, value5, EOC.START); bounds = restrictions.boundsAsComposites(Bound.END, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertComposite(bounds.get(0), value1,'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { BinaryMarshaller marsh = binaryMarshaller(Arrays.asList( new BinaryTypeConfiguration("org.apache.ignite.internal.binary.test.*"), new BinaryTypeConfiguration("unknown.*") )); BinaryContext ctx = binaryContext(marsh); Map<Integer, Class> typeIds = U.field(ctx, "userTypes"); assertEquals(3, typeIds.size()); assertTrue(typeIds.containsKey("gridbinarytestclass1".hashCode())); assertTrue(typeIds.containsKey("gridbinarytestclass2".hashCode())); }', 'ground_truth': 'private void checkTypeConfigurationJar(BinaryNameMapper nameMapper, BinaryIdMapper idMapper) throws IgniteCheckedException { BinaryMarshaller marsh = binaryMarshaller(nameMapper, idMapper, Arrays.asList( new BinaryTypeConfiguration("org.apache.ignite.internal.binary.test.*"), new BinaryTypeConfiguration("unknown.*") )); BinaryContext ctx = binaryContext(marsh); Map<Integer, Class> typeIds = U.field(ctx, "userTypes"); assertEquals(3, typeIds.size()); assertTrue(typeIds.containsKey(typeId(CLASS1_FULL_NAME, nameMapper, idMapper))); assertTrue(typeIds.containsKey(typeId(CLASS2_FULL_NAME, nameMapper, idMapper))); }', 'output': 'public void testBinaryTypes() throws Exception { BinaryMarshaller marsh = binaryMarshaller(Arrays.asList( new BinaryTypeConfiguration("org.apache.ignite.internal.binary.test.*"), new BinaryTypeConfiguration("unknown.*") )); BinaryContext ctx = binaryContext(marsh); Map<Integer, Class> typeIds = U.field(ctx, "userTypes"); assertEquals(3, typeIds.size()); assertTrue(typeIds.containsKey("gridbinarytestclass1".hashCode())); assertTrue(typeIds.containsKey("gridbinarytestclass2".hashCode())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected int XXX(Collector<Integer> context) { List<Integer> list = Lists.newArrayList(data.get()); Collections.sort(list); for (Integer i : list) { context.collect(i); } return list.size(); }', 'ground_truth': 'protected int flushAndGetSize(Collector<Integer> context) { List<Integer> list = Lists.newArrayList(data.get()); Collections.sort(list); for (Integer i : list) { context.collect(i); } return list.size(); }', 'output': 'protected int collect(Collector<Integer> context) { List<Integer> list = Lists.newArrayList(data.get()); Collections.sort(list); for (Integer i : list) { context.collect(i); } return list.size(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private String XXX(boolean addFootnote) { return addFootnote ? "\\n  %s : "\\n  %s\\n"; }', 'ground_truth': 'private String javaSourceMemberFormat(boolean addFootnote) { return addFootnote ? "\\n  %s : "\\n  %s\\n"; }', 'output': 'private String footnote(boolean addFootnote) { return addFootnote ? "\\n  %s : "\\n  %s\\n"; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ApplicationFeatureId XXX(String fqn) { val feat = new ApplicationFeatureId(ApplicationFeatureType.PACKAGE); ApplicationFeatureType.PACKAGE.init(feat, fqn); return feat; }', 'ground_truth': 'public static ApplicationFeatureId createNamespace(String namespace) { val feat = new ApplicationFeatureId(ApplicationFeatureType.NAMESPACE); ApplicationFeatureType.NAMESPACE.init(feat, namespace); return feat; }', 'output': 'public static ApplicationFeatureId fromFqn(String fqn) { val feat = new ApplicationFeatureId(ApplicationFeatureType.PACKAGE); ApplicationFeatureType.PACKAGE.init(feat, fqn); return feat; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX( SqlOperator operator, ExpressionType expressionType) { map.put(operator, new UnaryImplementor(expressionType)); }', 'ground_truth': 'private void defineImplementor( SqlOperator operator, NullPolicy nullPolicy, NotNullImplementor implementor, boolean harmonize) { CallImplementor callImplementor = createImplementor(implementor, nullPolicy, harmonize); map.put(operator, callImplementor); }', 'output': 'private void registerUnaryOperator(SqlOperator operator, ExpressionType expressionType) { map.put(operator, new UnaryImplementor(expressionType)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean XXX() { BaselineTopology topology = cctx.discovery().discoCache().state().baselineTopology(); return topology!= null && !topology.consistentIds().contains(cctx.localNode().consistentId()); }', 'ground_truth': 'private boolean isLocalNodeInBaseline() { BaselineTopology topology = cctx.discovery().discoCache().state().baselineTopology(); return topology != null && topology.consistentIds().contains(cctx.localNode().consistentId()); }', 'output': 'private boolean isBaselineActive() { BaselineTopology topology = cctx.discovery().discoCache().state().baselineTopology(); return topology != null && !topology.consistentIds().contains(cctx.localNode().consistentId()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static String XXX(EndpointState epState) { if (epState == null || epState.XXX(ApplicationState.STATUS) == null) return ""; String value = epState.XXX(ApplicationState.STATUS).value; String[] pieces = value.split(VersionedValue.DELIMITER_STR, -1); assert (pieces.length > 0); return pieces[0]; }', 'ground_truth': 'private static String getGossipStatus(EndpointState epState) { if (epState == null || epState.getApplicationState(ApplicationState.STATUS) == null) return ""; String value = epState.getApplicationState(ApplicationState.STATUS).value; String[] pieces = value.split(VersionedValue.DELIMITER_STR, -1); assert (pieces.length > 0); return pieces[0]; }', 'output': 'private static String getStatus(EndpointState epState) { if (epState == null || epState.getStatus(ApplicationState.STATUS) == null) return ""; String value = epState.getStatus(ApplicationState.STATUS).value; String[] pieces = value.split(VersionedValue.DELIMITER_STR, -1); assert (pieces.length > 0); return pieces[0]; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { fail("https: if (cacheMode() != PARTITIONED) return; nearCache = true; testAccess(); Integer key = primaryKeys(jcache(0), 1, 500_000).get(0); IgniteCache<Integer, Integer> cache0 = jcache(0); cache0.put(key, 1); checkTtl(key, 60_000L); assertEquals(1, jcache(1).get(key)); checkTtl(key, 62_000L, true); assertEquals(1, jcache(2).withExpiryPolicy(new TestPolicy(1100L, 1200L, TTL_FOR_EXPIRE)).get(key)); checkTtl(key, TTL_FOR_EXPIRE, true); waitExpired(key); key = nearKeys(jcache(0), 1, 600_000).get(0); cache0.put(key, 1); checkTtl(key, 60_000L); IgniteCache<Object, Object> cache = grid(0).affinity(DEFAULT_CACHE_NAME).isPrimary(grid(1).localNode(), key) ? jcache(1) : jcache(2); assertEquals(1, cache.get(key)); checkTtl(key, 62_000L, true); }', 'ground_truth': 'public void testNearAccess() throws Exception { if (cacheMode() != PARTITIONED) return; nearCache = true; testAccess(); Integer key = primaryKeys(jcache(0), 1, 500_000).get(0); IgniteCache<Integer, Integer> cache0 = jcache(0); cache0.put(key, 1); checkTtl(key, 60_000L); assertEquals(1, jcache(1).get(key)); checkTtl(key, 62_000L, true); assertEquals(1, jcache(2).withExpiryPolicy(new TestPolicy(1100L, 1200L, TTL_FOR_EXPIRE)).get(key)); checkTtl(key, TTL_FOR_EXPIRE, true); waitExpired(key); key = nearKeys(jcache(0), 1, 600_000).get(0); cache0.put(key, 1); checkTtl(key, 60_000L); IgniteCache<Object, Object> cache = grid(0).affinity(DEFAULT_CACHE_NAME).isPrimary(grid(1).localNode(), key) ? jcache(1) : jcache(2); assertEquals(1, cache.get(key)); checkTtl(key, 62_000L, true); }', 'output': 'public void testAccess() throws Exception { fail("https: if (cacheMode() != PARTITIONED) return; nearCache = true; testAccess(); Integer key = primaryKeys(jcache(0), 1, 500_000).get(0); IgniteCache<Integer, Integer> cache0 = jcache(0); cache0.put(key, 1); checkTtl(key, 60_000L); assertEquals(1, jcache(1).get(key)); checkTtl(key, 62_000L, true); assertEquals(1, jcache(2).withExpiryPolicy(new TestPolicy(1100L, 1200L, TTL_FOR_EXPIRE)).get(key)); checkTtl(key, TTL_FOR_EXPIRE, true); waitExpired(key); key = nearKeys(jcache(0), 1, 600_000).get(0); cache0.put(key, 1); checkTtl(key, 60_000L); IgniteCache<Object, Object> cache = grid(0).affinity(DEFAULT_CACHE_NAME).isPrimary(grid(1).localNode(), key) ? jcache(1) : jcache(2); assertEquals(1, cache.get(key)); checkTtl(key, 62_000L, true); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(KerberosPrincipalEntity principalEntity) { this.principalEntity = principalEntity; if (principalEntity != null) { principalName = principalEntity.getPrincipalName(); } }', 'ground_truth': 'public void setKerberosPrincipalEntity(KerberosPrincipalEntity kerberosPrincipalEntity) { this.kerberosPrincipalEntity = kerberosPrincipalEntity; if (kerberosPrincipalEntity != null) { principalName = kerberosPrincipalEntity.getPrincipalName(); } }', 'output': 'public void setPrincipalEntity(KerberosPrincipalEntity principalEntity) { this.principalEntity = principalEntity; if (principalEntity != null) { principalName = principalEntity.getPrincipalName(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(GridSqlElement expression, boolean visible) { if (expression == null) throw new NullPointerException(); if (visible) { if (select.size() != allExprs.size()) throw new IllegalStateException("Already started adding invisible columns."); select.add(expression); } else if (select.isEmpty()) throw new IllegalStateException("No visible columns."); allExprs.add(expression); }', 'ground_truth': 'public GridSqlSelect addColumn(GridSqlElement expression, boolean visible) { if (expression == null) throw new NullPointerException(); if (visible) { if (visibleCols != cols.size()) throw new IllegalStateException("Already started adding invisible columns."); visibleCols++; } cols.add(expression); return this; }', 'output': 'public void addExpression(GridSqlElement expression, boolean visible) { if (expression == null) throw new NullPointerException(); if (visible) { if (select.size() != allExprs.size()) throw new IllegalStateException("Already started adding invisible columns."); select.add(expression); } else if (select.isEmpty()) throw new IllegalStateException("No visible columns."); allExprs.add(expression); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static String XXX(String s, int length) { return s + Util.spaces(Math.max(length - s.length(), 0)); } private static ByteString XXX(ByteString s, int length) { if (s.length() >= length) { return s; } return new ByteString(Arrays.copyOf(s.getBytes(), length)); }', 'ground_truth': 'private static ByteString padRight(ByteString s, int length) { if (s.length() >= length) { return s; } return new ByteString(Arrays.copyOf(s.getBytes(), length)); }', 'output': 'private static String pad(String s, int length) { return s + Util.spaces(Math.max(length - s.length(), 0)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(String key) { for (int i = 0; i < hashes_; ++i) { ISimpleHash hash = hashLibrary_.get(i); int hashValue = hash.hash(key); int index = Math.abs(hashValue % size_); filter_.set(index); } }', 'ground_truth': 'public void add(String key) { for (int i = 0; i < hashes_; ++i) { ISimpleHash hash = hashLibrary_.get(i); int hashValue = hash.hash(key); int index = Math.abs(hashValue % size_); filter_.set(index); } }', 'output': 'public void add(String key) { for (int i = 0; i < hashes_; ++i) { ISimpleHash hash = hashLibrary_.get(i); int hashValue = hash.hash(key); int index = Math.abs(hashValue % size_); filter_.set(index); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected JdoPersistenceSession XXX() { return isisInteractionTracker.get().currentInteractionSession() .map(interaction->interaction.getAttribute(JdoPersistenceSession.class)) .orElse(null); }', 'ground_truth': 'protected JdoPersistenceSession getJdoPersistenceSession() { return isisInteractionTrackerLazy.get().currentInteractionSession() .map(interactionSession->interactionSession.getAttribute(JdoPersistenceSession.class)) .orElse(null); }', 'output': 'protected JdoPersistenceSession getCurrentInteractionSession() { return isisInteractionTracker.get().currentInteractionSession() .map(interaction->interaction.getAttribute(JdoPersistenceSession.class)) .orElse(null); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(int port) throws Exception { final String failMsg = "IGFS endpoint TCP port is out of range"; g1Cfg.setCacheConfiguration(concat(dataCaches(1024), metaCaches(), CacheConfiguration.class)); final String igfsCfgName = "igfs-cfg"; final IgfsIpcEndpointConfiguration igfsEndpointCfg = new IgfsIpcEndpointConfiguration(); igfsEndpointCfg.setPort(port); g1IgfsCfg1.setName(igfsCfgName); g1IgfsCfg1.setIpcEndpointConfiguration(igfsEndpointCfg); checkGridStartFails(g1Cfg, failMsg, true); }', 'ground_truth': 'private void checkInvalidPort(int port) throws Exception { final String failMsg = "IGFS endpoint TCP port is out of range"; final String igfsCfgName = "igfs-cfg"; final IgfsIpcEndpointConfiguration igfsEndpointCfg = new IgfsIpcEndpointConfiguration(); igfsEndpointCfg.setPort(port); g1IgfsCfg1.setName(igfsCfgName); g1IgfsCfg1.setIpcEndpointConfiguration(igfsEndpointCfg); checkGridStartFails(g1Cfg, failMsg, true); }', 'output': 'private void setTcpPort(int port) throws Exception { final String failMsg = "IGFS endpoint TCP port is out of range"; g1Cfg.setCacheConfiguration(concat(dataCaches(1024), metaCaches(), CacheConfiguration.class)); final String igfsCfgName = "igfs-cfg"; final IgfsIpcEndpointConfiguration igfsEndpointCfg = new IgfsIpcEndpointConfiguration(); igfsEndpointCfg.setPort(port); g1IgfsCfg1.setName(igfsCfgName); g1IgfsCfg1.setIpcEndpointConfiguration(igfsEndpointCfg); checkGridStartFails(g1Cfg, failMsg, true); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public SaslAuthenticator XXX() { if (saslAuthenticator == null) { IAuthenticator authenticator = DatabaseDescriptor.XXX(); assert authenticator instanceof ISaslAwareAuthenticator : "Configured IAuthenticator does not support SASL authentication"; saslAuthenticator = ((ISaslAwareAuthenticator)authenticator).newAuthenticator(); } return saslAuthenticator; }', 'ground_truth': 'public IAuthenticator.SaslNegotiator getSaslNegotiator() { if (saslNegotiator == null) saslNegotiator = DatabaseDescriptor.getAuthenticator().newSaslNegotiator(); return saslNegotiator; }', 'output': 'public SaslAuthenticator getSaslAuthenticator() { if (saslAuthenticator == null) { IAuthenticator authenticator = DatabaseDescriptor.getAuthenticator(); assert authenticator instanceof ISaslAwareAuthenticator : "Configured IAuthenticator does not support SASL authentication"; saslAuthenticator = ((ISaslAwareAuthenticator)authenticator).newAuthenticator(); } return saslAuthenticator; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public AuditTrailEntry XXX() { E entry = factoryService.detachedEntity(auditTrailEntryClass); return repositoryService.persistAndFlush(entry); }', 'ground_truth': 'public AuditTrailEntry createFor(final EntityPropertyChange change) { E entry = factoryService.detachedEntity(auditTrailEntryClass); entry.init(change); return repositoryService.persistAndFlush(entry); }', 'output': 'public AuditTrailEntry persistAndFlush() { E entry = factoryService.detachedEntity(auditTrailEntryClass); return repositoryService.persistAndFlush(entry); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { int threadCnt = 150; final int keyCnt = 2000; final int valCnt = 10000; final Ignite g = grid(0); final IgniteCache<Integer, String> c = g.jcache(null); final IgniteCache<Integer, Long> cl = g.jcache(null); assertEquals(0, g.cache(null).size()); assertEquals(0, c.query(new QuerySqlPredicate(String.class, "1 = 1")).getAll().size()); assertEquals(0, cl.query(new QuerySqlPredicate(Long.class, "1 = 1")).getAll().size()); Random rnd = new Random(); for (int i = 0; i < keyCnt; i += 1 + rnd.nextInt(3)) { c.put(i, String.valueOf(rnd.nextInt(valCnt))); if (evictsEnabled() && rnd.nextBoolean()) c.localEvict(Arrays.asList(i)); } final AtomicBoolean done = new AtomicBoolean(); IgniteInternalFuture<?> fut = multithreadedAsync(new CAX() { @Override public void applyx() throws IgniteCheckedException { Random rnd = new Random(); while (!done.get()) { switch (rnd.nextInt(5)) { case 0: c.put(rnd.nextInt(keyCnt), String.valueOf(rnd.nextInt(valCnt))); break; case 1: if (evictsEnabled()) c.localEvict(Arrays.asList(rnd.nextInt(keyCnt))); break; case 2: c.remove(rnd.nextInt(keyCnt)); break; case 3: c.get(rnd.nextInt(keyCnt)); break; case 4: int from = rnd.nextInt(valCnt); QueryCursor<Cache.Entry<Integer, String>> qry = c.query( new QuerySqlPredicate(String.class, "_val between ? and ?", String.valueOf(from), String.valueOf(from + 250))); Collection<Cache.Entry<Integer, String>> res = qry.getAll(); for (Cache.Entry<Integer, String> ignored : res) { } } } } }, threadCnt); Thread.sleep(DURATION); done.set(true); fut.get(); }', 'ground_truth': 'public void _testMultiThreadedSwapUnswapString() throws Exception { int threadCnt = 150; final int keyCnt = 2000; final int valCnt = 10000; final Ignite g = grid(0); final IgniteCache<Integer, String> c = g.jcache(null); final IgniteCache<Integer, Long> cl = g.jcache(null); assertEquals(0, g.cache(null).size()); assertEquals(0, c.query(new QuerySqlPredicate(String.class, "1 = 1")).getAll().size()); assertEquals(0, cl.query(new QuerySqlPredicate(Long.class, "1 = 1")).getAll().size()); Random rnd = new Random(); for (int i = 0; i < keyCnt; i += 1 + rnd.nextInt(3)) { c.put(i, String.valueOf(rnd.nextInt(valCnt))); if (evictsEnabled() && rnd.nextBoolean()) c.localEvict(Arrays.asList(i)); } final AtomicBoolean done = new AtomicBoolean(); IgniteInternalFuture<?> fut = multithreadedAsync(new CAX() { @Override public void applyx() throws IgniteCheckedException { Random rnd = new Random(); while (!done.get()) { switch (rnd.nextInt(5)) { case 0: c.put(rnd.nextInt(keyCnt), String.valueOf(rnd.nextInt(valCnt))); break; case 1: if (evictsEnabled()) c.localEvict(Arrays.asList(rnd.nextInt(keyCnt))); break; case 2: c.remove(rnd.nextInt(keyCnt)); break; case 3: c.get(rnd.nextInt(keyCnt)); break; case 4: int from = rnd.nextInt(valCnt); QueryCursor<Cache.Entry<Integer, String>> qry = c.query( new QuerySqlPredicate(String.class, "_val between ? and ?", String.valueOf(from), String.valueOf(from + 250))); Collection<Cache.Entry<Integer, String>> res = qry.getAll(); for (Cache.Entry<Integer, String> ignored : res) { } } } } }, threadCnt); Thread.sleep(DURATION); done.set(true); fut.get(); }', 'output': 'public void testMultithreadedPut() throws Exception { int threadCnt = 150; final int keyCnt = 2000; final int valCnt = 10000; final Ignite g = grid(0); final IgniteCache<Integer, String> c = g.jcache(null); final IgniteCache<Integer, Long> cl = g.jcache(null); assertEquals(0, g.cache(null).size()); assertEquals(0, c.query(new QuerySqlPredicate(String.class, "1 = 1")).getAll().size()); assertEquals(0, cl.query(new QuerySqlPredicate(Long.class, "1 = 1")).getAll().size()); Random rnd = new Random(); for (int i = 0; i < keyCnt; i += 1 + rnd.nextInt(3)) { c.put(i, String.valueOf(rnd.nextInt(valCnt))); if (evictsEnabled() && rnd.nextBoolean()) c.localEvict(Arrays.asList(i)); } final AtomicBoolean done = new AtomicBoolean(); IgniteInternalFuture<?> fut = multithreadedAsync(new CAX() { @Override public void applyx() throws IgniteCheckedException { Random rnd = new Random(); while (!done.get()) { switch (rnd.nextInt(5)) { case 0: c.put(rnd.nextInt(keyCnt), String.valueOf(rnd.nextInt(valCnt))); break; case 1: if (evictsEnabled()) c.localEvict(Arrays.asList(rnd.nextInt(keyCnt))); break; case 2: c.remove(rnd.nextInt(keyCnt)); break; case 3: c.get(rnd.nextInt(keyCnt)); break; case 4: int from = rnd.nextInt(valCnt); QueryCursor<Cache.Entry<Integer, String>> qry = c.query( new QuerySqlPredicate(String.class, "_val between ? and ?", String.valueOf(from), String.valueOf(from + 250))); Collection<Cache.Entry<Integer, String>> res = q'}
{'instruction': 'Please refactor the following code:', 'input': 'private WarningCounter XXX() { WarningCounter current; do { current = warningCounter; if (current != null) return current; current = new WarningCounter(); } while (!warningsUpdater.compareAndSet(this, null, current)); return current; }', 'ground_truth': 'private WarningContext getWarningContext() { WarningContext current; do { current = warningContext; if (current != null) return current; current = new WarningContext(); } while (!warningsUpdater.compareAndSet(this, null, current)); return current; }', 'output': 'private WarningCounter getWarningCounter() { WarningCounter current; do { current = warningCounter; if (current != null) return current; current = new WarningCounter(); } while (!warningsUpdater.compareAndSet(this, null, current)); return current; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private PipelineResult XXX() throws Exception { MetricsEnvironment.setMetricsSupported(false); FlinkPortablePipelineTranslator<?> translator; if (!pipelineOptions.isStreaming() && !hasUnboundedPCollections(pipeline)) { translator = FlinkBatchPortablePipelineTranslator.createTranslator(); } else { translator = new FlinkStreamingPortablePipelineTranslator(); } return XXXWithTranslator(translator); }', 'ground_truth': 'public PipelineResult run(final Pipeline pipeline) throws Exception { MetricsEnvironment.setMetricsSupported(false); FlinkPortablePipelineTranslator<?> translator; if (!pipelineOptions.isStreaming() && !hasUnboundedPCollections(pipeline)) { translator = FlinkBatchPortablePipelineTranslator.createTranslator(); } else { translator = new FlinkStreamingPortablePipelineTranslator(); } return runPipelineWithTranslator(pipeline, translator); }', 'output': 'private PipelineResult translate(String pipelineName) throws Exception { MetricsEnvironment.setMetricsSupported(false); FlinkPortablePipelineTranslator<?> translator; if (!pipelineOptions.isStreaming() && !hasUnboundedPCollections(pipeline)) { translator = FlinkBatchPortablePipelineTranslator.createTranslator(); } else { translator = new FlinkStreamingPortablePipelineTranslator(); } return translateWithTranslator(translator, pipelineName); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(Row row) throws IOException { Table table = Table.open(table_); for (ColumnFamily columnFamily : row.getColumnFamilies()) { int id = table.getColumnFamilyId(columnFamily.name()); if ( clHeader_.get(id) == 0 || ( clHeader_.get(id) == 1 && clHeader_.getPosition(id) == 0 ) ) { if ( clHeader_.get(id) == 0 || ( clHeader_.get(id) == 1 && clHeader_.getPosition(id) == 0 ) ) { clHeader_.turnOn( id, logWriter_.getCurrentPosition() ); writeCommitLogHeader(clHeader_.toByteArray(), true); } } } }', 'ground_truth': 'private void maybeUpdateHeader(Row row) throws IOException { Table table = Table.open(table_); for (ColumnFamily columnFamily : row.getColumnFamilies()) { int id = table.getColumnFamilyId(columnFamily.name()); if (!clHeader_.isDirty(id) || (clHeader_.isDirty(id) && clHeader_.getPosition(id) == 0)) { if (!clHeader_.isDirty(id) || (clHeader_.isDirty(id) && clHeader_.getPosition(id) == 0)) { clHeader_.turnOn(id, logWriter_.getCurrentPosition()); writeCommitLogHeader(clHeader_.toByteArray(), true); } } } }', 'output': 'private void writeTable(Row row) throws IOException { Table table = Table.open(table_); for (ColumnFamily columnFamily : row.getColumnFamilies()) { int id = table.getColumnFamilyId(columnFamily.name()); if ( clHeader_.get(id) == 0 || ( clHeader_.get(id) == 1 && clHeader_.getPosition(id) == 0 ) ) { if ( clHeader_.get(id) == 0 || ( clHeader_.get(id) == 1 && clHeader_.getPosition(id) == 0 ) ) { clHeader_.turnOn( id, logWriter_.getCurrentPosition() ); writeCommitLogHeader(clHeader_.toByteArray(), true); } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Read<T> XXX(ValueProvider<String> topic) { if (topic.isAccessible()) { PubsubTopic.fromPath(topic.get()); } return toBuilder() .setTopicProvider(NestedValueProvider.of(topic, new TopicTranslator())) .build(); } public Write<T> topic(ValueProvider<String> topic) { return toBuilder() .setTopicProvider(NestedValueProvider.of(topic, new TopicTranslator())) .build(); }', 'ground_truth': 'public Write<T> to(ValueProvider<String> topic) { return toBuilder() .setTopicProvider(NestedValueProvider.of(topic, new TopicTranslator())) .build(); }', 'output': 'public Read<T> topic(ValueProvider<String> topic) { if (topic.isAccessible()) { PubsubTopic.fromPath(topic.get()); } return toBuilder() .setTopicProvider(NestedValueProvider.of(topic, new TopicTranslator())) .build(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override protected BinaryObjectExImpl XXX(BinaryMarshaller marsh, Object obj) throws Exception { byte[] arr = marsh.marshal(obj); long ptr = UNSAFE.allocateMemory(arr.length); ptrs.add(ptr); UNSAFE.copyMemory(arr, BYTE_ARR_OFF, null, ptr, arr.length); return new BinaryObjectOffheapImpl(ctx, ptr, 0, arr.length); } } }', 'ground_truth': '@Override protected BinaryObjectExImpl toBinary(BinaryMarshaller marsh, Object obj) throws Exception { byte[] arr = marsh.marshal(obj); long ptr = UNSAFE.allocateMemory(arr.length); ptrs.add(ptr); UNSAFE.copyMemory(arr, BYTE_ARR_OFF, null, ptr, arr.length); return new BinaryObjectOffheapImpl(ctx, ptr, 0, arr.length); }', 'output': '@Override protected BinaryObjectExImpl marshal(BinaryMarshaller marsh, Object obj) throws Exception { byte[] arr = marsh.marshal(obj); long ptr = UNSAFE.allocateMemory(arr.length); ptrs.add(ptr); UNSAFE.copyMemory(arr, BYTE_ARR_OFF, null, ptr, arr.length); return new BinaryObjectOffheapImpl(ctx, ptr, 0, arr.length); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(int cacheId, CacheState state) { if (cacheStates == null) cacheStates = new HashMap<>(); cacheStates.put(cacheId, state); }', 'ground_truth': 'public void addCacheGroupState(int grpId, CacheState state) { if (cacheGrpStates == null) cacheGrpStates = new HashMap<>(); cacheGrpStates.put(grpId, state); }', 'output': 'public void putCacheState(int cacheId, CacheState state) { if (cacheStates == null) cacheStates = new HashMap<>(); cacheStates.put(cacheId, state); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected IgfsConfiguration XXX(String gridName) throws IgniteCheckedException { IgfsConfiguration cfg = new IgfsConfiguration(); cfg.setDataCacheName("partitioned"); cfg.setMetaCacheName("replicated"); cfg.setName("igfs"); cfg.setPrefetchBlocks(1); cfg.setMaxSpaceSize(64 * 1024 * 1024); cfg.setDefaultMode(mode); if (mode != PRIMARY) cfg.setSecondaryFileSystem(new IgfsHadoopFileSystemWrapper(secondaryFileSystemUriPath(), secondaryFileSystemConfigPath())); cfg.setIpcEndpointConfiguration(primaryIpcEndpointConfiguration(gridName)); cfg.setManagementPort(-1); cfg.setBlockSize(512 * 1024); return cfg; }', 'ground_truth': 'protected IgfsConfiguration igfsConfiguration(String gridName) throws IgniteCheckedException { IgfsConfiguration cfg = new IgfsConfiguration(); cfg.setDataCacheName("partitioned"); cfg.setMetaCacheName("replicated"); cfg.setName("igfs"); cfg.setPrefetchBlocks(1); cfg.setMaxSpaceSize(64 * 1024 * 1024); cfg.setDefaultMode(mode); if (mode != PRIMARY) cfg.setSecondaryFileSystem(new IgfsHadoopFileSystemWrapper(secondaryFileSystemUriPath(), secondaryFileSystemConfigPath())); cfg.setIpcEndpointConfiguration(primaryIpcEndpointConfiguration(gridName)); cfg.setManagementPort(-1); cfg.setBlockSize(512 * 1024); return cfg; }', 'output': 'protected IgfsConfiguration configure(String gridName) throws IgniteCheckedException { IgfsConfiguration cfg = new IgfsConfiguration(); cfg.setDataCacheName("partitioned"); cfg.setMetaCacheName("replicated"); cfg.setName("igfs"); cfg.setPrefetchBlocks(1); cfg.setMaxSpaceSize(64 * 1024 * 1024); cfg.setDefaultMode(mode); if (mode != PRIMARY) cfg.setSecondaryFileSystem(new IgfsHadoopFileSystemWrapper(secondaryFileSystemUriPath(), secondaryFileSystemConfigPath())); cfg.setIpcEndpointConfiguration(primaryIpcEndpointConfiguration(gridName)); cfg.setManagementPort(-1); cfg.setBlockSize(512 * 1024); return cfg; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': "public static String XXX(int length) { Random random = new Random(); char[] chars = new char[26]; int i = 0; for (char c = 'a'; c <= 'z'; c++) chars[i++] = c; return new String(chars); }", 'ground_truth': "public static String makeRandomSt(int length) { Random random = new Random(); char[] chars = new char[26]; int i = 0; for (char c = 'a'; c <= 'z'; c++) chars[i++] = c; return new String(chars); }", 'output': "public static String uuid() { Random random = new Random(); char[] chars = new char[4]; int i = 0; for (char c = 'a'; c <= 'z'; c++) chars[i++] = c; return new String(chars); }</s>"}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws IOException { GcsUtil gcsUtil = gcsOptionsWithTestCredential().getGcsUtil(); thrown.expect(IllegalArgumentException.class); thrown.expectMessage("Number of source files 3"); gcsUtil.makeCopyBatches(makeStrings("s", 3), makeStrings("d", 1)); }', 'ground_truth': 'public void testMakeRewriteOpsInvalid() throws IOException { GcsUtil gcsUtil = gcsOptionsWithTestCredential().getGcsUtil(); thrown.expect(IllegalArgumentException.class); thrown.expectMessage("Number of source files 3"); gcsUtil.makeRewriteOps(makeStrings("s", 3), makeStrings("d", 1)); }', 'output': 'public void testMakeCopyBatchesThrowsIllegalArgumentExceptionForTooManySources() throws IOException { GcsUtil gcsUtil = gcsOptionsWithTestCredential().getGcsUtil(); thrown.expect(IllegalArgumentException.class); thrown.expectMessage("Number of source files 3"); gcsUtil.makeCopyBatches(makeStrings("s", 3), makeStrings("d", 1)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public long XXX() { return SIZE; }', 'ground_truth': 'public long unsharedHeapSizeExcludingData() { return SIZE; }', 'output': 'public long getSize() { return SIZE; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static KeyspaceMetadata XXX(RowIterator serializedParams, RowIterator serializedTables, RowIterator serializedViews, RowIterator serializedTypes, RowIterator serializedFunctions, RowIterator serializedAggregates) { String name = AsciiType.instance.compose(serializedParams.partitionKey().getKey()); KeyspaceParams params = createKeyspaceParamsFromSchemaPartition(serializedParams); Tables tables = createTablesFromTablesPartition(serializedTables); Views views = createViewsFromViewsPartition(serializedViews); Types types = createTypesFromPartition(serializedTypes); Collection<UDFunction> udfs = createFunctionsFromFunctionsPartition(serializedFunctions); Functions functions = org.apache.cassandra.schema.Functions.builder().add(udfs).build(); functions = createAggregatesFromAggregatesPartition(functions, serializedAggregates); return KeyspaceMetadata.create(name, params, tables, views, types, functions); }', 'ground_truth': 'private static KeyspaceMetadata fetchKeyspace(String keyspaceName) { KeyspaceParams params = fetchKeyspaceParams(keyspaceName); Types types = fetchTypes(keyspaceName); Tables tables = fetchTables(keyspaceName, types); Views views = fetchViews(keyspaceName, types); Functions functions = fetchFunctions(keyspaceName, types); return KeyspaceMetadata.create(keyspaceName, params, tables, views, types, functions); }', 'output': 'private static KeyspaceMetadata readKeyspaceMetadata(RowIterator serializedParams, RowIterator serializedTables, RowIterator serializedViews, RowIterator serializedTypes, RowIterator serializedFunctions, RowIterator serializedAggregates) { String name = AsciiType.instance.compose(serializedParams.partitionKey().getKey()); KeyspaceParams params = createKeyspaceParamsFromSchemaPartition(serializedParams); Tables tables = createTablesFromTablesPartition(serializedTables); Views views = createViewsFromViewsPartition(serializedViews); Types types = createTypesFromPartition(serializedTypes); Collection<UDFunction> udfs = createFunctionsFromFunctionsPartition(serializedFunctions); Functions functions = org.apache.cassandra.schema.Functions.builder().add(udfs).build(); functions = createAggregatesFromAggregatesPartition(functions, serializedAggregates); return KeyspaceMetadata.create(name, params, tables, views, types, functions); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void XXX(PortableReader reader) throws PortableException { PortableRawReader raw = reader.rawReader(); typeName = raw.readString(); affKeyFieldName = raw.readString(); fields = raw.readMap(); } /** {@inheritDoc} */ @Override public String toString() { return S.toString(PortableMetaDataImpl.class, this); } } }', 'ground_truth': '@Override public void readBinary(BinaryReader reader) throws BinaryObjectException { BinaryRawReader raw = reader.rawReader(); typeName = raw.readString(); affKeyFieldName = raw.readString(); fields = raw.readMap(); }', 'output': '@Override public void read(PortableReader reader) throws PortableException { PortableRawReader raw = reader.rawReader(); typeName = raw.readString(); affKeyFieldName = raw.readString(); fields = raw.readMap(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { checkVisitErrors("FinalClass", ACC_FINAL, false); checkVisitErrors("AbstractClass", ACC_ABSTRACT, false); checkVisitErrors(ABSTRACT_FINAL_CLASS, ACC_ABSTRACT | ACC_FINAL, true); checkErrorMessage(EXPECTED_ERROR_MESSAGE); }', 'ground_truth': 'public void testDetectsFinalAbstractClass() throws Exception { checkVisitErrors("FinalClass", ACC_FINAL, false); checkVisitErrors("AbstractClass", ACC_ABSTRACT, false); checkVisitErrors(ABSTRACT_FINAL_CLASS, ACC_ABSTRACT | ACC_FINAL, true); checkErrorMessage(EXPECTED_ERROR_MESSAGE_FOR_CLASS); }', 'output': 'public void testVisitErrors() throws Exception { checkVisitErrors("FinalClass", ACC_FINAL, false); checkVisitErrors("AbstractClass", ACC_ABSTRACT, false); checkVisitErrors(ABSTRACT_FINAL_CLASS, ACC_ABSTRACT | ACC_FINAL, true); checkErrorMessage(EXPECTED_ERROR_MESSAGE); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX(ColumnFamilyStore baseCfs) { this.baseCfs = baseCfs; }', 'ground_truth': 'private void setBaseCfs(ColumnFamilyStore baseCfs) { this.baseCfs = baseCfs; }', 'output': 'private void setBaseCfs(ColumnFamilyStore baseCfs) { this.baseCfs = baseCfs; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<DataflowPackage> XXX() { checkNotNull(options.getStagingLocation()); String windmillBinary = options.as(DataflowPipelineDebugOptions.class).getOverrideWindmillBinary(); if (windmillBinary != null) { options.getFilesToStage().add("windmill_main=" + windmillBinary); } int uploadSizeBytes = firstNonNull(options.getGcsUploadBufferSizeBytes(), 1024 * 1024); checkArgument(uploadSizeBytes > 0, "gcsUploadBufferSizeBytes must be > 0"); uploadSizeBytes = Math.min(uploadSizeBytes, 1024 * 1024); GcsCreateOptions createOptions = GcsCreateOptions.builder() .setGcsUploadBufferSizeBytes(uploadSizeBytes) .setMimeType(MimeTypes.BINARY) .build(); try (PackageUtil packageUtil = PackageUtil.withDefaultThreadPool()) { return packageUtil.stageClasspathElements( options.getFilesToStage(), options.getStagingLocation(), createOptions); } }', 'ground_truth': 'public List<DataflowPackage> stageDefaultFiles() { checkNotNull(options.getStagingLocation()); String windmillBinary = options.as(DataflowPipelineDebugOptions.class).getOverrideWindmillBinary(); List<String> filesToStage = options.getFilesToStage(); if (windmillBinary != null) { filesToStage.add("windmill_main=" + windmillBinary); } return stageFiles(filesToStage); }', 'output': 'public List<DataflowPackage> getPackages() { checkNotNull(options.getStagingLocation()); String windmillBinary = options.as(DataflowPipelineDebugOptions.class).getOverrideWindmillBinary(); if (windmillBinary != null) { options.getFilesToStage().add("windmill_main=" + windmillBinary); } int uploadSizeBytes = firstNonNull(options.getGcsUploadBufferSizeBytes(), 1024 * 1024); checkArgument(uploadSizeBytes > 0, "gcsUploadBufferSizeBytes must be > 0"); uploadSizeBytes = Math.min(uploadSizeBytes, 1024 * 1024); GcsCreateOptions createOptions = GcsCreateOptions.builder() .setGcsUploadBufferSizeBytes(uploadSizeBytes) .setMimeType(MimeTypes.BINARY) .build(); try (PackageUtil packageUtil = PackageUtil.withDefaultThreadPool()) { return packageUtil.stageClasspathElements( options.getFilesToStage(), options.getStagingLocation(), createOptions); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ColumnDefinition XXX(ByteBuffer newName) { return new ColumnDefinition(newName, validator, indexType, indexOptions, indexName, componentIndex, type); }', 'ground_truth': 'public ColumnDefinition withNewName(ColumnIdentifier newName) { return new ColumnDefinition(ksName, cfName, newName, type, indexType, indexOptions, indexName, componentIndex, kind); }', 'output': 'public ColumnDefinition copy(ByteBuffer newName) { return new ColumnDefinition(newName, validator, indexType, indexOptions, indexName, componentIndex, type); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Map<String, String> XXX(KsDef ks_def) { Map<String, String> options; if (ks_def.isSetReplication_factor()) { options = new HashMap<String, String>(ks_def.strategy_options == null ? Collections.<String, String>emptyMap() : ks_def.strategy_options); options.put("replication_factor", String.valueOf(ks_def.replication_factor)); } else { options = ks_def.strategy_options; } return options; }', 'ground_truth': 'public static Map<String, String> forwardsCompatibleOptions(KsDef ks_def) { Map<String, String> options; if (ks_def.isSetReplication_factor()) { options = new HashMap<String, String>(ks_def.strategy_options == null ? Collections.<String, String>emptyMap() : ks_def.strategy_options); options.put("replication_factor", String.valueOf(ks_def.replication_factor)); } else { options = ks_def.strategy_options; } return options; }', 'output': 'public static Map<String, String> strategyOptions(KsDef ks_def) { Map<String, String> options; if (ks_def.isSetReplication_factor()) { options = new HashMap<String, String>(ks_def.strategy_options == null ? Collections.<String, String>emptyMap() : ks_def.strategy_options); options.put("replication_factor", String.valueOf(ks_def.replication_factor)); } else { options = ks_def.strategy_options; } return options; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ApplicationFeatureId XXX(final String packageFqn) { final ApplicationFeatureId featureId = new ApplicationFeatureId(ApplicationFeatureType.PACKAGE); featureId.setNamespace(packageFqn); return featureId; }', 'ground_truth': 'public static ApplicationFeatureId newNamespace(final String namespace) { final ApplicationFeatureId featureId = new ApplicationFeatureId(ApplicationFeatureType.NAMESPACE); featureId.setNamespace(namespace); return featureId; }', 'output': 'public static ApplicationFeatureId getPackageFeatureId(final String packageFqn) { final ApplicationFeatureId featureId = new ApplicationFeatureId(ApplicationFeatureType.PACKAGE); featureId.setNamespace(packageFqn); return featureId; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static CFMetaData XXX(String ks, String cf, TriggerMetadata trigger) { CFMetaData metadata = CFMetaData.Builder.create(ks, cf) .addPartitionKey("pkey", UTF8Type.instance) .addRegularColumn("c1", UTF8Type.instance) .addRegularColumn("c2", UTF8Type.instance) .build(); try { if (trigger != null) metadata.triggers(metadata.getTriggers().with(trigger)); } catch (InvalidRequestException e) { throw new AssertionError(e); } return metadata; }', 'ground_truth': 'private static TableMetadata makeTableMetadata(String ks, String cf, TriggerMetadata trigger) { TableMetadata.Builder builder = TableMetadata.builder(ks, cf) .addPartitionKeyColumn("pkey", UTF8Type.instance) .addRegularColumn("c1", UTF8Type.instance) .addRegularColumn("c2", UTF8Type.instance); if (trigger != null) builder.triggers(Triggers.of(trigger)); return builder.build(); }', 'output': 'private static CFMetaData withTrigger(String ks, String cf, TriggerMetadata trigger) { CFMetaData metadata = CFMetaData.Builder.create(ks, cf) .addPartitionKey("pkey", UTF8Type.instance) .addRegularColumn("c1", UTF8Type.instance) .addRegularColumn("c2", UTF8Type.instance) .build(); try { if (trigger != null) metadata.triggers(metadata.getTriggers().with(trigger)); } catch (InvalidRequestException e) { throw new AssertionError(e); } return metadata; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static UDAggregate XXX(UntypedResultSet.Row row) { return XXX(Schema.instance.getKSMetaData(row.getString("keyspace_name")).functions, row); } private static UDAggregate XXX(Functions functions, UntypedResultSet.Row row)\n    { { String ksName = row.getString("keyspace_name"); String functionName = row.getString("aggregate_name"); FunctionName name = new FunctionName(ksName, functionName); List<String> types = row.getFrozenList("argument_types", UTF8Type.instance); List<AbstractType<?>> argTypes; if (types == null) { argTypes = Collections.emptyList(); } else { argTypes = new ArrayList<>(types.size()); for (String type : types) argTypes.add(parseType(type)); } AbstractType<?> returnType = parseType(row.getString("return_type")); FunctionName stateFunc = new FunctionName(ksName, (row.getString("state_func"))); FunctionName finalFunc = row.has("final_func") ? new FunctionName(ksName, row.getString("final_func")) : null; AbstractType<?> stateType = row.has("state_type") ? parseType(row.getString("state_type")) : null; ByteBuffer initcond = row.has("initcond") ? row.getBytes("initcond") : null; try { return UDAggregate.create(functions, name, argTypes, returnType, stateFunc, finalFunc, stateType, initcond); } catch (InvalidRequestException reason) { return UDAggregate.createBroken(name, argTypes, returnType, initcond, reason); } }', 'ground_truth': 'private static UDAggregate createUDAFromRow(UntypedResultSet.Row row, Functions functions, Types types) { String ksName = row.getString("keyspace_name"); String functionName = row.getString("aggregate_name"); FunctionName name = new FunctionName(ksName, functionName); List<AbstractType<?>> argTypes = row.getFrozenList("argument_types", UTF8Type.instance) .stream() .map(t -> parse(ksName, t, types)) .collect(toList()); AbstractType<?> returnType = parse(ksName, row.getString("return_type"), types); FunctionName stateFunc = new FunctionName(ksName, (row.getString("state_func"))); FunctionName finalFunc = row.has("final_func") ? new FunctionName(ksName, row.getString("final_func")) : null; AbstractType<?> stateType = row.has("state_type") ? parse(ksName, row.getString("state_type"), types) : null; ByteBuffer initcond = row.has("initcond") ? row.getBytes("initcond") : null; try { return UDAggregate.create(functions, name, argTypes, returnType, stateFunc, finalFunc, stateType, initcond); } catch (InvalidRequestException reason) { return UDAggregate.createBroken(name, argTypes, returnType, initcond, reason); } }', 'output': 'private static UDAggregate aggregate(UntypedResultSet.Row row) { return aggregate(Schema.instance.getKSMetaData(row.getString("keyspace_name")).functions, row); } private static UDAggregate aggregate(Functions functions, UntypedResultSet.Row row)\n    { { String ksName = row.getString("keyspace_name"); String functionName = row.getString("aggregate_name"); FunctionName name = new FunctionName(ksName, functionName); List<String> types = row.getFrozenList("argument_types", UTF8Type.instance); List<AbstractType<?>> argTypes; if (types == null) { argTypes = Collections.emptyList(); } else { argTypes = new ArrayList<>(types.size()); for (String type : types) argTypes.add(parseType(type)); } AbstractType<?> returnType = parseType(row.getString("return_type")); FunctionName stateFunc = new FunctionName(ksName, (row.getString("state_func"))); FunctionName finalFunc = row.has("final_func") ? new FunctionName(ksName, row.getString("final_func")) : null; AbstractType<?> stateType = row.has("state_type") ? parseType(row.getString("state_type")) : null; ByteBuffer initcond = row.has("initcond") ? row.getBytes("initcond") : null; try { return UDAggregate.create(functions, name, argTypes, returnType, stateFunc, finalFunc, stateType, initcond); } catch (InvalidRequestException reason) { return UDAggregate.createBroken(name, argTypes, returnType, initcond, reason); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { RuntimeTabOperator rto; rto = RuntimeTabOperator.invoke(); Node derbynode = new Node(rto.tree(), "Servers|Payara V3|Resources|JDBC|Connection Pools|DerbyPool"); if (derbynode.isPresent()) { System.out.printf("Found JDBC Connection DerbyPool"); } else { fail("JDBC Connection DerbyPool missing!"); } }', 'ground_truth': 'public void VerifyDefaultH2Pool() { RuntimeTabOperator rto; rto = RuntimeTabOperator.invoke(); Node h2Node = new Node(rto.tree(), "Servers|Payara V3|Resources|JDBC|Connection Pools|H2Pool"); if (h2Node.isPresent()) { System.out.printf("Found JDBC Connection H2Pool"); } else { fail("JDBC Connection H2Pool missing!"); } }', 'output': 'public void test001() { RuntimeTabOperator rto; rto = RuntimeTabOperator.invoke(); Node derbynode = new Node(rto.tree(), "Servers|Payara V3|Resources|JDBC|Connection Pools|DerbyPool"); if (derbynode.isPresent()) { System.out.printf("Found JDBC Connection DerbyPool"); } else { fail("JDBC Connection DerbyPool missing!"); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws InvalidRequestException { CFMetaData cfMetaData = newCFMetaData(2); ByteBuffer clustering_0 = ByteBufferUtil.bytes(1); Restriction eq = newSingleEq(cfMetaData, 0, clustering_0); PrimaryKeyRestrictions restrictions = new PrimaryKeyRestrictionSet(cfMetaData.comparator); restrictions = restrictions.mergeWith(eq); List<Composite> bounds = restrictions.boundsAsComposites(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertComposite(bounds.get(0), clustering_0, EOC.START); bounds = restrictions.boundsAsComposites(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertComposite(bounds.get(0), clustering_0, EOC.END); }', 'ground_truth': 'public void testboundsAsClusteringWithOneEqRestrictionsAndTwoClusteringColumns() throws InvalidRequestException { CFMetaData cfMetaData = newCFMetaData(2); ByteBuffer clustering_0 = ByteBufferUtil.bytes(1); Restriction eq = newSingleEq(cfMetaData, 0, clustering_0); PrimaryKeyRestrictions restrictions = new PrimaryKeyRestrictionSet(cfMetaData.comparator, false); restrictions = restrictions.mergeWith(eq); SortedSet<Slice.Bound> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), true, clustering_0); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, clustering_0); }', 'output': 'public void testClusteringStartEnd() throws InvalidRequestException { CFMetaData cfMetaData = newCFMetaData(2); ByteBuffer clustering_0 = ByteBufferUtil.bytes(1); Restriction eq = newSingleEq(cfMetaData, 0, clustering_0); PrimaryKeyRestrictions restrictions = new PrimaryKeyRestrictionSet(cfMetaData.comparator); restrictions = restrictions.mergeWith(eq); List<Composite> bounds = restrictions.boundsAsComposites(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertComposite(bounds.get(0), clustering_0, EOC.START); bounds = restrictions.boundsAsComposites(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertComposite(bounds.get(0), clustering_0, EOC.END); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private PCollection<SellerPrice> XXX(PCollection<Event> events) { return events .apply(Filter.by(new AuctionOrBid())) .apply(new WinningBids(name + ".WinningBids", configuration)) .apply( name + ".Rekey", ParDo.of( new DoFn<AuctionBid, KV<Long, Bid>>() { @ProcessElement public void processElement(ProcessContext c) { Auction auction = c.element().auction; Bid bid = c.element().bid; c.output(KV.of(auction.seller, bid)); } })) .apply( Window.<KV<Long, Bid>>into(new GlobalWindows()) .triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1))) .accumulatingFiredPanes() .withAllowedLateness(Duration.ZERO)) .apply(Combine.perKey(new MovingMeanSellingPrice(10))) .apply( name + ".Select", ParDo.of( new DoFn<KV<Long, Long>, SellerPrice>() { @ProcessElement public void processElement(ProcessContext c) { c.output(new SellerPrice(c.element().getKey(), c.element().getValue())); } })); }', 'ground_truth': 'public PCollection<SellerPrice> expand(PCollection<Event> events) { return events .apply(Filter.by(new AuctionOrBid())) .apply(new WinningBids(name + ".WinningBids", configuration)) .apply( name + ".Rekey", ParDo.of( new DoFn<AuctionBid, KV<Long, Bid>>() { @ProcessElement public void processElement(ProcessContext c) { Auction auction = c.element().auction; Bid bid = c.element().bid; c.output(KV.of(auction.seller, bid)); } })) .apply( Window.<KV<Long, Bid>>into(new GlobalWindows()) .triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1))) .accumulatingFiredPanes() .withAllowedLateness(Duration.ZERO)) .apply(Combine.perKey(new MovingMeanSellingPrice(10))) .apply( name + ".Select", ParDo.of( new DoFn<KV<Long, Long>, SellerPrice>() { @ProcessElement public void processElement(ProcessContext c) { c.output(new SellerPrice(c.element().getKey(), c.element().getValue())); } })); }', 'output': 'private PCollection<SellerPrice> winningBids(PCollection<Event> events) { return events .apply(Filter.by(new AuctionOrBid())) .apply(new WinningBids(name + ".WinningBids", configuration)) .apply( name + ".Rekey", ParDo.of( new DoFn<AuctionBid, KV<Long, Bid>>() { @ProcessElement public void processElement(ProcessContext c) { Auction auction = c.element().auction; Bid bid = c.element().bid; c.output(KV.of(auction.seller, bid)); } })) .apply( Window.<KV<Long, Bid>>into(new GlobalWindows()) .triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1))) .accumulatingFiredPanes() .withAllowedLateness(Duration.ZERO)) .apply(Combine.perKey(new MovingMeanSellingPrice(10))) .apply( name + ".Select", ParDo.of( new DoFn<KV<Long, Long>, SellerPrice>() { @ProcessElement public void processElement(ProcessContext c) { c.output(new SellerPrice(c.element().getKey(), c.element().getValue())); } })); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Set<SSTableReader> XXX(int level) { Set<SSTableReader> sstables = new HashSet<>(); Set<SSTableReader> levelSSTables = new HashSet<>(getLevel(level)); for (SSTableReader sstable : cfs.getTracker().XXX()) { if (levelSSTables.contains(sstable)) sstables.add(sstable); } return sstables; }', 'ground_truth': 'private Set<SSTableReader> getCompactingL0() { Set<SSTableReader> sstables = new HashSet<>(); Set<SSTableReader> levelSSTables = new HashSet<>(generations.get(0)); for (SSTableReader sstable : cfs.getTracker().getCompacting()) { if (levelSSTables.contains(sstable)) sstables.add(sstable); } return sstables; }', 'output': 'private Set<SSTableReader> getSSTables(int level) { Set<SSTableReader> sstables = new HashSet<>(); Set<SSTableReader> levelSSTables = new HashSet<>(getLevel(level)); for (SSTableReader sstable : cfs.getTracker().getSSTables()) { if (levelSSTables.contains(sstable)) sstables.add(sstable); } return sstables; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(UpgradeContext upgradeContext) throws AmbariException { Cluster cluster = upgradeContext.getCluster(); Set<String> services = upgradeContext.getSupportedServices(); for (String serviceName : services) { Service service = cluster.getService(serviceName); RepositoryVersionEntity targetRepositoryVersion = upgradeContext.getTargetRepositoryVersion(serviceName); StackId targetStack = targetRepositoryVersion.getStackId(); service.setDesiredRepositoryVersion(targetRepositoryVersion); Collection<ServiceComponent> components = service.getServiceComponents().values(); for (ServiceComponent serviceComponent : components) { boolean versionAdvertised = false; try { ComponentInfo ci = m_ambariMetaInfo.get().getComponent(targetStack.getStackName(), targetStack.getStackVersion(), serviceComponent.getServiceName(), serviceComponent.getName()); versionAdvertised = ci.isVersionAdvertised(); } catch (AmbariException e) { LOG.warn("Component {}/{} doesn\'t exist for stack {}.  Setting version to {}", serviceComponent.getServiceName(), serviceComponent.getName(), targetStack, StackVersionListener.UNKNOWN_VERSION); } UpgradeState upgradeStateToSet = UpgradeState.IN_PROGRESS; if (!versionAdvertised) { upgradeStateToSet = UpgradeState.NONE; } for (ServiceComponentHost serviceComponentHost : serviceComponent.getServiceComponentHosts().values()) { if (serviceComponentHost.getUpgradeState() != upgradeStateToSet) { serviceComponentHost.setUpgradeState(upgradeStateToSet); } if (!versionAdvertised && !StringUtils.equals(StackVersionListener.UNKNOWN_VERSION, serviceComponentHost.getVersion())) { serviceComponentHost.setVersion(StackVersionListener.UNKNOWN_VERSION); } } serviceComponent.setDesiredRepositoryVersion(targetRepositoryVersion); } } }', 'ground_truth': 'private void setDesiredRepositories(UpgradeContext upgradeContext) throws AmbariException { Cluster cluster = upgradeContext.getCluster(); Set<String> services = upgradeContext.getSupportedServices(); for (String serviceName : services) { Service service = cluster.getService(serviceName); RepositoryVersionEntity targetRepositoryVersion = upgradeContext.getTargetRepositoryVersion(serviceName); StackId targetStack = targetRepositoryVersion.getStackId(); service.setDesiredRepositoryVersion(targetRepositoryVersion); Collection<ServiceComponent> components = service.getServiceComponents().values(); for (ServiceComponent serviceComponent : components) { boolean versionAdvertised = false; try { ComponentInfo ci = m_ambariMetaInfoProvider.get().getComponent(targetStack.getStackName(), targetStack.getStackVersion(), serviceComponent.getServiceName(), serviceComponent.getName()); versionAdvertised = ci.isVersionAdvertised(); } catch (AmbariException e) { LOG.warn("Component {}/{} doesn\'t exist for stack {}.  Setting version to {}", serviceComponent.getServiceName(), serviceComponent.getName(), targetStack, StackVersionListener.UNKNOWN_VERSION); } UpgradeState upgradeStateToSet = UpgradeState.IN_PROGRESS; if (!versionAdvertised) { upgradeStateToSet = UpgradeState.NONE; } for (ServiceComponentHost serviceComponentHost : serviceComponent.getServiceComponentHosts().values()) { if (serviceComponentHost.getUpgradeState() != upgradeStateToSet) { serviceComponentHost.setUpgradeState(upgradeStateToSet); } if (!versionAdvertised && !StringUtils.equals(StackVersionListener.UNKNOWN_VERSION, serviceComponentHost.getVersion())) { serviceComponentHost.setVersion(StackVersionListener.UNKNOWN_VERSION); } } serviceComponent.setDesiredRepositoryVersion(targetRepositoryVersion); } } }', 'output': 'public void upgrade(UpgradeContext upgradeContext) throws AmbariException { Cluster cluster = upgradeContext.getCluster(); Set<String> services = upgradeContext.getSupportedServices(); for (String serviceName : services) { Service service = cluster.getService(serviceName); RepositoryVersionEntity targetRepositoryVersion = upgradeContext.getTargetRepositoryVersion(serviceName); StackId targetStack = targetRepositoryVersion.getStackId(); service.setDesiredRepositoryVersion(targetRepositoryVersion); Collection<ServiceComponent> components = service.getServiceComponents().values(); for (ServiceComponent serviceComponent : components) { boolean versionAdvertised = false; try { ComponentInfo ci = m_ambariMetaInfo.get().getComponent(targetStack.getStackName(), targetStack.getStackVersion(), serviceComponent.getServiceName(), serviceComponent.getName()); versionAdvertised = ci.isVersionAdvertised(); } catch (AmbariException e) { LOG.warn("Component {}/{} doesn\'t exist for stack {}.  Setting version to {}", serviceComponent.getServiceName(), serviceComponent.getName(), targetStack, StackVersionListener.UNKNOWN_VERSION); } UpgradeState upgradeStateToSet = UpgradeState.IN_PROGRESS; if (!versionAdvertised) { upgradeStateToSet = UpgradeState.NONE; } for (ServiceComponentHost serviceComponentHost : serviceComponent.getServiceComponentHosts().values()) { if (serviceComponentHost.getUpgradeState() != upgradeStateToSet) { serviceComponentHost.setUpgradeState(upgradeStateToSet); } if (!versionAdvertised && !StringUtils.equals(StackVersionListener.UNKNOWN_VERSION, serviceComponentHost.getVersion())) { serviceComponentHost.setVersion(StackVersionListener.UNKNOWN_VERSION); } } serviceComponent.setDesiredRepositoryVersion(targetRepositoryVersion); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(DecoratedKey key) { UUID cfId = Schema.instance.getId(keyspace.getName(), this.name); if (cfId == null) return; XXX(new RowCacheKey(cfId, key)); }', 'ground_truth': 'public void invalidateCachedPartition(RowCacheKey key) { CacheService.instance.rowCache.remove(key); }', 'output': 'public void update(DecoratedKey key) { UUID cfId = Schema.instance.getId(keyspace.getName(), this.name); if (cfId == null) return; update(new RowCacheKey(cfId, key)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Expression XXX(MacroContext ctx, final Expression... exps) { List<Expression> expList = Arrays.stream(exps).map(exp -> callX(exp, "dump")) .collect(Collectors.toList()); return new GStringExpression("", makeLabels(exps), expList); }', 'ground_truth': 'public static Expression SVD(MacroContext ctx, final Expression... exps) { List<Expression> expList = Arrays.stream(exps).map(exp -> callX(exp, "dump")) .collect(Collectors.toList()); return new GStringExpression("", makeLabels(exps), expList); }', 'output': 'public static Expression dump(MacroContext ctx, final Expression... exps) { List<Expression> expList = Arrays.stream(exps).map(exp -> callX(exp, "dump")) .collect(Collectors.toList()); return new GStringExpression("", makeLabels(exps), expList); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public DefaultObjectWrapper XXX() { return (DefaultObjectWrapper) _BeansAPI.getBeansWrapperSubclassSingleton( this, INSTANCE_CACHE, INSTANCE_CACHE_REF_QUEUE, DefaultObjectWrapperFactory.INSTANCE); }', 'ground_truth': 'public DefaultObjectWrapper getResult() { return (DefaultObjectWrapper) _BeansAPI.getBeansWrapperSubclassSingleton( this, INSTANCE_CACHE, INSTANCE_CACHE_REF_QUEUE, DefaultObjectWrapperFactory.INSTANCE); }', 'output': 'public <T> T xxx() { return (T) _BeansAPI.getBeansWrapperSubclassSingleton( this, INSTANCE_CACHE, INSTANCE_CACHE_REF_QUEUE, DefaultObjectWrapperFactory.INSTANCE ); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public synchronized static void XXX(InetAddress key) { streamNotificationHandlers_.remove(key); }', 'ground_truth': 'public synchronized static void removeFileStatusHandler(InetAddress key) { streamNotificationHandlers_.remove(key); }', 'output': 'public synchronized static void removeStreamNotificationHandler(InetAddress key) { streamNotificationHandlers_.remove(key); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { final InMemFileSystem inmemfs = InMemFileSystem.get(); inmemfs.reset() .setFile("/tmp/foo.txt", Duration.ofSeconds(2), asList( "one two three four four two two", "one one one two two three")); Settings settings = new Settings(); settings.setClass("euphoria.io.datasource.factory.inmem", InMemFileSystem.SourceFactory.class); settings.setClass("euphoria.io.datasink.factory.inmem", InMemFileSystem.SinkFactory.class); Flow flow = Flow.create("Test", settings); Dataset<String> lines = flow.createInput(URI.create("inmem: Dataset<Pair<String, Long>> words = FlatMap.of(lines) .by((String s, Collector<Pair<String, Long>> c) -> { for (String part : s.split(" ")) { c.collect(Pair.of(part, 1L)); }}) .output(); Dataset<Pair<String, Long>> streamOutput = ReduceByKey .of(words) .keyBy(Pair::getFirst) .valueBy(Pair::getSecond) .combineBy(Sums.ofLongs()) .windowBy(Windowing.Time.seconds(1)) .output(); streamOutput.persist(URI.create("inmem: executor.waitForCompletion(flow); @SuppressWarnings("unchecked") List<Pair<String, Long>> f = new ArrayList<>(inmemfs.getFile("/tmp/output/0")); assertEquals(7, f.size()); assertEquals(asList("four-2", "one-1", "three-1", "two-3"), sublist(f, 0, 4)); assertEquals(asList("one-3", "three-1", "two-2"), sublist(f, 4, -1)); }', 'ground_truth': 'public void testWordCountStreamNonAggregating() throws Exception { final InMemFileSystem inmemfs = InMemFileSystem.get(); inmemfs.reset() .setFile("/tmp/foo.txt", Duration.ofSeconds(2), asList( "one two three four four two two", "one one one two two three")); Settings settings = new Settings(); settings.setClass("euphoria.io.datasource.factory.inmem", InMemFileSystem.SourceFactory.class); settings.setClass("euphoria.io.datasink.factory.inmem", InMemFileSystem.SinkFactory.class); Flow flow = Flow.create("Test", settings); Dataset<String> lines = flow.createInput(URI.create("inmem: Dataset<Pair<String, Long>> words = FlatMap.of(lines) .by((String s, Collector<Pair<String, Long>> c) -> { for (String part : s.split(" ")) { c.collect(Pair.of(part, 1L)); }}) .output(); Dataset<Pair<String, Long>> streamOutput = ReduceByKey .of(words) .keyBy(Pair::getFirst) .valueBy(Pair::getSecond) .combineBy(Sums.ofLongs()) .windowBy(Windowing.Time.seconds(1)) .output(); streamOutput.persist(URI.create("inmem: executor.waitForCompletion(flow); @SuppressWarnings("unchecked") List<Pair<String, Long>> f = new ArrayList<>(inmemfs.getFile("/tmp/output/0")); assertEquals(7, f.size()); assertEquals(asList("four-2", "one-1", "three-1", "two-3"), sublist(f, 0, 4)); assertEquals(asList("one-3", "three-1", "two-2"), sublist(f, 4, -1)); }', 'output': 'public void testInMemFlow() throws Exception { final InMemFileSystem inmemfs = InMemFileSystem.get(); inmemfs.reset() .setFile("/tmp/foo.txt", Duration.ofSeconds(2), asList( "one two three four four two two", "one one one two two three")); Settings settings = new Settings(); settings.setClass("euphoria.io.datasource.factory.inmem", InMemFileSystem.SourceFactory.class); settings.setClass("euphoria.io.datasink.factory.inmem", InMemFileSystem.SinkFactory.class); Flow flow = Flow.create("Test", settings); Dataset<String> lines = flow.createInput(URI.create("inmem: Dataset<Pair<String, Long>> words = FlatMap.of(lines) .by((String s, Collector<Pair<String, Long>> c) -> { for (String part : s.split(" ")) { c.collect(Pair.of(part, 1L)); }}) .output(); Dataset<Pair<String, Long>> streamOutput = ReduceByKey .of(words) .keyBy(Pair::getFirst) .valueBy(Pair::getSecond) .combineBy(Sums.ofLongs()) .windowBy(Windowing.Time.seconds(1)) .output(); streamOutput.persist(URI.create("inmem: executor.waitForCompletion(flow); @SuppressWarnings("unchecked") List<Pair<String, Long>> f = new ArrayList<>(inmemfs.getFile("/tmp/output/0")); assertEquals(7, f.size()); assertEquals(asList("four-2", "one-1", "three-1", "two-3"), sublist(f, 0, 4)); assertEquals(asList("one-3", "three-1", "two-2"), sublist(f, 4, -1)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean XXX(long now) { return (int) (now / 1000) >= getLocalDeletionTime(); }', 'ground_truth': 'public boolean isLive(long now) { return (int) (now / 1000) < getLocalDeletionTime(); }', 'output': 'public boolean isExpired(long now) { return (int) (now / 1000) >= getLocalDeletionTime(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Map<String, String> XXX(String configurationType) { Map<String, String> properties = m_configurations.get(configurationType); if (null == properties) { return Maps.newHashMap(); } return properties; }', 'ground_truth': 'public Map<String, String> getConfigurationProperties(String configurationType) { Map<String, String> properties = m_configurations.get(configurationType); if (null == properties) { return Maps.newHashMap(); } return properties; }', 'output': 'public Map<String, String> getProperties(String configurationType) { Map<String, String> properties = m_configurations.get(configurationType); if (null == properties) { return Maps.newHashMap(); } return properties; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { FileUtils.closeQuietly(iwriter); FileUtils.closeQuietly(dataFile); try { Set<Component> components = SSTable.componentsFor(descriptor, Descriptor.TempState.TEMP); if (!components.isEmpty()) SSTable.delete(descriptor, components); } catch (Exception e) { logger.error(String.format("Failed deleting temp components for %s", descriptor), e); } }', 'ground_truth': 'public void abort() { assert descriptor.temporary; FileUtils.closeQuietly(iwriter); FileUtils.closeQuietly(dataFile); try { Set<Component> components = SSTable.componentsFor(descriptor); if (!components.isEmpty()) SSTable.delete(descriptor, components); } catch (Exception e) { logger.error(String.format("Failed deleting temp components for %s", descriptor), e); } }', 'output': 'public void cleanup() { FileUtils.closeQuietly(iwriter); FileUtils.closeQuietly(dataFile); try { Set<Component> components = SSTable.componentsFor(descriptor, Descriptor.TempState.TEMP); if (!components.isEmpty()) SSTable.delete(descriptor, components); } catch (Exception e) { logger.error(String.format("Failed deleting temp components for %s", descriptor), e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Set<K> XXX() { return map.keySet(); }', 'ground_truth': 'public Iterator<K> keyIterator() { return map.keyIterator(); }', 'output': 'public Set<K> keySet() { return map.keySet(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void XXX( final java.sql.Timestamp timestamp, final String user, final AuditEntry auditEntry) { val adapterAndProperty = auditEntry.getAdapterAndProperty(); val spec = adapterAndProperty.getAdapter().getSpecification(); final Bookmark target = adapterAndProperty.getBookmark(); final String propertyId = adapterAndProperty.getPropertyId(); final String memberId = adapterAndProperty.getMemberId(); final PreAndPostValues papv = auditEntry.getPreAndPostValues(); final String preValue = papv.getPreString(); final String postValue = papv.getPostString(); final String targetClass = CommandUtil.targetClassNameFor(spec); val txId = transactionService.currentTransactionId(); final UUID transactionId = txId.getUniqueId(); final int sequence = txId.getSequence(); for (val auditListener : enabledAuditListeners) { auditListener .audit(transactionId, sequence, targetClass, target, memberId, propertyId, preValue, postValue, user, timestamp); } }', 'ground_truth': 'private void publishChangedProperty( final java.sql.Timestamp timestamp, final String user, final AuditEntry auditEntry) { val adapterAndProperty = auditEntry.getAdapterAndProperty(); val spec = adapterAndProperty.getAdapter().getSpecification(); final Bookmark target = adapterAndProperty.getBookmark(); final String propertyId = adapterAndProperty.getPropertyId(); final String memberId = adapterAndProperty.getMemberId(); final PreAndPostValues papv = auditEntry.getPreAndPostValues(); final String preValue = papv.getPreString(); final String postValue = papv.getPostString(); final String targetClass = CommandUtil.targetClassNameFor(spec); val txId = transactionService.currentTransactionId(); final UUID transactionId = txId.getUniqueId(); final int sequence = txId.getSequence(); for (val subscriber : enabledSubscribers) { subscriber.onChanging( EntityPropertyChange .of(transactionId, sequence, targetClass, target, memberId, propertyId, preValue, postValue, user, timestamp)); } }', 'output': 'private void audit( final java.sql.Timestamp timestamp, final String user, final AuditEntry auditEntry) { val adapterAndProperty = auditEntry.getAdapterAndProperty(); val spec = adapterAndProperty.getAdapter().getSpecification(); final Bookmark target = adapterAndProperty.getBookmark(); final String propertyId = adapterAndProperty.getPropertyId(); final String memberId = adapterAndProperty.getMemberId(); final PreAndPostValues papv = auditEntry.getPreAndPostValues(); final String preValue = papv.getPreString(); final String postValue = papv.getPostString(); final String targetClass = CommandUtil.targetClassNameFor(spec); val txId = transactionService.currentTransactionId(); final UUID transactionId = txId.getUniqueId(); final int sequence = txId.getSequence(); for (val auditListener : enabledAuditListeners) { auditListener .audit(transactionId, sequence, targetClass, target, memberId, propertyId, preValue, postValue, user, timestamp); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public CompoundNaiveBayesModel XXX(double[] priorProbabilities) { this.priorProbabilities = priorProbabilities.clone(); return this; }', 'ground_truth': 'public CompoundNaiveBayesModel withPriorProbabilities(double[] priorProbabilities) { this.priorProbabilities = priorProbabilities.clone(); return this; }', 'output': 'public CompoundNaiveBayesModel setPriorProbabilities(double[] priorProbabilities) { this.priorProbabilities = priorProbabilities.clone(); return this; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX(final URL base) { this.reader.XXX(new EntityResolver() { public InputSource resolveEntity(final String publicId, final String systemId) throws IOException { return new InputSource(new URL(base, systemId).openStream()); } }); }', 'ground_truth': 'public void setEntityBaseUrl(final URL base) { this.reader.setEntityResolver(new EntityResolver() { public InputSource resolveEntity(final String publicId, final String systemId) throws IOException { return new InputSource(new URL(base, systemId).openStream()); } }); }', 'output': 'public void setBaseUrl(final URL base) { this.reader.setBaseUrl(new EntityResolver() { public InputSource resolveEntity(final String publicId, final String systemId) throws IOException { return new InputSource(new URL(base, systemId).openStream()); } }); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected IgfsConfiguration XXX(String gridName) throws IgniteCheckedException { IgfsConfiguration cfg = new IgfsConfiguration(); cfg.setDataCacheName("partitioned"); cfg.setMetaCacheName("replicated"); cfg.setName("igfs"); cfg.setPrefetchBlocks(1); cfg.setDefaultMode(mode); if (mode != PRIMARY) cfg.setSecondaryFileSystem(new IgfsHadoopFileSystemWrapper(SECONDARY_URI, SECONDARY_CFG_PATH)); cfg.setIpcEndpointConfiguration(primaryIpcEndpointConfiguration(gridName)); cfg.setManagementPort(-1); cfg.setBlockSize(512 * 1024); return cfg; }', 'ground_truth': 'protected IgfsConfiguration igfsConfiguration(String gridName) throws IgniteCheckedException { IgfsConfiguration cfg = new IgfsConfiguration(); cfg.setDataCacheName("partitioned"); cfg.setMetaCacheName("replicated"); cfg.setName("igfs"); cfg.setPrefetchBlocks(1); cfg.setDefaultMode(mode); if (mode != PRIMARY) cfg.setSecondaryFileSystem(new IgfsHadoopFileSystemWrapper(SECONDARY_URI, SECONDARY_CFG_PATH)); cfg.setIpcEndpointConfiguration(primaryIpcEndpointConfiguration(gridName)); cfg.setManagementPort(-1); cfg.setBlockSize(512 * 1024); return cfg; }', 'output': 'protected IgfsConfiguration configure(String gridName) throws IgniteCheckedException { IgfsConfiguration cfg = new IgfsConfiguration(); cfg.setDataCacheName("partitioned"); cfg.setMetaCacheName("replicated"); cfg.setName("igfs"); cfg.setPrefetchBlocks(1); cfg.setDefaultMode(mode); if (mode != PRIMARY) cfg.setSecondaryFileSystem(new IgfsHadoopFileSystemWrapper(SECONDARY_URI, SECONDARY_CFG_PATH)); cfg.setIpcEndpointConfiguration(primaryIpcEndpointConfiguration(gridName)); cfg.setManagementPort(-1); cfg.setBlockSize(512 * 1024); return cfg; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Backlog XXX() { if (NO_KEYS.equals(range)) { return Backlog.of(BigDecimal.ZERO); } if (lastAttemptedKey == null) { return Backlog.of(BigDecimal.ONE); } if (lastAttemptedKey.isEmpty() || !(range.getEndKey().isEmpty() || range.getEndKey().compareTo(lastAttemptedKey) > 0)) { return Backlog.of(BigDecimal.ZERO); } return Backlog.of(BigDecimal.valueOf(range.estimateFractionForKey(lastAttemptedKey))); }', 'ground_truth': 'public double getSize() { if (NO_KEYS.equals(range)) { return 0; } if (lastAttemptedKey == null) { return 1; } if (lastAttemptedKey.isEmpty() || !(range.getEndKey().isEmpty() || range.getEndKey().compareTo(lastAttemptedKey) > 0)) { return 0; } return range.estimateFractionForKey(lastAttemptedKey); }', 'output': 'public Backlog getBacklog() { if (NO_KEYS.equals(range)) { return Backlog.of(BigDecimal.ZERO); } if (lastAttemptedKey == null) { return Backlog.of(BigDecimal.ONE); } if (lastAttemptedKey.isEmpty() || !(range.getEndKey().isEmpty() || range.getEndKey().compareTo(lastAttemptedKey) > 0)) { return Backlog.of(BigDecimal.ZERO); } return Backlog.of(BigDecimal.valueOf(range.estimateFractionForKey(lastAttemptedKey))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() { List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, RANGE_1, "one", RANGE_2, "one", RANGE_3, "one"), treeResponse(addr2, RANGE_1, "two", RANGE_2, "two", RANGE_3, "two"), treeResponse(addr3, RANGE_1, "three", RANGE_2, "three", RANGE_3, "three")); Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createOptimisedSyncingSyncTasks(JOB_DESC, treeResponses, addr1, noTransient(), addr -> "DC1", false, PreviewKind.ALL)); for (SyncNodePair pair : new SyncNodePair[]{ pair(addr1, addr2), pair(addr1, addr3), pair(addr2, addr1), pair(addr2, addr3), pair(addr3, addr1), pair(addr3, addr2) }) { assertThat(tasks.get(pair)).hasRanges(RANGE_1, RANGE_2, RANGE_3); } }', 'ground_truth': 'public void testOptimisedCreateStandardSyncTasksAllDifferent() { List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, RANGE_1, "one", RANGE_2, "one", RANGE_3, "one"), treeResponse(addr2, RANGE_1, "two", RANGE_2, "two", RANGE_3, "two"), treeResponse(addr3, RANGE_1, "three", RANGE_2, "three", RANGE_3, "three")); Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createOptimisedSyncingSyncTasks(JOB_DESC, treeResponses, addr1, noTransient(), addr -> "DC1", false, PreviewKind.ALL)); for (SyncNodePair pair : new SyncNodePair[]{ pair(addr1, addr2), pair(addr1, addr3), pair(addr2, addr1), pair(addr2, addr3), pair(addr3, addr1), pair(addr3, addr2) }) { assertThat(tasks.get(pair)).hasRanges(RANGE_1, RANGE_2, RANGE_3); } }', 'output': 'public void testMultipleAddresses() { List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, RANGE_1, "one", RANGE_2, "one", RANGE_3, "one"), treeResponse(addr2, RANGE_1, "two", RANGE_2, "two", RANGE_3, "two"), treeResponse(addr3, RANGE_1, "three", RANGE_2, "three", RANGE_3, "three")); Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createOptimisedSyncingSyncTasks(JOB_DESC, treeResponses, addr1, noTransient(), addr -> "DC1", false, PreviewKind.ALL)); for (SyncNodePair pair : new SyncNodePair[]{ pair(addr1, addr2), pair(addr1, addr3), pair(addr2, addr1), pair(addr2, addr3), pair(addr3, addr1), pair(addr3, addr2) }) { assertThat(tasks.get(pair)).hasRanges(RANGE_1, RANGE_2, RANGE_3); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected boolean XXX() { return true; }', 'ground_truth': 'protected boolean respectTombstoneThresholds() { return true; }', 'output': 'protected boolean isInitialized() { return true; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected ReadCommand XXX(int pageSize) throws RequestExecutionException { DataLimits limits; DataRange fullRange = ((PartitionRangeReadCommand)command).dataRange(); DataRange pageRange; if (lastReturnedKey == null) { pageRange = fullRange; limits = command.limits().forPaging(pageSize); } else { boolean includeLastKey = remainingInPartition() > 0 && lastReturnedRow != null; AbstractBounds<PartitionPosition> bounds = makeKeyBounds(lastReturnedKey, includeLastKey); if (includeLastKey) { pageRange = fullRange.forPaging(bounds, command.metadata().comparator, lastReturnedRow.clustering(command.metadata()), false); limits = command.limits().forPaging(pageSize, lastReturnedKey.getKey(), remainingInPartition()); } else { pageRange = fullRange.forSubRange(bounds); limits = command.limits().forPaging(pageSize); } } return ((PartitionRangeReadCommand) command).withUpdatedLimitsAndDataRange(limits, pageRange); }', 'ground_truth': 'protected PartitionRangeReadQuery nextPageReadQuery(int pageSize) { DataLimits limits; DataRange fullRange = query.dataRange(); DataRange pageRange; if (lastReturnedKey == null) { pageRange = fullRange; limits = query.limits().forPaging(pageSize); } else { boolean includeLastKey = remainingInPartition() > 0 && lastReturnedRow != null; AbstractBounds<PartitionPosition> bounds = makeKeyBounds(lastReturnedKey, includeLastKey); if (includeLastKey) { pageRange = fullRange.forPaging(bounds, query.metadata().comparator, lastReturnedRow.clustering(query.metadata()), false); limits = query.limits().forPaging(pageSize, lastReturnedKey.getKey(), remainingInPartition()); } else { pageRange = fullRange.forSubRange(bounds); limits = query.limits().forPaging(pageSize); } } return query.withUpdatedLimitsAndDataRange(limits, pageRange); }', 'output': 'protected ReadCommand read(int pageSize) throws RequestExecutionException { DataLimits limits; DataRange fullRange = ((PartitionRangeReadCommand)command).dataRange(); DataRange pageRange; if (lastReturnedKey == null) { pageRange = fullRange; limits = command.limits().forPaging(pageSize); } else { boolean includeLastKey = remainingInPartition() > 0 && lastReturnedRow != null; AbstractBounds<PartitionPosition> bounds = makeKeyBounds(lastReturnedKey, includeLastKey); if (includeLastKey) { pageRange = fullRange.forPaging(bounds, command.metadata().comparator, lastReturnedRow.clustering(command.metadata()), false); limits = command.limits().forPaging(pageSize, lastReturnedKey.getKey(), remainingInPartition()); } else { pageRange = fullRange.forSubRange(bounds); limits = command.limits().forPaging(pageSize); } } return ((PartitionRangeReadCommand) command).withUpdatedLimitsAndDataRange(limits, pageRange); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Throwable { List<RowMutation> rms = new LinkedList<RowMutation>(); RowMutation rm = new RowMutation(tablename, "key"); rm.add(new QueryPath(cfname, null, "Column1".getBytes()), "asdf".getBytes(), 0); rms.add(rm); ColumnFamilyStoreUtils.writeColumnFamily(rms); ColumnFamilyStore store = ColumnFamilyStoreUtils.writeColumnFamily(rms); MerkleTree old = aes.getCachedTree(tablename, cfname, LOCAL); CompactionManager.instance.submitMajor(store, 0).get(5000, TimeUnit.MILLISECONDS); flushAES().get(5000, TimeUnit.MILLISECONDS); assert old != aes.getCachedTree(tablename, cfname, LOCAL); }', 'ground_truth': 'public void testTreeStore() throws Throwable { List<RowMutation> rms = new LinkedList<RowMutation>(); RowMutation rm = new RowMutation(tablename, "key"); rm.add(new QueryPath(cfname, null, "Column1".getBytes()), "asdf".getBytes(), 0); rms.add(rm); ColumnFamilyStoreUtils.writeColumnFamily(rms); ColumnFamilyStore store = ColumnFamilyStoreUtils.writeColumnFamily(rms); TreePair old = aes.getRendezvousPair(tablename, cfname, REMOTE); CompactionManager.instance.submitReadonly(store, REMOTE).get(5000, TimeUnit.MILLISECONDS); flushAES().get(5000, TimeUnit.MILLISECONDS); assert old != aes.getRendezvousPair(tablename, cfname, REMOTE); }', 'output': 'public void testWriteColumnFamily() throws Throwable { List<RowMutation> rms = new LinkedList<RowMutation>(); RowMutation rm = new RowMutation(tablename, "key"); rm.add(new QueryPath(cfname, null, "Column1".getBytes()), "asdf".getBytes(), 0); rms.add(rm); ColumnFamilyStoreUtils.writeColumnFamily(rms); ColumnFamilyStore store = ColumnFamilyStoreUtils.writeColumnFamily(rms); MerkleTree old = aes.getCachedTree(tablename, cfname, LOCAL); CompactionManager.instance.submitMajor(store, 0).get(5000, TimeUnit.MILLISECONDS); flushAES().get(5000, TimeUnit.MILLISECONDS); assert old != aes.getCachedTree(tablename, cfname, LOCAL); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public MetadataCollector XXX(long rowSize) { estimatedRowSize.add(rowSize); return this; }', 'ground_truth': 'public MetadataCollector addPartitionSizeInBytes(long partitionSize) { estimatedPartitionSize.add(partitionSize); return this; }', 'output': 'public MetadataCollector addEstimatedRowSize(long rowSize) { estimatedRowSize.add(rowSize); return this; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws UnknownHostException { failThreshold(CQL_TABLE_SCAN); }', 'ground_truth': 'public void failThresholdScanTrackingEnabled() throws UnknownHostException { failThresholdEnabled(CQL_TABLE_SCAN); }', 'output': 'public void testTableScanOnUnknownHost() throws UnknownHostException { failThreshold(CQL_TABLE_SCAN); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected Statement XXX(FieldNode field, Expression fieldExpression) { return new ExpressionStatement( new MethodCallExpression( new FieldExpression(pcsField), "firePropertyChange", new ArgumentListExpression( new Expression[]{ new ConstantExpression(field.getName()), fieldExpression, new BinaryExpression( fieldExpression, Token.newSymbol(Types.EQUAL, 0, 0), new VariableExpression("value"))}))); }', 'ground_truth': 'protected Statement createBindableStatement(FieldNode field, Expression fieldExpression) { return new ExpressionStatement( new MethodCallExpression( new FieldExpression(pcsField), "firePropertyChange", new ArgumentListExpression( new Expression[]{ new ConstantExpression(field.getName()), fieldExpression, new BinaryExpression( fieldExpression, Token.newSymbol(Types.EQUAL, 0, 0), new VariableExpression("value"))}))); }', 'output': 'protected Statement firePropertyChange(FieldNode field, Expression fieldExpression) { return new ExpressionStatement( new MethodCallExpression( new FieldExpression(pcsField), "firePropertyChange", new ArgumentListExpression( new Expression[]{ new ConstantExpression(field.getName()), fieldExpression, new BinaryExpression( fieldExpression, Token.newSymbol(Types.EQUAL, 0, 0), new VariableExpression("value"))}))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public synchronized void XXX() { if (!initialised) { fillMethodIndex(); addProperties(); initialised = true; } if (reflector == null) { generateReflector(); } }', 'ground_truth': 'public synchronized void initialise() { if (!initialised) { fillMethodIndex(); addProperties(); initialised = true; } if (reflector == null) { generateReflector(); } }', 'output': 'public synchronized void init() { if (!initialised) { fillMethodIndex(); addProperties(); initialised = true; } if (reflector == null) { generateReflector(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void XXX(Iterator<OnDiskAtom> columns, PrintStream out, AbstractType<?> comparator, CFMetaData cfMetaData) { while (columns.hasNext()) { writeJSON(out, serializeColumn(columns.next(), comparator, cfMetaData)); if (columns.hasNext()) out.print(", "); } } private static List<Object> serializeColumn(OnDiskAtom column, AbstractType<?> comparator, CFMetaData cfMetaData)\n    { { if (column instanceof IColumn) { return serializeColumn((IColumn)column, comparator, cfMetaData); } else { assert column instanceof RangeTombstone; RangeTombstone rt = (RangeTombstone)column; ArrayList<Object> serializedColumn = new ArrayList<Object>(); serializedColumn.add(comparator.getString(rt.min)); serializedColumn.add(comparator.getString(rt.max)); serializedColumn.add(rt.data.markedForDeleteAt); serializedColumn.add("t"); serializedColumn.add(rt.data.localDeletionTime); return serializedColumn; } }', 'ground_truth': 'private static List<Object> serializeAtom(OnDiskAtom atom, AbstractType<?> comparator, CFMetaData cfMetaData) { if (atom instanceof Column) { return serializeColumn((Column)atom, comparator, cfMetaData); } else { assert atom instanceof RangeTombstone; RangeTombstone rt = (RangeTombstone)atom; ArrayList<Object> serializedColumn = new ArrayList<Object>(); serializedColumn.add(comparator.getString(rt.min)); serializedColumn.add(comparator.getString(rt.max)); serializedColumn.add(rt.data.markedForDeleteAt); serializedColumn.add("t"); serializedColumn.add(rt.data.localDeletionTime); return serializedColumn; } }', 'output': 'private static List<Object> serializeColumn(OnDiskAtom column, AbstractType<?> comparator, CFMetaData cfMetaData) { if (column instanceof IColumn) { return serializeColumn((IColumn)column, comparator, cfMetaData); } else { assert column instanceof RangeTombstone; RangeTombstone rt = (RangeTombstone)column; List<Object> serializedColumn = new ArrayList<Object>(); serializedColumn.add(comparator.getString(rt.min)); serializedColumn.add(comparator.getString(rt.max)); serializedColumn.add(rt.data.markedForDeleteAt); serializedColumn.add("t"); serializedColumn.add(rt.data.localDeletionTime); return serializedColumn; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public TrackActionResponse XXX(HostRequest request) throws AmbariException { return null; } public TrackActionResponse XXXComponent(ServiceComponentHostRequest request) throws AmbariException { return null; }', 'ground_truth': 'public TrackActionResponse deleteComponents( Set<ServiceComponentRequest> request) throws AmbariException { return null; }', 'output': 'public TrackActionResponse trackAction(HostRequest request) throws AmbariException { return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void XXX(DataStorageSpec.LongBytesBound spec) { conf.min_tracked_partition_size_bytes = spec; }', 'ground_truth': 'public static void setMinTrackedPartitionSizeInBytes(DataStorageSpec.LongBytesBound spec) { conf.min_tracked_partition_size = spec; }', 'output': 'public static void setMinTrackedPartitionSizeBytes(DataStorageSpec.LongBytesBound spec) { conf.min_tracked_partition_size_bytes = spec; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<String> XXX() throws TException { TMessage msg = iprot_.readMessageBegin(); if (msg.type == TMessageType.EXCEPTION) { TApplicationException x = TApplicationException.read(iprot_); iprot_.readMessageEnd(); throw x; } getStringListProperty_result result = new getStringListProperty_result(); result.read(iprot_); iprot_.readMessageEnd(); if (result.isSetSuccess()) { return result.success; } throw new TApplicationException(TApplicationException.MISSING_RESULT, "getStringListProperty failed: unknown result"); }', 'ground_truth': 'public List<String> recv_get_string_list_property() throws TException { TMessage msg = iprot_.readMessageBegin(); if (msg.type == TMessageType.EXCEPTION) { TApplicationException x = TApplicationException.read(iprot_); iprot_.readMessageEnd(); throw x; } get_string_list_property_result result = new get_string_list_property_result(); result.read(iprot_); iprot_.readMessageEnd(); if (result.isSetSuccess()) { return result.success; } throw new TApplicationException(TApplicationException.MISSING_RESULT, "get_string_list_property failed: unknown result"); }', 'output': 'public List<String> getStringListProperty(String name) throws TException { TMessage msg = iprot_.readMessageBegin(); if (msg.type == TMessageType.EXCEPTION) { TApplicationException x = TApplicationException.read(iprot_); iprot_.readMessageEnd(); throw x; } getStringListProperty_result result = new getStringListProperty_result(); result.read(iprot_); iprot_.readMessageEnd(); if (result.isSetSuccess()) { return result.success; } throw new TApplicationException(TApplicationException.MISSING_RESULT, "getStringListProperty failed: unknown result"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected Job XXX(final String opId) throws ItemNotFound { List<StoredOperationHandle> operationHandles = operationHandleResourceManager.readAll(new FilteringStrategy() { @Override public boolean isConform(Indexed item) { StoredOperationHandle opHandle = (StoredOperationHandle) item; return opHandle.getGuid().equals(opId); } @Override public String whereStatement() { return "guid=\'" + opId + "\'"; } }); if (operationHandles.size() != 1) throw new ItemNotFound(); return viewJobResourceManager.read(operationHandles.get(0).getJobId()); }', 'ground_truth': 'private StoredOperationHandle getStoredOperationHandleByGuid(final String optId) throws ItemNotFound { LOG.debug("stored procedure with operation id : {} in DB", optId); List<StoredOperationHandle> operationHandles = operationHandleResourceManager.readAll(new FilteringStrategy() { @Override public boolean isConform(Indexed item) { StoredOperationHandle soh = (StoredOperationHandle) item; return soh.getGuid().equals(optId); } @Override public String whereStatement() { return " guid = \'" + optId + "\'"; } }); if (null != operationHandles && !operationHandles.isEmpty()) { return operationHandles.get(0); } throw new ItemNotFound(String.format("Stored operation handle with id %s not found", optId)); }', 'output': 'protected Job readJob(final String opId) throws ItemNotFound { List<StoredOperationHandle> operationHandles = operationHandleResourceManager.readAll(new FilteringStrategy() { @Override public boolean isConform(Indexed item) { StoredOperationHandle opHandle = (StoredOperationHandle) item; return opHandle.getGuid().equals(opId); } @Override public String whereStatement() { return "guid=\'" + opId + "\'"; } }); if (operationHandles.size() != 1) throw new ItemNotFound(); return viewJobResourceManager.read(operationHandles.get(0).getJobId()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void XXX() throws Exception { try (@SuppressWarnings("unused") AutoCloseable bundleFactoryCloser = stageBundleFactory; @SuppressWarnings("unused") AutoCloseable closable = stageContext) {} stageContext = null; super.XXX(); }', 'ground_truth': 'public void dispose() throws Exception { super.dispose(); try (@SuppressWarnings("unused") AutoCloseable bundleFactoryCloser = stageBundleFactory; @SuppressWarnings("unused") AutoCloseable closable = stageContext) {} stageContext = null; }', 'output': 'public void start() throws Exception { try (@SuppressWarnings("unused") AutoCloseable bundleFactoryCloser = stageBundleFactory; @SuppressWarnings("unused") AutoCloseable closable = stageContext) {} stageContext = null; super.start(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void fairMerge(long prevId, long pageId, long nextId, ByteBuffer nextBuf) throws IgniteCheckedException { try (Page prev = page(prevId)) { ByteBuffer prevBuf = prev.getForWrite(); try { assert getPageId(prevBuf) == prevId; PagesListNodeIO prevIO = PagesListNodeIO.VERSIONS.forPage(prevBuf); PagesListNodeIO nextIO = PagesListNodeIO.VERSIONS.forPage(nextBuf); assert prevIO.getNextId(prevBuf) == pageId; assert nextIO.getPreviousId(nextBuf) == pageId; prevIO.setNextId(prevBuf, nextId); nextIO.setPreviousId(nextBuf, prevId); } finally { prev.releaseWrite(true); } } }', 'ground_truth': 'private void fairMerge(long prevId, long pageId, long nextId, Page next, ByteBuffer nextBuf) throws IgniteCheckedException { try (Page prev = page(prevId)) { ByteBuffer prevBuf = prev.getForWrite(); try { assert getPageId(prevBuf) == prevId; PagesListNodeIO prevIO = PagesListNodeIO.VERSIONS.forPage(prevBuf); PagesListNodeIO nextIO = PagesListNodeIO.VERSIONS.forPage(nextBuf); assert prevIO.getNextId(prevBuf) == pageId; assert nextIO.getPreviousId(nextBuf) == pageId; prevIO.setNextId(prevBuf, nextId); if (isWalDeltaRecordNeeded(wal, prev)) wal.log(new PagesListSetNextRecord(cacheId, prevId, nextId)); nextIO.setPreviousId(nextBuf, prevId); if (isWalDeltaRecordNeeded(wal, next)) wal.log(new PagesListSetPreviousRecord(cacheId, nextId, prevId)); } finally { prev.releaseWrite(true); } } }', 'output': 'private void fairMerge(long prevId, long pageId, long nextId, ByteBuffer nextBuf) throws IgniteCheckedException { try (Page prev = page(prevId)) { ByteBuffer prevBuf = prev.getForWrite(); try { assert getPageId(prevBuf) == prevId; PagesListNodeIO prevIO = PagesListNodeIO.VERSIONS.forPage(prevBuf); PagesListNodeIO nextIO = PagesListNodeIO.VERSIONS.forPage(nextBuf); assert prevIO.getNextId(prevBuf) == pageId; assert nextIO.getPreviousId(nextBuf) == pageId; prevIO.setNextId(prevBuf, nextId); nextIO.setPreviousId(nextBuf, prevId); } finally { prev.releaseWrite(true); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public AlertSummaryDTO(Number ok, Number warning, Number critical, Number unknown) { okCount = null == ok ? 0 : ok.intValue(); warningCount = null == warning ? 0 : warning.intValue(); criticalCount = null == critical ? 0 : critical.intValue(); unknownCount = null == unknown ? 0 : unknown.intValue(); }', 'ground_truth': 'public AlertSummaryDTO(Number ok, Number warning, Number critical, Number unknown, Number maintenance) { okCount = null == ok ? 0 : ok.intValue(); warningCount = null == warning ? 0 : warning.intValue(); criticalCount = null == critical ? 0 : critical.intValue(); unknownCount = null == unknown ? 0 : unknown.intValue(); maintenanceCount = null == maintenance ? 0 : maintenance.intValue(); }', 'output': 'public AlertSummaryDTO(Integer ok, Integer warning, Integer critical, Integer unknown) { okCount = ok == null ? 0 : ok.intValue(); warningCount = warning == null ? 0 : warning.intValue(); criticalCount = critical == null ? 0 : critical.intValue(); unknownCount = unknown == null ? 0 : unknown.intValue(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private PartitionEvictionTask( GridDhtLocalPartition part, GroupEvictionContext grpEvictionCtx, EvictReason reason ) { this.part = part; this.grpEvictionCtx = grpEvictionCtx; this.reason = reason; size = part.fullSize(); }', 'ground_truth': 'private PartitionEvictionTask( GridDhtLocalPartition part, GroupEvictionContext grpEvictionCtx, EvictReason reason, GridFutureAdapter<?> finishFut ) { this.part = part; this.grpEvictionCtx = grpEvictionCtx; this.reason = reason; this.finishFut = finishFut; }', 'output': 'private PartitionEvictionTask( GridDhtLocalPartition part, GroupEvictionContext grpEvictionCtx, EvictReason reason, Collection<ClusterNode> evictedNodes ) { this.part = part; this.grpEvictionCtx = grpEvictionCtx; this.reason = reason; size = part.fullSize(); this.evictedNodes = evictedNodes; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RegistrationResponse register(Register message) throws WebApplicationException, InvalidStateTransitionException { /* Call into the heartbeat handler */ RegistrationResponse response = null; try { response = hh.handleRegistration(message); LOG.debug("Sending registration response " + response); } catch (AmbariException ex) { response = new RegistrationResponse(); response.setResponseId(-1); response.setResponseStatus(RegistrationStatus.FAILED); response.setExitstatus(1); response.setLog(ex.getMessage()); return response; } return response; }', 'ground_truth': 'public RegistrationResponse register(@Header String simpSessionId, Register message) throws WebApplicationException, InvalidStateTransitionException, AmbariException { /* Call into the heartbeat handler */ RegistrationResponse response = null; try { response = hh.handleRegistration(message); LOG.debug("Sending registration response " + response); } catch (AmbariException ex) { response = new RegistrationResponse(); response.setResponseId(-1); response.setResponseStatus(RegistrationStatus.FAILED); response.setExitstatus(1); response.setLog(ex.getMessage()); return response; } stateUpdateEventPublisher.publish(hh.getInitialClusterTopology()); return response; }', 'output': 'public RegistrationResponse register(Register message) throws WebApplicationException, InvalidStateTransitionException { /* Call into the heartbeat handler */ RegistrationResponse response = null; try { response = hh.handleRegistration(message); LOG.debug("Sending registration response " + response); } catch (AmbariException ex) { response = new RegistrationResponse(); response.setResponseId(-1); response.setResponseStatus(RegistrationStatus.FAILED); response.setExitstatus(1); response.setLog(ex.getMessage()); return response; } return response; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void lockLocally( final Collection<KeyCacheObject> keys, XX XXX, @Nullable final Deque<GridNearLockMapping> mappings ) { if (log.isDebugEnabled()) log.debug("Before locally locking keys : " + keys); IgniteInternalFuture<Exception> fut = cctx.colocated().lockAllAsync(cctx, tx, threadId, lockVer, topVer, keys, read, retval, timeout, accessTtl, filter, skipStore); add(new GridEmbeddedFuture<>( new C2<Exception, Exception, Boolean>() { @Override public Boolean apply(Exception resEx, Exception e) { if (CU.isLockTimeoutOrCancelled(e) || (resEx != null && CU.isLockTimeoutOrCancelled(resEx))) return false; if (e != null) { onError(e); return false; } if (resEx != null) { onError(resEx); return false; } if (log.isDebugEnabled()) log.debug("Acquired lock for local DHT mapping [locId=" + cctx.nodeId() + ", mappedKeys=" + keys + ", fut=" + GridDhtColocatedLockFuture.this + \']\'); if (inTx()) { for (KeyCacheObject key : keys) tx.entry(cctx.txKey(key)).markLocked(); } else { for (KeyCacheObject key : keys) cctx.mvcc().markExplicitOwner(cctx.txKey(key), threadId); } try { if (mappings != null) proceedMapping(mappings); } catch (IgniteCheckedException ex) { onError(ex); return false; } return true; } }, fut)); }', 'ground_truth': 'private void lockLocally( final Collection<KeyCacheObject> keys, AffinityTopologyVersion topVer ) { if (log.isDebugEnabled()) log.debug("Before locally locking keys : " + keys); IgniteInternalFuture<Exception> fut = cctx.colocated().lockAllAsync(cctx, tx, threadId, lockVer, topVer, keys, read, retval, timeout, accessTtl, filter, skipStore); add(new GridEmbeddedFuture<>( new C2<Exception, Exception, Boolean>() { @Override public Boolean apply(Exception resEx, Exception e) { if (CU.isLockTimeoutOrCancelled(e) || (resEx != null && CU.isLockTimeoutOrCancelled(resEx))) return false; if (e != null) { onError(e); return false; } if (resEx != null) { onError(resEx); return false; } if (log.isDebugEnabled()) log.debug("Acquired lock for local DHT mapping [locId=" + cctx.nodeId() + ", mappedKeys=" + keys + ", fut=" + GridDhtColocatedLockFuture.this + \']\'); if (inTx()) { for (KeyCacheObject key : keys) tx.entry(cctx.txKey(key)).markLocked(); } else { for (KeyCacheObject key : keys) cctx.mvcc().markExplicitOwner(cctx.txKey(key), threadId); } try { if (mappings != null) proceedMapping(); } catch (IgniteCheckedException ex) { onError(ex); return false; } return true; } }, fut)); }', 'output': 'private void lockLocally( final Collection<KeyCacheObject> keys, IgniteInternalFuture<Exception> fut, @Nullable final Deque<GridNearLockMapping> mappings ) { if (log.isDebugEnabled()) log.debug("Before locally locking keys : " + keys); IgniteInternalFuture<Exception> f = cctx.colocated().lockAllAsync(cctx, tx, threadId, lockVer, topVer, keys, read, retval, timeout, accessTtl, filter, skipStore); add(new GridEmbeddedFuture<>( new C2<Exception, Exception, Boolean>() { @Override public Boolean apply(Exception resEx, Exception e) { if (CU.isLockTimeoutOrCancelled(e) || (resEx != null && CU.isLockTimeoutOrCancelled(resEx))) return false; if (e != null) { onError(e); return false; } if (resEx != null) { onError(resEx); return false; } if (log.isDebugEnabled()) log.debug("Acquired lock for local DHT mapping [locId=" + cctx.nodeId() + ", mappedKeys=" + keys + ", fut=" + GridDhtColocatedLockFuture.this + \']\'); if (inTx()) { for (KeyCacheObject key : keys) tx.entry(cctx.txKey(key)).markLocked(); } else { for (KeyCacheObject key : keys) cctx.mvcc().markExplicitOwner(cctx.txKey(key), threadId); } try { if (mappings != null) proceedMapping(mappings); } catch (IgniteCheckedException ex) { onError(ex); return false; } return true; } }, f)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public <T> ListStateStorage<T> getListStorageFor(Class<T> what) { try { initKryo(); return new ListStorage(MAX_ELEMENTS_IN_MEMORY, kryo, serializers); } catch (IOException ex) { throw new RuntimeException(ex); } }', 'ground_truth': 'public <T> ListStateStorage<T> getListStorage( String name, Class<T> what) { try { initKryo(); return new ListStorage(MAX_ELEMENTS_IN_MEMORY, kryo, serializers); } catch (IOException ex) { throw new RuntimeException(ex); } }', 'output': 'public <T> ListStateStorage<T> getListStorageFor(Class<T> what, int maxElementsInMemory) { try { initKryo(); return new ListStorage(maxElementsInMemory, kryo, serializers); } catch (IOException ex) { throw new RuntimeException(ex); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void incrementalRepair(UUID parentSession, long startTime, TraceState traceState, NeighborsAndRanges neighborsAndRanges, String... cfnames) { Set<InetAddressAndPort> allParticipants = ImmutableSet.<InetAddressAndPort>builder() .addAll(neighborsAndRanges.participants) .add(FBUtilities.getBroadcastAddressAndPort()) .build(); List<CommonRange> allRanges = neighborsAndRanges.filterCommonRanges(keyspace, cfnames); CoordinatorSession coordinatorSession = ActiveRepairService.instance.consistent.coordinated.registerSession(parentSession, allParticipants, neighborsAndRanges.shouldExcludeDeadParticipants); ListeningExecutorService executor = createExecutor(); AtomicBoolean hasFailure = new AtomicBoolean(false); ListenableFuture repairResult = coordinatorSession.execute(() -> submitRepairSessions(parentSession, true, executor, allRanges, cfnames), hasFailure); Collection<Range<Token>> ranges = new HashSet<>(); for (Collection<Range<Token>> range : Iterables.transform(allRanges, cr -> cr.ranges)) { ranges.addAll(range); } Futures.addCallback(repairResult, new RepairCompleteCallback(parentSession, ranges, startTime, traceState, hasFailure, executor), MoreExecutors.directExecutor()); }', 'ground_truth': 'private void incrementalRepair(UUID parentSession, long startTime, TraceState traceState, NeighborsAndRanges neighborsAndRanges, Set<InetAddressAndPort> preparedEndpoints, String... cfnames) { Set<InetAddressAndPort> allParticipants = ImmutableSet.<InetAddressAndPort>builder() .addAll(neighborsAndRanges.participants) .add(FBUtilities.getBroadcastAddressAndPort()) .build(); List<CommonRange> allRanges = neighborsAndRanges.filterCommonRanges(keyspace, cfnames); CoordinatorSession coordinatorSession = ActiveRepairService.instance.consistent.coordinated.registerSession(parentSession, allParticipants, neighborsAndRanges.shouldExcludeDeadParticipants); ListeningExecutorService executor = createExecutor(); AtomicBoolean hasFailure = new AtomicBoolean(false); ListenableFuture repairResult = coordinatorSession.execute(() -> submitRepairSessions(parentSession, true, executor, allRanges, cfnames), hasFailure); Collection<Range<Token>> ranges = new HashSet<>(); for (Collection<Range<Token>> range : Iterables.transform(allRanges, cr -> cr.ranges)) { ranges.addAll(range); } Futures.addCallback(repairResult, new RepairCompleteCallback(parentSession, ranges, preparedEndpoints, startTime, traceState, hasFailure, executor), MoreExecutors.directExecutor()); }', 'output': 'private void incrementalRepair(UUID parentSession, long startTime, TraceState traceState, NeighborsAndRanges neighborsAndRanges, String... cfnames) { Set<InetAddressAndPort> allParticipants = ImmutableSet.<InetAddressAndPort>builder() .addAll(neighborsAndRanges.participants) .add(FBUtilities.getBroadcastAddressAndPort()) .build(); List<CommonRange> allRanges = neighborsAndRanges.filterCommonRanges(keyspace, cfnames); CoordinatorSession coordinatorSession = ActiveRepairService.instance.consistent.coordinated.registerSession(parentSession, allParticipants, neighborsAndRanges.shouldExcludeDeadParticipants); ListeningExecutorService executor = createExecutor(); AtomicBoolean hasFailure = new AtomicBoolean(false); ListenableFuture repairResult = coordinatorSession.execute(() -> submitRepairSessions(parentSession, true, executor, allRanges, cfnames), hasFailure); Collection<Range<Token>> ranges = new HashSet<>(); for (Collection<Range<Token>> range : Iterables.transform(allRanges, cr -> cr.ranges)) { ranges.addAll(range); } Futures.addCallback(repairResult, new RepairCompleteCallback(parentSession, ranges, startTime, traceState, hasFailure, executor), MoreExecutors.directExecutor()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private ReadQuery getRangeCommand(QueryOptions options, ColumnFilter columnFilter, DataLimits limit, int nowInSec) { ClusteringIndexFilter clusteringIndexFilter = makeClusteringIndexFilter(options, columnFilter); if (clusteringIndexFilter == null) return ReadQuery.empty(table); RowFilter rowFilter = getRowFilter(options); AbstractBounds<PartitionPosition> keyBounds = restrictions.getPartitionKeyBounds(options); if (keyBounds == null) return ReadQuery.empty(table); ReadQuery command = PartitionRangeReadQuery.create(table, nowInSec, columnFilter, rowFilter, limit, new DataRange(keyBounds, clusteringIndexFilter)); command.maybeValidateIndex(); return command; }', 'ground_truth': 'private ReadQuery getRangeCommand(QueryOptions options, ClientState state, ColumnFilter columnFilter, DataLimits limit, int nowInSec) { ClusteringIndexFilter clusteringIndexFilter = makeClusteringIndexFilter(options, state, columnFilter); if (clusteringIndexFilter == null) return ReadQuery.empty(table); RowFilter rowFilter = getRowFilter(options); AbstractBounds<PartitionPosition> keyBounds = restrictions.getPartitionKeyBounds(options); if (keyBounds == null) return ReadQuery.empty(table); ReadQuery command = PartitionRangeReadQuery.create(table, nowInSec, columnFilter, rowFilter, limit, new DataRange(keyBounds, clusteringIndexFilter)); command.maybeValidateIndex(); return command; }', 'output': 'private ReadQuery getRangeCommand(QueryOptions options, ColumnFilter columnFilter, DataLimits limit, long nowInSec) { ClusteringIndexFilter clusteringIndexFilter = makeClusteringIndexFilter(options, columnFilter); if (clusteringIndexFilter == null) return ReadQuery.empty(table); RowFilter rowFilter = getRowFilter(options); AbstractBounds<PartitionPosition> keyBounds = restrictions.getPartitionKeyBounds(options); if (keyBounds == null) return ReadQuery.empty(table); ReadQuery command = PartitionRangeReadQuery.create(table, nowInSec, columnFilter, rowFilter, limit, new DataRange(keyBounds, clusteringIndexFilter)); command.maybeValidateIndex(); return command; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void appendDirect(ByteBuffer bb) { try { ByteBuffer toAppend = bb.duplicate(); toAppend.mark(); FBUtilities.directCheckSum(incrementalChecksum, toAppend); toAppend.reset(); incrementalOut.writeInt((int) incrementalChecksum.getValue()); incrementalChecksum.reset(); FBUtilities.directCheckSum(fullChecksum, toAppend); } catch (IOException e) { throw new IOError(e); } }', 'ground_truth': 'public void appendDirect(ByteBuffer bb, boolean checksumIncrementalResult) { try { ByteBuffer toAppend = bb.duplicate(); toAppend.mark(); FBUtilities.directCheckSum(incrementalChecksum, toAppend); toAppend.reset(); int incrementalChecksumValue = (int) incrementalChecksum.getValue(); incrementalOut.writeInt(incrementalChecksumValue); FBUtilities.directCheckSum(fullChecksum, toAppend); if (checksumIncrementalResult) { ByteBuffer byteBuffer = ByteBuffer.allocate(4); byteBuffer.putInt(incrementalChecksumValue); fullChecksum.update(byteBuffer.array(), 0, byteBuffer.array().length); } incrementalChecksum.reset(); } catch (IOException e) { throw new IOError(e); } }', 'output': 'public void appendDirect(ByteBuffer bb, int length) { try { ByteBuffer toAppend = bb.duplicate(); toAppend.mark(); FBUtilities.directCheckSum(incrementalChecksum, toAppend); toAppend.limit(length); incrementalOut.writeInt((int) incrementalChecksum.getValue()); incrementalChecksum.reset(); FBUtilities.directCheckSum(fullChecksum, toAppend); } catch (IOException e) { throw new IOError(e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static ByteBuffer serializeMutations(Collection<Mutation> mutations) { DataOutputBuffer buf = new DataOutputBuffer(); try { buf.writeInt(mutations.size()); for (Mutation mutation : mutations) Mutation.serializer.serialize(mutation, buf, MessagingService.VERSION_12); } catch (IOException e) { throw new AssertionError(); } return buf.asByteBuffer(); }', 'ground_truth': 'private static ByteBuffer serializeMutations(Collection<Mutation> mutations, int version) { DataOutputBuffer buf = new DataOutputBuffer(); try { buf.writeInt(mutations.size()); for (Mutation mutation : mutations) Mutation.serializer.serialize(mutation, buf, version); } catch (IOException e) { throw new AssertionError(); } return buf.asByteBuffer(); }', 'output': 'private static ByteBuffer serializeMutations(List<Mutation> mutations) { DataOutputBuffer buf = new DataOutputBuffer(); try { buf.writeInt(mutations.size()); for (Mutation mutation : mutations) Mutation.serializer.serialize(mutation, buf, MessagingService.VERSION_12); } catch (IOException e) { throw new AssertionError(); } return buf.asByteBuffer(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ParsedStatement.Prepared prepare(boolean forView) throws InvalidRequestException { CFMetaData cfm = ThriftValidation.validateColumnFamily(keyspace(), columnFamily()); VariableSpecifications boundNames = getBoundVariables(); Selection selection = selectClause.isEmpty() ? Selection.wildcard(cfm) : Selection.fromSelectors(cfm, selectClause); StatementRestrictions restrictions = prepareRestrictions(cfm, boundNames, selection, forView); if (parameters.isDistinct) validateDistinctSelection(cfm, selection, restrictions); Comparator<List<ByteBuffer>> orderingComparator = null; boolean isReversed = false; if (!parameters.orderings.isEmpty()) { assert !forView; verifyOrderingIsAllowed(restrictions); orderingComparator = getOrderingComparator(cfm, selection, restrictions); isReversed = isReversed(cfm); if (isReversed) orderingComparator = Collections.reverseOrder(orderingComparator); } checkNeedsFiltering(restrictions); SelectStatement stmt = new SelectStatement(cfm, boundNames.size(), parameters, selection, restrictions, isReversed, orderingComparator, prepareLimit(boundNames)); return new ParsedStatement.Prepared(stmt, boundNames, boundNames.getPartitionKeyBindIndexes(cfm)); }', 'ground_truth': 'public ParsedStatement.Prepared prepare(boolean forView, ClientState clientState) throws InvalidRequestException { CFMetaData cfm = ThriftValidation.validateColumnFamilyWithCompactMode(keyspace(), columnFamily(), clientState.isNoCompactMode()); VariableSpecifications boundNames = getBoundVariables(); Selection selection = selectClause.isEmpty() ? Selection.wildcard(cfm) : Selection.fromSelectors(cfm, selectClause); StatementRestrictions restrictions = prepareRestrictions(cfm, boundNames, selection, forView); if (parameters.isDistinct) validateDistinctSelection(cfm, selection, restrictions); Comparator<List<ByteBuffer>> orderingComparator = null; boolean isReversed = false; if (!parameters.orderings.isEmpty()) { assert !forView; verifyOrderingIsAllowed(restrictions); orderingComparator = getOrderingComparator(cfm, selection, restrictions); isReversed = isReversed(cfm); if (isReversed) orderingComparator = Collections.reverseOrder(orderingComparator); } checkNeedsFiltering(restrictions); SelectStatement stmt = new SelectStatement(cfm, boundNames.size(), parameters, selection, restrictions, isReversed, orderingComparator, prepareLimit(boundNames)); return new ParsedStatement.Prepared(stmt, boundNames, boundNames.getPartitionKeyBindIndexes(cfm)); }', 'output': 'public ParsedStatement.Prepared prepare(boolean forView) throws InvalidRequestException { CFMetaData cfm = ThriftValidation.validateColumnFamily(keyspace(), columnFamily()); VariableSpecifications boundNames = getBoundVariables(); Selection selection = selectClause.isEmpty() ? Selection.wildcard(cfm) : Selection.fromSelectors(cfm, selectClause); StatementRestrictions restrictions = prepareRestrictions(cfm, boundNames, selection, forView); if (parameters.isDistinct) validateDistinctSelection(cfm, selection, restrictions); Comparator<List<ByteBuffer>> orderingComparator = null; boolean isReversed = false; if (!parameters.orderings.isEmpty()) { assert !forView; verifyOrderingIsAllowed(restrictions); orderingComparator = getOrderingComparator(cfm, selection, restrictions); isReversed = isReversed(cfm); if (isReversed) orderingComparator = Collections.reverseOrder(orderingComparator); } checkNeedsFiltering(restrictions); SelectStatement stmt = new SelectStatement(cfm, boundNames.size(), parameters, selection, restrictions, isReversed, orderingComparator, prepareLimit(boundNames)); return new ParsedStatement.Prepared(stmt, boundNames, boundNames.getPartitionKeyBindIndexes(cfm)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public int forceRepairRangeAsync(String beginToken, String endToken, final String keyspaceName, boolean isSequential, Collection<String> dataCenters, final String... columnFamilies) { Token parsedBeginToken = getPartitioner().getTokenFactory().fromString(beginToken); Token parsedEndToken = getPartitioner().getTokenFactory().fromString(endToken); logger.info("starting user-requested repair of range ({}, {}] for keyspace {} and column families {}", parsedBeginToken, parsedEndToken, keyspaceName, columnFamilies); return forceRepairAsync(keyspaceName, isSequential, dataCenters, Collections.singleton(new Range<Token>(parsedBeginToken, parsedEndToken)), columnFamilies); }', 'ground_truth': 'public int forceRepairRangeAsync(String beginToken, String endToken, final String keyspaceName, boolean isSequential, Collection<String> dataCenters, final Collection<String> hosts, final String... columnFamilies) { Token parsedBeginToken = getPartitioner().getTokenFactory().fromString(beginToken); Token parsedEndToken = getPartitioner().getTokenFactory().fromString(endToken); logger.info("starting user-requested repair of range ({}, {}] for keyspace {} and column families {}", parsedBeginToken, parsedEndToken, keyspaceName, columnFamilies); return forceRepairAsync(keyspaceName, isSequential, dataCenters, hosts, Collections.singleton(new Range<Token>(parsedBeginToken, parsedEndToken)), columnFamilies); }', 'output': 'public int forceRepairRangeAsync(String beginToken, String endToken, final String keyspaceName, boolean isSequential, Collection<String> dataCenters, final String... columnFamilies) { Token parsedBeginToken = getPartitioner().getTokenFactory().fromString(beginToken); Token parsedEndToken = getPartitioner().getTokenFactory().fromString(endToken); logger.info("starting user-requested repair of range ({}, {}] for keyspace {} and column families {}", parsedBeginToken, parsedEndToken, keyspaceName, columnFamilies); return forceRepairAsync(keyspaceName, isSequential, dataCenters, Collections.singleton(new Range<Token>(parsedBeginToken, parsedEndToken)), columnFamilies); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private int[] evaluateIntervalLiteralAsSecond( int sign, String value, String originalValue) throws SqlValidatorException { BigDecimal second; BigDecimal secondFrac; boolean hasFractionalSecond; String intervalPatternWithFracSec = "(\\\\d+)\\\\.(\\\\d{1," + fractionalSecondPrecision + "})"; String intervalPatternWithoutFracSec = "(\\\\d+)"; Matcher m = Pattern.compile(intervalPatternWithFracSec).matcher(value); if (m.matches()) { hasFractionalSecond = true; } else { m = Pattern.compile(intervalPatternWithoutFracSec).matcher(value); hasFractionalSecond = false; } if (m.matches()) { try { second = parseField(m, 1); } catch (NumberFormatException e) { throw intervalidValueException(originalValue); } if (hasFractionalSecond) { secondFrac = normalizeSecondFraction(m.group(2)); } else { secondFrac = ZERO; } checkLeadFieldInRange(sign, second, TimeUnit.SECOND); if (!(isFractionalSecondFieldInRange(secondFrac))) { throw intervalidValueException(originalValue); } return fillIntervalValueArray( sign, ZERO, ZERO, ZERO, second, secondFrac); } else { throw intervalidValueException(originalValue); } }', 'ground_truth': 'private int[] evaluateIntervalLiteralAsSecond( int sign, String value, String originalValue, SqlParserPos pos) { BigDecimal second; BigDecimal secondFrac; boolean hasFractionalSecond; String intervalPatternWithFracSec = "(\\\\d+)\\\\.(\\\\d{1," + fractionalSecondPrecision + "})"; String intervalPatternWithoutFracSec = "(\\\\d+)"; Matcher m = Pattern.compile(intervalPatternWithFracSec).matcher(value); if (m.matches()) { hasFractionalSecond = true; } else { m = Pattern.compile(intervalPatternWithoutFracSec).matcher(value); hasFractionalSecond = false; } if (m.matches()) { try { second = parseField(m, 1); } catch (NumberFormatException e) { throw invalidValueException(pos, originalValue); } if (hasFractionalSecond) { secondFrac = normalizeSecondFraction(m.group(2)); } else { secondFrac = ZERO; } checkLeadFieldInRange(sign, second, TimeUnit.SECOND, pos); if (!(isFractionalSecondFieldInRange(secondFrac))) { throw invalidValueException(pos, originalValue); } return fillIntervalValueArray( sign, ZERO, ZERO, ZERO, second, secondFrac); } else { throw invalidValueException(pos, originalValue); } }', 'output': 'private int[] evaluateIntervalLiteralAsSecond( int sign, String value, String originalValue) throws SqlValidatorException { BigDecimal second; BigDecimal secondFrac; boolean hasFractionalSecond; String intervalPatternWithFracSec = "(\\\\d+)\\\\.(\\\\d{1," + fractionalSecondPrecision + "})"; String intervalPatternWithoutFracSec = "(\\\\d+)"; Matcher m = Pattern.compile(intervalPatternWithFracSec).matcher(value); if (m.matches()) { hasFractionalSecond = true; } else { m = Pattern.compile(intervalPatternWithoutFracSec).matcher(value); hasFractionalSecond = false; } if (m.matches()) { try { second = parseField(m, 1); } catch (NumberFormatException e) { throw intervalidValueException(originalValue); } if (hasFractionalSecond) { secondFrac = normalizeSecondFraction(m.group(2)); } else { secondFrac = ZERO; } checkLeadFieldInRange(sign, second, TimeUnit.SECOND); if (!(isFractionalSecondFieldInRange(secondFrac))) { throw intervalidValueException(originalValue); } return fillIntervalValueArray( sign, ZERO, ZERO, ZERO, second, secondFrac); } else { throw intervalidValueException(originalValue); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void writeBinary(BinaryWriterExImpl writer) throws BinaryObjectException { super.writeBinary(writer); writer.writeLong(order); } /** {@inheritDoc} */ @Override public void readBinary(BinaryReaderExImpl reader) throws BinaryObjectException { super.readBinary(reader); order = reader.readLong(); } /** {@inheritDoc} */ @Override public String toString() { return S.toString(JdbcOrderedBatchExecuteResult.class, this); } } }', 'ground_truth': '@Override public void writeBinary(BinaryWriterExImpl writer, ClientListenerProtocolVersion ver) throws BinaryObjectException { super.writeBinary(writer, ver); writer.writeLong(order); }', 'output': '@Override public void writeBinary(BinaryWriterExImpl writer) throws BinaryObjectException { super.writeBinary(writer); writer.writeLong(this.order); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Prepared prepare() { return new Prepared(this); }', 'ground_truth': 'public Prepared prepare(ClientState clientState) { return new Prepared(this); }', 'output': 'public Prepared prepare(String query) { return new Prepared(this, query); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ReuseListImpl(int cacheId, String name, PageMemory pageMem, IgniteWriteAheadLogManager wal, long metaPageId, boolean initNew) throws IgniteCheckedException { super(cacheId, name, pageMem, 1, wal, metaPageId); reuseList = this; init(metaPageId, initNew); }', 'ground_truth': 'public ReuseListImpl( int cacheId, String name, PageMemory pageMem, IgniteWriteAheadLogManager wal, long metaPageId, boolean initNew, PageLockListener lockLsnr ) throws IgniteCheckedException { super( cacheId, name, pageMem, 1, wal, metaPageId, lockLsnr ); reuseList = this; init(metaPageId, initNew); }', 'output': 'public ReuseListImpl(int cacheId, String name, PageMemory pageMem, IgniteWriteAheadLogManager wal, long metaPageId, boolean initNew) throws IgniteCheckedException { super(cacheId, name, pageMem, 1, wal, metaPageId); reuseList = this; init(metaPageId, initNew); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private ClientListenerConnectionContext prepareContext(byte clientType) throws IgniteCheckedException { long connId = nextConnectionId(); switch (clientType) { case ODBC_CLIENT: return new OdbcConnectionContext(ctx, busyLock, connId, maxCursors); case JDBC_CLIENT: return new JdbcConnectionContext(ctx, busyLock, connId, maxCursors); case THIN_CLIENT: return new ClientConnectionContext(ctx, connId, maxCursors, thinCfg); } throw new IgniteCheckedException("Unknown client type: " + clientType); }', 'ground_truth': 'private ClientListenerConnectionContext prepareContext(byte clientType, GridNioSession ses) throws IgniteCheckedException { long connId = nextConnectionId(); switch (clientType) { case ODBC_CLIENT: return new OdbcConnectionContext(ctx, ses, busyLock, connId, maxCursors); case JDBC_CLIENT: return new JdbcConnectionContext(ctx, ses, busyLock, connId, maxCursors); case THIN_CLIENT: return new ClientConnectionContext(ctx, ses, connId, maxCursors, thinCfg); } throw new IgniteCheckedException("Unknown client type: " + clientType); }', 'output': 'private ClientConnectionContext prepareContext(byte clientType) throws IgniteCheckedException { long connId = nextConnectionId(); switch (clientType) { case ODBC_CLIENT: return new OdbcConnectionContext(ctx, busyLock, connId, maxCursors); case JDBC_CLIENT: return new JdbcConnectionContext(ctx, busyLock, connId, maxCursors); case THIN_CLIENT: return new ClientConnectionContext(ctx, connId, maxCursors, thinCfg); } throw new IgniteCheckedException("Unknown client type: " + clientType); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Collection<? extends IMutation> getMutations(BatchQueryOptions options, boolean local, long now) throws RequestExecutionException, RequestValidationException { Set<String> tablesWithZeroGcGs = null; UpdatesCollector collector = new UpdatesCollector(updatedColumns, updatedRows()); for (int i = 0; i < statements.size(); i++) { ModificationStatement statement = statements.get(i); if (isLogged() && statement.cfm.params.gcGraceSeconds == 0) { if (tablesWithZeroGcGs == null) tablesWithZeroGcGs = new HashSet<>(); tablesWithZeroGcGs.add(String.format("%s.%s", statement.cfm.ksName, statement.cfm.cfName)); } QueryOptions statementOptions = options.forStatement(i); long timestamp = attrs.getTimestamp(now, statementOptions); statement.addUpdates(collector, statementOptions, local, timestamp); } if (tablesWithZeroGcGs != null) { String suffix = tablesWithZeroGcGs.size() == 1 ? "" : "s"; NoSpamLogger.log(logger, NoSpamLogger.Level.WARN, 1, TimeUnit.MINUTES, LOGGED_BATCH_LOW_GCGS_WARNING, suffix, tablesWithZeroGcGs); ClientWarn.instance.warn(MessageFormatter.arrayFormat(LOGGED_BATCH_LOW_GCGS_WARNING, new Object[] { suffix, tablesWithZeroGcGs }) .getMessage()); } collector.validateIndexedColumns(); return collector.toMutations(); }', 'ground_truth': 'private Collection<? extends IMutation> getMutations(BatchQueryOptions options, boolean local, long now, long queryStartNanoTime) throws RequestExecutionException, RequestValidationException { Set<String> tablesWithZeroGcGs = null; UpdatesCollector collector = new UpdatesCollector(updatedColumns, updatedRows()); for (int i = 0; i < statements.size(); i++) { ModificationStatement statement = statements.get(i); if (isLogged() && statement.cfm.params.gcGraceSeconds == 0) { if (tablesWithZeroGcGs == null) tablesWithZeroGcGs = new HashSet<>(); tablesWithZeroGcGs.add(String.format("%s.%s", statement.cfm.ksName, statement.cfm.cfName)); } QueryOptions statementOptions = options.forStatement(i); long timestamp = attrs.getTimestamp(now, statementOptions); statement.addUpdates(collector, statementOptions, local, timestamp, queryStartNanoTime); } if (tablesWithZeroGcGs != null) { String suffix = tablesWithZeroGcGs.size() == 1 ? "" : "s"; NoSpamLogger.log(logger, NoSpamLogger.Level.WARN, 1, TimeUnit.MINUTES, LOGGED_BATCH_LOW_GCGS_WARNING, suffix, tablesWithZeroGcGs); ClientWarn.instance.warn(MessageFormatter.arrayFormat(LOGGED_BATCH_LOW_GCGS_WARNING, new Object[] { suffix, tablesWithZeroGcGs }) .getMessage()); } collector.validateIndexedColumns(); return collector.toMutations(); }', 'output': 'private Collection<? extends IMutation> getMutations(BatchQueryOptions options, boolean local, long now) throws RequestExecutionException, RequestValidationException { Set<String> tablesWithZeroGcGs = null; UpdatesCollector collector = new UpdatesCollector(updatedColumns, updatedRows()); for (int i = 0; i < statements.size(); i++) { ModificationStatement statement = statements.get(i); if (isLogged() && statement.cfm.params.gcGraceSeconds == 0) { if (tablesWithZeroGcGs == null) tablesWithZeroGcGs = new HashSet<>(); tablesWithZeroGcGs.add(String.format("%s.%s", statement.cfm.ksName, statement.cfm.cfName)); } QueryOptions statementOptions = options.forStatement(i); long timestamp = attrs.getTimestamp(now, statementOptions); statement.addUpdates(collector, statementOptions, local, timestamp); } if (tablesWithZeroGcGs != null) { String suffix = tablesWithZeroGcGs.size() == 1 ? "" : "s"; NoSpamLogger.log(logger, NoSpamLogger.Level.WARN, 1, TimeUnit.MINUTES, LOGGED_BATCH_LOW_GCGS_WARNING, suffix, tablesWithZeroGcGs); ClientWarn.instance.warn(MessageFormatter.arrayFormat(LOGGED_BATCH_LOW_GCGS_WARNING, new Object[] { suffix, tablesWithZeroGcGs }) .getMessage()); } collector.validateIndexedColumns(); return collector.toMutations(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void write(BinaryRawWriter writer) { writer.writeString(schemaName); writer.writeString(tableName); writer.writeString(columnName); byte typeId = getTypeId(dataType); writer.writeByte(typeId); if (ver.compareTo(OdbcConnectionContext.VER_2_7_0) >= 0) { writer.writeInt(precision); writer.writeInt(scale); } }', 'ground_truth': 'public void write(BinaryRawWriter writer, ClientListenerProtocolVersion ver) { writer.writeString(schemaName); writer.writeString(tableName); writer.writeString(columnName); byte typeId = getTypeId(dataType); writer.writeByte(typeId); if (ver.compareTo(OdbcConnectionContext.VER_2_7_0) >= 0) { writer.writeInt(precision); writer.writeInt(scale); } if (ver.compareTo(OdbcConnectionContext.VER_2_8_0) >= 0) { writer.writeByte((byte)nullability); } }', 'output': 'public void write(BinaryRawWriter writer) { writer.writeString(schemaName); writer.writeString(tableName); writer.writeString(columnName); int typeId = getTypeId(dataType); writer.writeInt(typeId); if (ver.compareTo(OdbcConnectionContext.VER_2_7_0) >= 0) { writer.writeInt(precision); writer.writeInt(scale); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean isPrefixOf(Composite c) { return true; }', 'ground_truth': 'public boolean isPrefixOf(CType type, Composite c) { return true; }', 'output': 'public boolean isPrefixOf(Composite c, int level) { return true; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void makeActionStage(Cluster cluster, RequestStageContainer request, UpgradeItemEntity entity, StageWrapper wrapper) throws AmbariException { Map<String, String> hostLevelParams = new HashMap<String, String>(); hostLevelParams.put(JDK_LOCATION, getManagementController().getJdkResourceUrl()); Stage stage = stageFactory.get().createNew(request.getId().longValue(), "/tmp/ambari", cluster.getClusterName(), cluster.getClusterId(), entity.getText(), "{}", "{}", StageUtils.getGson().toJson(hostLevelParams)); long stageId = request.getLastStageId() + 1; if (0L == stageId) { stageId = 1L; } stage.setStageId(stageId); entity.setStageId(Long.valueOf(stageId)); RequestResourceFilter filter = new RequestResourceFilter("", "", new ArrayList<String>(wrapper.getHosts())); Map<String, String> params = new HashMap<String, String>(); params.put("tasks", entity.getTasks()); ActionExecutionContext actionContext = new ActionExecutionContext( cluster.getClusterName(), "ru_execute_tasks", Collections.singletonList(filter), params); actionContext.setTimeout(Short.valueOf((short)60)); actionExecutionHelper.get().addExecutionCommandsToStage(actionContext, stage); for (Map<String, HostRoleCommand> map : stage.getHostRoleCommands().values()) { for (HostRoleCommand hrc : map.values()) { hrc.setCommandDetail(entity.getText()); } } request.addStages(Collections.singletonList(stage)); }', 'ground_truth': 'private void makeActionStage(Cluster cluster, RequestStageContainer request, final String version, UpgradeItemEntity entity, StageWrapper wrapper) throws AmbariException { Map<String, String> hostLevelParams = new HashMap<String, String>(); hostLevelParams.put(JDK_LOCATION, getManagementController().getJdkResourceUrl()); Stage stage = stageFactory.get().createNew(request.getId().longValue(), "/tmp/ambari", cluster.getClusterName(), cluster.getClusterId(), entity.getText(), "{}", "{}", StageUtils.getGson().toJson(hostLevelParams)); long stageId = request.getLastStageId() + 1; if (0L == stageId) { stageId = 1L; } stage.setStageId(stageId); entity.setStageId(Long.valueOf(stageId)); RequestResourceFilter filter = new RequestResourceFilter("", "", new ArrayList<String>(wrapper.getHosts())); Map<String, String> params = new HashMap<String, String>(); params.put("tasks", entity.getTasks()); ActionExecutionContext actionContext = new ActionExecutionContext( cluster.getClusterName(), "ru_execute_tasks", Collections.singletonList(filter), params); actionContext.setTimeout(Short.valueOf((short)60)); actionExecutionHelper.get().addExecutionCommandsToStage(actionContext, stage); for (Map<String, HostRoleCommand> map : stage.getHostRoleCommands().values()) { for (HostRoleCommand hrc : map.values()) { hrc.setCommandDetail(entity.getText()); } } request.addStages(Collections.singletonList(stage)); }', 'output': 'private void makeActionStage(Cluster cluster, RequestStageContainer request, UpgradeItemEntity entity, StageWrapper wrapper) throws AmbariException { Map<String, String> hostLevelParams = new HashMap<String, String>(); hostLevelParams.put(JDK_LOCATION, getManagementController().getJdkResourceUrl()); Stage stage = stageFactory.get().createNew(request.getId().longValue(), "/tmp/ambari", cluster.getClusterName(), cluster.getClusterId(), entity.getText(), "{}", "{}", StageUtils.getGson().toJson(hostLevelParams)); long stageId = request.getLastStageId() + 1; if (0L == stageId) { stageId = 1L; } stage.setStageId(stageId); entity.setStageId(Long.valueOf(stageId)); RequestResourceFilter filter = new RequestResourceFilter("", "", new ArrayList<String>(wrapper.getHosts())); Map<String, String> params = new HashMap<String, String>(); params.put("tasks", entity.getTasks()); ActionExecutionContext actionContext = new ActionExecutionContext( cluster.getClusterName(), "ru_execute_tasks", Collections.singletonList(filter), params); actionContext.setTimeout(Short.valueOf((short)60)); actionExecutionHelper.get().addExecutionCommandsToStage(actionContext, stage); for (Map<String, HostRoleCommand> map : stage.getHostRoleCommands().values()) { for (HostRoleCommand hrc : map.values()) { hrc.setCommandDetail(entity.getText()); } } request.addStages(Collections.singletonList(stage)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void encode(BinaryRawWriterEx writer) { super.encode(writer); writer.writeInt(res.size()); for (Map.Entry e : res.entrySet()) { writer.writeObjectDetached(e.getKey()); writer.writeObjectDetached(e.getValue()); } } } }', 'ground_truth': '@Override public void encode(ClientConnectionContext ctx, BinaryRawWriterEx writer) { super.encode(ctx, writer); writer.writeInt(res.size()); for (Map.Entry e : res.entrySet()) { writer.writeObjectDetached(e.getKey()); writer.writeObjectDetached(e.getValue()); } }', 'output': '@Override public void encode(BinaryRawWriterEx writer, int version) { super.encode(writer, version); writer.writeInt(res.size()); for (Map.Entry e : res.entrySet()) { writer.writeObjectDetached(e.getKey()); writer.writeObjectDetached(e.getValue()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public H2FieldsIterator(ResultSet data, MvccQueryTracker mvccTracker, boolean forUpdate) throws IgniteCheckedException { super(data, forUpdate); this.mvccTracker = mvccTracker; }', 'ground_truth': 'public H2FieldsIterator(ResultSet data, MvccQueryTracker mvccTracker, boolean forUpdate, ThreadLocalObjectPool.Reusable<H2ConnectionWrapper> detachedConn) throws IgniteCheckedException { super(data, forUpdate); this.mvccTracker = mvccTracker; this.detachedConn = detachedConn; }', 'output': 'public H2FieldsIterator(ResultSet data, MvccQueryTracker mvccTracker, boolean forUpdate, int pageSize) throws IgniteCheckedException { super(data, forUpdate, pageSize); this.mvccTracker = mvccTracker; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean update(KeyCacheObject key, int partId, CacheObject val, GridCacheVersion ver, long expirationTime, boolean rmv) throws IgniteCheckedException { assert desc != null; GridH2Row row = desc.createRow(key, partId, val, ver, expirationTime); return doUpdate(row, rmv); }', 'ground_truth': 'public boolean update(KeyCacheObject key, int partId, CacheObject val, GridCacheVersion ver, long expirationTime, boolean rmv, long link) throws IgniteCheckedException { assert desc != null; GridH2Row row = desc.createRow(key, partId, val, ver, expirationTime); row.link = link; return doUpdate(row, rmv); }', 'output': 'public boolean update(KeyCacheObject key, int partId, CacheObject val, CacheObject prevVal, GridCacheVersion ver, long expirationTime, boolean rmv) throws IgniteCheckedException { assert desc != null; GridH2Row row = desc.createRow(key, partId, val, prevVal, ver, expirationTime); return doUpdate(row, rmv); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void decommission() throws InterruptedException { ssProxy.decommission(); }', 'ground_truth': 'public void decommission(boolean force) throws InterruptedException { ssProxy.decommission(force); }', 'output': 'public void decommission() throws InterruptedException, IOException { ssProxy.decommission(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void createStage(Cluster cluster, RequestStageContainer request, UpgradeItemEntity entity, StageWrapper wrapper) throws AmbariException { if (wrapper.hasCommand()) { makeRestartStage(cluster, request, entity, wrapper); } else { makeActionStage(cluster, request, entity, wrapper); } }', 'ground_truth': 'private void createStage(Cluster cluster, RequestStageContainer request, final String version, UpgradeItemEntity entity, StageWrapper wrapper) throws AmbariException { if (wrapper.hasCommand()) { makeRestartStage(cluster, request, version, entity, wrapper); } else { makeActionStage(cluster, request, version, entity, wrapper); } }', 'output': 'private void createStage(Cluster cluster, RequestStageContainer request, UpgradeItemEntity entity, StageWrapper wrapper, String stageName) throws AmbariException { if (wrapper.hasCommand()) { makeRestartStage(cluster, request, entity, wrapper, stageName); } else { makeActionStage(cluster, request, entity, wrapper, stageName); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void validateCellValue(ByteBuffer cellValue) throws MarshalException { if (isMultiCell()) valueComparator().validateCellValue(cellValue); else super.validateCellValue(cellValue); }', 'ground_truth': 'public <V> void validateCellValue(V cellValue, ValueAccessor<V> accessor) throws MarshalException { if (isMultiCell()) valueComparator().validateCellValue(cellValue, accessor); else super.validateCellValue(cellValue, accessor); }', 'output': 'public void validateCellValue(ByteBuffer cellValue, CellMetadata metadata) throws MarshalException { if (isMultiCell()) valueComparator().validateCellValue(cellValue, metadata); else super.validateCellValue(cellValue, metadata); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void finish(Iterable<GridDistributedTxMapping> mappings, boolean commit) { assert !hasFutures() : futures(); int miniId = 0; for (GridDistributedTxMapping m : mappings) finish(++miniId, m, commit); }', 'ground_truth': 'private void finish(Iterable<GridDistributedTxMapping> mappings, boolean commit, boolean useCompletedVer) { assert !hasFutures() : futures(); int miniId = 0; for (GridDistributedTxMapping m : mappings) finish(++miniId, m, commit, useCompletedVer); }', 'output': 'private void finish(List<GridDistributedTxMapping> mappings, boolean commit) { assert !hasFutures() : futures(); int miniId = 0; for (GridDistributedTxMapping m : mappings) finish(++miniId, m, commit); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public <V> V runWithCompactionsDisabled(Callable<V> callable, boolean interruptValidation) { synchronized (this) { logger.debug("Cancelling in-progress compactions for {}", metadata.cfName); Iterable<ColumnFamilyStore> selfWithIndexes = concatWithIndexes(); for (ColumnFamilyStore cfs : selfWithIndexes) cfs.getCompactionStrategyManager().pause(); try { CompactionManager.instance.interruptCompactionForCFs(selfWithIndexes, interruptValidation); CompactionManager.instance.waitForCessation(selfWithIndexes); for (ColumnFamilyStore cfs : selfWithIndexes) { if (!cfs.getTracker().getCompacting().isEmpty()) { logger.warn("Unable to cancel in-progress compactions for {}.  Perhaps there is an unusually large row in progress somewhere, or the system is simply overloaded.", metadata.cfName); return null; } } logger.debug("Compactions successfully cancelled"); try { return callable.call(); } catch (Exception e) { throw new RuntimeException(e); } } finally { for (ColumnFamilyStore cfs : selfWithIndexes) cfs.getCompactionStrategyManager().resume(); } } }', 'ground_truth': 'public <V> V runWithCompactionsDisabled(Callable<V> callable, boolean interruptValidation, boolean interruptViews) { synchronized (this) { logger.debug("Cancelling in-progress compactions for {}", metadata.cfName); Iterable<ColumnFamilyStore> selfWithAuxiliaryCfs = interruptViews ? Iterables.concat(concatWithIndexes(), materializedViewManager.allViewsCfs()) : concatWithIndexes(); for (ColumnFamilyStore cfs : selfWithAuxiliaryCfs) cfs.getCompactionStrategyManager().pause(); try { CompactionManager.instance.interruptCompactionForCFs(selfWithAuxiliaryCfs, interruptValidation); CompactionManager.instance.waitForCessation(selfWithAuxiliaryCfs); for (ColumnFamilyStore cfs : selfWithAuxiliaryCfs) { if (!cfs.getTracker().getCompacting().isEmpty()) { logger.warn("Unable to cancel in-progress compactions for {}.  Perhaps there is an unusually large row in progress somewhere, or the system is simply overloaded.", metadata.cfName); return null; } } logger.debug("Compactions successfully cancelled"); try { return callable.call(); } catch (Exception e) { throw new RuntimeException(e); } } finally { for (ColumnFamilyStore cfs : selfWithAuxiliaryCfs) cfs.getCompactionStrategyManager().resume(); } } }', 'output': 'public <T> T runWithCompactionsDisabled(Callable<T> callable, boolean interruptValidation) { synchronized (this) { logger.debug("Cancelling in-progress compactions for {}", metadata.cfName); Iterable<ColumnFamilyStore> selfWithIndexes = concatWithIndexes(); for (ColumnFamilyStore cfs : selfWithIndexes) cfs.getCompactionStrategyManager().pause(); try { CompactionManager.instance.interruptCompactionForCFs(selfWithIndexes, interruptValidation); CompactionManager.instance.waitForCessation(selfWithIndexes); for (ColumnFamilyStore cfs : selfWithIndexes) { if (!cfs.getTracker().getCompacting().isEmpty()) { logger.warn("Unable to cancel in-progress compactions for {}.  Perhaps there is an unusually large row in progress somewhere, or the system is simply overloaded.", metadata.cfName); return null; } } logger.debug("Compactions successfully cancelled"); try { return callable.call(); } catch (Exception e) { throw new RuntimeException(e); } } finally { for (ColumnFamilyStore cfs : selfWithIndexes) cfs.getCompactionStrategyManager().resume(); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static UDAggregate readAggregateMetadata(String keyspaceName, String functionName, List<String> signature) { String query = format("SELECT * FROM %s.%s WHERE keyspace_name = ? AND function_name = ? AND signature = ?", SystemKeyspace.NAME, SystemKeyspace.LEGACY_AGGREGATES); UntypedResultSet.Row row = query(query, keyspaceName, functionName, signature).one(); FunctionName name = new FunctionName(keyspaceName, functionName); List<String> types = row.getList("argument_types", UTF8Type.instance); List<AbstractType<?>> argTypes = new ArrayList<>(); if (types != null) { argTypes = new ArrayList<>(types.size()); for (String type : types) argTypes.add(parseType(type)); } AbstractType<?> returnType = parseType(row.getString("return_type")); FunctionName stateFunc = new FunctionName(keyspaceName, row.getString("state_func")); FunctionName finalFunc = row.has("final_func") ? new FunctionName(keyspaceName, row.getString("final_func")) : null; AbstractType<?> stateType = row.has("state_type") ? parseType(row.getString("state_type")) : null; ByteBuffer initcond = row.has("initcond") ? row.getBytes("initcond") : null; try { return UDAggregate.create(name, argTypes, returnType, stateFunc, finalFunc, stateType, initcond); } catch (InvalidRequestException reason) { return UDAggregate.createBroken(name, argTypes, returnType, initcond, reason); } }', 'ground_truth': 'private static UDAggregate readAggregateMetadata(Functions functions, String keyspaceName, String functionName, List<String> signature) { String query = format("SELECT * FROM %s.%s WHERE keyspace_name = ? AND aggregate_name = ? AND signature = ?", SystemKeyspace.NAME, SystemKeyspace.LEGACY_AGGREGATES); UntypedResultSet.Row row = query(query, keyspaceName, functionName, signature).one(); FunctionName name = new FunctionName(keyspaceName, functionName); List<String> types = row.getList("argument_types", UTF8Type.instance); List<AbstractType<?>> argTypes = new ArrayList<>(); if (types != null) { argTypes = new ArrayList<>(types.size()); for (String type : types) argTypes.add(parseType(type)); } AbstractType<?> returnType = parseType(row.getString("return_type")); FunctionName stateFunc = new FunctionName(keyspaceName, row.getString("state_func")); AbstractType<?> stateType = parseType(row.getString("state_type")); FunctionName finalFunc = row.has("final_func") ? new FunctionName(keyspaceName, row.getString("final_func")) : null; ByteBuffer initcond = row.has("initcond") ? row.getBytes("initcond") : null; try { return UDAggregate.create(functions, name, argTypes, returnType, stateFunc, finalFunc, stateType, initcond); } catch (InvalidRequestException reason) { return UDAggregate.createBroken(name, argTypes, returnType, initcond, reason); } }', 'output': 'private static UDAggregate readAggregateMetadata(String keyspaceName, String functionName, List<String> signature) { String query = format("SELECT * FROM %s.%s WHERE keyspace_name = ? AND function_name = ? AND signature = ?", SystemKeyspace.NAME, SystemKeyspace.LEGACY_AGGREGATES); UntypedResultSet.Row row = query(query, keyspaceName, functionName, signature).one(); FunctionName name = new FunctionName(keyspaceName, functionName); List<String> types = row.getList("argument_types", UTF8Type.instance); List<AbstractType<?>> argTypes = new ArrayList<>(); if (types != null) { argTypes = new ArrayList<>(types.size()); for (String type : types) argTypes.add(parseType(type)); } AbstractType<?> returnType = parseType(row.getString("return_type")); FunctionName stateFunc = new FunctionName(keyspaceName, row.getString("state_func")); FunctionName finalFunc = row.has("final_func") ? new FunctionName(keyspaceName, row.getString("final_func")) : null; AbstractType<?> stateType = row.has("state_type") ? parseType(row.getString("state_type")) : null; ByteBuffer initcond = row.has("initcond") ? row.getBytes("initcond") : null; try { return UDAggregate.create(name, argTypes, returnType, stateFunc, finalFunc, stateType, initcond); } catch (InvalidRequestException reason) { return UDAggregate.createBroken(name, argTypes, returnType, initcond, reason); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void insertDataRow(CacheDataRow row) throws IgniteCheckedException { super.insertDataRow(row); assert row.key().partition() == PageIdUtils.partId(row.link()) : "Constructed a link with invalid partition ID [partId=" + row.key().partition() + ", link=" + U.hexLong(row.link()) + \']\'; } /** {@inheritDoc} */ @Override public String toString() { return "FreeList [name=" + name + \']\'; } } }', 'ground_truth': '@Override public void insertDataRow(CacheDataRow row, IoStatisticsHolder statHolder) throws IgniteCheckedException { super.insertDataRow(row, statHolder); assert row.key().partition() == PageIdUtils.partId(row.link()) : "Constructed a link with invalid partition ID [partId=" + row.key().partition() + ", link=" + U.hexLong(row.link()) + \']\'; }', 'output': '@Override public void insertDataRow(CacheDataRow row) throws IgniteCheckedException { super.insertDataRow(row); assert row.key().partition() == PageIdUtils.partId(row.link()) : "Constructed a link with invalid partition ID [partId=" + row.key().partition() + ", link=" + U.hexLong(row.link()) + \']\'; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RepairSession getRepairSession(String tablename, String... cfnames) { return new RepairSession(tablename, cfnames); }', 'ground_truth': 'public RepairSession getRepairSession(Range range, String tablename, String... cfnames) { return new RepairSession(range, tablename, cfnames); }', 'output': 'public RepairSession getRepairSession(String tablename, Collection<String> cfnames) { return new RepairSession(tablename, cfnames); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static Map<Descriptor,Set<Component>> files(String keyspace, final String columnFamily, final boolean includeCompacted) { final Map<Descriptor,Set<Component>> sstables = new HashMap<Descriptor,Set<Component>>(); for (String directory : DatabaseDescriptor.getAllDataFileLocationsForTable(keyspace)) { for (Pair<Descriptor, Component> component : files(new File(directory))) { if (component != null && component.left.cfname.equals(columnFamily)) { if (includeCompacted || !new File(component.left.filenameFor(Component.COMPACTED_MARKER)).exists()) { Set<Component> components = sstables.get(component.left); if (components == null) { components = new HashSet<Component>(); sstables.put(component.left, components); } components.add(component.right); } else logger.debug("not including compacted sstable " + component.left.cfname + "-" + component.left.generation); } } } return sstables; }', 'ground_truth': 'private static Map<Descriptor,Set<Component>> files(String keyspace, final String columnFamily, final boolean includeCompacted, final boolean includeTemporary) { final Map<Descriptor,Set<Component>> sstables = new HashMap<Descriptor,Set<Component>>(); for (String directory : DatabaseDescriptor.getAllDataFileLocationsForTable(keyspace)) { for (Pair<Descriptor, Component> component : files(new File(directory))) { if (component != null && component.left.cfname.equals(columnFamily)) { if ((includeCompacted || !new File(component.left.filenameFor(Component.COMPACTED_MARKER)).exists()) && (includeTemporary || !component.left.temporary)) { Set<Component> components = sstables.get(component.left); if (components == null) { components = new HashSet<Component>(); sstables.put(component.left, components); } components.add(component.right); } else logger.debug("not including compacted sstable " + component.left.cfname + "-" + component.left.generation); } } } return sstables; }', 'output': 'private static Map<Descriptor,Set<Component>> files(String keyspace, final String columnFamily, final boolean includeCompacted) { final Map<Descriptor,Set<Component>> sstables = new HashMap<Descriptor,Set<Component>>(); for (String directory : DatabaseDescriptor.getAllDataFileLocationsForTable(keyspace)) { for (Pair<Descriptor, Component> component : files(new File(directory))) { if (component != null && component.left.cfname.equals(columnFamily)) { if (includeCompacted || !new File(component.left.filenameFor(Component.COMPACTED_MARKER)).exists()) { Set<Component> components = sstables.get(component.left); if (components == null) { components = new HashSet<Component>(); sstables.put(component.left, components); } components.add(component.right); } else logger.debug("not including compacted sstable " + component.left.cfname + "-" + component.left.generation); } } } return sstables; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Map<Range, Collection<InetAddress>> getPendingRanges() { return pendingRanges.asMap(); } public List<Range> getPendingRanges(InetAddress endpoint)\n    { { List<Range> ranges = new ArrayList<Range>(); for (Map.Entry<Range, InetAddress> entry : pendingRanges.entries()) { if (entry.getValue().equals(endpoint)) { ranges.add(entry.getKey()); } } return ranges; }', 'ground_truth': 'public Map<Range, Collection<InetAddress>> getPendingRanges(String table) { return getPendingRangesMM(table).asMap(); }', 'output': 'public Map<Range, Set<InetAddress>> getPendingRanges() { return pendingRanges.asMap(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ThriftReader(Timer timer, PartitionGenerator generator, StressSettings settings) { super(Command.READ, timer, generator, settings); }', 'ground_truth': 'public ThriftReader(Timer timer, PartitionGenerator generator, SeedManager seedManager, StressSettings settings) { super(Command.READ, timer, generator, seedManager, settings); }', 'output': 'public ThriftReader(Timer timer, PartitionGenerator generator, Settings settings) { super(Command.READ, timer, generator, settings); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static AbstractWriteResponseHandler<IMutation> applyCounterMutationOnLeader(CounterMutation cm, String localDataCenter, Runnable callback) throws UnavailableException, OverloadedException { return performWrite(cm, cm.consistency(), localDataCenter, counterWritePerformer, callback, WriteType.COUNTER); }', 'ground_truth': 'public static AbstractWriteResponseHandler<IMutation> applyCounterMutationOnLeader(CounterMutation cm, String localDataCenter, Runnable callback, long queryStartNanoTime) throws UnavailableException, OverloadedException { return performWrite(cm, cm.consistency(), localDataCenter, counterWritePerformer, callback, WriteType.COUNTER, queryStartNanoTime); }', 'output': 'public static AbstractWriteResponseHandler<IMutation> applyCounterMutationOnLeader(CounterMutation cm, String localDataCenter, int consistency, Runnable callback) throws UnavailableException, OverloadedException { return performWrite(cm, consistency, localDataCenter, counterWritePerformer, callback, WriteType.COUNTER); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected int textHeight(int index) { if (textHeight == -1) { String testStr = "Zgj"; Font f = getFont(); textHeight = new Double(f.getStringBounds(testStr, BasicScrollingTabDisplayerUI.getOffscreenGraphics().getFontRenderContext()).getWidth()).intValue() + 2; } return textHeight; }', 'ground_truth': 'protected int textHeight(int index, JComponent component) { if (textHeight == -1) { String testStr = "Zgj"; Font f = getFont(); textHeight = new Double(f.getStringBounds(testStr, BasicScrollingTabDisplayerUI.getOffscreenGraphics(component).getFontRenderContext()).getWidth()).intValue() + 2; } return textHeight; }', 'output': 'public int textHeight(int index) { if (textHeight == -1) { String testStr = "Zgj"; Font f = getFont(); textHeight = new Double(f.getStringBounds(testStr, BasicScrollingTabDisplayerUI.getOffscreenGraphics().getFontRenderContext()).getWidth()).intValue() + 2; } return textHeight; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void assertFails(CheckedFunction function, String message, boolean thrown) throws Throwable { ClientWarn.instance.captureWarnings(); try { function.apply(); if (thrown) fail("Expected to fail, but it did not"); } catch (InvalidRequestException | InvalidQueryException e) { assertTrue("Expect no exception thrown", thrown); assertTrue(format("Full error message \'%s\' does not contain expected message \'%s\'", e.getMessage(), message), e.getMessage().contains(message)); assertWarnings(message); } finally { ClientWarn.instance.resetWarnings(); } }', 'ground_truth': 'protected void assertFails(CheckedFunction function, boolean thrown, String... messages) throws Throwable { ClientWarn.instance.captureWarnings(); try { function.apply(); if (thrown) fail("Expected to fail, but it did not"); } catch (InvalidRequestException | InvalidQueryException e) { assertTrue("Expect no exception thrown", thrown); String failMessage = messages[messages.length - 1]; assertTrue(format("Full error message \'%s\' does not contain expected message \'%s\'", e.getMessage(), failMessage), e.getMessage().contains(failMessage)); assertWarnings(messages); } finally { ClientWarn.instance.resetWarnings(); } }', 'output': 'protected void assertFails(CheckedFunction function, String message, boolean thrown) throws Throwable { ClientWarn.instance.captureWarnings(); try { function.apply(); if (thrown) fail("Expected to fail, but it did not"); } catch (InvalidRequestException | InvalidQueryException e) { assertTrue("Expect no exception thrown", thrown); assertTrue(format("Full error message \'%s\' does not contain expected message \'%s\'", e.getMessage(), message), e.getMessage().contains(message)); assertWarnings(message); } finally { ClientWarn.instance.resetWarnings(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private MixinFacetForDomainObjectAnnotation( final Class<?> mixinType, final String value, final Constructor<?> constructorType, final FacetHolder holder) { super(mixinType, value, constructorType, holder); }', 'ground_truth': 'private MixinFacetForDomainObjectAnnotation( final Class<?> mixinType, final String value, final Constructor<?> constructorType, final FacetHolder holder, MetaModelValidatorForMixinTypes mixinTypeValidator) { super(mixinType, value, constructorType, holder, mixinTypeValidator); }', 'output': 'private MixinFacetForDomainObjectAnnotation( final Class<?> mixinType, final String value, final Constructor<?> constructorType, final FacetHolder holder, final Class<?> domainClass) { super(mixinType, value, constructorType, holder, domainClass); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static RuntimeVariablesExpression runtimeVariables( Iterable<ParameterExpression> expressions) { throw Extensions.todo(); } public static RuntimeVariablesExpression runtimeVariables(\n      ParameterExpression... arguments) { ParameterExpression... arguments) { throw Extensions.todo(); }', 'ground_truth': 'public static RuntimeVariablesExpression runtimeVariables( Iterable<? extends ParameterExpression> expressions) { throw Extensions.todo(); }', 'output': 'public static RuntimeVariablesExpression runtimeVariables( Iterable<ParameterExpression> expressions, String name) { throw Extensions.todo(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void addInput(ResultSetBuilder rs) { if (isWritetime) { long ts = rs.timestamps[idx]; current = ts != Long.MIN_VALUE ? ByteBufferUtil.bytes(ts) : null; } else { int ttl = rs.ttls[idx]; current = ttl > 0 ? ByteBufferUtil.bytes(ttl) : null; } }', 'ground_truth': 'public void addInput(int protocolVersion, ResultSetBuilder rs) { if (isWritetime) { long ts = rs.timestamps[idx]; current = ts != Long.MIN_VALUE ? ByteBufferUtil.bytes(ts) : null; } else { int ttl = rs.ttls[idx]; current = ttl > 0 ? ByteBufferUtil.bytes(ttl) : null; } }', 'output': 'public void addInput(ResultSetBuilder rs, boolean isWritetime) { if (isWritetime) { long ts = rs.timestamps[idx]; current = ts != Long.MIN_VALUE ? ByteBufferUtil.bytes(ts) : null; } else { int ttl = rs.ttls[idx]; current = ttl > 0 ? ByteBufferUtil.bytes(ttl) : null; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<IMutation> getMutations(ClientState clientState, List<ByteBuffer> variables) throws RequestExecutionException, RequestValidationException { List<ByteBuffer> keys = UpdateStatement.buildKeyNames(cfDef, processedKeys, variables); ColumnNameBuilder builder = cfDef.getColumnNameBuilder(); CFDefinition.Name firstEmpty = UpdateStatement.buildColumnNames(cfDef, processedKeys, builder, variables, false); boolean fullKey = builder.componentCount() == cfDef.columns.size(); boolean isRange = cfDef.isCompact ? !fullKey : (!fullKey || toRemove.isEmpty()); if (!toRemove.isEmpty() && isRange) throw new InvalidRequestException(String.format("Missing mandatory PRIMARY KEY part %s since %s specified", firstEmpty, toRemove.iterator().next().left)); boolean needsReading = false; for (Pair<CFDefinition.Name, Term> p : toRemove) { CFDefinition.Name name = p.left; Term value = p.right; if ((name.type instanceof ListType) && value != null) { needsReading = true; break; } } Map<ByteBuffer, ColumnGroupMap> rows = needsReading ? readRows(keys, builder, (CompositeType)cfDef.cfm.comparator) : null; List<IMutation> rowMutations = new ArrayList<IMutation>(keys.size()); UpdateParameters params = new UpdateParameters(variables, getTimestamp(clientState), -1); for (ByteBuffer key : keys) rowMutations.add(mutationForKey(cfDef, key, builder, isRange, params, rows == null ? null : rows.get(key))); return rowMutations; }', 'ground_truth': 'public List<IMutation> getMutations(ClientState clientState, List<ByteBuffer> variables, boolean local) throws RequestExecutionException, RequestValidationException { List<ByteBuffer> keys = UpdateStatement.buildKeyNames(cfDef, processedKeys, variables); ColumnNameBuilder builder = cfDef.getColumnNameBuilder(); CFDefinition.Name firstEmpty = UpdateStatement.buildColumnNames(cfDef, processedKeys, builder, variables, false); boolean fullKey = builder.componentCount() == cfDef.columns.size(); boolean isRange = cfDef.isCompact ? !fullKey : (!fullKey || toRemove.isEmpty()); if (!toRemove.isEmpty() && isRange) throw new InvalidRequestException(String.format("Missing mandatory PRIMARY KEY part %s since %s specified", firstEmpty, toRemove.iterator().next().left)); boolean needsReading = false; for (Pair<CFDefinition.Name, Term> p : toRemove) { CFDefinition.Name name = p.left; Term value = p.right; if ((name.type instanceof ListType) && value != null) { needsReading = true; break; } } Map<ByteBuffer, ColumnGroupMap> rows = needsReading ? readRows(keys, builder, (CompositeType)cfDef.cfm.comparator, local) : null; List<IMutation> rowMutations = new ArrayList<IMutation>(keys.size()); UpdateParameters params = new UpdateParameters(variables, getTimestamp(clientState), -1); for (ByteBuffer key : keys) rowMutations.add(mutationForKey(cfDef, key, builder, isRange, params, rows == null ? null : rows.get(key))); return rowMutations; }', 'output': 'public List<IMutation> getMutations(ClientState clientState, List<ByteBuffer> variables) throws RequestExecutionException, RequestValidationException { List<ByteBuffer> keys = UpdateStatement.buildKeyNames(cfDef, processedKeys, variables); ColumnNameBuilder builder = cfDef.getColumnNameBuilder(); CFDefinition.Name firstEmpty = UpdateStatement.buildColumnNames(cfDef, processedKeys, builder, variables, false); boolean fullKey = builder.componentCount() == cfDef.columns.size(); boolean isRange = cfDef.isCompact ? !fullKey : (!fullKey || toRemove.isEmpty()); if (!toRemove.isEmpty() && isRange) throw new InvalidRequestException(String.format("Missing mandatory PRIMARY KEY part %s since %s specified", firstEmpty, toRemove.iterator().next().left)); boolean needsReading = false; for (Pair<CFDefinition.Name, Term> p : toRemove) { CFDefinition.Name name = p.left; Term value = p.right; if ((name.type instanceof ListType) && value != null) { needsReading = true; break; } } Map<ByteBuffer, ColumnGroupMap> rows = needsReading ? readRows(keys, builder, (CompositeType)cfDef.cfm.comparator) : null; List<IMutation> rowMutations = new ArrayList<IMutation>(keys.size()); UpdateParameters params = new UpdateParameters(variables, getTimestamp(clientState), -1); for (ByteBuffer key : keys) rowMutations.add(mutationForKey(cfDef, key, builder, isRange, params, rows == null ? null : rows.get(key))); return rowMutations; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private DataSource<I> asListDataSource(List<I> inputData, boolean bounded) { List<List<I>> splits = inputData.stream() .map(Arrays::asList).collect(Collectors.toList()); return ListDataSource.of(bounded, splits); }', 'ground_truth': 'private DataSource<I> asListDataSource( List<I> inputData, boolean bounded, boolean canParallelize) { final List<List<I>> splits; if (canParallelize) { splits = inputData.stream() .map(Arrays::asList).collect(Collectors.toList()); } else { splits = Collections.singletonList(inputData); } return ListDataSource.of(bounded, splits); }', 'output': 'private DataSource<I> asListDataSource(List<I> inputData, boolean bounded) { List<List<I>> splits = inputData.stream() .map(Arrays::asList).collect(Collectors.toList()); return ListDataSource.of(bounded, splits); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private MaintenanceModeEvent(MaintenanceState state, Service service, Host host, ServiceComponentHost serviceComponentHost) { super(AmbariEventType.MAINTENANCE_MODE); m_state = state; m_service = service; m_host = host; m_serviceComponentHost = serviceComponentHost; }', 'ground_truth': 'private MaintenanceModeEvent(MaintenanceState state, long clusterId, Service service, Host host, ServiceComponentHost serviceComponentHost) { super(AmbariEventType.MAINTENANCE_MODE); m_state = state; m_clusterId = clusterId; m_service = service; m_host = host; m_serviceComponentHost = serviceComponentHost; }', 'output': 'private MaintenanceModeEvent(long timestamp, MaintenanceState state, Service service, Host host, ServiceComponentHost serviceComponentHost) { super(AmbariEventType.MAINTENANCE_MODE, timestamp); m_state = state; m_service = service; m_host = host; m_serviceComponentHost = serviceComponentHost; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static WriteResponseHandlerWrapper wrapViewBatchResponseHandler(Mutation mutation, ConsistencyLevel consistency_level, ConsistencyLevel batchConsistencyLevel, List<InetAddress> naturalEndpoints, AtomicLong baseComplete, WriteType writeType, BatchlogResponseHandler.BatchlogCleanup cleanup) { Keyspace keyspace = Keyspace.open(mutation.getKeyspaceName()); AbstractReplicationStrategy rs = keyspace.getReplicationStrategy(); String keyspaceName = mutation.getKeyspaceName(); Token tk = mutation.key().getToken(); Collection<InetAddress> pendingEndpoints = StorageService.instance.getTokenMetadata().pendingEndpointsFor(tk, keyspaceName); AbstractWriteResponseHandler<IMutation> writeHandler = rs.getWriteResponseHandler(naturalEndpoints, pendingEndpoints, consistency_level, () -> { long delay = Math.max(0, System.currentTimeMillis() - baseComplete.get()); viewWriteMetrics.viewWriteLatency.update(delay, TimeUnit.MILLISECONDS); }, writeType); BatchlogResponseHandler<IMutation> batchHandler = new ViewWriteMetricsWrapped(writeHandler, batchConsistencyLevel.blockFor(keyspace), cleanup); return new WriteResponseHandlerWrapper(batchHandler, mutation); }', 'ground_truth': 'private static WriteResponseHandlerWrapper wrapViewBatchResponseHandler(Mutation mutation, ConsistencyLevel consistency_level, ConsistencyLevel batchConsistencyLevel, List<InetAddress> naturalEndpoints, AtomicLong baseComplete, WriteType writeType, BatchlogResponseHandler.BatchlogCleanup cleanup, long queryStartNanoTime) { Keyspace keyspace = Keyspace.open(mutation.getKeyspaceName()); AbstractReplicationStrategy rs = keyspace.getReplicationStrategy(); String keyspaceName = mutation.getKeyspaceName(); Token tk = mutation.key().getToken(); Collection<InetAddress> pendingEndpoints = StorageService.instance.getTokenMetadata().pendingEndpointsFor(tk, keyspaceName); AbstractWriteResponseHandler<IMutation> writeHandler = rs.getWriteResponseHandler(naturalEndpoints, pendingEndpoints, consistency_level, () -> { long delay = Math.max(0, System.currentTimeMillis() - baseComplete.get()); viewWriteMetrics.viewWriteLatency.update(delay, TimeUnit.MILLISECONDS); }, writeType, queryStartNanoTime); BatchlogResponseHandler<IMutation> batchHandler = new ViewWriteMetricsWrapped(writeHandler, batchConsistencyLevel.blockFor(keyspace), cleanup, queryStartNanoTime); return new WriteResponseHandlerWrapper(batchHandler, mutation); }', 'output': 'private static WriteResponseHandlerWrapper wrapViewBatchResponseHandler(Mutation mutation, ConsistencyLevel consistency_level, ConsistencyLevel batchConsistencyLevel, List<InetAddress> naturalEndpoints, AtomicLong baseComplete, WriteType writeType, BatchlogResponseHandler.BatchlogCleanup cleanup) { Keyspace keyspace = Keyspace.open(mutation.getKeyspaceName()); AbstractReplicationStrategy rs = keyspace.getReplicationStrategy(); String keyspaceName = mutation.getKeyspaceName(); Token tk = mutation.key().getToken(); Collection<InetAddress> pendingEndpoints = StorageService.instance.getTokenMetadata().pendingEndpointsFor(tk, keyspaceName); AbstractWriteResponseHandler<IMutation> writeHandler = rs.getWriteResponseHandler(naturalEndpoints, pendingEndpoints, consistency_level, () -> { long delay = Math.max(0, System.currentTimeMillis() - baseComplete.get()); viewWriteMetrics.viewWriteLatency.update(delay, TimeUnit.MILLISECONDS); }, writeType); BatchlogResponseHandler<IMutation> batchHandler = new ViewWriteMetricsWrapped(writeHandler, batchConsistencyLevel.blockFor(keyspace), cleanup); return new WriteResponseHandlerWrapper(batchHandler, mutation); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ObjectViewFx fromObject( @NonNull final UiComponentFactoryFx uiComponentFactory, @NonNull final ManagedObject managedObject) { return new ObjectViewFx(uiComponentFactory, managedObject); }', 'ground_truth': 'public static ObjectViewFx fromObject( @NonNull final UiComponentFactoryFx uiComponentFactory, @NonNull final Consumer<ManagedAction> actionEventHandler, @NonNull final ManagedObject managedObject) { return new ObjectViewFx(uiComponentFactory, actionEventHandler, managedObject); }', 'output': 'public static ObjectViewFx fromManagedObject( @NonNull final UiComponentFactoryFx uiComponentFactory, @NonNull final ManagedObject managedObject) { return new ObjectViewFx(uiComponentFactory, managedObject); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Map<TokenRange, Set<Host>> getRangeMap(String keyspace, Metadata metadata) { return metadata.getTokenRanges() .stream() .collect(toMap(p -> p, p -> metadata.getReplicas(\'"\' + keyspace + \'"\', p))); }', 'ground_truth': 'private static Map<TokenRange, List<Host>> getRangeMap(String keyspace, Metadata metadata, String targetDC) { return CqlClientHelper.getLocalPrimaryRangeForDC(keyspace, metadata, targetDC); }', 'output': 'private Map<TokenRange, Collection<Host>> getRangeMap(String keyspace, Metadata metadata) { return metadata.getTokenRanges() .stream() .collect(toMap(p -> p, p -> metadata.getReplicas(\'"\' + keyspace + \'"\', p))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public IndexStorageImpl( final PageMemory pageMem, final IgniteWriteAheadLogManager wal, final AtomicLong globalRmvId, final int grpId, final int allocPartId, final byte allocSpace, final ReuseList reuseList, final long rootPageId, final boolean initNew ) { try { this.pageMem = pageMem; this.grpId = grpId; this.allocPartId = allocPartId; this.allocSpace = allocSpace; this.reuseList = reuseList; metaTree = new MetaTree(grpId, allocPartId, allocSpace, pageMem, wal, globalRmvId, rootPageId, reuseList, MetaStoreInnerIO.VERSIONS, MetaStoreLeafIO.VERSIONS, initNew); } catch (IgniteCheckedException e) { throw new IgniteException(e); } }', 'ground_truth': 'public IndexStorageImpl( final PageMemory pageMem, final IgniteWriteAheadLogManager wal, final AtomicLong globalRmvId, final int grpId, final int allocPartId, final byte allocSpace, final ReuseList reuseList, final long rootPageId, final boolean initNew, final FailureProcessor failureProcessor ) { try { this.pageMem = pageMem; this.grpId = grpId; this.allocPartId = allocPartId; this.allocSpace = allocSpace; this.reuseList = reuseList; metaTree = new MetaTree(grpId, allocPartId, allocSpace, pageMem, wal, globalRmvId, rootPageId, reuseList, MetaStoreInnerIO.VERSIONS, MetaStoreLeafIO.VERSIONS, initNew, failureProcessor); } catch (IgniteCheckedException e) { throw new IgniteException(e); } }', 'output': 'public IndexStorageImpl( final PageMemory pageMem, final WriteAheadLogManager wal, final AtomicLong globalRmvId, final int grpId, final int allocPartId, final byte allocSpace, final ReuseList reuseList, final long rootPageId, final boolean initNew ) { try { this.pageMem = pageMem; this.grpId = grpId; this.allocPartId = allocPartId; this.allocSpace = allocSpace; this.reuseList = reuseList; metaTree = new MetaTree(grpId, allocPartId, allocSpace, pageMem, wal, globalRmvId, rootPageId, reuseList, MetaStoreInnerIO.VERSIONS, MetaStoreLeafIO.VERSIONS, initNew); } catch (IgniteCheckedException e) { throw new IgniteException(e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ReflectiveTable( ReflectiveSchema schema, Type elementType, Expression expression) { super(schema.getQueryProvider(), elementType, expression); this.schema = schema; }', 'ground_truth': 'public ReflectiveTable( ReflectiveSchema schema, Type elementType, RelDataType relDataType, Expression expression) { super(schema.getQueryProvider(), elementType, expression); this.schema = schema; this.relDataType = relDataType; }', 'output': 'public ReflectiveTable( ReflectiveSchema schema, Type elementType, Expression expression, String name) { super(schema.getQueryProvider(), elementType, expression, name); this.schema = schema; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RangeSliceCommand(String keyspace, ColumnParent column_parent, SlicePredicate predicate, AbstractBounds<RowPosition> range, int max_keys) { this(keyspace, column_parent.getColumn_family(), column_parent.super_column, predicate, range, max_keys); }', 'ground_truth': 'public RangeSliceCommand(String keyspace, ColumnParent column_parent, SlicePredicate predicate, AbstractBounds<RowPosition> range, List<IndexExpression> row_filter, int max_keys) { this(keyspace, column_parent.getColumn_family(), column_parent.super_column, predicate, range, row_filter, max_keys); }', 'output': 'public RangeSliceCommand(String keyspace, String column_family, String super_column, SlicePredicate predicate, AbstractBounds<RowPosition> range, int max_keys) { this(keyspace, column_family, super_column, predicate, range, max_keys); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ExtendedFilter create(ColumnFamilyStore cfs, IDiskAtomFilter filter, List<IndexExpression> clause, int maxResults, boolean countCQL3Rows, boolean isPaging) { if (clause == null || clause.isEmpty()) { return new EmptyClauseFilter(cfs, filter, maxResults, countCQL3Rows, isPaging); } else { if (isPaging) throw new IllegalArgumentException("Cross-row paging is not supported along with index clauses"); return cfs.getComparator() instanceof CompositeType ? new FilterWithCompositeClauses(cfs, filter, clause, maxResults, countCQL3Rows) : new FilterWithClauses(cfs, filter, clause, maxResults, countCQL3Rows); } }', 'ground_truth': 'public static ExtendedFilter create(ColumnFamilyStore cfs, List<IndexExpression> clause, IDiskAtomFilter filter, int maxResults, long timestamp, boolean countCQL3Rows, boolean isPaging) { if (clause == null || clause.isEmpty()) { return new EmptyClauseFilter(cfs, filter, maxResults, timestamp, countCQL3Rows, isPaging); } else { if (isPaging) throw new IllegalArgumentException("Cross-row paging is not supported along with index clauses"); return cfs.getComparator() instanceof CompositeType ? new FilterWithCompositeClauses(cfs, clause, filter, maxResults, timestamp, countCQL3Rows) : new FilterWithClauses(cfs, clause, filter, maxResults, timestamp, countCQL3Rows); } }', 'output': 'public static ExtendedFilter create(ColumnFamilyStore cfs, IDiskAtomFilter filter, Collection<IndexExpression> clause, int maxResults, boolean countCQL3Rows, boolean isPaging) { if (clause == null || clause.isEmpty()) { return new EmptyClauseFilter(cfs, filter, maxResults, countCQL3Rows, isPaging); } else { if (isPaging) throw new IllegalArgumentException("Cross-row paging is not supported along with index clauses"); return cfs.getComparator() instanceof CompositeType ? new FilterWithCompositeClauses(cfs, filter, clause, maxResults, countCQL3Rows) : new FilterWithClauses(cfs, filter, clause, maxResults, countCQL3Rows); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Statement createConstructorStatementGuarded(FieldNode fNode) { final FieldExpression fieldExpr = new FieldExpression(fNode); Expression initExpr = fNode.getInitialValueExpression(); if (initExpr == null) initExpr = ConstantExpression.NULL; Expression unknown = findArg(fNode.getName()); return new IfStatement( equalsNullExpr(unknown), new IfStatement( equalsNullExpr(initExpr), new EmptyStatement(), assignStatement(fieldExpr, checkUnresolved(fNode, initExpr))), assignStatement(fieldExpr, checkUnresolved(fNode, unknown))); }', 'ground_truth': 'private Statement createConstructorStatementGuarded(ClassNode cNode, FieldNode fNode) { final FieldExpression fieldExpr = new FieldExpression(fNode); Expression initExpr = fNode.getInitialValueExpression(); if (initExpr == null) initExpr = ConstantExpression.NULL; Expression unknown = findArg(fNode.getName()); return new IfStatement( equalsNullExpr(unknown), new IfStatement( equalsNullExpr(initExpr), new EmptyStatement(), assignStatement(fieldExpr, checkUnresolved(cNode, fNode, initExpr))), assignStatement(fieldExpr, checkUnresolved(cNode, fNode, unknown))); }', 'output': 'private Statement createConstructorStatementGuarded(FieldNode fNode) { final FieldExpression fieldExpr = new FieldExpression(fNode); Expression initExpr = fNode.getInitialValueExpression(); if (initExpr == null) initExpr = ConstantExpression.NULL; Expression unknown = findArg(fNode.getName()); return new IfStatement( equalsNullExpr(unknown), new IfStatement( equalsNullExpr(initExpr), new EmptyStatement(), assignStatement(fieldExpr, checkUnresolved(fNode, initExpr))), assignStatement(fieldExpr, checkUnresolved(fNode, unknown))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public CustomUiPanel( final String id, final EntityModel model, final ComponentFactory componentFactory) { super(id, model); }', 'ground_truth': 'public CustomUiPanel( final String id, final EntityModel model, final ComponentFactory componentFactory, final GeoapifyClient geoapifyClient) { super(id, model); this.geoapifyClient = geoapifyClient; }', 'output': 'public CustomUiPanel( final String id, final EntityModel model, final ComponentFactory componentFactory, final int width, final int height) { super(id, model, componentFactory, width, height); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void run() { SliceRange sliceRange = new SliceRange(); sliceRange.setStart(new byte[] {}).setFinish(new byte[] {}); sliceRange.setReversed(false).setCount(session.getColumnsPerKey()); SlicePredicate predicate = new SlicePredicate().setSlice_range(sliceRange); if (session.getColumnFamilyType() == ColumnFamilyType.Super) { runSuperColumnReader(predicate); } else { runColumnReader(predicate); } }', 'ground_truth': 'public void run(Cassandra.Client client) throws IOException { SliceRange sliceRange = new SliceRange(); sliceRange.setStart(new byte[] {}).setFinish(new byte[] {}); sliceRange.setReversed(false).setCount(session.getColumnsPerKey()); SlicePredicate predicate = new SlicePredicate().setSlice_range(sliceRange); if (session.getColumnFamilyType() == ColumnFamilyType.Super) { runSuperColumnReader(predicate, client); } else { runColumnReader(predicate, client); } }', 'output': 'public void run() { SliceRange sliceRange = new SliceRange(); sliceRange.setStart(new byte[] {}).setFinish(new byte[] {}); sliceRange.setReversed(false).setCount(session.getColumnsPerKey()); SlicePredicate predicate = new SlicePredicate().setSlice_range(sliceRange); if (session.getColumnFamilyType() == ColumnFamilyType.Super) { runSuperColumnReader(predicate); } else { runColumnReader(predicate); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean store(final String space, final KeyCacheObject key, int partId, final CacheObject val, GridCacheVersion ver, long expirationTime) throws IgniteCheckedException { assert key != null; assert val != null; if (log.isDebugEnabled()) log.debug("Store [space=" + space + ", key=" + key + ", val=" + val + "]"); CacheObjectContext coctx = null; if (ctx.indexing().enabled()) { coctx = cacheObjectContext(space); ctx.indexing().store(space, key.value(coctx, false), val.value(coctx, false), expirationTime); } if (idx == null) return false; if (!busyLock.enterBusy()) throw new IllegalStateException("Failed to write to index (grid is stopping)."); try { if (coctx == null) coctx = cacheObjectContext(space); Class<?> valCls = null; TypeId id; boolean binaryVal = ctx.cacheObjects().isBinaryObject(val); if (binaryVal) { int typeId = ctx.cacheObjects().typeId(val); id = new TypeId(space, typeId); } else { valCls = val.value(coctx, false).getClass(); id = new TypeId(space, valCls); } TypeDescriptor desc = types.get(id); if (desc == null || !desc.registered()) return false; if (!binaryVal && !desc.valueClass().isAssignableFrom(valCls)) throw new IgniteCheckedException("Failed to update index due to class name conflict" + "(multiple classes with same simple name are stored in the same cache) " + "[expCls=" + desc.valueClass().getName() + ", actualCls=" + valCls.getName() + \']\'); if (!ctx.cacheObjects().isBinaryObject(key)) { Class<?> keyCls = key.value(coctx, false).getClass(); if (!desc.keyClass().isAssignableFrom(keyCls)) throw new IgniteCheckedException("Failed to update index, incorrect key class [expCls=" + desc.keyClass().getName() + ", actualCls=" + keyCls.getName() + "]"); } return idx.store(space, desc, key, partId, val, ver, expirationTime); } finally { busyLock.leaveBusy(); } }', 'ground_truth': 'public boolean store(final String space, final KeyCacheObject key, int partId, final CacheObject val, GridCacheVersion ver, long expirationTime, long link) throws IgniteCheckedException { assert key != null; assert val != null; if (log.isDebugEnabled()) log.debug("Store [space=" + space + ", key=" + key + ", val=" + val + "]"); CacheObjectContext coctx = null; if (ctx.indexing().enabled()) { coctx = cacheObjectContext(space); ctx.indexing().store(space, key.value(coctx, false), val.value(coctx, false), expirationTime); } if (idx == null) return false; if (!busyLock.enterBusy()) throw new IllegalStateException("Failed to write to index (grid is stopping)."); try { if (coctx == null) coctx = cacheObjectContext(space); Class<?> valCls = null; TypeId id; boolean binaryVal = ctx.cacheObjects().isBinaryObject(val); if (binaryVal) { int typeId = ctx.cacheObjects().typeId(val); id = new TypeId(space, typeId); } else { valCls = val.value(coctx, false).getClass(); id = new TypeId(space, valCls); } TypeDescriptor desc = types.get(id); if (desc == null || !desc.registered()) return false; if (!binaryVal && !desc.valueClass().isAssignableFrom(valCls)) throw new IgniteCheckedException("Failed to update index due to class name conflict" + "(multiple classes with same simple name are stored in the same cache) " + "[expCls=" + desc.valueClass().getName() + ", actualCls=" + valCls.getName() + \']\'); if (!ctx.cacheObjects().isBinaryObject(key)) { Class<?> keyCls = key.value(coctx, false).getClass(); if (!desc.keyClass().isAssignableFrom(keyCls)) throw new IgniteCheckedException("Failed to update index, incorrect key class [expCls=" + desc.keyClass().getName() + ", actualCls=" + keyCls.getName() + "]"); } return idx.store(space, desc, key, partId, val, ver, expirationTime, link); } finally { busyLock.leaveBusy(); } }', 'output': 'public boolean store(final String space, final KeyCacheObject key, int partId, final CacheObject val, GridCacheVersion ver, long expirationTime) throws IgniteCheckedException { assert key != null; assert val != null; if (log.isDebugEnabled()) log.debug("Store [space=" + space + ", key=" + key + ", val=" + val + "]"); CacheObjectContext coctx = null; if (ctx.indexing().enabled()) { coctx = cacheObjectContext(space); ctx.indexing().store(space, key.value(coctx, false), val.value(coctx, false), expirationTime); } if (idx == null) return false; if (!busyLock.enterBusy()) throw new IllegalStateException("Failed to write to index (grid is stopping)."); try { if (coctx == null) coctx = cacheObjectContext(space); Class<?> valCls = null; TypeId id; boolean binaryVal = ctx.cacheObjects().isBinaryObject(val); if (binaryVal) { int typeId = ctx.cacheObjects().typeId(val); id = new TypeId(space, typeId); } else { valCls = val.value(coctx, false).getClass(); id = new TypeId(space, valCls); } TypeDescriptor desc = types.get(id); if (desc == null || !desc.registered()) return false; if (!binaryVal && !desc.valueClass().isAssignableFrom(valCls)) throw new IgniteCheckedException("Failed to update index due to class name conflict" + "(multiple classes with same simple name are stored in the same cache) " + "[expCls=" + desc.valueClass().getName() + ", actualCls=" + valCls.getName() + \']\'); if (!ctx.cacheObjects().isBinaryObject(key)) { Class<?> keyCls = key.value(coctx, false).getClass(); if (!desc.keyClass().isAssignableFrom(keyCls)) throw new IgniteCheckedException("Failed to update index, incorrect key class [expCls=" + desc.keyClass().getName() + ", actualCls=" + keyCls.getName() + "]"); } return idx.store(space, desc, key, partId, val, ver,'}
{'instruction': 'Please refactor the following code:', 'input': 'public void localWalEnabled(boolean enabled) { if (localWalEnabled != enabled){ log.info("Local WAL state for group=" + cacheOrGroupName() + " changed from " + localWalEnabled + " to " + enabled); persistLocalWalState(enabled); localWalEnabled = enabled; } }', 'ground_truth': 'public void localWalEnabled(boolean enabled, boolean persist) { if (localWalEnabled != enabled){ log.info("Local WAL state for group=" + cacheOrGroupName() + " changed from " + localWalEnabled + " to " + enabled); if (persist) persistLocalWalState(enabled); localWalEnabled = enabled; } }', 'output': 'public void localWalEnabled(boolean enabled, boolean force) { if (localWalEnabled != enabled || force) { log.info("Local WAL state for group=" + cacheOrGroupName() + " changed from " + localWalEnabled + " to " + enabled); persistLocalWalState(enabled); localWalEnabled = enabled; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Memtable memtable() { return new Memtable(cfs.metadata); }', 'ground_truth': 'public static Memtable memtable(ColumnFamilyStore cfs) { return new Memtable(cfs.metadata); }', 'output': 'public static Memtable memtable(CFMetaData metadata) { return new Memtable(metadata); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void newRow() throws InvalidRequestException { if (current != null) { selectors.addInputRow(this); if (!selectors.isAggregate()) { resultSet.addRow(selectors.getOutputRow()); selectors.reset(); } } current = new ArrayList<ByteBuffer>(columns.size()); }', 'ground_truth': 'public void newRow(int protocolVersion) throws InvalidRequestException { if (current != null) { selectors.addInputRow(protocolVersion, this); if (!selectors.isAggregate()) { resultSet.addRow(selectors.getOutputRow(protocolVersion)); selectors.reset(); } } current = new ArrayList<ByteBuffer>(columns.size()); }', 'output': 'public void newRow() throws InvalidRequestException { if (current != null) { selectors.addInputRow(this); if (!selectors.isAggregate()) { resultSet.addRow(selectors.getOutputRow()); selectors.reset(); } } current = new ArrayList<ByteBuffer>(columns.size()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private List<ColumnOrSuperColumn> thriftifyCounterSuperColumns(Collection<org.apache.cassandra.db.Column> columns, boolean reverseOrder) { ArrayList<ColumnOrSuperColumn> thriftSuperColumns = new ArrayList<ColumnOrSuperColumn>(columns.size()); CounterSuperColumn current = null; for (org.apache.cassandra.db.Column column : columns) { if (column.isMarkedForDelete()) continue; ByteBuffer scName = SuperColumns.scName(column.name()); if (current == null || !scName.equals(current.bufferForName())) { current = new CounterSuperColumn(scName, new ArrayList<CounterColumn>()); thriftSuperColumns.add(new ColumnOrSuperColumn().setCounter_super_column(current)); } current.getColumns().add(thriftifySubCounter(column).setName(SuperColumns.subName(column.name()))); } if (reverseOrder) Collections.reverse(thriftSuperColumns); return thriftSuperColumns; }', 'ground_truth': 'private List<ColumnOrSuperColumn> thriftifyCounterSuperColumns(Collection<org.apache.cassandra.db.Column> columns, boolean reverseOrder, long now) { ArrayList<ColumnOrSuperColumn> thriftSuperColumns = new ArrayList<ColumnOrSuperColumn>(columns.size()); CounterSuperColumn current = null; for (org.apache.cassandra.db.Column column : columns) { if (column.isMarkedForDelete(now)) continue; ByteBuffer scName = SuperColumns.scName(column.name()); if (current == null || !scName.equals(current.bufferForName())) { current = new CounterSuperColumn(scName, new ArrayList<CounterColumn>()); thriftSuperColumns.add(new ColumnOrSuperColumn().setCounter_super_column(current)); } current.getColumns().add(thriftifySubCounter(column).setName(SuperColumns.subName(column.name()))); } if (reverseOrder) Collections.reverse(thriftSuperColumns); return thriftSuperColumns; }', 'output': 'private List<ColumnOrSuperColumn> thriftifyCounterSuperColumns(Collection<org.apache.cassandra.db.Column> columns, boolean reverseOrder) { ArrayList<ColumnOrSuperColumn> thriftSuperColumns = new ArrayList<ColumnOrSuperColumn>(columns.size()); CounterSuperColumn current = null; for (org.apache.cassandra.db.Column column : columns) { if (column.isMarkedForDelete()) continue; ByteBuffer scName = SuperColumns.scName(column.name()); if (current == null || !scName.equals(current.bufferForName())) { current = new CounterSuperColumn(scName, new ArrayList<CounterColumn>()); thriftSuperColumns.add(new ColumnOrSuperColumn().setCounter_super_column(current)); } current.getColumns().add(thriftifySubCounter(column).setName(SuperColumns.subName(column.name()))); } if (reverseOrder) Collections.reverse(thriftSuperColumns); return thriftSuperColumns; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void rollbackTx(IgniteInternalTx tx, boolean clearThreadMap) { assert tx != null; if (log.isDebugEnabled()) log.debug("Rolling back from TM [locNodeId=" + cctx.localNodeId() + ", tx=" + tx + \']\'); addRolledbackTx(tx); ConcurrentMap<GridCacheVersion, IgniteInternalTx> txIdMap = transactionMap(tx); if (txIdMap.remove(tx.xidVersion(), tx)) { unlockMultiple(tx, tx.writeEntries()); if (unlockReadEntries(tx)) unlockMultiple(tx, tx.readEntries()); notifyEvictions(tx); removeObsolete(tx); if (clearThreadMap) clearThreadMap(tx); if (!tx.alternateVersions().isEmpty()) for (GridCacheVersion ver : tx.alternateVersions()) idMap.remove(ver); if (tx instanceof GridCacheMappedVersion) mappedVers.remove(((GridCacheMappedVersion)tx).mappedVersion()); resetContext(); if (!tx.dht() && tx.local()) { if (!tx.system()) cctx.txMetrics().onTxRollback(); tx.txState().onTxEnd(cctx, tx, false); } if (log.isDebugEnabled()) log.debug("Rolled back from TM: " + tx); } else if (log.isDebugEnabled()) log.debug("Did not rollback from TM (was already rolled back): " + tx); }', 'ground_truth': 'public void rollbackTx(IgniteInternalTx tx, boolean clearThreadMap, boolean skipCompletedVers) { assert tx != null; if (log.isDebugEnabled()) log.debug("Rolling back from TM [locNodeId=" + cctx.localNodeId() + ", tx=" + tx + \']\'); if (!skipCompletedVers) addRolledbackTx(tx); ConcurrentMap<GridCacheVersion, IgniteInternalTx> txIdMap = transactionMap(tx); if (txIdMap.remove(tx.xidVersion(), tx)) { unlockMultiple(tx, tx.writeEntries()); if (unlockReadEntries(tx)) unlockMultiple(tx, tx.readEntries()); notifyEvictions(tx); removeObsolete(tx); if (clearThreadMap) clearThreadMap(tx); if (!tx.alternateVersions().isEmpty()) for (GridCacheVersion ver : tx.alternateVersions()) idMap.remove(ver); if (tx instanceof GridCacheMappedVersion) mappedVers.remove(((GridCacheMappedVersion)tx).mappedVersion()); resetContext(); if (!tx.dht() && tx.local()) { if (!tx.system()) cctx.txMetrics().onTxRollback(); tx.txState().onTxEnd(cctx, tx, false); } if (log.isDebugEnabled()) log.debug("Rolled back from TM: " + tx); } else if (log.isDebugEnabled()) log.debug("Did not rollback from TM (was already rolled back): " + tx); }', 'output': 'public void rollbackTx(IgniteInternalTx tx, boolean clearThreadMap) { assert tx != null; if (log.isDebugEnabled()) log.debug("Rolling back from TM [locNodeId=" + cctx.localNodeId() + ", tx=" + tx + \']\'); addRolledbackTx(tx); ConcurrentMap<GridCacheVersion, IgniteInternalTx> txIdMap = transactionMap(tx); if (txIdMap.remove(tx.xidVersion(), tx)) { unlockMultiple(tx, tx.writeEntries()); if (unlockReadEntries(tx)) unlockMultiple(tx, tx.readEntries()); notifyEvictions(tx); removeObsolete(tx); if (clearThreadMap) clearThreadMap(tx); if (!tx.alternateVersions().isEmpty()) for (GridCacheVersion ver : tx.alternateVersions()) idMap.remove(ver); if (tx instanceof GridCacheMappedVersion) mappedVers.remove(((GridCacheMappedVersion)tx).mappedVersion()); resetContext(); if (!tx.dht() && tx.local()) { if (!tx.system()) cctx.txMetrics().onTxRollback(); tx.txState().onTxEnd(cctx, tx, false); } if (log.isDebugEnabled()) log.debug("Rolled back from TM: " + tx); } else if (log.isDebugEnabled()) log.debug("Did not rollback from TM (was already rolled back): " + tx); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void visitGetter(final PropertyNode node, final Statement getterBlock, final int getterModifiers, final String getterName) { MethodNode getter = new MethodNode(getterName, getterModifiers, node.getType(), Parameter.EMPTY_ARRAY, ClassNode.EMPTY_ARRAY, getterBlock); getter.setSynthetic(true); addPropertyMethod(getter); visitMethod(getter); }', 'ground_truth': 'private void visitGetter(final PropertyNode node, final FieldNode field, final Statement getterBlock, final int getterModifiers, final String getterName) { MethodNode getter = new MethodNode(getterName, getterModifiers, node.getType(), Parameter.EMPTY_ARRAY, ClassNode.EMPTY_ARRAY, getterBlock); getter.setSynthetic(true); addPropertyMethod(getter); if (!field.isSynthetic()) { copyMethodAnnotations(node, getter); } visitMethod(getter); }', 'output': 'private void visitGetter(final PropertyNode node, final Statement getterBlock, final int getterModifiers, final String getterName, final List<Parameter> parameters) { MethodNode getter = new MethodNode(getterName, getterModifiers, node.getType(), parameters, ClassNode.EMPTY_ARRAY, getterBlock); getter.setSynthetic(true); addPropertyMethod(getter); visitMethod(getter); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void setBootstrapped() { ColumnFamily cf = ColumnFamily.create(Table.SYSTEM_TABLE, STATUS_CF); cf.addColumn(new Column(BOOTSTRAP, new byte[] {1}, System.currentTimeMillis())); RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, BOOTSTRAP_KEY); rm.add(cf); try { rm.apply(); } catch (IOException e) { throw new RuntimeException(e); } }', 'ground_truth': 'public static void setBootstrapped(boolean isBootstrapped) { ColumnFamily cf = ColumnFamily.create(Table.SYSTEM_TABLE, STATUS_CF); cf.addColumn(new Column(BOOTSTRAP, new byte[] { (byte) (isBootstrapped ? 1 : 0) }, System.currentTimeMillis())); RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, BOOTSTRAP_KEY); rm.add(cf); try { rm.apply(); } catch (IOException e) { throw new RuntimeException(e); } }', 'output': 'public static void setBootstrapped() { ColumnFamily cf = ColumnFamily.create(Table.SYSTEM_TABLE, STATUS_CF); cf.addColumn(new Column(BOOTSTRAP, new byte[] {1}, System.currentTimeMillis())); RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, BOOTSTRAP_KEY); rm.add(cf); try { rm.apply(); } catch (IOException e) { throw new RuntimeException(e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private IgniteInternalFuture<?> rebuildIndexes(@Nullable final String space, @Nullable final TypeDescriptor desc) { if (idx == null) return new GridFinishedFuture<>(new IgniteCheckedException("Indexing is disabled.")); if (desc == null || !desc.registered()) return new GridFinishedFuture<Void>(); final GridWorkerFuture<?> fut = new GridWorkerFuture<Void>(); GridWorker w = new GridWorker(ctx.gridName(), "index-rebuild-worker", log) { @Override protected void body() { try { idx.rebuildIndexes(space, desc); fut.onDone(); } catch (Exception e) { fut.onDone(e); } catch (Throwable e) { log.error("Failed to rebuild indexes for type: " + desc.name(), e); fut.onDone(e); if (e instanceof Error) throw e; } } }; fut.setWorker(w); execSvc.execute(w); return fut; }', 'ground_truth': 'private IgniteInternalFuture<?> rebuildIndexes(@Nullable final String space, @Nullable final TypeDescriptor desc, final boolean fromHash) { if (idx == null) return new GridFinishedFuture<>(new IgniteCheckedException("Indexing is disabled.")); if (desc == null || !desc.registered()) return new GridFinishedFuture<Void>(); final GridWorkerFuture<?> fut = new GridWorkerFuture<Void>(); if (fromHash) idx.markForRebuildFromHash(space, desc); GridWorker w = new GridWorker(ctx.gridName(), "index-rebuild-worker", log) { @Override protected void body() { try { if (fromHash) idx.rebuildIndexesFromHash(space, desc); else idx.rebuildIndexes(space, desc); fut.onDone(); } catch (Exception e) { fut.onDone(e); } catch (Throwable e) { log.error("Failed to rebuild indexes for type: " + desc.name(), e); fut.onDone(e); if (e instanceof Error) throw e; } } }; fut.setWorker(w); execSvc.execute(w); return fut; }', 'output': 'private IgniteInternalFuture<?> rebuildIndexes(@Nullable final String space, @Nullable final TypeDescriptor desc) { if (idx == null) return new GridFinishedFuture<>(new IgniteCheckedException("Indexing is disabled.")); if (desc == null || !desc.registered()) return new GridFinishedFuture<Void>(); final GridWorkerFuture<?> fut = new GridWorkerFuture<Void>(); GridWorker w = new GridWorker(ctx.gridName(), "index-rebuild-worker", log) { @Override protected void body() { try { idx.rebuildIndexes(space, desc); fut.onDone(); } catch (Exception e) { fut.onDone(e); } catch (Throwable e) { log.error("Failed to rebuild indexes for type: " + desc.name(), e); fut.onDone(e); if (e instanceof Error) throw e; } } }; fut.setWorker(w); execSvc.execute(w); return fut; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public LocalPersistenceManagerFactoryBean getLocalPersistenceManagerFactoryBean( final IsisConfiguration isisConfiguration, final DataSource dataSource, final MetaModelContext metaModelContext, final EventBusService eventBusService, final Provider<EntityChangeTracker> entityChangeTrackerProvider, final IsisBeanTypeRegistry beanTypeRegistry, final DatanucleusSettings dnSettings) { _Assert.assertNotNull(dataSource, "a datasource is required"); final Set<String> classNamesNotEnhanced = new LinkedHashSet<>(); autoCreateSchemas(dataSource, isisConfiguration); val lpmfBean = new LocalPersistenceManagerFactoryBean() { @Override protected PersistenceManagerFactory newPersistenceManagerFactory(final java.util.Map<?,?> props) { val pu = createDefaultPersistenceUnit(beanTypeRegistry); val pmf = new JDOPersistenceManagerFactory(pu, props); pmf.setConnectionFactory(dataSource); integrateWithApplicationLayer(metaModelContext, eventBusService, entityChangeTrackerProvider, pmf); return pmf; } @Override protected PersistenceManagerFactory newPersistenceManagerFactory(final String name) { val pmf = super.newPersistenceManagerFactory(name); pmf.setConnectionFactory(dataSource); integrateWithApplicationLayer(metaModelContext, eventBusService, entityChangeTrackerProvider, pmf); return pmf; } }; lpmfBean.setJdoPropertyMap(dnSettings.getAsProperties()); return lpmfBean; }', 'ground_truth': 'public LocalPersistenceManagerFactoryBean getLocalPersistenceManagerFactoryBean( final IsisConfiguration isisConfiguration, final DataSource dataSource, final MetaModelContext metaModelContext, final EventBusService eventBusService, final ObjectLifecyclePublisher objectLifecyclePublisher, final Provider<EntityChangeTracker> entityChangeTrackerProvider, final IsisBeanTypeRegistry beanTypeRegistry, final DatanucleusSettings dnSettings) { _Assert.assertNotNull(dataSource, "a datasource is required"); final Set<String> classNamesNotEnhanced = new LinkedHashSet<>(); autoCreateSchemas(dataSource, isisConfiguration); val lpmfBean = new LocalPersistenceManagerFactoryBean() { @Override protected PersistenceManagerFactory newPersistenceManagerFactory(final java.util.Map<?,?> props) { val pu = createDefaultPersistenceUnit(beanTypeRegistry); val pmf = new JDOPersistenceManagerFactory(pu, props); pmf.setConnectionFactory(dataSource); integrateWithApplicationLayer(metaModelContext, entityChangeTrackerProvider, objectLifecyclePublisher, pmf); return pmf; } @Override protected PersistenceManagerFactory newPersistenceManagerFactory(final String name) { val pmf = super.newPersistenceManagerFactory(name); pmf.setConnectionFactory(dataSource); integrateWithApplicationLayer(metaModelContext, entityChangeTrackerProvider, objectLifecyclePublisher, pmf); return pmf; } }; lpmfBean.setJdoPropertyMap(dnSettings.getAsProperties()); return lpmfBean; }', 'output': 'public LocalPersistenceManagerFactoryBean getLocalPersistenceManagerFactoryBean( final IsisConfiguration isisConfiguration, final DataSource dataSource, final MetaModelContext metaModelContext, final EventBusService eventBusService, final Provider<EntityChangeTracker> entityChangeTrackerProvider, final IsisBeanTypeRegistry beanTypeRegistry, final DatanucleusSettings dnSettings) { _Assert.assertNotNull(dataSource, "a datasource is required"); final Set<String> classNamesNotEnhanced = new LinkedHashSet<>(); autoCreateSchemas(dataSource, isisConfiguration); val lpmfBean = new LocalPersistenceManagerFactoryBean() { @Override protected PersistenceManagerFactory newPersistenceManagerFactory(final Map<?,?> props) { val pu = createDefaultPersistenceUnit(beanTypeRegistry); val pmf = new JDOPersistenceManagerFactory(pu, props); pmf.setConnectionFactory(dataSource); integrateWithApplicationLayer(metaModelContext, eventBusService, entityChangeTrackerProvider, pmf); return pmf; } @Override protected PersistenceManagerFactory newPersistenceManagerFactory(final String name) { val pmf = super.newPersistenceManagerFactory(name); pmf.setConnectionFactory(dataSource); integrateWithApplicationLayer(metaModelContext, eventBusService, entityChangeTrackerProvider, pmf); return pmf; } }; lpmfBean.setJdoPropertyMap(dnSettings.getAsProperties()); return lpmfBean; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private IgniteInternalFuture<IgniteInternalTx> chainFinishFuture(final NearTxFinishFuture fut, final boolean commit, final boolean clearThreadMap) { assert fut != null; if (fut.commit() != commit) { final GridNearTxLocal tx = this; if (!commit) { final GridNearTxFinishFuture rollbackFut = new GridNearTxFinishFuture<>(cctx, this, false); fut.listen(new IgniteInClosure<IgniteInternalFuture<IgniteInternalTx>>() { @Override public void apply(IgniteInternalFuture<IgniteInternalTx> fut0) { if (FINISH_FUT_UPD.compareAndSet(tx, fut, rollbackFut)) { if (tx.state() == COMMITTED) { if (log.isDebugEnabled()) log.debug("Failed to rollback, transaction is already committed: " + tx); rollbackFut.forceFinish(); assert rollbackFut.isDone() : rollbackFut; } else { if (!cctx.mvcc().addFuture(rollbackFut, rollbackFut.futureId())) return; rollbackFut.finish(false, clearThreadMap); } } } }); return rollbackFut; } else { final GridFutureAdapter<IgniteInternalTx> fut0 = new GridFutureAdapter<>(); fut.listen(new IgniteInClosure<IgniteInternalFuture<IgniteInternalTx>>() { @Override public void apply(IgniteInternalFuture<IgniteInternalTx> fut) { if (timedOut()) fut0.onDone(new IgniteTxTimeoutCheckedException("Failed to commit transaction, " + "transaction is concurrently rolled back on timeout: " + tx)); else fut0.onDone(new IgniteCheckedException("Failed to commit transaction, " + "transaction is concurrently rolled back: " + tx)); } }); return fut0; } } return fut; }', 'ground_truth': 'private IgniteInternalFuture<IgniteInternalTx> chainFinishFuture(final NearTxFinishFuture fut, final boolean commit, final boolean clearThreadMap, final boolean onTimeout) { assert fut != null; if (fut.commit() != commit) { final GridNearTxLocal tx = this; if (!commit) { final GridNearTxFinishFuture rollbackFut = new GridNearTxFinishFuture<>(cctx, this, false); fut.listen(new IgniteInClosure<IgniteInternalFuture<IgniteInternalTx>>() { @Override public void apply(IgniteInternalFuture<IgniteInternalTx> fut0) { if (FINISH_FUT_UPD.compareAndSet(tx, fut, rollbackFut)) { if (tx.state() == COMMITTED) { if (log.isDebugEnabled()) log.debug("Failed to rollback, transaction is already committed: " + tx); rollbackFut.forceFinish(); assert rollbackFut.isDone() : rollbackFut; } else { if (!cctx.mvcc().addFuture(rollbackFut, rollbackFut.futureId())) return; rollbackFut.finish(false, clearThreadMap, onTimeout); } } else { finishFut.listen(new IgniteInClosure<IgniteInternalFuture<IgniteInternalTx>>() { @Override public void apply(IgniteInternalFuture<IgniteInternalTx> fut) { try { fut.get(); rollbackFut.markInitialized(); } catch (IgniteCheckedException e) { rollbackFut.onDone(e); } } }); } } }); return rollbackFut; } else { final GridFutureAdapter<IgniteInternalTx> fut0 = new GridFutureAdapter<>(); fut.listen(new IgniteInClosure<IgniteInternalFuture<IgniteInternalTx>>() { @Override public void apply(IgniteInternalFuture<IgniteInternalTx> fut) { if (timedOut()) fut0.onDone(new IgniteTxTimeoutCheckedException("Failed to commit transaction, " + "transaction is concurrently rolled back on timeout: " + tx)); else fut0.onDone(new IgniteTxRollbackCheckedException("Failed to commit transaction, " + "transaction is concurrently rolled back: " + tx)); } }); return fut0; } } return fut; }', 'output': 'private IgniteInternalFuture<IgniteInternalTx> chainFinishFuture(final NearTxFinishFuture fut, final boolean commit, final boolean clearThreadMap) { assert fut != null; if (fut.commit() != commit) { final GridNearTxLocal tx = this; if (!commit) { final GridNearTxFinishFuture rollbackFut = new GridNearTxFinishFuture<>(cctx, this, false); fut.listen(new IgniteInClosure<IgniteInternalFuture<IgniteInternalTx>>() { @Override public void apply(IgniteInternalFuture<IgniteInternalTx> fut0) { if (FINISH_FUT_UPD.compareAndSet(tx, fut, rollbackFut)) { if (tx.state() == COMMITTED) { if (log.isDebugEnabled()) log.debug("Failed to rollback, transaction is already committed: " + tx); rollbackFut.forceFinish(); assert rollbackFut.isDone() : rollbackFut; } else { if (!cctx.mvcc().addFuture(rollbackFut, rollbackFut.futureId())) return; rollbackFut.finish(false, clearThreadMap); } } } }); return rollbackFut; } else { final GridFutureAdapter<IgniteInternalTx> fut0 = new GridFutureAdapter<>(); fut.listen(new IgniteInClosure<IgniteInternalFuture<IgniteInternalTx>>() { @Override public void apply(IgniteInternalFuture<IgniteInternalTx> fut) { if (timedOut()) fut0.onDone(new IgniteTxTimeoutCheckedException("Failed to commit transaction, " + "transaction is concurrently rolled back on timeout: " + tx)); else fut0.onDone(new IgniteCheckedException("Failed to commit transaction, " + "transaction is concurrently rolled back: " + tx)); } }); return fut0; } } return fut; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private synchronized LogLevelFilter findLogFilter(String logId) { LogLevelFilter logFilter = filters.get(logId); if (logFilter != null) { return logFilter; } LOG.info("Filter is not present for log " + logId + ", creating default filter"); LogLevelFilter defaultFilter = new LogLevelFilter(); defaultFilter.setLabel(logId); defaultFilter.setDefaultLevels(logFeederProps.getIncludeDefaultLogLevels()); try { config.getLogLevelFilterManager().createLogLevelFilter(logFeederProps.getClusterName(), logId, defaultFilter); filters.put(logId, defaultFilter); } catch (Exception e) { LOG.warn("Could not persist the default filter for log " + logId, e); } return defaultFilter; }', 'ground_truth': 'private synchronized LogLevelFilter findLogFilter(String logId, List<String> defaultLogLevels) { LogLevelFilter logFilter = filters.get(logId); if (logFilter != null) { return logFilter; } LOG.info("Filter is not present for log " + logId + ", creating default filter"); LogLevelFilter defaultFilter = new LogLevelFilter(); defaultFilter.setLabel(logId); defaultFilter.setDefaultLevels(defaultLogLevels); try { config.getLogLevelFilterManager().createLogLevelFilter(logFeederProps.getClusterName(), logId, defaultFilter); filters.put(logId, defaultFilter); } catch (Exception e) { LOG.warn("Could not persist the default filter for log " + logId, e); } return defaultFilter; }', 'output': 'private synchronized LogLevelFilter findOrCreateLogFilter(String logId) { LogLevelFilter logFilter = filters.get(logId); if (logFilter != null) { return logFilter; } LOG.info("Filter is not present for log " + logId + ", creating default filter"); LogLevelFilter defaultFilter = new LogLevelFilter(); defaultFilter.setLabel(logId); defaultFilter.setDefaultLevels(logFeederProps.getIncludeDefaultLogLevels()); try { config.getLogLevelFilterManager().createLogLevelFilter(logFeederProps.getClusterName(), logId, defaultFilter); filters.put(logId, defaultFilter); } catch (Exception e) { LOG.warn("Could not persist the default filter for log " + logId, e); } return defaultFilter; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean managesRepairedGroup(boolean isRepaired, boolean isPendingRepair) { Preconditions.checkArgument(!isPendingRepair || !isRepaired, "SSTables cannot be both repaired and pending repair"); return !isPendingRepair && (isRepaired == this.isRepaired); }', 'ground_truth': 'public boolean managesRepairedGroup(boolean isRepaired, boolean isPendingRepair, boolean isTransient) { if (!isPendingRepair) { Preconditions.checkArgument(!isTransient, "isTransient can only be true for sstables pending repairs"); return this.isRepaired == isRepaired; } else { Preconditions.checkArgument(!isRepaired, "SSTables cannot be both repaired and pending repair"); return false; } }', 'output': 'public boolean managesRepairedGroup(boolean isRepaired, boolean isPendingRepair) { Preconditions.checkArgument(isPendingRepair != isRepaired, "SSTables cannot be both repaired and pending repair"); return !isPendingRepair && (isRepaired == this.isRepaired); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public H2TreeTest( GridCacheContext cctx, GridH2Table table, String name, String idxName, String cacheName, String tblName, ReuseList reuseList, int grpId, String grpName, PageMemory pageMem, IgniteWriteAheadLogManager wal, AtomicLong globalRmvId, long metaPageId, boolean initNew, List<IndexColumn> unwrappedCols, List<IndexColumn> wrappedCols, AtomicInteger maxCalculatedInlineSize, boolean pk, boolean affinityKey, boolean mvccEnabled, @Nullable H2RowCache rowCache, @Nullable FailureProcessor failureProcessor, IgniteLogger log, IoStatisticsHolder stats, InlineIndexColumnFactory factory, int configuredInlineSize ) throws IgniteCheckedException { super( cctx, table, name, idxName, cacheName, tblName, reuseList, grpId, grpName, pageMem, wal, globalRmvId, metaPageId, initNew, unwrappedCols, wrappedCols, maxCalculatedInlineSize, pk, affinityKey, mvccEnabled, rowCache, failureProcessor, log, stats, factory, configuredInlineSize ); }', 'ground_truth': 'public H2TreeTest( GridCacheContext cctx, GridH2Table table, String name, String idxName, String cacheName, String tblName, ReuseList reuseList, int grpId, String grpName, PageMemory pageMem, IgniteWriteAheadLogManager wal, AtomicLong globalRmvId, long metaPageId, boolean initNew, List<IndexColumn> unwrappedCols, List<IndexColumn> wrappedCols, AtomicInteger maxCalculatedInlineSize, boolean pk, boolean affinityKey, boolean mvccEnabled, @Nullable H2RowCache rowCache, @Nullable FailureProcessor failureProcessor, IgniteLogger log, IoStatisticsHolder stats, InlineIndexColumnFactory factory, int configuredInlineSize, PageIoResolver pageIoRslvr ) throws IgniteCheckedException { super( cctx, table, name, idxName, cacheName, tblName, reuseList, grpId, grpName, pageMem, wal, globalRmvId, metaPageId, initNew, unwrappedCols, wrappedCols, maxCalculatedInlineSize, pk, affinityKey, mvccEnabled, rowCache, failureProcessor, log, stats, factory, configuredInlineSize, pageIoRslvr ); }', 'output': 'public H2TreeTest( GridCacheContext cctx, H2Table table, String name, String idxName, String cacheName, String tblName, ReuseList reuseList, int grpId, String grpName, PageMemory pageMem, IgniteWriteAheadLogManager wal, AtomicLong globalRmvId, long metaPageId, boolean initNew, List<IndexColumn> unwrappedCols, List<IndexColumn> wrappedCols, AtomicInteger maxCalculatedInlineSize, boolean pk, boolean affinityKey, boolean mvccEnabled, @Nullable H2RowCache rowCache, @Nullable FailureProcessor failureProcessor, IgniteLogger log, IoStatisticsHolder stats, InlineIndexColumnFactory factory, int configuredInlineSize ) throws IgniteCheckedException { super( cctx, table, name, idxName, cacheName, tblName, reuseList, grpId, grpName, pageMem, wal, globalRmvId, metaPageId, initNew, unwrappedCols, wrappedCols, maxCalculatedInlineSize, pk, affinityKey, mvccEnabled, rowCache, failureProcessor, log, stats, factory, configuredInlineSize ); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private IgniteInternalFuture<Void> loadMissing( final GridCacheContext cacheCtx, final Set<KeyCacheObject> keys, final CacheEntryPredicate[] filter, final GridCacheReturn ret, final boolean needReadVer, final boolean singleRmv, final boolean hasFilters, final boolean skipStore, final boolean retval) { GridInClosure3<KeyCacheObject, Object, GridCacheVersion> c = new GridInClosure3<KeyCacheObject, Object, GridCacheVersion>() { @Override public void apply(KeyCacheObject key, @Nullable Object val, @Nullable GridCacheVersion loadVer) { if (log.isDebugEnabled()) log.debug("Loaded value from remote node [key=" + key + ", val=" + val + \']\'); IgniteTxEntry e = entry(new IgniteTxKey(key, cacheCtx.cacheId())); assert e != null; if (needReadVer) { assert loadVer != null; e.serializableReadVersion(singleRmv && val != null ? SER_READ_NOT_EMPTY_VER : loadVer); } if (singleRmv) { assert !hasFilters && !retval; assert val == null || Boolean.TRUE.equals(val) : val; ret.set(cacheCtx, null, val != null, keepBinary); } else { CacheObject cacheVal = cacheCtx.toCacheObject(val); if (e.op() == TRANSFORM) { GridCacheVersion ver; try { ver = e.cached().version(); } catch (GridCacheEntryRemovedException ex) { assert optimistic() : e; if (log.isDebugEnabled()) log.debug("Failed to get entry version: [msg=" + ex.getMessage() + \']\'); ver = null; } addInvokeResult(e, cacheVal, ret, ver); } else { boolean success = !hasFilters || isAll(e.context(), key, cacheVal, filter); ret.set(cacheCtx, cacheVal, success); } } } }; return loadMissing( cacheCtx, /*read through*/cacheCtx.config().isLoadPreviousValue() && !skipStore, /*async*/true, keys, /*skipVals*/singleRmv, needReadVer, c); }', 'ground_truth': 'private IgniteInternalFuture<Void> loadMissing( final GridCacheContext cacheCtx, final Set<KeyCacheObject> keys, final CacheEntryPredicate[] filter, final GridCacheReturn ret, final boolean needReadVer, final boolean singleRmv, final boolean hasFilters, final boolean skipStore, final boolean retval, final boolean keepBinary) { GridInClosure3<KeyCacheObject, Object, GridCacheVersion> c = new GridInClosure3<KeyCacheObject, Object, GridCacheVersion>() { @Override public void apply(KeyCacheObject key, @Nullable Object val, @Nullable GridCacheVersion loadVer) { if (log.isDebugEnabled()) log.debug("Loaded value from remote node [key=" + key + ", val=" + val + \']\'); IgniteTxEntry e = entry(new IgniteTxKey(key, cacheCtx.cacheId())); assert e != null; if (needReadVer) { assert loadVer != null; e.serializableReadVersion(singleRmv && val != null ? SER_READ_NOT_EMPTY_VER : loadVer); } if (singleRmv) { assert !hasFilters && !retval; assert val == null || Boolean.TRUE.equals(val) : val; ret.set(cacheCtx, null, val != null, keepBinary); } else { CacheObject cacheVal = cacheCtx.toCacheObject(val); if (e.op() == TRANSFORM) { GridCacheVersion ver; try { ver = e.cached().version(); } catch (GridCacheEntryRemovedException ex) { assert optimistic() : e; if (log.isDebugEnabled()) log.debug("Failed to get entry version: [msg=" + ex.getMessage() + \']\'); ver = null; } addInvokeResult(e, cacheVal, ret, ver); } else { boolean success = !hasFilters || isAll(e.context(), key, cacheVal, filter); ret.set(cacheCtx, cacheVal, success, keepBinary); } } } }; return loadMissing( cacheCtx, /*read through*/cacheCtx.config().isLoadPreviousValue() && !skipStore, /*async*/true, keys, /*skipVals*/singleRmv, needReadVer, keepBinary, c); }', 'output': 'private IgniteInternalFuture<Void> loadMissing( final GridCacheContext cacheCtx, final Set<KeyCacheObject> keys, final CacheEntryPredicate[] filter, final GridCacheReturn ret, final boolean needReadVer, final boolean singleRmv, final boolean hasFilters, final boolean skipStore, final boolean retval) { GridInClosure3<KeyCacheObject, Object, GridCacheVersion> c = new GridInClosure3<KeyCacheObject, Object, GridCacheVersion>() { @Override public void apply(KeyCacheObject key, @Nullable Object val, @Nullable GridCacheVersion loadVer) { if (log.isDebugEnabled()) log.debug("Loaded value from remote node [key=" + key + ", val=" + val + \']\'); IgniteTxEntry e = entry(new IgniteTxKey(key, cacheCtx.cacheId())); assert e != null; if (needReadVer) { assert loadVer != null; e.serializableReadVersion(singleRmv && val != null ? SER_READ_NOT_EMPTY_VER : loadVer); } if (singleRmv) { assert !hasFilters && !retval; assert val == null || Boolean.TRUE.equals(val) : val; ret.set(cacheCtx, null, val != null, keepBinary); } else { CacheObject cacheVal = cacheCtx.toCacheObject(val); if (e.op() == TRANSFORM) { GridCacheVersion ver; try { ver = e.cached().version(); } catch (GridCacheEntryRemovedException ex) { assert optimistic() : e; if (log.isDebugEnabled()) log.debug("Failed to get entry version: [msg=" + ex.getMessage() + \']\'); ver = null; } addInvokeResult(e, cacheVal, ret, ver); } else { boolean success = !hasFilters || isAll(e.context(), key, cacheVal, filter); ret.set(cacheCtx, cacheVal, success); } } } }; return loadMissing( cacheCtx, /*read through*/cacheCtx.config().isLoadPreviousValue() && !skipStore, /*async*/true, keys, /*skipVals*/singleRmv, needReadVer, c); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public BeamSqlPrimitive evaluate(BeamRecord inputRow) { String str = opValueEvaluated(0, inputRow); int idx = opValueEvaluated(1, inputRow); int startIdx = idx; if (startIdx > 0) { startIdx -= 1; } else if (startIdx < 0) { startIdx += str.length(); } else { return BeamSqlPrimitive.of(SqlTypeName.VARCHAR, ""); } if (operands.size() == 3) { int length = opValueEvaluated(2, inputRow); if (length < 0) { length = 0; } int endIdx = Math.min(startIdx + length, str.length()); return BeamSqlPrimitive.of(SqlTypeName.VARCHAR, str.substring(startIdx, endIdx)); } else { return BeamSqlPrimitive.of(SqlTypeName.VARCHAR, str.substring(startIdx)); } } } }', 'ground_truth': '@Override public BeamSqlPrimitive evaluate(BeamRecord inputRow, BoundedWindow window) { String str = opValueEvaluated(0, inputRow, window); int idx = opValueEvaluated(1, inputRow, window); int startIdx = idx; if (startIdx > 0) { startIdx -= 1; } else if (startIdx < 0) { startIdx += str.length(); } else { return BeamSqlPrimitive.of(SqlTypeName.VARCHAR, ""); } if (operands.size() == 3) { int length = opValueEvaluated(2, inputRow, window); if (length < 0) { length = 0; } int endIdx = Math.min(startIdx + length, str.length()); return BeamSqlPrimitive.of(SqlTypeName.VARCHAR, str.substring(startIdx, endIdx)); } else { return BeamSqlPrimitive.of(SqlTypeName.VARCHAR, str.substring(startIdx)); } }', 'output': '@Override public BeamSqlPrimitive evaluate(BeamRecord inputRow) { String str = opValueEvaluated(0, inputRow); int idx = opValueEvaluated(1, inputRow); int startIdx = idx; if (startIdx > 0) { startIdx -= 1; } else if (startIdx < 0) { startIdx += str.length(); } else { return BeamSqlPrimitive.of(SqlTypeName.VARCHAR, ""); } if (operands.size() == 3) { int length = opValueEvaluated(2, inputRow); if (length < 0) { length = 0; } int endIdx = Math.min(startIdx + length, str.length()); return BeamSqlPrimitive.of(SqlTypeName.VARCHAR, str.substring(startIdx, endIdx)); } else { return BeamSqlPrimitive.of(SqlTypeName.VARCHAR, str.substring(startIdx)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ArrayAccess(int start, int end, VariableBase variableName, ArrayDimension dimension) { super(start, end, variableName); this.dimension = dimension; }', 'ground_truth': 'public ArrayAccess(int start, int end, VariableBase variableName, ArrayDimension dimension, ArrayAccess.Type arrayType) { super(start, end, variableName); this.dimension = dimension; this.arrayType = arrayType; }', 'output': 'public ArrayAccess(int start, int end, String variableName, ArrayDimension dimension) { super(start, end, variableName); this.dimension = dimension; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private CommandDto commandDtoFor( final ManagedObject targetAdapter, final Can<ManagedObject> argumentAdapters) { val commandTargetAdapters = commandTargetAdaptersHolder.get() != null ? Can.ofCollection(commandTargetAdaptersHolder.get()) : Can.ofSingleton(targetAdapter); return getCommandDtoServiceInternal() .asCommandDto(commandTargetAdapters, this, argumentAdapters); }', 'ground_truth': 'private CommandDto commandDtoFor( final UUID uniqueId, final ManagedObject targetAdapter, final Can<ManagedObject> argumentAdapters) { val commandTargetAdapters = commandTargetAdaptersHolder.get() != null ? Can.ofCollection(commandTargetAdaptersHolder.get()) : Can.ofSingleton(targetAdapter); return getCommandDtoFactory() .asCommandDto(uniqueId, commandTargetAdapters, this, argumentAdapters); }', 'output': 'private CommandDto commandDtoFor( final ManagedObject targetAdapter, final Can<? extends ManagedObject> argumentAdapters) { val commandTargetAdapters = commandTargetAdaptersHolder.get() != null ? Can.ofCollection(commandTargetAdaptersHolder.get()) : Can.ofSingleton(targetAdapter); return getCommandDtoServiceInternal() .asCommandDto(commandTargetAdapters, this, argumentAdapters); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private DataRegionConfiguration createSystemDataRegion(long sysCacheInitSize, long sysCacheMaxSize) { DataRegionConfiguration res = new DataRegionConfiguration(); res.setName(SYSTEM_DATA_REGION_NAME); res.setInitialSize(sysCacheInitSize); res.setMaxSize(sysCacheMaxSize); return res; }', 'ground_truth': 'private DataRegionConfiguration createSystemDataRegion( long sysCacheInitSize, long sysCacheMaxSize, boolean persistenceEnabled ) { DataRegionConfiguration res = new DataRegionConfiguration(); res.setName(SYSTEM_DATA_REGION_NAME); res.setInitialSize(sysCacheInitSize); res.setMaxSize(sysCacheMaxSize); res.setPersistenceEnabled(persistenceEnabled); return res; }', 'output': 'private DataRegionConfiguration createSystemDataRegion(long sysCacheInitSize, long sysCacheMaxSize, int numReplicas) { DataRegionConfiguration res = new DataRegionConfiguration(); res.setName(SYSTEM_DATA_REGION_NAME); res.setInitialSize(sysCacheInitSize); res.setMaxSize(sysCacheMaxSize); res.setNumReplicas(numReplicas); return res; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected ByteBuffer allocateBuffer(int bufferSize) { assert Integer.bitCount(bufferSize) == 1; return ByteBuffer.allocate(bufferSize); }', 'ground_truth': 'protected ByteBuffer allocateBuffer(int bufferSize, boolean useDirect) { assert Integer.bitCount(bufferSize) == 1; return useMmap && useDirect ? ByteBuffer.allocateDirect(bufferSize) : ByteBuffer.allocate(bufferSize); }', 'output': 'protected ByteBuffer allocateBuffer(int bufferSize, int alignment) { assert Integer.bitCount(bufferSize) == 1; return ByteBuffer.allocate(bufferSize, alignment); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void testQueryCancel(int keyCnt, int valSize, String sql, int timeoutUnits, TimeUnit timeUnit, boolean timeout) throws Exception { try (Ignite client = startGrid("client")) { IgniteCache<Object, Object> cache = client.cache(DEFAULT_CACHE_NAME); assertEquals(0, cache.localSize()); int p = 1; for (int i = 1; i <= keyCnt; i++) { char[] tmp = new char[valSize]; Arrays.fill(tmp, \' \'); cache.put(i, new String(tmp)); if (i/(float)keyCnt >= p/10f) { log().info("Loaded " + i + " of " + keyCnt); p++; } } assertEquals(0, cache.localSize()); SqlFieldsQuery qry = new SqlFieldsQuery(sql); final QueryCursor<List<?>> cursor; if (timeout) { qry.setTimeout(timeoutUnits, timeUnit); cursor = cache.query(qry); } else { cursor = cache.query(qry); client.scheduler().runLocal(new Runnable() { @Override public void run() { cursor.close(); } }, timeoutUnits, timeUnit); } try(QueryCursor<List<?>> ignored = cursor) { cursor.iterator(); } catch (CacheException ex) { log().error("Got expected exception", ex); assertTrue("Must throw correct exception", ex.getCause() instanceof QueryCancelledException); } Thread.sleep(TimeUnit.MILLISECONDS.convert(timeoutUnits, timeUnit) + 3_000); checkCleanState(); } }', 'ground_truth': 'private void testQueryCancel(int keyCnt, int valSize, String sql, int timeoutUnits, TimeUnit timeUnit, boolean timeout, boolean checkCanceled) throws Exception { try (Ignite client = startGrid("client")) { IgniteCache<Object, Object> cache = client.cache(DEFAULT_CACHE_NAME); assertEquals(0, cache.localSize()); int p = 1; for (int i = 1; i <= keyCnt; i++) { char[] tmp = new char[valSize]; Arrays.fill(tmp, \' \'); cache.put(i, new String(tmp)); if (i/(float)keyCnt >= p/10f) { log().info("Loaded " + i + " of " + keyCnt); p++; } } assertEquals(0, cache.localSize()); SqlFieldsQuery qry = new SqlFieldsQuery(sql); final QueryCursor<List<?>> cursor; if (timeout) { qry.setTimeout(timeoutUnits, timeUnit); cursor = cache.query(qry); } else { cursor = cache.query(qry); client.scheduler().runLocal(new Runnable() { @Override public void run() { cursor.close(); } }, timeoutUnits, timeUnit); } try (QueryCursor<List<?>> ignored = cursor) { cursor.getAll(); if (checkCanceled) fail("Query not canceled"); } catch (CacheException ex) { log().error("Got expected exception", ex); assertNotNull("Must throw correct exception", X.cause(ex, QueryCancelledException.class)); } Thread.sleep(TimeUnit.MILLISECONDS.convert(timeoutUnits, timeUnit) + 3_000); checkCleanState(); } }', 'output': 'private void testQueryCancel(int keyCnt, int valSize, String sql, int timeoutUnits, TimeUnit timeUnit, boolean timeout) throws Exception { try (Ignite client = startGrid("client")) { IgniteCache<Object, Object> cache = client.cache(DEFAULT_CACHE_NAME); assertEquals(0, cache.localSize()); int p = 1; for (int i = 1; i <= keyCnt; i++) { char[] tmp = new char[valSize]; Arrays.fill(tmp, \' \'); cache.put(i, new String(tmp)); if (i/(float)keyCnt >= p/10f) { log().info("Loaded " + i + " of " + keyCnt); p++; } } assertEquals(0, cache.localSize()); SqlFieldsQuery qry = new SqlFieldsQuery(sql); final QueryCursor<List<?>> cursor; if (timeout) { qry.setTimeout(timeoutUnits, timeUnit); cursor = cache.query(qry); } else { cursor = cache.query(qry); client.scheduler().runLocal(new Runnable() { @Override public void run() { cursor.close(); } }, timeoutUnits, timeUnit); } try(QueryCursor<List<?>> ignored = cursor) { cursor.iterator(); } catch (CacheException ex) { log().error("Got expected exception", ex); assertTrue("Must throw correct exception", ex.getCause() instanceof QueryCancelledException); } Thread.sleep(TimeUnit.MILLISECONDS.convert(timeoutUnits, timeUnit) + 3_000); checkCleanState(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static StreamOutSession create(InetAddress host, long sessionId, Runnable callback) { Pair<InetAddress, Long> context = new Pair<InetAddress, Long>(host, sessionId); StreamOutSession session = new StreamOutSession(context, callback); streams.put(context, session); return session; }', 'ground_truth': 'public static StreamOutSession create(String table, InetAddress host, long sessionId, Runnable callback) { Pair<InetAddress, Long> context = new Pair<InetAddress, Long>(host, sessionId); StreamOutSession session = new StreamOutSession(table, context, callback); streams.put(context, session); return session; }', 'output': 'public static StreamOutSession create(InetAddress host, long sessionId, Runnable callback, int port) { Pair<InetAddress, Long> context = new Pair<InetAddress, Long>(host, sessionId); StreamOutSession session = new StreamOutSession(context, callback, port); streams.put(context, session); return session; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void sendMessage(StreamMessage message) { try { StreamMessage.serialize(message, out, protocolVersion, session); } catch (SocketException e) { session.onError(e); close(); } catch (IOException e) { session.onError(e); } }', 'ground_truth': 'private void sendMessage(WritableByteChannel out, StreamMessage message) { try { StreamMessage.serialize(message, out, protocolVersion, session); } catch (SocketException e) { session.onError(e); close(); } catch (IOException e) { session.onError(e); } }', 'output': 'private void sendMessage(StreamMessage message) { try { StreamMessage.serialize(message, out, protocolVersion, session); } catch (SocketException e) { session.onError(e); close(); } catch (IOException e) { session.onError(e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public get_slice_by_names_args( String table, String key, ColumnParent column_parent, List<byte[]> column_names) { this(); this.table = table; this.key = key; this.column_parent = column_parent; this.column_names = column_names; }', 'ground_truth': 'public get_slice_by_names_args( String table, String key, ColumnParent column_parent, List<byte[]> column_names, int consistency_level) { this(); this.table = table; this.key = key; this.column_parent = column_parent; this.column_names = column_names; this.consistency_level = consistency_level; this.__isset.consistency_level = true; }', 'output': 'public get_slice_by_names_args( String table, String key, ColumnParent column_parent, List<ByteBuffer> column_names) { this(); this.table = table; this.key = key; this.column_parent = column_parent; this.column_names = column_names; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RingCache() { for (InetAddress seed : DatabaseDescriptor.getSeeds()) { seeds_.add(seed.getHostAddress()); } refreshEndPointMap(); }', 'ground_truth': 'public RingCache(String keyspace) { for (InetAddress seed : DatabaseDescriptor.getSeeds()) { seeds_.add(seed.getHostAddress()); } this.keyspace = keyspace; refreshEndPointMap(); }', 'output': 'public RingCache(List<InetAddress> seeds) { for (InetAddress seed : seeds) { seeds_.add(seed.getHostAddress()); } refreshEndPointMap(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private long checkFile(FileIO fileIO) throws IOException { ByteBuffer hdr = ByteBuffer.allocate(headerSize()).order(ByteOrder.LITTLE_ENDIAN); fileIO.readFully(hdr); hdr.rewind(); long signature = hdr.getLong(); String prefix = "Failed to verify, file=" + cfgFile.getAbsolutePath() + "\\" "; if (SIGNATURE != signature) throw new IOException(prefix + "(invalid file signature)" + " [expectedSignature=" + U.hexLong(SIGNATURE) + ", actualSignature=" + U.hexLong(signature) + \']\'); int ver = hdr.getInt(); if (version() != ver) throw new IOException(prefix + "(invalid file version)" + " [expectedVersion=" + version() + ", fileVersion=" + ver + "]"); byte type = hdr.get(); if (this.type != type) throw new IOException(prefix + "(invalid file type)" + " [expectedFileType=" + this.type + ", actualFileType=" + type + "]"); int pageSize = hdr.getInt(); if (dbCfg.getPageSize() != pageSize) throw new IOException(prefix + "(invalid page size)" + " [expectedPageSize=" + dbCfg.getPageSize() + ", filePageSize=" + pageSize + "]"); long fileSize = cfgFile.length(); if (fileSize == headerSize()) fileSize = pageSize + headerSize(); if (fileSize % pageSize != 0) fileSize = (fileSize / pageSize + 1) * pageSize; return fileSize; }', 'ground_truth': 'private long checkFile(FileIO fileIO, File cfgFile) throws IOException { ByteBuffer hdr = ByteBuffer.allocate(headerSize()).order(ByteOrder.LITTLE_ENDIAN); fileIO.readFully(hdr); hdr.rewind(); long signature = hdr.getLong(); String prefix = "Failed to verify, file=" + cfgFile.getAbsolutePath() + "\\" "; if (SIGNATURE != signature) throw new IOException(prefix + "(invalid file signature)" + " [expectedSignature=" + U.hexLong(SIGNATURE) + ", actualSignature=" + U.hexLong(signature) + \']\'); int ver = hdr.getInt(); if (version() != ver) throw new IOException(prefix + "(invalid file version)" + " [expectedVersion=" + version() + ", fileVersion=" + ver + "]"); byte type = hdr.get(); if (this.type != type) throw new IOException(prefix + "(invalid file type)" + " [expectedFileType=" + this.type + ", actualFileType=" + type + "]"); int pageSize = hdr.getInt(); if (dbCfg.getPageSize() != pageSize) throw new IOException(prefix + "(invalid page size)" + " [expectedPageSize=" + dbCfg.getPageSize() + ", filePageSize=" + pageSize + "]"); long fileSize = cfgFile.length(); if (fileSize == headerSize()) fileSize = pageSize + headerSize(); if (fileSize % pageSize != 0) fileSize = (fileSize / pageSize + 1) * pageSize; return fileSize; }', 'output': 'private long checkFile(FileIO fileIO) throws IOException { ByteBuffer hdr = ByteBuffer.allocate(headerSize()).order(ByteOrder.LITTLE_ENDIAN); fileIO.readFully(hdr); hdr.rewind(); long signature = hdr.getLong(); String prefix = "Failed to verify, file=" + cfgFile.getAbsolutePath() + "\\" "; if (SIGNATURE != signature) throw new IOException(prefix + "(invalid file signature)" + " [expectedSignature=" + U.hexLong(SIGNATURE) + ", actualSignature=" + U.hexLong(signature) + \']\'); int ver = hdr.getInt(); if (version() != ver) throw new IOException(prefix + "(invalid file version)" + " [expectedVersion=" + version() + ", fileVersion=" + ver + "]"); byte type = hdr.get(); if (this.type != type) throw new IOException(prefix + "(invalid file type)" + " [expectedFileType=" + this.type + ", actualFileType=" + type + "]"); int pageSize = hdr.getInt(); if (dbCfg.getPageSize() != pageSize) throw new IOException(prefix + "(invalid page size)" + " [expectedPageSize=" + dbCfg.getPageSize() + ", filePageSize=" + pageSize + "]"); long fileSize = cfgFile.length(); if (fileSize == headerSize()) fileSize = pageSize + headerSize(); if (fileSize % pageSize != 0) fileSize = (fileSize / pageSize + 1) * pageSize; return fileSize; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GridIoSecurityAwareMessage( UUID secSubjId, byte plc, Object topic, int topicOrd, Message msg, boolean ordered, long timeout, boolean skipOnTimeout) { super(plc, topic, topicOrd, msg, ordered, timeout, skipOnTimeout); this.secSubjId = secSubjId; }', 'ground_truth': 'public GridIoSecurityAwareMessage( UUID secSubjId, byte plc, Object topic, int topicOrd, Message msg, boolean ordered, long timeout, boolean skipOnTimeout, int connIdx ) { super(plc, topic, topicOrd, msg, ordered, timeout, skipOnTimeout, connIdx); this.secSubjId = secSubjId; }', 'output': 'public GridIoSecurityAwareMessage( UUID secSubjId, byte plc, Object topic, int topicOrd, Message msg, boolean ordered, long timeout, boolean skipOnTimeout, SecurityContext secCtx) { super(plc, topic, topicOrd, msg, ordered, timeout, skipOnTimeout, secCtx); this.secSubjId = secSubjId; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public MixinFacetAbstract( final Class<?> mixinType, final String value, final Constructor<?> constructor, final FacetHolder holder) { super(type(), value, holder); this.mixinType = mixinType; this.constructor = constructor; this.holderType = constructor.getParameterTypes()[0]; this.holderField = _Reflect.streamAllFields(mixinType, true) .filter(mixinField->mixinField.getType().isAssignableFrom(holderType)) .findFirst() .orElse(null); if(holderField==null) { log.warn("Could not find or access the \'mixed-in\' domain object within {}" + " (tried to guess by looking at all public and non-public fields " + "and matching one against the constructor parameter\'s type)", mixinType.getClass().getName()); } }', 'ground_truth': 'public MixinFacetAbstract( final Class<?> mixinType, final String value, final Constructor<?> constructor, final FacetHolder holder, final @Nullable MetaModelValidatorForMixinTypes mixinTypeValidator) { super(type(), value, holder); this.mixinType = mixinType; this.constructor = constructor; this.holderType = constructor.getParameterTypes()[0]; this.holderField = _Reflect.streamAllFields(mixinType, true) .filter(mixinField->mixinField.getType().isAssignableFrom(holderType)) .findFirst() .orElse(null); if(holderField==null) { val msg = String.format("Could not find the \'mixed-in\' domain object within %s" + " (tried to guess by looking at all public and non-public fields " + "and matching one against the constructor parameter\'s type)", mixinType.getClass().getName()); log.warn(msg); if(mixinTypeValidator!=null) { mixinTypeValidator.onFailure(holder, Identifier.classIdentifier(mixinType), msg); } } }', 'output': 'public MixinFacetAbstract( final Class<?> mixinType, final String value, final Constructor<?> constructor, final FacetHolder holder) { super(value, holder); this.mixinType = mixinType; this.constructor = constructor; this.holderType = constructor.getParameterTypes()[0]; this.holderField = _Reflect.streamAllFields(mixinType, true) .filter(mixinField->mixinField.getType().isAssignableFrom(holderType)) .findFirst() .orElse(null); if(holderField==null) { log.warn("Could not find or access the \'mixed-in\' domain object within {}" + " (tried to guess by looking at all public and non-public fields " + "and matching one against the constructor parameter\'s type)", mixinType.getClass().getName()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static StatsPrinter from(String format) { switch (format) { case "json": return new StatsPrinter.JsonPrinter(); case "yaml": return new StatsPrinter.YamlPrinter(); default: return new DefaultPrinter(); } }', 'ground_truth': 'public static StatsPrinter from(String format, boolean sorted) { switch (format) { case "json": return new StatsPrinter.JsonPrinter(); case "yaml": return new StatsPrinter.YamlPrinter(); default: if (sorted) return new SortedDefaultPrinter(); else return new DefaultPrinter(); } }', 'output': 'public static StatsPrinter from(Format format) { switch (format) { case Format.JSON: return new StatsPrinter.JsonPrinter(); case Format.YAML: return new StatsPrinter.YamlPrinter(); default: return new DefaultPrinter(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static UpdatePlan forUpdate(GridH2Table tbl, String[] colNames, int[] colTypes, KeyValueSupplier valSupplier, int valColIdx, String selectQry) { assert !F.isEmpty(colNames); return new UpdatePlan(UpdateMode.UPDATE, tbl, colNames, colTypes, null, valSupplier, -1, valColIdx, selectQry, false, 0, null); }', 'ground_truth': 'public static UpdatePlan forUpdate(GridH2Table tbl, String[] colNames, int[] colTypes, KeyValueSupplier valSupplier, int valColIdx, String selectQry, DistributedPlanInfo distributed) { assert !F.isEmpty(colNames); return new UpdatePlan(UpdateMode.UPDATE, tbl, colNames, colTypes, null, valSupplier, -1, valColIdx, selectQry, false, 0, null, distributed); }', 'output': 'public static UpdatePlan forUpdate(IgniteTable tbl, String[] colNames, int[] colTypes, KeyValueSupplier valSupplier, int valColIdx, String selectQry) { assert !F.isEmpty(colNames); return new UpdatePlan(UpdateMode.UPDATE, tbl, colNames, colTypes, null, valSupplier, -1, valColIdx, selectQry, false, 0, null); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<InetAddress> getLiveNaturalEndpoints(String key) { return getLiveNaturalEndpoints(partitioner_.getToken(key)); }', 'ground_truth': 'public List<InetAddress> getLiveNaturalEndpoints(String table, String key) { return getLiveNaturalEndpoints(table, partitioner_.getToken(key)); }', 'output': 'public List<InetAddress> getLiveNaturalEndpoints(byte[] key) { return getLiveNaturalEndpoints(partitioner_.getToken(key)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected ViewURLStreamProvider createURLStreamProvider() { ComponentSSLConfiguration configuration1 = ComponentSSLConfiguration.instance(); org.apache.ambari.server.controller.internal.URLStreamProvider streamProvider = new org.apache.ambari.server.controller.internal.URLStreamProvider( configuration.getRequestConnectTimeout(), configuration.getRequestReadTimeout(), configuration1.getTruststorePath(), configuration1.getTruststorePassword(), configuration1.getTruststoreType()); return new ViewURLStreamProvider(streamProvider); }', 'ground_truth': 'protected ViewURLStreamProvider createURLStreamProvider(ViewContext viewContext) { ComponentSSLConfiguration sslConfiguration = ComponentSSLConfiguration.instance(); org.apache.ambari.server.controller.internal.URLStreamProvider streamProvider = new org.apache.ambari.server.controller.internal.URLStreamProvider( configuration.getRequestConnectTimeout(), configuration.getRequestReadTimeout(), sslConfiguration.getTruststorePath(), sslConfiguration.getTruststorePassword(), sslConfiguration.getTruststoreType()); return new ViewURLStreamProvider(viewContext, streamProvider); }', 'output': 'protected ViewURLStreamProvider createURLStreamProvider() { ComponentSSLConfiguration configuration1 = ComponentSSLConfiguration.instance(); URLStreamProvider streamProvider = new URLStreamProvider( configuration.getRequestConnectTimeout(), configuration.getRequestReadTimeout(), configuration1.getTruststorePath(), configuration1.getTruststorePassword(), configuration1.getTruststoreType()); return new ViewURLStreamProvider(streamProvider); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ResultMessage processStatement(CQLStatement statement, QueryState queryState, QueryOptions options) throws RequestExecutionException, RequestValidationException { logger.trace("Process {} @CL.{}", statement, options.getConsistency()); ClientState clientState = queryState.getClientState(); statement.checkAccess(clientState); statement.validate(clientState); ResultMessage result = statement.execute(queryState, options); return result == null ? new ResultMessage.Void() : result; }', 'ground_truth': 'public ResultMessage processStatement(CQLStatement statement, QueryState queryState, QueryOptions options, long queryStartNanoTime) throws RequestExecutionException, RequestValidationException { logger.trace("Process {} @CL.{}", statement, options.getConsistency()); ClientState clientState = queryState.getClientState(); statement.checkAccess(clientState); statement.validate(clientState); ResultMessage result = statement.execute(queryState, options, queryStartNanoTime); return result == null ? new ResultMessage.Void() : result; }', 'output': 'public ResultMessage processStatement(CQLStatement statement, QueryState queryState, QueryOptions options, ConsistencyLevel consistency) throws RequestExecutionException, RequestValidationException { logger.trace("Process {} @CL.{}", statement, consistency); ClientState clientState = queryState.getClientState(); statement.checkAccess(clientState); statement.validate(clientState); ResultMessage result = statement.execute(queryState, options, consistency); return result == null ? new ResultMessage.Void() : result; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ColumnIndex writeAndBuildIndex(UnfilteredRowIterator iterator, SequentialWriter output, SerializationHeader header, Version version) throws IOException { assert !iterator.isEmpty() && version.storeRows(); Builder builder = new Builder(iterator, output, header, version.correspondingMessagingVersion()); return builder.build(); }', 'ground_truth': 'public static ColumnIndex writeAndBuildIndex(UnfilteredRowIterator iterator, SequentialWriter output, SerializationHeader header, Collection<SSTableFlushObserver> observers, Version version) throws IOException { assert !iterator.isEmpty() && version.storeRows(); Builder builder = new Builder(iterator, output, header, observers, version.correspondingMessagingVersion()); return builder.build(); }', 'output': 'public static ColumnIndex buildIndex(UnfilteredRowIterator iterator, SequentialWriter output, SerializationHeader header, Version version) throws IOException { assert !iterator.isEmpty() && version.storeRows(); Builder builder = new Builder(iterator, output, header, version.correspondingMessagingVersion()); return builder.build(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private CacheInvokeEntry.Operation applyEntryProcessor(GridCacheContext cctx, KeyCacheObject key, GridCacheVersion ver, EntryProcessor entryProc, Object[] invokeArgs, MvccUpdateDataRow updateRow, CacheDataRow oldRow) { Object procRes = null; Exception err = null; CacheObject oldVal = oldRow == null ? null : oldRow.value(); CacheInvokeEntry invokeEntry = new CacheInvokeEntry<>(key, oldVal, ver, cctx.keepBinary(), new GridDhtDetachedCacheEntry(cctx, key)); try { procRes = entryProc.process(invokeEntry, invokeArgs); if(invokeEntry.modified() && invokeEntry.op() != CacheInvokeEntry.Operation.REMOVE) { Object val = invokeEntry.getValue(true); CacheObject val0 = cctx.toCacheObject(val); val0.prepareForCache(cctx.cacheObjectContext()); updateRow.value(val0); } } catch (Exception e) { log.error("Exception was thrown during entry processing.", e); err = e; } CacheInvokeResult invokeRes = err == null ? CacheInvokeResult.fromResult(procRes) : CacheInvokeResult.fromError(err); updateRow.invokeResult(invokeRes); return invokeEntry.op(); }', 'ground_truth': 'private CacheInvokeEntry.Operation applyEntryProcessor(GridCacheContext cctx, KeyCacheObject key, GridCacheVersion ver, EntryProcessor entryProc, Object[] invokeArgs, MvccUpdateDataRow updateRow, CacheDataRow oldRow, boolean keepBinary) { Object procRes = null; Exception err = null; CacheObject oldVal = oldRow == null ? null : oldRow.value(); CacheInvokeEntry invokeEntry = new CacheInvokeEntry<>(key, oldVal, ver, keepBinary, new GridDhtDetachedCacheEntry(cctx, key)); try { procRes = entryProc.process(invokeEntry, invokeArgs); if(invokeEntry.modified() && invokeEntry.op() != CacheInvokeEntry.Operation.REMOVE) { Object val = invokeEntry.getValue(true); CacheObject val0 = cctx.toCacheObject(val); val0.prepareForCache(cctx.cacheObjectContext()); updateRow.value(val0); } } catch (Exception e) { log.error("Exception was thrown during entry processing.", e); err = e; } CacheInvokeResult invokeRes = err == null ? CacheInvokeResult.fromResult(procRes) : CacheInvokeResult.fromError(err); updateRow.invokeResult(invokeRes); return invokeEntry.op(); }', 'output': 'private CacheInvokeEntry.Operation applyEntryProcessor(GridCacheContext cctx, KeyCacheObject key, GridCacheVersion ver, EntryProcessor entryProc, Object[] invokeArgs, MvccUpdateDataRow updateRow, CacheDataRow oldRow) { Object procRes = null; Exception err = null; CacheObject oldVal = oldRow == null ? null : oldRow.value(); CacheInvokeEntry invokeEntry = new CacheInvokeEntry<>(key, oldVal, ver, cctx.keepBinary(), new GridDhtDetachedCacheEntry(cctx, key)); try { procRes = entryProc.process(invokeEntry, invokeArgs); if(invokeEntry.modified() && invokeEntry.op() != CacheInvokeEntry.Operation.REMOVE) { Object val = invokeEntry.getValue(true); CacheObject val0 = cctx.toCacheObject(val); val0.prepareForCache(cctx.cacheObjectContext()); updateRow.value(val0); } } catch (Exception e) { log.error("Exception was thrown during entry processing.", e); err = e; } CacheInvokeResult invokeRes = err == null ? CacheInvokeResult.fromResult(procRes) : CacheInvokeResult.fromError(err); updateRow.invokeResult(invokeRes); return invokeEntry.op(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected static @CheckForNull Pair<ActionProvider, String> findTarget(FileObject toRun, boolean debug) { ClassPath sourceCP = ClassPath.getClassPath(toRun, ClassPath.SOURCE); FileObject fileRoot = sourceCP != null ? sourceCP.findOwnerRoot(toRun) : null; boolean mainSource; if (fileRoot != null) { mainSource = UnitTestForSourceQuery.findUnitTests(fileRoot).length > 0; } else { mainSource = true; } ActionProvider provider = null; String command = null; Collection<ActionProvider> actionProviders = findActionProviders(toRun); Lookup testLookup = Lookups.singleton(toRun); String[] actions = debug ? mainSource ? new String[] {ActionProvider.COMMAND_DEBUG_SINGLE} : new String[] {ActionProvider.COMMAND_DEBUG_TEST_SINGLE, ActionProvider.COMMAND_DEBUG_SINGLE} : mainSource ? new String[] {ActionProvider.COMMAND_RUN_SINGLE} : new String[] {ActionProvider.COMMAND_TEST_SINGLE, ActionProvider.COMMAND_RUN_SINGLE}; for (String commandCandidate : actions) { provider = findActionProvider(commandCandidate, actionProviders, testLookup); if (provider != null) { command = commandCandidate; break; } } if (provider == null) { command = debug ? mainSource ? ActionProvider.COMMAND_DEBUG : ActionProvider.COMMAND_DEBUG : mainSource ? ActionProvider.COMMAND_RUN : ActionProvider.COMMAND_TEST; provider = findActionProvider(command, actionProviders, testLookup); } if (provider == null) { return null; } return Pair.of(provider, command); }', 'ground_truth': 'protected static @CheckForNull Pair<ActionProvider, String> findTarget(FileObject toRun, SingleMethod singleMethod, boolean debug) { ClassPath sourceCP = ClassPath.getClassPath(toRun, ClassPath.SOURCE); FileObject fileRoot = sourceCP != null ? sourceCP.findOwnerRoot(toRun) : null; boolean mainSource; if (fileRoot != null) { mainSource = UnitTestForSourceQuery.findUnitTests(fileRoot).length > 0; } else { mainSource = true; } ActionProvider provider = null; String command = null; Collection<ActionProvider> actionProviders = findActionProviders(toRun); Lookup testLookup = Lookups.singleton(toRun); String[] actions; if (mainSource && singleMethod != null) { actions = debug ? new String[] {SingleMethod.COMMAND_DEBUG_SINGLE_METHOD} : new String[] {SingleMethod.COMMAND_RUN_SINGLE_METHOD}; } else { actions = debug ? mainSource ? new String[] {ActionProvider.COMMAND_DEBUG_SINGLE} : new String[] {ActionProvider.COMMAND_DEBUG_TEST_SINGLE, ActionProvider.COMMAND_DEBUG_SINGLE} : mainSource ? new String[] {ActionProvider.COMMAND_RUN_SINGLE} : new String[] {ActionProvider.COMMAND_TEST_SINGLE, ActionProvider.COMMAND_RUN_SINGLE}; } for (String commandCandidate : actions) { provider = findActionProvider(commandCandidate, actionProviders, testLookup); if (provider != null) { command = commandCandidate; break; } } if (provider == null) { command = debug ? mainSource ? ActionProvider.COMMAND_DEBUG : ActionProvider.COMMAND_DEBUG : mainSource ? ActionProvider.COMMAND_RUN : ActionProvider.COMMAND_TEST; provider = findActionProvider(command, actionProviders, testLookup); } if (provider == null) { return null; } return Pair.of(provider, command); }', 'output': 'protected static @CheckForNull Pair<ActionProvider, String> findTarget(FileObject toRun, boolean debug) { ClassPath sourceCP = ClassPath.getClassPath(toRun, ClassPath.SOURCE); FileObject fileRoot = sourceCP != null ? sourceCP.findOwnerRoot(toRun) : null; boolean mainSource; if (fileRoot != null) { mainSource = UnitTestForSourceQuery.findUnitTests(fileRoot).length > 0; } else { mainSource = true; } ActionProvider provider = null; String command = null; Collection<ActionProvider> actionProviders = findActionProviders(toRun); Lookup testLookup = Lookups.singleton(toRun); String[] actions = debug ? mainSource ? new String[] {ActionProvider.COMMAND_DEBUG_SINGLE} : new String[] {ActionProvider.COMMAND_DEBUG_TEST_SINGLE, ActionProvider.COMMAND_DEBUG_SINGLE} : mainSource ? new String[] {ActionProvider.COMMAND_RUN_SINGLE} : new String[] {ActionProvider.COMMAND_TEST_SINGLE, ActionProvider.COMMAND_RUN_SINGLE}; for (String commandCandidate : actions) { provider = findActionProvider(commandCandidate, actionProviders, testLookup); if (provider != null) { command = commandCandidate; break; } } if (provider == null) { command = debug ? mainSource ? ActionProvider.COMMAND_DEBUG : ActionProvider.COMMAND_DEBUG : mainSource ? ActionProvider.COMMAND_RUN : ActionProvider.COMMAND_TEST; provider = findActionProvider(command, actionProviders, testLookup); } if (provider == null) { return null; } return Pair.of(provider, command); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ResultMessage process(String queryString, List<ByteBuffer> variables, ConsistencyLevel cl, QueryState queryState) throws RequestExecutionException, RequestValidationException { CQLStatement prepared = getStatement(queryString, queryState.getClientState()).statement; if (prepared.getBoundsTerms() != variables.size()) throw new InvalidRequestException("Invalid amount of bind variables"); return processStatement(prepared, cl, queryState, variables); }', 'ground_truth': 'public static ResultMessage process(String queryString, List<ByteBuffer> variables, ConsistencyLevel cl, QueryState queryState, int pageSize) throws RequestExecutionException, RequestValidationException { CQLStatement prepared = getStatement(queryString, queryState.getClientState()).statement; if (prepared.getBoundsTerms() != variables.size()) throw new InvalidRequestException("Invalid amount of bind variables"); return processStatement(prepared, cl, queryState, variables, pageSize); }', 'output': 'public static ResultMessage process(String queryString, Collection<ByteBuffer> variables, ConsistencyLevel cl, QueryState queryState) throws RequestExecutionException, RequestValidationException { CQLStatement prepared = getStatement(queryString, queryState.getClientState()).statement; if (prepared.getBoundsTerms() != variables.size()) throw new InvalidRequestException("Invalid amount of bind variables"); return processStatement(prepared, cl, queryState, variables); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void assertFails(CheckedFunction function, boolean thrown, String... messages) throws Throwable { ClientWarn.instance.captureWarnings(); try { function.apply(); if (thrown) fail("Expected to fail, but it did not"); } catch (GuardrailViolatedException e) { assertTrue("Expect no exception thrown", thrown); String failMessage = messages[messages.length - 1]; if (guardrail != null) { String prefix = guardrail.decorateMessage(""); assertTrue(format("Full error message \'%s\' doesn\'t start with the prefix \'%s\'", e.getMessage(), prefix), e.getMessage().startsWith(prefix)); } assertTrue(format("Full error message \'%s\' does not contain expected message \'%s\'", e.getMessage(), failMessage), e.getMessage().contains(failMessage)); assertWarnings(messages); } finally { ClientWarn.instance.resetWarnings(); } }', 'ground_truth': 'protected void assertFails(CheckedFunction function, boolean thrown, List<String> messages, List<String> redactedMessages) throws Throwable { ClientWarn.instance.captureWarnings(); try { function.apply(); if (thrown) fail("Expected to fail, but it did not"); } catch (GuardrailViolatedException e) { assertTrue("Expect no exception thrown", thrown); String failMessage = messages.get(messages.size() - 1); if (guardrail != null) { String prefix = guardrail.decorateMessage(""); assertTrue(format("Full error message \'%s\' doesn\'t start with the prefix \'%s\'", e.getMessage(), prefix), e.getMessage().startsWith(prefix)); } assertTrue(format("Full error message \'%s\' does not contain expected message \'%s\'", e.getMessage(), failMessage), e.getMessage().contains(failMessage)); assertWarnings(messages); if (messages.size() > 1) listener.assertWarned(redactedMessages.subList(0, messages.size() - 1)); else listener.assertNotWarned(); listener.assertFailed(redactedMessages.get(messages.size() - 1)); } finally { ClientWarn.instance.resetWarnings(); listener.clear(); } }', 'output': 'protected void assertFails(CheckedFunction function, boolean thrown, String... messages) throws Throwable { ClientWarn.instance.captureWarnings(); try { function.apply(); if (thrown) fail("Expected to fail, but it did not"); } catch (GuardrailViolatedException e) { assertTrue("Expect no exception thrown", thrown); String failMessage = messages[messages.length - 1]; if (guardrail != null) { String prefix = guardrail.decorateMessage(""); assertTrue(format("Full error message \'%s\' doesn\'t start with the prefix \'%s\'", e.getMessage(), prefix), e.getMessage().startsWith(prefix)); } assertTrue(format("Full error message \'%s\' does not contain expected message \'%s\'", e.getMessage(), failMessage), e.getMessage().contains(failMessage)); assertWarnings(messages); } finally { ClientWarn.instance.resetWarnings(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void interruptCompactionFor(Iterable<TableMetadata> columnFamilies, boolean interruptValidation) { assert columnFamilies != null; for (Holder compactionHolder : CompactionMetrics.getCompactions()) { CompactionInfo info = compactionHolder.getCompactionInfo(); if ((info.getTaskType() == OperationType.VALIDATION) && !interruptValidation) continue; if (Iterables.contains(columnFamilies, info.getTableMetadata())) compactionHolder.stop(); } }', 'ground_truth': 'public void interruptCompactionFor(Iterable<TableMetadata> columnFamilies, Predicate<SSTableReader> sstablePredicate, boolean interruptValidation) { assert columnFamilies != null; for (Holder compactionHolder : active.getCompactions()) { CompactionInfo info = compactionHolder.getCompactionInfo(); if ((info.getTaskType() == OperationType.VALIDATION) && !interruptValidation) continue; if (Iterables.contains(columnFamilies, info.getTableMetadata())) { if (info.shouldStop(sstablePredicate)) compactionHolder.stop(); } } }', 'output': 'public void interruptCompactionFor(Set<TableMetadata> columnFamilies, boolean interruptValidation) { assert columnFamilies != null; for (Holder compactionHolder : CompactionMetrics.getCompactions()) { CompactionInfo info = compactionHolder.getCompactionInfo(); if ((info.getTaskType() == OperationType.VALIDATION) && !interruptValidation) continue; if (columnFamilies.contains(info.getTableMetadata())) compactionHolder.stop(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void cache(String storageLevel) { LOG.warn("Provided StorageLevel ignored for stream, using default level"); cache(); }', 'ground_truth': 'public void cache(String storageLevel, Coder<?> coder) { if (!StorageLevel.fromString(storageLevel).equals(StorageLevel.MEMORY_ONLY_SER())) { LOG.warn("Provided StorageLevel: {} is ignored for streams, using the default level: {}", storageLevel, StorageLevel.MEMORY_ONLY_SER()); } Coder<WindowedValue<T>> wc = (Coder<WindowedValue<T>>) coder; this.dStream = dStream.map(CoderHelpers.toByteFunction(wc)) .cache() .map(CoderHelpers.fromByteFunction(wc)); }', 'output': 'public void cache(String storageLevel, TimeUnit unit) { LOG.warn("Provided StorageLevel ignored for stream, using default level"); cache(unit); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public PartitionUpdate makeUpdates(FilteredPartition current, ClientState state) throws InvalidRequestException { PartitionUpdate.Builder updateBuilder = new PartitionUpdate.Builder(metadata, key, updatedColumns(), conditions.size()); for (RowUpdate upd : updates) upd.applyUpdates(current, updateBuilder, state); for (RangeDeletion upd : rangeDeletions) upd.applyUpdates(current, updateBuilder, state); PartitionUpdate partitionUpdate = updateBuilder.build(); IndexRegistry.obtain(metadata).validate(partitionUpdate); return partitionUpdate; }', 'ground_truth': 'public PartitionUpdate makeUpdates(FilteredPartition current, ClientState state, TimeUUID ballot) throws InvalidRequestException { PartitionUpdate.Builder updateBuilder = new PartitionUpdate.Builder(metadata, key, updatedColumns(), conditions.size()); long timeUuidNanos = 0; for (RowUpdate upd : updates) timeUuidNanos = upd.applyUpdates(current, updateBuilder, state, ballot.msb(), timeUuidNanos); for (RangeDeletion upd : rangeDeletions) upd.applyUpdates(current, updateBuilder, state); PartitionUpdate partitionUpdate = updateBuilder.build(); IndexRegistry.obtain(metadata).validate(partitionUpdate); return partitionUpdate; }', 'output': 'public PartitionUpdate makeUpdates(ClientState state) throws InvalidRequestException { PartitionUpdate.Builder updateBuilder = new PartitionUpdate.Builder(metadata, key, updatedColumns(), conditions.size()); for (RowUpdate upd : updates) upd.applyUpdates(updateBuilder, state); for (RangeDeletion upd : rangeDeletions) upd.applyUpdates(updateBuilder, state); PartitionUpdate partitionUpdate = updateBuilder.build(); IndexRegistry.obtain(metadata).validate(partitionUpdate); return partitionUpdate; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static long calculateParameterDistance(Class[] arguments, Class[] parameters) { if (parameters.length == 0) return 0; long ret = 0; int noVargsLength = parameters.length-1; for (int i = 0; i < noVargsLength; i++) { ret += calculateParameterDistance(arguments[i], parameters[i]); } if (arguments.length==parameters.length) { Class baseType = parameters[noVargsLength]; if (!isAssignableFrom(parameters[noVargsLength],arguments[noVargsLength])) { baseType=baseType.getComponentType(); ret+=2l<<VARGS_SHIFT; } ret += calculateParameterDistance(arguments[noVargsLength], baseType); } else if (arguments.length>parameters.length) { ret += (2+arguments.length-parameters.length)<<VARGS_SHIFT; Class vargsType = parameters[noVargsLength].getComponentType(); for (int i = noVargsLength; i < arguments.length; i++) { ret += calculateParameterDistance(arguments[i], vargsType); } } else { ret+=1l<<VARGS_SHIFT; } return ret; }', 'ground_truth': 'public static long calculateParameterDistance(Class[] arguments, ParameterTypes pt) { CachedClass [] parameters = pt.getParameterTypes(); if (parameters.length == 0) return 0; long ret = 0; int noVargsLength = parameters.length-1; for (int i = 0; i < noVargsLength; i++) { ret += calculateParameterDistance(arguments[i], parameters[i]); } if (arguments.length==parameters.length) { CachedClass baseType = parameters[noVargsLength]; if (!parameters[noVargsLength].isAssignableFrom(arguments[noVargsLength])) { baseType= ReflectionCache.getCachedClass(baseType.getCachedClass().getComponentType()); ret+=2l<<VARGS_SHIFT; } ret += calculateParameterDistance(arguments[noVargsLength], baseType); } else if (arguments.length>parameters.length) { ret += (2+arguments.length-parameters.length)<<VARGS_SHIFT; CachedClass vargsType = ReflectionCache.getCachedClass(parameters[noVargsLength].getCachedClass().getComponentType()); for (int i = noVargsLength; i < arguments.length; i++) { ret += calculateParameterDistance(arguments[i], vargsType); } } else { ret+=1l<<VARGS_SHIFT; } return ret; }', 'output': 'public static long calculateParameterDistance(Class<?>[] arguments, Class<?>[] parameters) { if (parameters.length == 0) return 0; long ret = 0; int noVargsLength = parameters.length-1; for (int i = 0; i < noVargsLength; i++) { ret += calculateParameterDistance(arguments[i], parameters[i]); } if (arguments.length==parameters.length) { Class baseType = parameters[noVargsLength]; if (!isAssignableFrom(parameters[noVargsLength],arguments[noVargsLength])) { baseType=baseType.getComponentType(); ret+=2l<<VARGS_SHIFT; } ret += calculateParameterDistance(arguments[noVargsLength], baseType); } else if (arguments.length>parameters.length) { ret += (2+arguments.length-parameters.length)<<VARGS_SHIFT; Class vargsType = parameters[noVargsLength].getComponentType(); for (int i = noVargsLength; i < arguments.length; i++) { ret += calculateParameterDistance(arguments[i], vargsType); } } else { ret+=1l<<VARGS_SHIFT; } return ret; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public TreeMap<String, String> getTopologyCommandParams(StackId stackId, String serviceName, String componentName) throws AmbariException { ServiceInfo serviceInfo = ambariMetaInfo.getService(stackId.getStackName(), stackId.getStackVersion(), serviceName); ComponentInfo componentInfo = ambariMetaInfo.getComponent( stackId.getStackName(), stackId.getStackVersion(), serviceName, componentName); String scriptName = null; String scriptCommandTimeout = ""; CommandScriptDefinition script = componentInfo.getCommandScript(); if (serviceInfo.getSchemaVersion().equals(AmbariMetaInfo.SCHEMA_VERSION_2)) { if (script != null) { scriptName = script.getScript(); if (script.getTimeout() > 0) { scriptCommandTimeout = String.valueOf(script.getTimeout()); } } else { String message = String.format("Component %s of service %s has not " + "command script defined", componentName, serviceName); throw new AmbariException(message); } } String agentDefaultCommandTimeout = configs.getDefaultAgentTaskTimeout(false); String actualTimeout = (!scriptCommandTimeout.equals("") ? scriptCommandTimeout : agentDefaultCommandTimeout); TreeMap<String, String> commandParams = new TreeMap<>(); commandParams.put(COMMAND_TIMEOUT, actualTimeout); commandParams.put(SCRIPT, scriptName); commandParams.put(SCRIPT_TYPE, script.getScriptType().toString()); return commandParams; }', 'ground_truth': 'public TreeMap<String, String> getTopologyCommandParams(Long clusterId, String serviceName, String componentName) throws AmbariException { TreeMap<String, String> commandParams = new TreeMap<>(); RepositoryVersionEntity repositoryVersion = getComponentRepositoryVersion(clusterId, serviceName, componentName); if (null != repositoryVersion) { StackId stackId = repositoryVersion.getStackId(); ServiceInfo serviceInfo = ambariMetaInfo.getService(stackId.getStackName(), stackId.getStackVersion(), serviceName); ComponentInfo componentInfo = ambariMetaInfo.getComponent( stackId.getStackName(), stackId.getStackVersion(), serviceName, componentName); commandParams.put(SERVICE_PACKAGE_FOLDER, serviceInfo.getServicePackageFolder()); String scriptName = null; String scriptCommandTimeout = ""; CommandScriptDefinition script = componentInfo.getCommandScript(); if (serviceInfo.getSchemaVersion().equals(AmbariMetaInfo.SCHEMA_VERSION_2)) { if (script != null) { scriptName = script.getScript(); if (script.getTimeout() > 0) { scriptCommandTimeout = String.valueOf(script.getTimeout()); } } else { String message = String.format("Component %s of service %s has not " + "command script defined", componentName, serviceName); throw new AmbariException(message); } } String agentDefaultCommandTimeout = configs.getDefaultAgentTaskTimeout(false); String actualTimeout = (!scriptCommandTimeout.equals("") ? scriptCommandTimeout : agentDefaultCommandTimeout); commandParams.put(COMMAND_TIMEOUT, actualTimeout); commandParams.put(SCRIPT, scriptName); commandParams.put(SCRIPT_TYPE, script.getScriptType().toString()); } return commandParams; }', 'output': 'public Map<String, String> getTopologyCommandParams(StackId stackId, String serviceName, String componentName) throws AmbariException { ServiceInfo serviceInfo = ambariMetaInfo.getService(stackId.getStackName(), stackId.getStackVersion(), serviceName); ComponentInfo componentInfo = ambariMetaInfo.getComponent( stackId.getStackName(), stackId.getStackVersion(), serviceName, componentName); String scriptName = null; String scriptCommandTimeout = ""; CommandScriptDefinition script = componentInfo.getCommandScript(); if (serviceInfo.getSchemaVersion().equals(AmbariMetaInfo.SCHEMA_VERSION_2)) { if (script != null) { scriptName = script.getScript(); if (script.getTimeout() > 0) { scriptCommandTimeout = String.valueOf(script.getTimeout()); } } else { String message = String.format("Component %s of service %s has not " + "command script defined", componentName, serviceName); throw new AmbariException(message); } } String agentDefaultCommandTimeout = configs.getDefaultAgentTaskTimeout(false); String actualTimeout = (!scriptCommandTimeout.equals("") ? scriptCommandTimeout : agentDefaultCommandTimeout); Map<String, String> commandParams = new HashMap<>(); commandParams.put(COMMAND_TIMEOUT, actualTimeout); commandParams.put(SCRIPT, scriptName); commandParams.put(SCRIPT_TYPE, script.getScriptType().toString()); return commandParams; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ReuseTree(int cacheId, PageMemory pageMem, FullPageId metaPageId, boolean initNew) throws IgniteCheckedException { super(cacheId, pageMem, metaPageId); if (initNew) initNew(); }', 'ground_truth': 'public ReuseTree(ReuseList reuseList, int cacheId, PageMemory pageMem, FullPageId metaPageId, boolean initNew) throws IgniteCheckedException { super(cacheId, pageMem, metaPageId, reuseList); if (initNew) initNew(); }', 'output': 'public ReuseTree(int cacheId, PageMemory pageMem, FullPageId metaPageId, boolean initNew, int nodeId) throws IgniteCheckedException { super(cacheId, pageMem, metaPageId, nodeId); if (initNew) initNew(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void performLocally(Stage stage, final Runnable runnable, final IAsyncCallbackWithFailure<?> handler) { StageManager.getStage(stage).maybeExecuteImmediately(new LocalMutationRunnable() { public void runMayThrow() { try { runnable.run(); handler.response(null); } catch (Exception ex) { if (!(ex instanceof WriteTimeoutException)) logger.error("Failed to apply mutation locally : {}", ex); handler.onFailure(FBUtilities.getBroadcastAddress()); } } @Override protected Verb verb() { return MessagingService.Verb.MUTATION; } }); }', 'ground_truth': 'private static void performLocally(Stage stage, Optional<IMutation> mutation, final Runnable runnable, final IAsyncCallbackWithFailure<?> handler) { StageManager.getStage(stage).maybeExecuteImmediately(new LocalMutationRunnable(mutation) { public void runMayThrow() { try { runnable.run(); handler.response(null); } catch (Exception ex) { if (!(ex instanceof WriteTimeoutException)) logger.error("Failed to apply mutation locally : {}", ex); handler.onFailure(FBUtilities.getBroadcastAddress()); } } @Override protected Verb verb() { return MessagingService.Verb.MUTATION; } }); }', 'output': 'private static void performLocally(Stage stage, final Runnable runnable, final IAsyncCallbackWithFailure<?> handler) { StageManager.getStage(stage).maybeExecuteImmediately(new LocalMutationRunnable() { public void runMayThrow() { try { runnable.run(); handler.response(null); } catch (Exception ex) { if (!(ex instanceof WriteTimeoutException)) logger.error("Failed to apply mutation locally : {}", ex); handler.onFailure(FBUtilities.getBroadcastAddress()); } } @Override protected Verb verb() { return MessagingService.Verb.MUTATION; } }); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public SqlClientContext(GridKernalContext ctx, Factory<GridWorker> orderedBatchWorkerFactory, boolean distributedJoins, boolean enforceJoinOrder, boolean collocated, boolean replicatedOnly, boolean lazy, boolean skipReducerOnUpdate, @Nullable Boolean dataPageScanEnabled, @Nullable Integer updateBatchSize ) { this.ctx = ctx; this.orderedBatchWorkerFactory = orderedBatchWorkerFactory; this.distributedJoins = distributedJoins; this.enforceJoinOrder = enforceJoinOrder; this.collocated = collocated; this.replicatedOnly = replicatedOnly; this.lazy = lazy; this.skipReducerOnUpdate = skipReducerOnUpdate; this.dataPageScanEnabled = dataPageScanEnabled; this.updateBatchSize = updateBatchSize; log = ctx.log(SqlClientContext.class.getName()); }', 'ground_truth': 'public SqlClientContext(GridKernalContext ctx, Factory<GridWorker> orderedBatchWorkerFactory, boolean distributedJoins, boolean enforceJoinOrder, boolean collocated, boolean replicatedOnly, boolean lazy, boolean skipReducerOnUpdate, @Nullable Boolean dataPageScanEnabled, @Nullable Integer updateBatchSize, @Nullable String qryEngine ) { this.ctx = ctx; this.orderedBatchWorkerFactory = orderedBatchWorkerFactory; this.distributedJoins = distributedJoins; this.enforceJoinOrder = enforceJoinOrder; this.collocated = collocated; this.replicatedOnly = replicatedOnly; this.lazy = lazy; this.skipReducerOnUpdate = skipReducerOnUpdate; this.dataPageScanEnabled = dataPageScanEnabled; this.updateBatchSize = updateBatchSize; this.qryEngine = qryEngine; log = ctx.log(SqlClientContext.class.getName()); }', 'output': 'public SqlClientContext(IgniteKernalContext ctx, Factory<GridWorker> orderedBatchWorkerFactory, boolean distributedJoins, boolean enforceJoinOrder, boolean collocated, boolean replicatedOnly, boolean lazy, boolean skipReducerOnUpdate, @Nullable Boolean dataPageScanEnabled, @Nullable Integer updateBatchSize ) { this.ctx = ctx; this.orderedBatchWorkerFactory = orderedBatchWorkerFactory; this.distributedJoins = distributedJoins; this.enforceJoinOrder = enforceJoinOrder; this.collocated = collocated; this.replicatedOnly = replicatedOnly; this.lazy = lazy; this.skipReducerOnUpdate = skipReducerOnUpdate; this.dataPageScanEnabled = dataPageScanEnabled; this.updateBatchSize = updateBatchSize; log = ctx.log(SqlClientContext.class.getName()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void finishAndStartNext() throws IOException { PendingFile pf = files.remove(0); fileMap.remove(pf.getFilename()); if (files.size() > 0) startNext(); else { if (logger.isDebugEnabled()) logger.debug("Signalling that streaming is done for " + to); condition.signalAll(); } }', 'ground_truth': 'public void finishAndStartNext(String pfname) throws IOException { PendingFile pf = fileMap.remove(pfname); files.remove(pf); if (files.size() > 0) streamFile(files.get(0)); else { if (logger.isDebugEnabled()) logger.debug("Signalling that streaming is done for {} session {}", context.host, context.sessionId); remove(context); condition.signalAll(); } }', 'output': 'public void finishAndStartNext() throws IOException, InterruptedException { PendingFile pf = files.remove(0); fileMap.remove(pf.getFilename()); if (files.size() > 0) startNext(); else { if (logger.isDebugEnabled()) logger.debug("Signalling that streaming is done for " + to); condition.signalAll(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public final ActionInvocationDto recordActionResultScalar( final @NonNull ActionInvocationDto invocationDto, final @NonNull ObjectSpecification returnType, final @NonNull ManagedObject value) { val feature = getSpecificationLoader().loadFeatureElseFail(actionIdentifier(invocationDto)); val valueCls = feature.getElementType().getCorrespondingClass(); val context = newContext(valueCls, feature); invocationDto.setReturned( recordValue(context, new ValueWithTypeDto(), value)); return invocationDto; }', 'ground_truth': 'public final ActionInvocationDto recordActionResultScalar( final @NonNull ActionInvocationDto invocationDto, final @NonNull ObjectAction objectAction, final @NonNull ManagedObject value) { val feature = objectAction; val elementTypeAsClass = feature.getElementType().getCorrespondingClass(); val context = newContext(elementTypeAsClass, feature); invocationDto.setReturned( recordValue(context, new ValueWithTypeDto(), value)); return invocationDto; }', 'output': 'public final ActionInvocationDto recordActionResultScalar( final @NonNull ActionInvocationDto invocationDto, final @NonNull Class<?> returnType, final @NonNull ManagedObject value) { val feature = getSpecificationLoader().loadFeatureElseFail(actionIdentifier(invocationDto)); val valueCls = feature.getElementType().getCorrespondingClass(); val context = newContext(valueCls, feature); invocationDto.setReturned( recordValue(context, new ValueWithTypeDto(), value)); return invocationDto; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public SingleValuedTimelineMetric getAggregatedTimelineMetricFromResultSet(ResultSet rs, Function f) throws SQLException, IOException { byte[] uuid = rs.getBytes("UUID"); TimelineMetric timelineMetric = metadataManagerInstance.getMetricFromUuid(uuid); Function function = (f != null) ? f : Function.DEFAULT_VALUE_FUNCTION; SingleValuedTimelineMetric metric = new SingleValuedTimelineMetric( timelineMetric.getMetricName() + function.getSuffix(), timelineMetric.getAppId(), timelineMetric.getInstanceId(), timelineMetric.getHostName(), rs.getLong("SERVER_TIME") ); double value; switch(function.getReadFunction()){ case AVG: value = rs.getDouble("METRIC_SUM") / rs.getInt("METRIC_COUNT"); break; case MIN: value = rs.getDouble("METRIC_MIN"); break; case MAX: value = rs.getDouble("METRIC_MAX"); break; case SUM: value = rs.getDouble("METRIC_SUM"); break; default: value = rs.getDouble("METRIC_SUM") / rs.getInt("METRIC_COUNT"); break; } metric.setSingleTimeseriesValue(rs.getLong("SERVER_TIME"), value); return metric; }', 'ground_truth': 'public SingleValuedTimelineMetric getAggregatedTimelineMetricFromResultSet(ResultSet rs, Function f, boolean shouldSumMetricAcrossTime) throws SQLException, IOException { byte[] uuid = rs.getBytes("UUID"); TimelineMetric timelineMetric = metadataManagerInstance.getMetricFromUuid(uuid); Function function = (f != null) ? f : Function.DEFAULT_VALUE_FUNCTION; SingleValuedTimelineMetric metric = new SingleValuedTimelineMetric( timelineMetric.getMetricName() + function.getSuffix(), timelineMetric.getAppId(), timelineMetric.getInstanceId(), timelineMetric.getHostName(), rs.getLong("SERVER_TIME") ); double value; switch(function.getReadFunction()){ case AVG: value = rs.getDouble("METRIC_SUM") / rs.getInt("METRIC_COUNT"); break; case MIN: value = rs.getDouble("METRIC_MIN"); break; case MAX: value = rs.getDouble("METRIC_MAX"); break; case SUM: value = rs.getDouble("METRIC_SUM"); if (!shouldSumMetricAcrossTime) { value = value / rs.getInt("METRIC_COUNT"); } break; default: value = rs.getDouble("METRIC_SUM") / rs.getInt("METRIC_COUNT"); break; } metric.setSingleTimeseriesValue(rs.getLong("SERVER_TIME"), value); return metric; }', 'output': 'public SingleValuedTimelineMetric getAggregatedTimelineMetricFromResultSet(ResultSet rs, Function f) throws SQLException, IOException { byte[] uuid = rs.getBytes("UUID"); TimelineMetric timelineMetric = metadataManagerInstance.getMetricFromUuid(uuid); Function function = (f != null) ? f : Function.DEFAULT_VALUE_FUNCTION; SingleValuedTimelineMetric metric = new SingleValuedTimelineMetric( timelineMetric.getMetricName() + function.getSuffix(), timelineMetric.getAppId(), timelineMetric.getInstanceId(), timelineMetric.getHostName(), rs.getLong("SERVER_TIME") ); double value; switch(function.getReadFunction()){ case AVG: value = rs.getDouble("METRIC_SUM") / rs.getInt("METRIC_COUNT"); break; case MIN: value = rs.getDouble("METRIC_MIN"); break; case MAX: value = rs.getDouble("METRIC_MAX"); break; case SUM: value = rs.getDouble("METRIC_SUM"); break; default: value = rs.getDouble("METRIC_SUM") / rs.getInt("METRIC_COUNT"); break; } metric.setSingleTimeseriesValue(rs.getLong("SERVER_TIME"), value); return metric; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public synchronized Long executeBatchRequest(Long executionId, Long batchId) throws AmbariException { String type = null; String uri = null; String body = null; try { RequestScheduleBatchRequestEntityPK batchRequestEntityPK = new RequestScheduleBatchRequestEntityPK(); batchRequestEntityPK.setScheduleId(executionId); batchRequestEntityPK.setBatchId(batchId); RequestScheduleBatchRequestEntity batchRequestEntity = batchRequestDAO.findByPk(batchRequestEntityPK); type = batchRequestEntity.getRequestType(); uri = batchRequestEntity.getRequestUri(); body = batchRequestEntity.getRequestBodyAsString(); } catch (Exception e) { } return -1L; }', 'ground_truth': 'public Long executeBatchRequest(Long executionId, Long batchId, String clusterName) throws AmbariException { String type = null; String uri = null; String body = null; try { RequestExecution requestExecution = clusters.getCluster(clusterName).getAllRequestExecutions().get(executionId); BatchRequest batchRequest = requestExecution.getBatchRequest(batchId); type = batchRequest.getType(); uri = batchRequest.getUri(); body = requestExecution.getRequestBody(batchId); BatchRequestResponse batchRequestResponse = performApiRequest(uri, body, type); updateBatchRequest(executionId, batchId, clusterName, batchRequestResponse, false); return batchRequestResponse.getRequestId(); } catch (Exception e) { throw new AmbariException("Exception occurred while performing request", e); } }', 'output': 'public synchronized Long executeBatchRequest(long executionId, long batchId) throws AmbariException { String type = null; String uri = null; String body = null; try { RequestScheduleBatchRequestEntityPK batchRequestEntityPK = new RequestScheduleBatchRequestEntityPK(); batchRequestEntityPK.setScheduleId(executionId); batchRequestEntityPK.setBatchId(batchId); RequestScheduleBatchRequestEntity batchRequestEntity = batchRequestDAO.findByPk(batchRequestEntityPK); type = batchRequestEntity.getRequestType(); uri = batchRequestEntity.getRequestUri(); body = batchRequestEntity.getRequestBodyAsString(); } catch (Exception e) { } return -1L; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Collection<DemoItem> autoComplete1Act( Parity parity, @MinLength(3) String search) { if(parity == null) { return holder.getItems() .stream() .filter(item->item.getName().toLowerCase().contains(search.toLowerCase())) .collect(Collectors.toList()); } return holder.getItems() .stream() .filter(item->parity == item.getParity()) .filter(item->item.getName().toLowerCase().contains(search.toLowerCase())) .collect(Collectors.toList()); }', 'ground_truth': 'public Collection<DemoItem> autoComplete1Act( Parameters params, @MinLength(3) String search) { val parity = params.parity(); if(parity == null) { return holder.getItems() .stream() .filter(item->item.getName().toLowerCase().contains(search.toLowerCase())) .collect(Collectors.toList()); } return holder.getItems() .stream() .filter(item->parity == item.getParity()) .filter(item->item.getName().toLowerCase().contains(search.toLowerCase())) .collect(Collectors.toList()); }', 'output': 'public Collection<String> autoComplete1Act( Parity parity, @MinLength(3) String search) { if(parity == null) { return holder.getItems() .stream() .filter(item->item.getName().toLowerCase().contains(search.toLowerCase())) .collect(Collectors.toList()); } return holder.getItems() .stream() .filter(item->parity == item.getParity()) .filter(item->item.getName().toLowerCase().contains(search.toLowerCase())) .collect(Collectors.toList()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void setInfo(FeatureInfo info) { this.featureInfo = info; boolean activateNow; if (autoActivate != null) { activateNow = Boolean.TRUE.equals(autoActivate); } else { activateNow = Boolean.getBoolean("noActivateButton"); } if (activateNow) { infoLabel.setVisible(false); downloadButton.setVisible(false); downloadButtonActionPerformed(null); } }', 'ground_truth': 'public void setInfo(FeatureInfo info, Collection<UpdateElement> toInstall) { this.featureInfo = info; this.featureInstall = toInstall; boolean activateNow = toInstall.isEmpty(); if (activateNow) { infoLabel.setVisible(false); downloadLabel.setVisible(false); activateButton.setVisible(false); downloadButton.setVisible(false); activateButtonActionPerformed(null); } else { infoLabel.setVisible(true); downloadLabel.setVisible(true); activateButton.setVisible(true); downloadButton.setVisible(true); } }', 'output': 'public void setInfo(FeatureInfo info, boolean autoActivate) { this.featureInfo = info; boolean activateNow; if (autoActivate != null) { activateNow = Boolean.TRUE.equals(autoActivate); } else { activateNow = Boolean.getBoolean("noActivateButton"); } if (activateNow) { infoLabel.setVisible(false); downloadButton.setVisible(false); downloadButtonActionPerformed(null); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RepairFuture submitRepairSession(Range<Token> range, String keyspace, boolean isSequential, Collection<String> dataCenters, String... cfnames) { RepairSession session = new RepairSession(range, keyspace, isSequential, dataCenters, cfnames); if (session.endpoints.isEmpty()) return null; RepairFuture futureTask = new RepairFuture(session); executor.execute(futureTask); return futureTask; }', 'ground_truth': 'public RepairFuture submitRepairSession(Range<Token> range, String keyspace, boolean isSequential, Collection<String> dataCenters, Collection<String> hosts, String... cfnames) { RepairSession session = new RepairSession(range, keyspace, isSequential, dataCenters, hosts, cfnames); if (session.endpoints.isEmpty()) return null; RepairFuture futureTask = new RepairFuture(session); executor.execute(futureTask); return futureTask; }', 'output': 'public RepairFuture submitRepairSession(Range<Token> range, String keyspace, boolean isSequential, Collection<String> dataCenters, String... cfnames) { RepairSession session = new RepairSession(range, keyspace, isSequential, dataCenters, cfnames); if (session.endpoints.isEmpty()) return null; RepairFuture futureTask = new RepairFuture(session); executor.execute(futureTask); return futureTask; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private int[] evaluateIntervalLiteralAsYearToMonth( int sign, String value, String originalValue, SqlParserPos pos) { BigDecimal year; BigDecimal month; String intervalPattern = "(\\\\d+)-(\\\\d{1,2})"; Matcher m = Pattern.compile(intervalPattern).matcher(value); if (m.matches()) { try { year = parseField(m, 1); month = parseField(m, 2); } catch (NumberFormatException e) { throw invalidValueException(pos, originalValue); } checkLeadFieldInRange(sign, year, TimeUnit.YEAR, pos); if (!(isSecondaryFieldInRange(month, TimeUnit.MONTH))) { throw invalidValueException(pos, originalValue); } return fillIntervalValueArray(sign, year, month); } else { throw invalidValueException(pos, originalValue); } }', 'ground_truth': 'private int[] evaluateIntervalLiteralAsYearToMonth( RelDataTypeSystem typeSystem, int sign, String value, String originalValue, SqlParserPos pos) { BigDecimal year; BigDecimal month; String intervalPattern = "(\\\\d+)-(\\\\d{1,2})"; Matcher m = Pattern.compile(intervalPattern).matcher(value); if (m.matches()) { try { year = parseField(m, 1); month = parseField(m, 2); } catch (NumberFormatException e) { throw invalidValueException(pos, originalValue); } checkLeadFieldInRange(typeSystem, sign, year, TimeUnit.YEAR, pos); if (!(isSecondaryFieldInRange(month, TimeUnit.MONTH))) { throw invalidValueException(pos, originalValue); } return fillIntervalValueArray(sign, year, month); } else { throw invalidValueException(pos, originalValue); } }', 'output': 'private int[] evaluateIntervalLiteralAsYearToMonth( int sign, String value, String originalValue, SqlParserPos pos) { BigDecimal year; BigDecimal month; String intervalPattern = "(\\\\d+)-(\\\\d{1,2})"; Matcher m = Pattern.compile(intervalPattern).matcher(value); if (m.matches()) { try { year = parseField(m, 1); month = parseField(m, 2); } catch (NumberFormatException e) { throw invalidValueException(pos, originalValue); } checkLeadFieldInRange(sign, year, TimeUnit.YEAR, pos); if (!(isSecondaryFieldInRange(month, TimeUnit.MONTH))) { throw invalidValueException(pos, originalValue); } return fillIntervalValueArray(sign, year, month); } else { throw invalidValueException(pos, originalValue); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public EnumerableTableAccessRel( RelOptCluster cluster, RelOptTable table, RelOptConnection connection) { super( cluster, cluster.traitSetOf(CallingConvention.ENUMERABLE), table, connection); }', 'ground_truth': 'public EnumerableTableAccessRel( RelOptCluster cluster, RelOptTable table, RelOptConnection connection, Expression expression) { super( cluster, cluster.traitSetOf(CallingConvention.ENUMERABLE), table, connection); this.expression = expression; }', 'output': 'public EnumerableTableAccessRel( RelOptCluster cluster, RelOptTable table, RelOptConnection connection, String catalogName) { super( cluster, cluster.traitSetOf(CallingConvention.ENUMERABLE), table, connection, catalogName); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Response getRequestSchedule(@Context HttpHeaders headers, @Context UriInfo ui, @PathParam("requestScheduleId") String requestScheduleId) { return handleRequest(headers, null, ui, Request.Type.GET, createRequestSchedule(m_clusterName, requestScheduleId)); }', 'ground_truth': 'public Response getRequestSchedule(String body, @Context HttpHeaders headers, @Context UriInfo ui, @PathParam("requestScheduleId") String requestScheduleId) { return handleRequest(headers, body, ui, Request.Type.GET, createRequestSchedule(m_clusterName, requestScheduleId)); }', 'output': 'public Response getRequestSchedule(HttpHeaders headers, @Context UriInfo ui, @PathParam("requestScheduleId") String requestScheduleId) { return handleRequest(headers, null, ui, Request.Type.GET, createRequestSchedule(m_clusterName, requestScheduleId)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean writeArray(Object arr, long off, int bytes) { assert arr != null; assert arr.getClass().isArray() && arr.getClass().getComponentType().isPrimitive(); assert off > 0; assert bytes >= 0; assert bytes >= arrOff; if (!buf.hasRemaining()) return false; int toWrite = bytes - arrOff; int pos = buf.position(); int remaining = buf.remaining(); if (toWrite <= remaining) { UNSAFE.copyMemory(arr, off + arrOff, heapArr, baseOff + pos, toWrite); pos += toWrite; buf.position(pos); arrOff = 0; return true; } else { UNSAFE.copyMemory(arr, off + arrOff, heapArr, baseOff + pos, remaining); pos += remaining; buf.position(pos); arrOff += remaining; return false; } }', 'ground_truth': 'private boolean writeArray(Object arr, long off, int len, int bytes) { assert arr != null; assert arr.getClass().isArray() && arr.getClass().getComponentType().isPrimitive(); assert off > 0; assert len >= 0; assert bytes >= 0; assert bytes >= arrOff; if (arrOff == 0) { if (remaining() < 4) return false; writeInt(len); } int toWrite = bytes - arrOff; int pos = buf.position(); int remaining = buf.remaining(); if (toWrite <= remaining) { UNSAFE.copyMemory(arr, off + arrOff, heapArr, baseOff + pos, toWrite); pos += toWrite; buf.position(pos); arrOff = 0; return true; } else { UNSAFE.copyMemory(arr, off + arrOff, heapArr, baseOff + pos, remaining); pos += remaining; buf.position(pos); arrOff += remaining; return false; } }', 'output': 'private boolean writeArray(Object arr, long off, int bytes) { assert arr != null; assert arr.getClass().isArray() && arr.getClass().getComponentType().isPrimitive(); assert off > 0; assert bytes >= 0; assert bytes >= arrOff; if (!buf.hasRemaining()) return false; int toWrite = bytes - arrOff; int pos = buf.position(); int remaining = buf.remaining(); if (toWrite <= remaining) { UNSAFE.copyMemory(arr, off + arrOff, heapArr, baseOff + pos, toWrite); pos += toWrite; buf.position(pos); arrOff = 0; return true; } else { UNSAFE.copyMemory(arr, off + arrOff, heapArr, baseOff + pos, remaining); pos += remaining; buf.position(pos); arrOff += remaining; return false; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void initialize(Cluster cluster) { StackId stackId = cluster.getCurrentStackVersion(); StackInfo stack = null; try { stack = ambariMetaInfo.getStack(stackId.getStackName(), stackId.getStackVersion()); } catch (AmbariException ignored) { } Map<String,Object> userData = stack.getRoleCommandOrder().getContent(); Map<String,Object> generalSection = (Map<String, Object>) userData.get(GENERAL_DEPS_KEY); addDependencies(generalSection); if (hasGLUSTERFS) { Map<String,Object> glusterfsSection = (Map<String, Object>) userData.get(GLUSTERFS_DEPS_KEY); addDependencies(glusterfsSection); } else { Map<String,Object> noGlusterFSSection = (Map<String, Object>) userData.get(NO_GLUSTERFS_DEPS_KEY); addDependencies(noGlusterFSSection); } if (isNameNodeHAEnabled) { Map<String,Object> NAMENODEHASection = (Map<String, Object>) userData.get(NAMENODE_HA_DEPS_KEY); addDependencies(NAMENODEHASection); } if (isResourceManagerHAEnabled) { Map<String,Object> ResourceManagerHASection = (Map<String, Object>) userData.get(RESOURCEMANAGER_HA_DEPS_KEY); addDependencies(ResourceManagerHASection); } extendTransitiveDependency(); addMissingRestartDependencies(); }', 'ground_truth': 'public void initialize(Cluster cluster, LinkedHashSet<String> sectionKeys) { this.sectionKeys = sectionKeys; dependencies.clear(); StackId stackId = cluster.getCurrentStackVersion(); StackInfo stack = null; try { stack = ambariMetaInfo.getStack(stackId.getStackName(), stackId.getStackVersion()); } catch (AmbariException ignored) { } Map<String,Object> userData = stack.getRoleCommandOrder().getContent(); Map<String,Object> generalSection = (Map<String, Object>) userData.get(GENERAL_DEPS_KEY); addDependencies(generalSection); for (String sectionKey : sectionKeys) { Map<String, Object> section = (Map<String, Object>) userData.get(sectionKey); addDependencies(section); } extendTransitiveDependency(); addMissingRestartDependencies(); }', 'output': 'public void initialize(Cluster cluster) { StackId stackId = cluster.getCurrentStackVersion(); StackInfo stack = null; try { stack = ambariMetaInfo.getStack(stackId.getStackName(), stackId.getStackVersion()); } catch (AmbariException ignored) { } Map<String,Object> userData = stack.getRoleCommandOrder().getContent(); Map<String,Object> generalSection = (Map<String, Object>) userData.get(GENERAL_DEPS_KEY); addDependencies(generalSection); if (hasGLUSTERFS) { Map<String,Object> glusterfsSection = (Map<String, Object>) userData.get(GLUSTERFS_DEPS_KEY); addDependencies(glusterfsSection); } else { Map<String,Object> noGlusterFSSection = (Map<String, Object>) userData.get(NO_GLUSTERFS_DEPS_KEY); addDependencies(noGlusterFSSection); } if (isNameNodeHAEnabled) { Map<String,Object> NAMENODEHASection = (Map<String, Object>) userData.get(NAMENODE_HA_DEPS_KEY); addDependencies(NAMENODEHASection); } if (isResourceManagerHAEnabled) { Map<String,Object> ResourceManagerHASection = (Map<String, Object>) userData.get(RESOURCEMANAGER_HA_DEPS_KEY); addDependencies(ResourceManagerHASection); } extendTransitiveDependency(); addMissingRestartDependencies(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void createCache(CacheMode cacheMode, CacheWriteSynchronizationMode writeSync, Ignite ignite, String name) { ignite.createCache(cacheConfiguration(name, cacheMode, writeSync)); }', 'ground_truth': 'protected void createCache(CacheMode cacheMode, CacheWriteSynchronizationMode writeSync, boolean nearCache, Ignite ignite, String name) { ignite.createCache(cacheConfiguration(name, cacheMode, writeSync, nearCache)); }', 'output': 'protected void createCache(CacheMode cacheMode, CacheWriteSynchronizationMode writeSync, IgniteEx ignite, String name) { ignite.createCache(cacheConfiguration(name, cacheMode, writeSync)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GridDhtLockResponse(GridCacheVersion lockVer, GridUuid futId, GridUuid miniId, Throwable err) { super(lockVer, futId, err); assert miniId != null; this.miniId = miniId; }', 'ground_truth': 'public GridDhtLockResponse(int cacheId, GridCacheVersion lockVer, GridUuid futId, GridUuid miniId, Throwable err) { super(cacheId, lockVer, futId, err); assert miniId != null; this.miniId = miniId; }', 'output': 'public GridDhtLockResponse(GridCacheVersion lockVer, GridFutId futId, GridUuid miniId, Throwable err) { super(lockVer, futId, err); assert miniId != null; this.miniId = miniId; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void assertValidProperty(Set<ConsistencyLevel> properties) { assertValidProperty(warnedSetter, warnedGetter, properties); assertValidProperty(disallowedSetter, disallowedGetter, properties); } private void assertValidPropertyCSV(String csv)\n    { { csv = sortCSV(csv); assertValidProperty(warnedCSVSetter, warnedCSVGetter, csv); assertValidProperty(disallowedCSVSetter, disallowedCSVGetter, csv); }', 'ground_truth': 'private void assertValidProperty(Set<ConsistencyLevel> input) { Set<String> properties = input.stream().map(ConsistencyLevel::name).collect(Collectors.toSet()); assertValidProperty(warnedSetter, warnedGetter, properties); assertValidProperty(disallowedSetter, disallowedGetter, properties); }', 'output': 'private void assertValidPropertyCSV(String csv, Set<ConsistencyLevel> properties) { csv = sortCSV(csv); assertValidProperty(warnedCSVSetter, warnedCSVGetter, csv, properties); assertValidProperty(disallowedCSVSetter, disallowedCSVGetter, csv, properties); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static DmlDistributedPlanInfo checkPlanCanBeDistributed(IgniteH2Indexing idx, boolean mvccEnabled, Connection conn, SqlFieldsQuery fieldsQry, boolean loc, String selectQry, String cacheName) throws IgniteCheckedException { if (loc || (!mvccEnabled && !isSkipReducerOnUpdateQuery(fieldsQry)) || DmlUtils.isBatched(fieldsQry)) return null; assert conn != null; try { try (PreparedStatement stmt = conn.prepareStatement(selectQry)) { H2Utils.bindParameters(stmt, F.asList(fieldsQry.getArgs())); GridCacheTwoStepQuery qry = GridSqlQuerySplitter.split( conn, GridSqlQueryParser.prepared(stmt), fieldsQry.getArgs(), fieldsQry.isCollocated(), fieldsQry.isDistributedJoins(), fieldsQry.isEnforceJoinOrder(), false, idx ); boolean distributed = !qry.isLocalSplit() && qry.hasCacheIds() && qry.skipMergeTable() && qry.mapQueries().size() == 1 && !qry.mapQueries().get(0).hasSubQueries(); if (distributed) { List<Integer> cacheIds = H2Utils.collectCacheIds(idx, CU.cacheId(cacheName), qry.tables()); H2Utils.checkQuery(idx, cacheIds, qry.mvccEnabled(), qry.forUpdate(), qry.tables()); return new DmlDistributedPlanInfo(qry.isReplicatedOnly(), cacheIds); } else return null; } } catch (SQLException e) { throw new IgniteCheckedException(e); } }', 'ground_truth': 'private static DmlDistributedPlanInfo checkPlanCanBeDistributed( IgniteH2Indexing idx, boolean mvccEnabled, String schemaName, SqlFieldsQuery fieldsQry, String selectQry, String cacheName ) throws IgniteCheckedException { if ((!mvccEnabled && !isSkipReducerOnUpdateQuery(fieldsQry)) || DmlUtils.isBatched(fieldsQry)) return null; try (Connection conn = idx.connections().connectionNoCache(schemaName)) { try (PreparedStatement stmt = conn.prepareStatement(selectQry)) { H2Utils.bindParameters(stmt, F.asList(fieldsQry.getArgs())); GridCacheTwoStepQuery qry = GridSqlQuerySplitter.split( conn, GridSqlQueryParser.prepared(stmt), fieldsQry.getArgs(), fieldsQry.isCollocated(), fieldsQry.isDistributedJoins(), fieldsQry.isEnforceJoinOrder(), false, idx ); boolean distributed = !qry.isLocalSplit() && qry.hasCacheIds() && qry.skipMergeTable() && qry.mapQueries().size() == 1 && !qry.mapQueries().get(0).hasSubQueries(); if (distributed) { List<Integer> cacheIds = H2Utils.collectCacheIds(idx, CU.cacheId(cacheName), qry.tables()); H2Utils.checkQuery(idx, cacheIds, qry.mvccEnabled(), qry.forUpdate(), qry.tables()); return new DmlDistributedPlanInfo(qry.isReplicatedOnly(), cacheIds); } else return null; } } catch (SQLException e) { throw new IgniteCheckedException(e); } }', 'output': 'private static DmlDistributedPlanInfo checkPlanCanBeDistributed(IgniteH2Indexing idx, boolean mvccEnabled, Connection conn, SqlFieldsQuery fieldsQry, boolean loc, String selectQry, String cacheName) throws IgniteCheckedException { if (loc || (!mvccEnabled && !isSkipReducerOnUpdateQuery(fieldsQry)) || DmlUtils.isBatched(fieldsQry)) return null; assert conn != null; try { try (PreparedStatement stmt = conn.prepareStatement(selectQry)) { H2Utils.bindParameters(stmt, F.asList(fieldsQry.getArgs())); GridCacheTwoStepQuery qry = GridSqlQuerySplitter.split( conn, GridSqlQueryParser.prepared(stmt), fieldsQry.getArgs(), fieldsQry.isCollocated(), fieldsQry.isDistributedJoins(), fieldsQry.isEnforceJoinOrder(), false, idx ); boolean distributed = !qry.isLocalSplit() && qry.hasCacheIds() && qry.skipMergeTable() && qry.mapQueries().size() == 1 && !qry.mapQueries().get(0).hasSubQueries(); if (distributed) { List<Integer> cacheIds = H2Utils.collectCacheIds(idx, CU.cacheId(cacheName), qry.tables()); H2Utils.checkQuery(idx, cacheIds, qry.mvccEnabled(), qry.forUpdate(), qry.tables()); return new DmlDistributedPlanInfo(qry.isReplicatedOnly(), cacheIds); } else return null; } } catch (SQLException e) { throw new IgniteCheckedException(e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static WebAppContext createBaseWebappContext() throws MalformedURLException { URI webResourceBase = findWebResourceBase(); WebAppContext context = new WebAppContext(); context.setBaseResource(Resource.newResource(webResourceBase)); context.setContextPath(ROOT_CONTEXT); context.setParentLoaderPriority(true); context.addEventListener(new ContextLoaderListener()); context.addEventListener(new RequestContextListener()); context.setInitParameter("contextClass", AnnotationConfigWebApplicationContext.class.getName()); context.setInitParameter("contextConfigLocation", InfraManagerConfig.class.getName()); ServletHolder jerseyServlet = context.addServlet(org.glassfish.jersey.servlet.ServletContainer.class, "/api/v1/*"); jerseyServlet.setInitOrder(1); jerseyServlet.setInitParameter("jersey.config.server.provider.packages","org.apache.ambari.infra.rest,io.swagger.jaxrs.listing"); context.getSessionHandler().getSessionManager().setMaxInactiveInterval(SESSION_TIMEOUT); context.getSessionHandler().getSessionManager().getSessionCookieConfig().setName(INFRA_MANAGER_SESSION_ID); return context; }', 'ground_truth': 'private static WebAppContext createBaseWebappContext(String dataFolder) throws MalformedURLException { URI webResourceBase = findWebResourceBase(); WebAppContext context = new WebAppContext(); ResourceCollection resources = new ResourceCollection(Resource.newResource(webResourceBase)); context.setBaseResource(resources); context.setContextPath(ROOT_CONTEXT); context.setParentLoaderPriority(true); ServletHolder dataServlet = new ServletHolder("static-data", DefaultServlet.class); dataServlet.setInitParameter("dirAllowed","true"); dataServlet.setInitParameter("pathInfoOnly","true"); dataServlet.setInitParameter("resourceBase", dataFolder); context.addServlet(dataServlet,"/files/*"); context.addEventListener(new ContextLoaderListener()); context.addEventListener(new RequestContextListener()); context.setInitParameter("contextClass", AnnotationConfigWebApplicationContext.class.getName()); context.setInitParameter("contextConfigLocation", InfraManagerConfig.class.getName()); ServletHolder jerseyServlet = context.addServlet(org.glassfish.jersey.servlet.ServletContainer.class, "/api/v1/*"); jerseyServlet.setInitOrder(1); jerseyServlet.setInitParameter("jersey.config.server.provider.packages","org.apache.ambari.infra.rest,io.swagger.jaxrs.listing"); context.getSessionHandler().getSessionManager().setMaxInactiveInterval(SESSION_TIMEOUT); context.getSessionHandler().getSessionManager().getSessionCookieConfig().setName(INFRA_MANAGER_SESSION_ID); return context; }', 'output': 'private static WebAppContext createBaseWebappContext() throws MalformedURLException { Resource webResourceBase = findWebResourceBase(); WebAppContext context = new WebAppContext(); context.setBaseResource(Resource.newResource(webResourceBase)); context.setContextPath(ROOT_CONTEXT); context.setParentLoaderPriority(true); context.addEventListener(new ContextLoaderListener()); context.addEventListener(new RequestContextListener()); context.setInitParameter("contextClass", AnnotationConfigWebApplicationContext.class.getName()); context.setInitParameter("contextConfigLocation", InfraManagerConfig.class.getName()); ServletHolder jerseyServlet = context.addServlet(org.glassfish.jersey.servlet.ServletContainer.class, "/api/v1/*"); jerseyServlet.setInitOrder(1); jerseyServlet.setInitParameter("jersey.config.server.provider.packages","org.apache.ambari.infra.rest,io.swagger.jaxrs.listing"); context.getSessionHandler().getSessionManager().setMaxInactiveInterval(SESSION_TIMEOUT); context.getSessionHandler().getSessionManager().getSessionCookieConfig().setName(INFRA_MANAGER_SESSION_ID); return context; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private OdbcResponse closeQuery(OdbcQueryCloseRequest req) { try { QueryCursor cur = qryCursors.get(req.queryId()).get1(); if (cur == null) return new OdbcResponse(OdbcResponse.STATUS_FAILED, "Failed to find query with ID: " + req.queryId()); cur.close(); qryCursors.remove(req.queryId()); OdbcQueryCloseResult res = new OdbcQueryCloseResult(req.queryId()); return new OdbcResponse(res); } catch (Exception e) { qryCursors.remove(req.queryId()); return new OdbcResponse(OdbcResponse.STATUS_FAILED, e.getMessage()); } }', 'ground_truth': 'private OdbcResponse closeQuery(long reqId, OdbcQueryCloseRequest req) { try { QueryCursor cur = qryCursors.get(req.queryId()).get1(); if (cur == null) return new OdbcResponse(OdbcResponse.STATUS_FAILED, "Failed to find query with ID: " + req.queryId()); cur.close(); qryCursors.remove(req.queryId()); OdbcQueryCloseResult res = new OdbcQueryCloseResult(req.queryId()); return new OdbcResponse(res); } catch (Exception e) { qryCursors.remove(req.queryId()); U.error(log, "Failed to close SQL query [reqId=" + reqId + ", req=" + req.queryId() + \']\', e); return new OdbcResponse(OdbcResponse.STATUS_FAILED, e.getMessage()); } }', 'output': 'private OdbcResponse closeQuery(OdbcQueryCloseRequest req) { try { QueryCursor cur = qryCursors.get(req.queryId()); if (cur == null) return new OdbcResponse(OdbcResponse.STATUS_FAILED, "Failed to find query with ID: " + req.queryId()); cur.close(); qryCursors.remove(req.queryId()); OdbcQueryCloseResult res = new OdbcQueryCloseResult(req.queryId()); return new OdbcResponse(res); } catch (Exception e) { qryCursors.remove(req.queryId()); return new OdbcResponse(OdbcResponse.STATUS_FAILED, e.getMessage()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ClientCacheSqlFieldsQueryRequest(BinaryRawReaderEx reader) { super(reader); String schema = reader.readString(); int pageSize = reader.readInt(); reader.readInt(); String sql = reader.readString(); Object[] args = PlatformCache.readQueryArgs(reader); JdbcStatementType stmtType = JdbcStatementType.fromOrdinal(reader.readByte()); boolean distributedJoins = reader.readBoolean(); boolean loc = reader.readBoolean(); boolean replicatedOnly = reader.readBoolean(); boolean enforceJoinOrder = reader.readBoolean(); boolean collocated = reader.readBoolean(); boolean lazy = reader.readBoolean(); int timeout = (int) reader.readLong(); includeFieldNames = reader.readBoolean(); SqlFieldsQuery qry = stmtType == JdbcStatementType.ANY_STATEMENT_TYPE ? new SqlFieldsQuery(sql) : new SqlFieldsQueryEx(sql,stmtType == JdbcStatementType.SELECT_STATEMENT_TYPE); qry.setSchema(schema) .setPageSize(pageSize) .setArgs(args) .setDistributedJoins(distributedJoins) .setLocal(loc) .setReplicatedOnly(replicatedOnly) .setEnforceJoinOrder(enforceJoinOrder) .setCollocated(collocated) .setLazy(lazy); if (timeout >= 0) qry.setTimeout(timeout, TimeUnit.MILLISECONDS); this.qry = qry; }', 'ground_truth': 'public ClientCacheSqlFieldsQueryRequest(BinaryRawReaderEx reader, ClientProtocolContext protocolCtx) { super(reader); String schema = reader.readString(); int pageSize = reader.readInt(); reader.readInt(); String sql = reader.readString(); Object[] args = PlatformCache.readQueryArgs(reader); JdbcStatementType stmtType = JdbcStatementType.fromOrdinal(reader.readByte()); boolean distributedJoins = reader.readBoolean(); boolean loc = reader.readBoolean(); boolean replicatedOnly = reader.readBoolean(); boolean enforceJoinOrder = reader.readBoolean(); boolean collocated = reader.readBoolean(); boolean lazy = reader.readBoolean(); int timeout = (int) reader.readLong(); includeFieldNames = reader.readBoolean(); SqlFieldsQuery qry = stmtType == JdbcStatementType.ANY_STATEMENT_TYPE ? new SqlFieldsQuery(sql) : new SqlFieldsQueryEx(sql,stmtType == JdbcStatementType.SELECT_STATEMENT_TYPE); qry.setSchema(schema) .setPageSize(pageSize) .setArgs(args) .setDistributedJoins(distributedJoins) .setLocal(loc) .setReplicatedOnly(replicatedOnly) .setEnforceJoinOrder(enforceJoinOrder) .setCollocated(collocated) .setLazy(lazy); if (protocolCtx.isFeatureSupported(ClientBitmaskFeature.DEFAULT_QRY_TIMEOUT) || timeout > 0) QueryUtils.withQueryTimeout(qry, timeout, TimeUnit.MILLISECONDS); this.qry = qry; }', 'output': 'public ClientCacheSqlFieldsQueryRequest(BinaryRawReaderEx reader) throws IOException { super(reader); String schema = reader.readString(); int pageSize = reader.readInt(); reader.readInt(); String sql = reader.readString(); Object[] args = PlatformCache.readQueryArgs(reader); JdbcStatementType stmtType = JdbcStatementType.fromOrdinal(reader.readByte()); boolean distributedJoins = reader.readBoolean(); boolean loc = reader.readBoolean(); boolean replicatedOnly = reader.readBoolean(); boolean enforceJoinOrder = reader.readBoolean(); boolean collocated = reader.readBoolean(); boolean lazy = reader.readBoolean(); int timeout = (int) reader.readLong(); includeFieldNames = reader.readBoolean(); SqlFieldsQuery qry = stmtType == JdbcStatementType.ANY_STATEMENT_TYPE ? new SqlFieldsQuery(sql) : new SqlFieldsQueryEx(sql,stmtType == JdbcStatementType.SELECT_STATEMENT_TYPE); qry.setSchema(schema) .setPageSize(pageSize) .setArgs(args) .setDistributedJoins(distributedJoins) .setLocal(loc) .setReplicatedOnly(replicatedOnly) .setEnforceJoinOrder(enforceJoinOrder) .setCollocated(collocated) .setLazy(lazy); if (timeout >= 0) qry.setTimeout(timeout, TimeUnit.MILLISECONDS); this.qry = qry; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static RowIterator readOne(SinglePartitionReadCommand command, ConsistencyLevel consistencyLevel) throws UnavailableException, IsBootstrappingException, ReadFailureException, ReadTimeoutException, InvalidRequestException { return readOne(command, consistencyLevel, null); } public static RowIterator readOne(SinglePartitionReadCommand command, ConsistencyLevel consistencyLevel, ClientState state)\n    throws UnavailableException, IsBootstrappingException, ReadFailureException, ReadTimeoutException, InvalidRequestException\n    { throws UnavailableException, IsBootstrappingException, ReadFailureException, ReadTimeoutException, InvalidRequestException { return PartitionIterators.getOnlyElement(read(SinglePartitionReadCommand.Group.one(command), consistencyLevel, state), command); }', 'ground_truth': 'public static RowIterator readOne(SinglePartitionReadCommand command, ConsistencyLevel consistencyLevel, long queryStartNanoTime) throws UnavailableException, IsBootstrappingException, ReadFailureException, ReadTimeoutException, InvalidRequestException { return readOne(command, consistencyLevel, null, queryStartNanoTime); }', 'output': 'public static Iterator<Row> readOne(SinglePartitionReadCommand command, ConsistencyLevel consistencyLevel) throws UnavailableException, IsBootstrappingException, ReadFailureException, ReadTimeoutException, InvalidRequestException { return readOne(command, consistencyLevel, null); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RepositoryResponse(String baseUrl, String osType, String repoId, String repoName, String distribution, String components, String mirrorsList, String defaultBaseUrl, Set<RepoTag> repoTags) { setBaseUrl(baseUrl); setOsType(osType); setRepoId(repoId); setRepoName(repoName); setDistribution(distribution); setComponents(components); setMirrorsList(mirrorsList); setDefaultBaseUrl(defaultBaseUrl); setTags(repoTags); }', 'ground_truth': 'public RepositoryResponse(String baseUrl, String osType, String repoId, String repoName, String distribution, String components, String mirrorsList, String defaultBaseUrl, Set<RepoTag> repoTags, List<String> applicableServices) { setBaseUrl(baseUrl); setOsType(osType); setRepoId(repoId); setRepoName(repoName); setDistribution(distribution); setComponents(components); setMirrorsList(mirrorsList); setDefaultBaseUrl(defaultBaseUrl); setTags(repoTags); setApplicableServices(applicableServices); }', 'output': 'public RepositoryResponse(String baseUrl, String osType, String repoId, String repoName, String distribution, String components, String mirrorsList, String defaultBaseUrl, Set<RepoTag> repoTags, Map<String, String> metadata) { setBaseUrl(baseUrl); setOsType(osType); setRepoId(repoId); setRepoName(repoName); setDistribution(distribution); setComponents(components); setMirrorsList(mirrorsList); setDefaultBaseUrl(defaultBaseUrl); setTags(repoTags); setMetadata(metadata); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void afterCall() { for (val sudoListener : sudoListeners) { sudoListener.afterCall(); } }', 'ground_truth': 'private void afterCall( final @NonNull ExecutionContext before, final @NonNull ExecutionContext after) { for (val sudoListener : sudoListeners) { sudoListener.afterCall(before, after); } }', 'output': 'private void afterCall() { for (val listener : listeners) { listener.afterCall(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static IndexSummary downsample(IndexSummary existing, int newSamplingLevel, IPartitioner partitioner) { int currentSamplingLevel = existing.getSamplingLevel(); assert currentSamplingLevel > newSamplingLevel; int[] startPoints = Downsampling.getStartPoints(currentSamplingLevel, newSamplingLevel); int removedKeyCount = 0; long newOffHeapSize = existing.getOffHeapSize(); for (int start : startPoints) { for (int j = start; j < existing.size(); j += currentSamplingLevel) { removedKeyCount++; newOffHeapSize -= existing.getEntry(j).length; } } int newKeyCount = existing.size() - removedKeyCount; Memory memory = Memory.allocate(newOffHeapSize - (removedKeyCount * 4)); int idxPosition = 0; int keyPosition = newKeyCount * 4; outer: for (int oldSummaryIndex = 0; oldSummaryIndex < existing.size(); oldSummaryIndex++) { for (int start : startPoints) { if ((oldSummaryIndex - start) % currentSamplingLevel == 0) continue outer; } memory.setInt(idxPosition, keyPosition); idxPosition += TypeSizes.NATIVE.sizeof(keyPosition); byte[] entry = existing.getEntry(oldSummaryIndex); memory.setBytes(keyPosition, entry, 0, entry.length); keyPosition += entry.length; } return new IndexSummary(partitioner, memory, newKeyCount, existing.getMaxNumberOfEntries(), existing.getIndexInterval(), newSamplingLevel); }', 'ground_truth': 'public static IndexSummary downsample(IndexSummary existing, int newSamplingLevel, int minIndexInterval, IPartitioner partitioner) { int currentSamplingLevel = existing.getSamplingLevel(); assert currentSamplingLevel > newSamplingLevel; assert minIndexInterval == existing.getMinIndexInterval(); int[] startPoints = Downsampling.getStartPoints(currentSamplingLevel, newSamplingLevel); int removedKeyCount = 0; long newOffHeapSize = existing.getOffHeapSize(); for (int start : startPoints) { for (int j = start; j < existing.size(); j += currentSamplingLevel) { removedKeyCount++; newOffHeapSize -= existing.getEntry(j).length; } } int newKeyCount = existing.size() - removedKeyCount; Memory memory = Memory.allocate(newOffHeapSize - (removedKeyCount * 4)); int idxPosition = 0; int keyPosition = newKeyCount * 4; outer: for (int oldSummaryIndex = 0; oldSummaryIndex < existing.size(); oldSummaryIndex++) { for (int start : startPoints) { if ((oldSummaryIndex - start) % currentSamplingLevel == 0) continue outer; } memory.setInt(idxPosition, keyPosition); idxPosition += TypeSizes.NATIVE.sizeof(keyPosition); byte[] entry = existing.getEntry(oldSummaryIndex); memory.setBytes(keyPosition, entry, 0, entry.length); keyPosition += entry.length; } return new IndexSummary(partitioner, memory, newKeyCount, existing.getMaxNumberOfEntries(), minIndexInterval, newSamplingLevel); }', 'output': 'public static IndexSummary downsample(IndexSummary existing, int newSamplingLevel, IPartitioner partitioner) { int currentSamplingLevel = existing.getSamplingLevel(); assert currentSamplingLevel > newSamplingLevel; int[] startPoints = Downsampling.getStartPoints(currentSamplingLevel, newSamplingLevel); int removedKeyCount = 0; long newOffHeapSize = existing.getOffHeapSize(); for (int start : startPoints) { for (int j = start; j < existing.size(); j += currentSamplingLevel) { removedKeyCount++; newOffHeapSize -= existing.getEntry(j).length; } } int newKeyCount = existing.size() - removedKeyCount; Memory memory = Memory.allocate(newOffHeapSize - (removedKeyCount * 4)); int idxPosition = 0; int keyPosition = newKeyCount * 4; outer: for (int oldSummaryIndex = 0; oldSummaryIndex < existing.size(); oldSummaryIndex++) { for (int start : startPoints) { if ((oldSummaryIndex - start) % currentSamplingLevel == 0) continue outer; } memory.setInt(idxPosition, keyPosition); idxPosition += TypeSizes.NATIVE.sizeof(keyPosition); byte[] entry = existing.getEntry(oldSummaryIndex); memory.setBytes(keyPosition, entry, 0, entry.length); keyPosition += entry.length; } return new IndexSummary(partitioner, memory, newKeyCount, existing.getMaxNumberOfEntries(), existing.getIndexInterval(), newSamplingLevel); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static List<org.apache.cassandra.db.Row> getSlice(CFMetaData metadata, SelectStatement select, List<ByteBuffer> variables) throws InvalidRequestException, ReadTimeoutException, UnavailableException, IsBootstrappingException, WriteTimeoutException { List<ReadCommand> commands = new ArrayList<ReadCommand>(); if (!select.isColumnRange()) { SortedSet<ByteBuffer> columnNames = getColumnNames(select, metadata, variables); validateColumnNames(columnNames); for (Term rawKey: select.getKeys()) { ByteBuffer key = rawKey.getByteBuffer(metadata.getKeyValidator(),variables); validateKey(key); commands.add(new SliceByNamesReadCommand(metadata.ksName, key, select.getColumnFamily(), new NamesQueryFilter(columnNames))); } } else { AbstractType<?> comparator = select.getComparator(metadata.ksName); ByteBuffer start = select.getColumnStart().getByteBuffer(comparator,variables); ByteBuffer finish = select.getColumnFinish().getByteBuffer(comparator,variables); for (Term rawKey : select.getKeys()) { ByteBuffer key = rawKey.getByteBuffer(metadata.getKeyValidator(),variables); validateKey(key); validateSliceFilter(metadata, start, finish, select.isColumnsReversed()); commands.add(new SliceFromReadCommand(metadata.ksName, key, select.getColumnFamily(), new SliceQueryFilter(start, finish, select.isColumnsReversed(), select.getColumnsLimit()))); } } return StorageProxy.read(commands, select.getConsistencyLevel()); }', 'ground_truth': 'private static List<org.apache.cassandra.db.Row> getSlice(CFMetaData metadata, SelectStatement select, List<ByteBuffer> variables, long now) throws InvalidRequestException, ReadTimeoutException, UnavailableException, IsBootstrappingException, WriteTimeoutException { List<ReadCommand> commands = new ArrayList<ReadCommand>(); if (!select.isColumnRange()) { SortedSet<ByteBuffer> columnNames = getColumnNames(select, metadata, variables); validateColumnNames(columnNames); for (Term rawKey: select.getKeys()) { ByteBuffer key = rawKey.getByteBuffer(metadata.getKeyValidator(),variables); validateKey(key); commands.add(new SliceByNamesReadCommand(metadata.ksName, key, select.getColumnFamily(), now, new NamesQueryFilter(columnNames))); } } else { AbstractType<?> comparator = select.getComparator(metadata.ksName); ByteBuffer start = select.getColumnStart().getByteBuffer(comparator,variables); ByteBuffer finish = select.getColumnFinish().getByteBuffer(comparator,variables); for (Term rawKey : select.getKeys()) { ByteBuffer key = rawKey.getByteBuffer(metadata.getKeyValidator(),variables); validateKey(key); validateSliceFilter(metadata, start, finish, select.isColumnsReversed()); commands.add(new SliceFromReadCommand(metadata.ksName, key, select.getColumnFamily(), now, new SliceQueryFilter(start, finish, select.isColumnsReversed(), select.getColumnsLimit()))); } } return StorageProxy.read(commands, select.getConsistencyLevel()); }', 'output': 'private static List<org.apache.cassandra.db.Row> getSlice(CFMetaData metadata, SelectStatement select, List<ByteBuffer> variables) throws InvalidRequestException, ReadTimeoutException, UnavailableException, IsBootstrappingException, WriteTimeoutException { List<ReadCommand> commands = new ArrayList<ReadCommand>(); if (!select.isColumnRange()) { SortedSet<ByteBuffer> columnNames = getColumnNames(select, metadata, variables); validateColumnNames(columnNames); for (Term rawKey: select.getKeys()) { ByteBuffer key = rawKey.getByteBuffer(metadata.getKeyValidator(),variables); validateKey(key); commands.add(new SliceByNamesReadCommand(metadata.ksName, key, select.getColumnFamily(), new NamesQueryFilter(columnNames))); } } else { AbstractType<?> comparator = select.getComparator(metadata.ksName); ByteBuffer start = select.getColumnStart().getByteBuffer(comparator,variables); ByteBuffer finish = select.getColumnFinish().getByteBuffer(comparator,variables); for (Term rawKey : select.getKeys()) { ByteBuffer key = rawKey.getByteBuffer(metadata.getKeyValidator(),variables); validateKey(key); validateSliceFilter(metadata, start, finish, select.isColumnsReversed()); commands.add(new SliceFromReadCommand(metadata.ksName, key, select.getColumnFamily(), new SliceQueryFilter(start, finish, select.isColumnsReversed(), select.getColumnsLimit()))); } } return StorageProxy.read(commands, select.getConsistencyLevel()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void send_insert(String tablename, String key, String columnFamily_column, byte[] cellData, long timestamp) throws TException { oprot_.writeMessageBegin(new TMessage("insert", TMessageType.CALL, seqid_)); insert_args args = new insert_args(); args.tablename = tablename; args.key = key; args.columnFamily_column = columnFamily_column; args.cellData = cellData; args.timestamp = timestamp; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); } public void send_insert_blocking(String tablename, String key, String columnFamily_column, byte[] cellData, long timestamp) throws TException\n    { { oprot_.writeMessageBegin(new TMessage("insert_blocking", TMessageType.CALL, seqid_)); insert_blocking_args args = new insert_blocking_args(); args.tablename = tablename; args.key = key; args.columnFamily_column = columnFamily_column; args.cellData = cellData; args.timestamp = timestamp; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'ground_truth': 'public void send_insert(String tablename, String key, String columnFamily_column, byte[] cellData, long timestamp, boolean block) throws TException { oprot_.writeMessageBegin(new TMessage("insert", TMessageType.CALL, seqid_)); insert_args args = new insert_args(); args.tablename = tablename; args.key = key; args.columnFamily_column = columnFamily_column; args.cellData = cellData; args.timestamp = timestamp; args.block = block; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'output': 'public void send_insert(String tablename, String key, String columnFamily_column, byte[] cellData, long timestamp, boolean blocking) throws TException { oprot_.writeMessageBegin(new TMessage("insert", TMessageType.CALL, seqid_)); insert_args args = new insert_args(); args.tablename = tablename; args.key = key; args.columnFamily_column = columnFamily_column; args.cellData = cellData; args.timestamp = timestamp; args.blocking = blocking; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ConfigurationRecommendationCommand(File recommendationsDir, String stackAdvisorScript, int requestId, StackAdvisorRunner saRunner, AmbariMetaInfo metaInfo) { super(recommendationsDir, stackAdvisorScript, requestId, saRunner, metaInfo); }', 'ground_truth': 'public ConfigurationRecommendationCommand(File recommendationsDir, String recommendationsArtifactsLifetime, String stackAdvisorScript, int requestId, StackAdvisorRunner saRunner, AmbariMetaInfo metaInfo) { super(recommendationsDir, recommendationsArtifactsLifetime, stackAdvisorScript, requestId, saRunner, metaInfo); }', 'output': 'public ConfigurationRecommendationCommand(File recommendationsDir, String stackAdvisorScript, int requestId, StackAdvisorRunner saRunner, AmbariMetaInfo metaInfo, RecommendationContext context) { super(recommendationsDir, stackAdvisorScript, requestId, saRunner, metaInfo, context); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean checkRangerUser(String rangerUserUrl, String username, String password, String userToSearch, PrerequisiteCheck check, PrereqCheckRequest request, List<String> warnReasons) { String url = String.format("%s?name=%s", rangerUserUrl, userToSearch); Map<String, List<String>> headers = getHeaders(username, password); try { HttpURLConnection conn = m_streamProvider.processURL(url, "GET", (InputStream) null, headers); int result = conn.getResponseCode(); if (result == 200) { Gson gson = new Gson(); Object o = gson.fromJson(new InputStreamReader(conn.getInputStream()), Object.class); Map<?, ?> map = (Map<?,?>) o; if (!map.containsKey("vXUsers")) { String reason = getFailReason(KEY_RANGER_USERS_ELEMENT_MISSING, check, request); warnReasons.add(String.format(reason, url)); return false; } @SuppressWarnings("unchecked") List<Map<?, ?>> list = (List<Map<?, ?>>) map.get("vXUsers"); for (Map<?, ?> listMap : list) { if (listMap.containsKey("name") && listMap.get("name").equals(userToSearch)) { return true; } } } } catch (IOException e) { LOG.warn("Could not determine user {}.  Error is {}", userToSearch, e.getMessage()); LOG.debug("Could not determine user {}.  Error is {}", userToSearch, e.getMessage(), e); String reason = getFailReason(KEY_RANGER_COULD_NOT_ACCESS, check, request); warnReasons.add(String.format(reason, username, url, e.getMessage())); } catch (Exception e) { LOG.warn("Could not determine user {}.  Error is {}", userToSearch, e.getMessage()); LOG.debug("Could not determine user {}.  Error is {}", userToSearch, e.getMessage(), e); String reason = getFailReason(KEY_RANGER_OTHER_ISSUE, check, request); warnReasons.add(String.format(reason, e.getMessage(), url)); } return false; }', 'ground_truth': 'private boolean checkRangerUser(URLStreamProvider streamProvider, String rangerUserUrl, String username, String password, String userToSearch, PrerequisiteCheck check, PrereqCheckRequest request, List<String> warnReasons) { String url = String.format("%s?name=%s", rangerUserUrl, userToSearch); Map<String, List<String>> headers = getHeaders(username, password); try { HttpURLConnection conn = streamProvider.processURL(url, "GET", (InputStream) null, headers); int result = conn.getResponseCode(); if (result == 200) { Gson gson = new Gson(); Object o = gson.fromJson(new InputStreamReader(conn.getInputStream()), Object.class); Map<?, ?> map = (Map<?,?>) o; if (!map.containsKey("vXUsers")) { String reason = getFailReason(KEY_RANGER_USERS_ELEMENT_MISSING, check, request); warnReasons.add(String.format(reason, url)); return false; } @SuppressWarnings("unchecked") List<Map<?, ?>> list = (List<Map<?, ?>>) map.get("vXUsers"); for (Map<?, ?> listMap : list) { if (listMap.containsKey("name") && listMap.get("name").equals(userToSearch)) { return true; } } } } catch (IOException e) { LOG.warn("Could not determine user {}.  Error is {}", userToSearch, e.getMessage()); LOG.debug("Could not determine user {}.  Error is {}", userToSearch, e.getMessage(), e); String reason = getFailReason(KEY_RANGER_COULD_NOT_ACCESS, check, request); warnReasons.add(String.format(reason, username, url, e.getMessage())); } catch (Exception e) { LOG.warn("Could not determine user {}.  Error is {}", userToSearch, e.getMessage()); LOG.debug("Could not determine user {}.  Error is {}", userToSearch, e.getMessage(), e); String reason = getFailReason(KEY_RANGER_OTHER_ISSUE, check, request); warnReasons.add(String.format(reason, e.getMessage(), url)); } return false; }', 'output': 'private boolean checkRangerUser(String rangerUserUrl, String username, String password, String userToSearch, PrerequisiteCheck check, PrereqCheckRequest request, List<String> warnReasons) { String url = String.format("%s?name=%s", rangerUserUrl, userToSearch); Map<String, List<String>> headers = getHeaders(username, password); try { HttpURLConnection conn = m_streamProvider.processURL(url, "GET", (InputStream) null, headers); int result = conn.getResponseCode(); if (result == 200) { Gson gson = new Gson(); Object o = gson.fromJson(new InputStreamReader(conn.getInputStream()), Object.class); Map<?, ?> map = (Map<?,?>) o; if (!map.containsKey("vXUsers")) { String reason = getFailReason(KEY_RANGER_USERS_ELEMENT_MISSING, check, request); warnReasons.add(String.format(reason, url)); return false; } @SuppressWarnings("unchecked") List<Map<?, ?>> list = (List<Map<?, ?>>) map.get("vXUsers"); for (Map<?, ?> listMap : list) { if (listMap.containsKey("name") && listMap.get("name").equals(userToSearch)) { return true; } } } } catch (IOException e) { LOG.warn("Could not determine user {}.  Error is {}", userToSearch, e.getMessage()); LOG.debug("Could not determine user {}.  Error is {}", userToSearch, e.getMessage(), e); String reason = getFailReason(KEY_RANGER_COULD_NOT_ACCESS, check, request); warnReasons.add(String.format(reason, username, url, e.getMessage())); } catch (Exception e) { LOG.warn("Could not determine user {}.  Error is {}", userToSearch, e.getMessage()); LOG.debug("Could not determine user {}.  Error is {}", userToSearch, e.getMessage(), e); String reason = getFailReason(KEY_RANGER_OTHER_ISSUE, check, request); warnReasons.add(String.format(reason, e.getMessage'}
{'instruction': 'Please refactor the following code:', 'input': 'public void prepareKeyspace(ClientState state) throws InvalidRequestException { for (ModificationStatement.Parsed statement : parsedStatements) statement.prepareKeyspace(state); } public ParsedStatement.Prepared prepare() throws InvalidRequestException\n        { { VariableSpecifications boundNames = getBoundVariables(); String firstKS = null; String firstCF = null; boolean haveMultipleCFs = false; List<ModificationStatement> statements = new ArrayList<>(parsedStatements.size()); for (ModificationStatement.Parsed parsed : parsedStatements) { if (firstKS == null) { firstKS = parsed.keyspace(); firstCF = parsed.columnFamily(); } else if (!haveMultipleCFs) { haveMultipleCFs = !firstKS.equals(parsed.keyspace()) || !firstCF.equals(parsed.columnFamily()); } statements.add(parsed.prepare(boundNames)); } Attributes prepAttrs = attrs.prepare("[batch]", "[batch]"); prepAttrs.collectMarkerSpecification(boundNames); BatchStatement batchStatement = new BatchStatement(boundNames.size(), type, statements, prepAttrs); batchStatement.validate(); Short[] partitionKeyBindIndexes = (haveMultipleCFs || batchStatement.statements.isEmpty())? null : boundNames.getPartitionKeyBindIndexes(batchStatement.statements.get(0).cfm); return new ParsedStatement.Prepared(batchStatement, boundNames, partitionKeyBindIndexes); }', 'ground_truth': 'public ParsedStatement.Prepared prepare(ClientState clientState) throws InvalidRequestException { VariableSpecifications boundNames = getBoundVariables(); String firstKS = null; String firstCF = null; boolean haveMultipleCFs = false; List<ModificationStatement> statements = new ArrayList<>(parsedStatements.size()); for (ModificationStatement.Parsed parsed : parsedStatements) { if (firstKS == null) { firstKS = parsed.keyspace(); firstCF = parsed.columnFamily(); } else if (!haveMultipleCFs) { haveMultipleCFs = !firstKS.equals(parsed.keyspace()) || !firstCF.equals(parsed.columnFamily()); } statements.add(parsed.prepare(boundNames, clientState)); } Attributes prepAttrs = attrs.prepare("[batch]", "[batch]"); prepAttrs.collectMarkerSpecification(boundNames); BatchStatement batchStatement = new BatchStatement(boundNames.size(), type, statements, prepAttrs); batchStatement.validate(); Short[] partitionKeyBindIndexes = (haveMultipleCFs || batchStatement.statements.isEmpty())? null : boundNames.getPartitionKeyBindIndexes(batchStatement.statements.get(0).cfm); return new ParsedStatement.Prepared(batchStatement, boundNames, partitionKeyBindIndexes); }', 'output': 'public void prepareKeyspace(ClientState state) throws InvalidRequestException { for (ModificationStatement.Parsed statement : preparedStatements) statement.prepareKeyspace(state); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public final IgniteUuid getGridUuid() { byte[] arr = getByteArray(); if (arr == BYTE_ARR_NOT_READ) return GRID_UUID_NOT_READ; else if (arr == null) return null; else { long most = UNSAFE.getLong(arr, BYTE_ARR_OFF); long least = UNSAFE.getLong(arr, BYTE_ARR_OFF + 8); long loc = UNSAFE.getLong(arr, BYTE_ARR_OFF + 16); return new IgniteUuid(new UUID(most, least), loc); } }', 'ground_truth': 'public final IgniteUuid getGridUuid(String name) { byte[] arr = getByteArray(name); if (arr == BYTE_ARR_NOT_READ) return GRID_UUID_NOT_READ; else if (arr == null) return null; else { long most = UNSAFE.getLong(arr, BYTE_ARR_OFF); long least = UNSAFE.getLong(arr, BYTE_ARR_OFF + 8); long loc = UNSAFE.getLong(arr, BYTE_ARR_OFF + 16); return new IgniteUuid(new UUID(most, least), loc); } }', 'output': 'public final GridUuid getGridUuid() { byte[] arr = getByteArray(); if (arr == BYTE_ARR_NOT_READ) return GRID_UUID_NOT_READ; else if (arr == null) return null; else { long most = UNSAFE.getLong(arr, BYTE_ARR_OFF); long least = UNSAFE.getLong(arr, BYTE_ARR_OFF + 8); long loc = UNSAFE.getLong(arr, BYTE_ARR_OFF + 16); return new GridUuid(new UUID(most, least), loc); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public BSRunner(BootStrapImpl impl, SshHostInfo sshHostInfo, String bootDir, String bsScript, String agentSetupScript, String agentSetupPassword, int requestId, long timeout, String hostName, boolean isVerbose, String clusterOsType, String projectVersion) { this.requestId = requestId; this.sshHostInfo = sshHostInfo; this.bsScript = bsScript; this.bootDir = new File(bootDir); this.requestIdDir = new File(bootDir, Integer.toString(requestId)); this.sshKeyFile = new File(this.requestIdDir, "sshKey"); this.agentSetupScript = agentSetupScript; this.agentSetupPassword = agentSetupPassword; this.ambariHostname = hostName; this.verbose = isVerbose; this.clusterOsType = clusterOsType; this.projectVersion = projectVersion; this.bsImpl = impl; BootStrapStatus status = new BootStrapStatus(); status.setLog("RUNNING"); status.setStatus(BSStat.RUNNING); bsImpl.updateStatus(requestId, status); }', 'ground_truth': 'public BSRunner(BootStrapImpl impl, SshHostInfo sshHostInfo, String bootDir, String bsScript, String agentSetupScript, String agentSetupPassword, int requestId, long timeout, String hostName, boolean isVerbose, String clusterOsType, String projectVersion, int serverPort) { this.requestId = requestId; this.sshHostInfo = sshHostInfo; this.bsScript = bsScript; this.bootDir = new File(bootDir); this.requestIdDir = new File(bootDir, Integer.toString(requestId)); this.sshKeyFile = new File(this.requestIdDir, "sshKey"); this.agentSetupScript = agentSetupScript; this.agentSetupPassword = agentSetupPassword; this.ambariHostname = hostName; this.verbose = isVerbose; this.clusterOsType = clusterOsType; this.projectVersion = projectVersion; this.bsImpl = impl; this.serverPort = serverPort; BootStrapStatus status = new BootStrapStatus(); status.setLog("RUNNING"); status.setStatus(BSStat.RUNNING); bsImpl.updateStatus(requestId, status); }', 'output': 'public BSRunner(BootstrapImpl impl, SshHostInfo sshHostInfo, String bootDir, String bsScript, String agentSetupScript, String agentSetupPassword, int requestId, long timeout, String hostName, boolean isVerbose, String clusterOsType, String projectVersion) { this.requestId = requestId; this.sshHostInfo = sshHostInfo; this.bsScript = bsScript; this.bootDir = new File(bootDir); this.requestIdDir = new File(bootDir, Integer.toString(requestId)); this.sshKeyFile = new File(this.requestIdDir, "sshKey"); this.agentSetupScript = agentSetupScript; this.agentSetupPassword = agentSetupPassword; this.ambariHostname = hostName; this.verbose = isVerbose; this.clusterOsType = clusterOsType; this.projectVersion = projectVersion; this.bsImpl = impl; BootstrapStatus status = new BootstrapStatus(); status.setLog("RUNNING"); status.setStatus(BSStat.RUNNING); impl.updateStatus(requestId, status); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ViewRegistry getRegistry(ViewDAO viewDAO, ViewInstanceDAO viewInstanceDAO, UserDAO userDAO, MemberDAO memberDAO, PrivilegeDAO privilegeDAO, ResourceDAO resourceDAO, ResourceTypeDAO resourceTypeDAO, SecurityHelper securityHelper, ViewInstanceHandlerList handlerList, ViewExtractor viewExtractor, ViewArchiveUtility archiveUtility) { ViewRegistry instance = new ViewRegistry(); instance.viewDAO = viewDAO; instance.resourceDAO = resourceDAO; instance.instanceDAO = viewInstanceDAO; instance.userDAO = userDAO; instance.memberDAO = memberDAO; instance.privilegeDAO = privilegeDAO; instance.resourceTypeDAO = resourceTypeDAO; instance.securityHelper = securityHelper; instance.configuration = configuration; instance.handlerList = handlerList; instance.extractor = viewExtractor == null ? new ViewExtractor() : viewExtractor; instance.archiveUtility = archiveUtility == null ? new ViewArchiveUtility() : archiveUtility; instance.extractor.archiveUtility = instance.archiveUtility; return instance; }', 'ground_truth': 'public static ViewRegistry getRegistry(ViewDAO viewDAO, ViewInstanceDAO viewInstanceDAO, UserDAO userDAO, MemberDAO memberDAO, PrivilegeDAO privilegeDAO, ResourceDAO resourceDAO, ResourceTypeDAO resourceTypeDAO, SecurityHelper securityHelper, ViewInstanceHandlerList handlerList, ViewExtractor viewExtractor, ViewArchiveUtility archiveUtility, AmbariMetaInfo ambariMetaInfo) { ViewRegistry instance = new ViewRegistry(); instance.viewDAO = viewDAO; instance.resourceDAO = resourceDAO; instance.instanceDAO = viewInstanceDAO; instance.userDAO = userDAO; instance.memberDAO = memberDAO; instance.privilegeDAO = privilegeDAO; instance.resourceTypeDAO = resourceTypeDAO; instance.securityHelper = securityHelper; instance.configuration = configuration; instance.handlerList = handlerList; instance.extractor = viewExtractor == null ? new ViewExtractor() : viewExtractor; instance.archiveUtility = archiveUtility == null ? new ViewArchiveUtility() : archiveUtility; instance.extractor.archiveUtility = instance.archiveUtility; final AmbariMetaInfo finalMetaInfo = ambariMetaInfo; instance.ambariMetaInfo = new Provider<AmbariMetaInfo>() { @Override public AmbariMetaInfo get() { return finalMetaInfo; } }; return instance; }', 'output': 'public static ViewRegistry getRegistry(final ViewDAO viewDAO, final ViewInstanceDAO viewInstanceDAO, final UserDAO userDAO, final MemberDAO memberDAO, final PrivilegeDAO privilegeDAO, final ResourceDAO resourceDAO, final ResourceTypeDAO resourceTypeDAO, final SecurityHelper securityHelper, final ViewInstanceHandlerList handlerList, final ViewExtractor viewExtractor, final ViewArchiveUtility archiveUtility) { ViewRegistry instance = new ViewRegistry(); instance.viewDAO = viewDAO; instance.resourceDAO = resourceDAO; instance.instanceDAO = viewInstanceDAO; instance.userDAO = userDAO; instance.memberDAO = memberDAO; instance.privilegeDAO = privilegeDAO; instance.resourceTypeDAO = resourceTypeDAO; instance.securityHelper = securityHelper; instance.configuration = configuration; instance.handlerList = handlerList; instance.extractor = viewExtractor == null ? new ViewExtractor() : viewExtractor; instance.archiveUtility = archiveUtility == null ? new ViewArchiveUtility() : archiveUtility; instance.extractor.archiveUtility = instance.archiveUtility; return instance; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void updateData(Distribution distribution, Integer featureVersion, Latest latest, PackageType bundleType) { if (distribution == null) return; if (featureVersion == null) return; OperatingSystem operatingSystem = getOperatingSystem(); XX XXX = Architecture.NONE; ArchiveType extension = ArchiveType.NONE; Boolean fx = false; this.setEnabled(false); submit(() -> { List<Pkg> bundles = discoClient.getPkgs(distribution, new VersionNumber(featureVersion), latest, operatingSystem, architecture, extension, bundleType, fx); return bundles; }).then(this::setPackages) .handle(Exceptions::printStackTrace) .execute(); }', 'ground_truth': 'protected void updateData(Distribution distribution, Integer featureVersion, Architecture architecture, Latest latest, PackageType bundleType) { if (distribution == null) return; if (featureVersion == null) return; OperatingSystem operatingSystem = getOperatingSystem(); ArchiveType extension = ArchiveType.NONE; Boolean fx = false; this.setEnabled(false); submit(() -> { List<Pkg> bundles = discoClient.getPkgs(distribution, new VersionNumber(featureVersion), latest, operatingSystem, architecture, extension, bundleType, fx); return bundles; }).then(this::setPackages) .handle(Exceptions::printStackTrace) .execute(); }', 'output': 'protected void updateData(Distribution distribution, Integer featureVersion, Latest latest, PackageType bundleType) { if (distribution == null) return; if (featureVersion == null) return; OperatingSystem operatingSystem = getOperatingSystem(); Architecture architecture = Architecture.NONE; ArchiveType extension = ArchiveType.NONE; Boolean fx = false; this.setEnabled(false); submit(() -> { List<Pkg> bundles = discoClient.getPkgs(distribution, new VersionNumber(featureVersion), latest, operatingSystem, architecture, extension, bundleType, fx); return bundles; }).then(this::setPackages) .handle(Exceptions::printStackTrace) .execute(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Response updateService(@Context HttpHeaders headers, @Context UriInfo ui, @PathParam("serviceName") String serviceName) { return handleRequest(headers, ui, Request.Type.POST, createResourceDefinition(serviceName, m_clusterName)); }', 'ground_truth': 'public Response updateService(String body, @Context HttpHeaders headers, @Context UriInfo ui, @PathParam("serviceName") String serviceName) { return handleRequest(headers, body, ui, Request.Type.POST, createResourceDefinition(serviceName, m_clusterName)); }', 'output': 'public Response updateService(@Context HttpHeaders headers, @Context UriInfo ui, @PathParam("serviceName") String serviceName, @QueryParam("description") String description) { return handleRequest(headers, ui, Request.Type.POST, createResourceDefinition(serviceName, m_clusterName, description)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ByteBuffer getSliceFromSerialized(ByteBuffer collection, ByteBuffer from, ByteBuffer to, AbstractType<?> comparator) { if (from == ByteBufferUtil.UNSET_BYTE_BUFFER && to == ByteBufferUtil.UNSET_BYTE_BUFFER) return collection; try { ByteBuffer input = collection.duplicate(); int n = readCollectionSize(input, ProtocolVersion.V3); int startPos = input.position(); int count = 0; boolean inSlice = from == ByteBufferUtil.UNSET_BYTE_BUFFER; for (int i = 0; i < n; i++) { int pos = input.position(); ByteBuffer kbb = readValue(input, ProtocolVersion.V3); if (!inSlice) { int comparison = comparator.compareForCQL(from, kbb); if (comparison <= 0) { inSlice = true; startPos = pos; } else { skipValue(input, ProtocolVersion.V3); continue; } } int comparison = to == ByteBufferUtil.UNSET_BYTE_BUFFER ? -1 : comparator.compareForCQL(kbb, to); if (comparison > 0) { input.position(pos); break; } skipValue(input, ProtocolVersion.V3); ++count; if (comparison == 0) break; } return copyAsNewCollection(collection, count, startPos, input.position(), ProtocolVersion.V3); } catch (BufferUnderflowException e) { throw new MarshalException("Not enough bytes to read a map"); } }', 'ground_truth': 'public ByteBuffer getSliceFromSerialized(ByteBuffer collection, ByteBuffer from, ByteBuffer to, AbstractType<?> comparator, boolean frozen) { if (from == ByteBufferUtil.UNSET_BYTE_BUFFER && to == ByteBufferUtil.UNSET_BYTE_BUFFER) return collection; try { ByteBuffer input = collection.duplicate(); int n = readCollectionSize(input, ProtocolVersion.V3); int startPos = input.position(); int count = 0; boolean inSlice = from == ByteBufferUtil.UNSET_BYTE_BUFFER; for (int i = 0; i < n; i++) { int pos = input.position(); ByteBuffer kbb = readValue(input, ProtocolVersion.V3); if (!inSlice) { int comparison = comparator.compareForCQL(from, kbb); if (comparison <= 0) { inSlice = true; startPos = pos; } else { skipValue(input, ProtocolVersion.V3); continue; } } int comparison = to == ByteBufferUtil.UNSET_BYTE_BUFFER ? -1 : comparator.compareForCQL(kbb, to); if (comparison > 0) { input.position(pos); break; } skipValue(input, ProtocolVersion.V3); ++count; if (comparison == 0) break; } if (count == 0 && !frozen) return null; return copyAsNewCollection(collection, count, startPos, input.position(), ProtocolVersion.V3); } catch (BufferUnderflowException e) { throw new MarshalException("Not enough bytes to read a map"); } }', 'output': 'public ByteBuffer getSliceFromSerialized(ByteBuffer collection, ByteBuffer from, ByteBuffer to, AbstractType<?> comparator) { if (from == ByteBufferUtil.UNSET_BYTE_BUFFER && to == ByteBufferUtil.UNSET_BYTE_BUFFER) return collection; try { ByteBuffer input = collection.duplicate(); int n = readCollectionSize(input, ProtocolVersion.V3); int startPos = input.position(); int count = 0; boolean inSlice = from == ByteBufferUtil.UNSET_BYTE_BUFFER; for (int i = 0; i < n; i++) { int pos = input.position(); ByteBuffer kbb = readValue(input, ProtocolVersion.V3); if (!inSlice) { int comparison = comparator.compareForCQL(from, kbb); if (comparison <= 0) { inSlice = true; startPos = pos; } else { skipValue(input, ProtocolVersion.V3); continue; } } int comparison = to == ByteBufferUtil.UNSET_BYTE_BUFFER ? -1 : comparator.compareForCQL(kbb, to); if (comparison > 0) { input.position(pos); break; } skipValue(input, ProtocolVersion.V3); ++count; if (comparison == 0) break; } return copyAsNewCollection(collection, count, startPos, input.position(), ProtocolVersion.V3); } catch (BufferUnderflowException e) { throw new MarshalException("Not enough bytes to read a map"); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static String translate(TranslationService translationService, String msg) { if(translationService!=null) { return translationService.translate(MessageRegistry.class.getName(), msg); } return msg; }', 'ground_truth': 'private static String translate(TranslationService translationService, TranslationContext context, String msg) { if(translationService!=null) { return translationService.translate(context, msg); } return msg; }', 'output': 'private static String translate(TranslationService translationService, String key) { if(translationService!=null) { return translationService.translate(key); } return key; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected H2Tree( String name, ReuseList reuseList, int grpId, PageMemory pageMem, IgniteWriteAheadLogManager wal, AtomicLong globalRmvId, H2RowFactory rowStore, long metaPageId, boolean initNew, IndexColumn[] cols, List<InlineIndexHelper> inlineIdxs, int inlineSize ) throws IgniteCheckedException { super(name, grpId, pageMem, wal, globalRmvId, metaPageId, reuseList); if (!initNew) { inlineSize = getMetaInlineSize(); } this.inlineSize = inlineSize; assert rowStore != null; this.rowStore = rowStore; this.inlineIdxs = inlineIdxs; this.cols = cols; this.columnIds = new int[cols.length]; for (int i = 0; i < cols.length; i++) columnIds[i] = cols[i].column.getColumnId(); setIos(H2ExtrasInnerIO.getVersions(inlineSize), H2ExtrasLeafIO.getVersions(inlineSize)); initTree(initNew, inlineSize); }', 'ground_truth': 'protected H2Tree( String name, ReuseList reuseList, int grpId, PageMemory pageMem, IgniteWriteAheadLogManager wal, AtomicLong globalRmvId, H2RowFactory rowStore, long metaPageId, boolean initNew, IndexColumn[] cols, List<InlineIndexHelper> inlineIdxs, int inlineSize, @Nullable H2RowCache rowCache ) throws IgniteCheckedException { super(name, grpId, pageMem, wal, globalRmvId, metaPageId, reuseList); if (!initNew) { inlineSize = getMetaInlineSize(); } this.inlineSize = inlineSize; assert rowStore != null; this.rowStore = rowStore; this.inlineIdxs = inlineIdxs; this.cols = cols; this.columnIds = new int[cols.length]; for (int i = 0; i < cols.length; i++) columnIds[i] = cols[i].column.getColumnId(); this.rowCache = rowCache; setIos(H2ExtrasInnerIO.getVersions(inlineSize), H2ExtrasLeafIO.getVersions(inlineSize)); initTree(initNew, inlineSize); }', 'output': 'protected H2Tree( String name, ReuseList reuseList, int grpId, PageMemory pageMem, IgniteWriteAheadLogManager wal, AtomicLong globalRmvId, H2RowFactory rowStore, long metaPageId, boolean initNew, IndexColumn[] cols, List<InlineIndexHelper> inlineIdxs, int inlineSize ) throws IgniteCheckedException { super(name, grpId, pageMem, wal, globalRmvId, metaPageId, reuseList); if (!initNew) { inlineSize = getMetaInlineSize(); } this.inlineSize = inlineSize; assert rowStore != null; this.rowStore = rowStore; this.inlineIdxs = inlineIdxs; this.cols = cols; this.columnIds = new int[cols.length]; for (int i = 0; i < cols.length; i++) columnIds[i] = cols[i].column.getColumnId(); setIos(H2ExtrasInnerIO.getVersions(inlineSize), H2ExtrasLeafIO.getVersions(inlineSize)); initTree(initNew, inlineSize); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected CollectionInteraction startCollectionInteractionOn(Class<?> type, String collectionId) { val viewModel = factoryService.viewModel(type); val managedObject = objectManager.adapt(viewModel); return CollectionInteraction.start(managedObject, collectionId); }', 'ground_truth': 'protected CollectionInteraction startCollectionInteractionOn(Class<?> type, String collectionId, Where where) { val viewModel = factoryService.viewModel(type); val managedObject = objectManager.adapt(viewModel); return CollectionInteraction.start(managedObject, collectionId, where); }', 'output': 'protected <T> CollectionInteraction<T> startCollectionInteractionOn(Class<T> type, String collectionId) { val viewModel = factoryService.viewModel(type); val managedObject = objectManager.adapt(viewModel); return CollectionInteraction.start(managedObject, collectionId); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void registerExecutorMBeans(ExecutorService execSvc, ExecutorService sysExecSvc, ExecutorService p2pExecSvc, ExecutorService mgmtExecSvc, ExecutorService restExecSvc) throws IgniteCheckedException { pubExecSvcMBean = registerExecutorMBean(execSvc, "GridExecutionExecutor"); sysExecSvcMBean = registerExecutorMBean(sysExecSvc, "GridSystemExecutor"); mgmtExecSvcMBean = registerExecutorMBean(mgmtExecSvc, "GridManagementExecutor"); p2PExecSvcMBean = registerExecutorMBean(p2pExecSvc, "GridClassLoadingExecutor"); ConnectorConfiguration clientCfg = cfg.getConnectorConfiguration(); if (clientCfg != null) restExecSvcMBean = registerExecutorMBean(restExecSvc, "GridRestExecutor"); }', 'ground_truth': 'private void registerExecutorMBeans(ExecutorService execSvc, ExecutorService sysExecSvc, ExecutorService p2pExecSvc, ExecutorService mgmtExecSvc, ExecutorService restExecSvc, ExecutorService qryExecSvc) throws IgniteCheckedException { pubExecSvcMBean = registerExecutorMBean(execSvc, "GridExecutionExecutor"); sysExecSvcMBean = registerExecutorMBean(sysExecSvc, "GridSystemExecutor"); mgmtExecSvcMBean = registerExecutorMBean(mgmtExecSvc, "GridManagementExecutor"); p2PExecSvcMBean = registerExecutorMBean(p2pExecSvc, "GridClassLoadingExecutor"); qryExecSvcMBean = registerExecutorMBean(qryExecSvc, "GridQueryExecutor"); ConnectorConfiguration clientCfg = cfg.getConnectorConfiguration(); if (clientCfg != null) restExecSvcMBean = registerExecutorMBean(restExecSvc, "GridRestExecutor"); }', 'output': 'private void registerExecutorMBeans(ExecutorService execSvc, ExecutorService sysExecSvc, ExecutorService p2pExecSvc, ExecutorService mgmtExecSvc, ExecutorService restExecSvc, IgniteConfiguration cfg) throws IgniteCheckedException { pubExecSvcMBean = registerExecutorMBean(execSvc, "GridExecutionExecutor"); sysExecSvcMBean = registerExecutorMBean(sysExecSvc, "GridSystemExecutor"); mgmtExecSvcMBean = registerExecutorMBean(mgmtExecSvc, "GridManagementExecutor"); p2PExecSvcMBean = registerExecutorMBean(p2pExecSvc, "GridClassLoadingExecutor"); ConnectorConfiguration clientCfg = cfg.getConnectorConfiguration(); if (clientCfg != null) restExecSvcMBean = registerExecutorMBean(restExecSvc, "GridRestExecutor"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Iterable<Object> lifecycleAwares(CacheConfiguration ccfg, Object... objs) { Collection<Object> ret = new ArrayList<>(7 + objs.length); ret.add(ccfg.getAffinity()); ret.add(ccfg.getAffinityMapper()); ret.add(ccfg.getEvictionFilter()); ret.add(ccfg.getEvictionPolicy()); ret.add(ccfg.getInterceptor()); ret.add(ccfg.getTopologyValidator()); NearCacheConfiguration nearCfg = ccfg.getNearConfiguration(); if (nearCfg != null) ret.add(nearCfg.getNearEvictionPolicy()); Collections.addAll(ret, objs); return ret; }', 'ground_truth': 'private Iterable<Object> lifecycleAwares(CacheGroupContext grp, CacheConfiguration ccfg, Object... objs) { Collection<Object> ret = new ArrayList<>(7 + objs.length); if (grp.affinityFunction() != ccfg.getAffinity()) ret.add(ccfg.getAffinity()); ret.add(ccfg.getAffinityMapper()); ret.add(ccfg.getEvictionFilter()); ret.add(ccfg.getEvictionPolicy()); ret.add(ccfg.getInterceptor()); NearCacheConfiguration nearCfg = ccfg.getNearConfiguration(); if (nearCfg != null) ret.add(nearCfg.getNearEvictionPolicy()); Collections.addAll(ret, objs); return ret; }', 'output': 'private Iterable<Object> lifecycleAwares(CacheConfiguration ccfg, Object... objs) { List<Object> ret = new ArrayList<>(7 + objs.length); ret.add(ccfg.getAffinity()); ret.add(ccfg.getAffinityMapper()); ret.add(ccfg.getEvictionFilter()); ret.add(ccfg.getEvictionPolicy()); ret.add(ccfg.getInterceptor()); ret.add(ccfg.getTopologyValidator()); NearCacheConfiguration nearCfg = ccfg.getNearConfiguration(); if (nearCfg != null) ret.add(nearCfg.getNearEvictionPolicy()); Collections.addAll(ret, objs); return ret; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void onMessage(UUID nodeId, Object msg) { GridCheckpointRequest req = (GridCheckpointRequest)msg; if (log.isDebugEnabled()) log.debug("Received checkpoint request: " + req); if (!enabled()) return; IgniteUuid sesId = req.getSessionId(); if (closedSess.contains(sesId)) { getSpi(req.getCheckpointSpi()).removeCheckpoint(req.getKey()); return; } Set<String> keys = keyMap.get(sesId); if (keys == null) { GridTaskSessionImpl ses = ctx.session().getSession(sesId); if (ses == null) { getSpi(req.getCheckpointSpi()).removeCheckpoint(req.getKey()); return; } Set<String> old = keyMap.putIfAbsent(sesId, (CheckpointSet)(keys = new CheckpointSet(ses))); if (old != null) keys = old; } keys.add(req.getKey()); if (closedSess.contains(sesId)) { keyMap.remove(sesId, keys); getSpi(req.getCheckpointSpi()).removeCheckpoint(req.getKey()); } } } } }', 'ground_truth': '@Override public void onMessage(UUID nodeId, Object msg, byte plc) { GridCheckpointRequest req = (GridCheckpointRequest)msg; if (log.isDebugEnabled()) log.debug("Received checkpoint request: " + req); if (!enabled()) return; IgniteUuid sesId = req.getSessionId(); if (closedSess.contains(sesId)) { getSpi(req.getCheckpointSpi()).removeCheckpoint(req.getKey()); return; } Set<String> keys = keyMap.get(sesId); if (keys == null) { GridTaskSessionImpl ses = ctx.session().getSession(sesId); if (ses == null) { getSpi(req.getCheckpointSpi()).removeCheckpoint(req.getKey()); return; } Set<String> old = keyMap.putIfAbsent(sesId, (CheckpointSet)(keys = new CheckpointSet(ses))); if (old != null) keys = old; } keys.add(req.getKey()); if (closedSess.contains(sesId)) { keyMap.remove(sesId, keys); getSpi(req.getCheckpointSpi()).removeCheckpoint(req.getKey()); } }', 'output': '@Override public void onMessage(UUID nodeId, Object msg) { CheckpointRequest req = (CheckpointRequest)msg; if (log.isDebugEnabled()) log.debug("Received checkpoint request: " + req); if (!enabled()) return; IgniteUuid sesId = req.getSessionId(); if (closedSess.contains(sesId)) { getSpi(req.getCheckpointSpi()).removeCheckpoint(req.getKey()); return; } Set<String> keys = keyMap.get(sesId); if (keys == null) { GridTaskSessionImpl ses = ctx.session().getSession(sesId); if (ses == null) { getSpi(req.getCheckpointSpi()).removeCheckpoint(req.getKey()); return; } Set<String> old = keyMap.putIfAbsent(sesId, (CheckpointSet)(keys = new CheckpointSet(ses))); if (old != null) keys = old; } keys.add(req.getKey()); if (closedSess.contains(sesId)) { keyMap.remove(sesId, keys); getSpi(req.getCheckpointSpi()).removeCheckpoint(req.getKey()); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Response getStackService(@Context HttpHeaders headers, @Context UriInfo ui, @PathParam("stackName") String stackName, @PathParam("stackVersion") String stackVersion, @PathParam("serviceName") String serviceName) { return handleRequest(headers, null, new StackUriInfo(ui), Request.Type.GET, createStackServiceResource(stackName, stackVersion, serviceName)); }', 'ground_truth': 'public Response getStackService(String body, @Context HttpHeaders headers, @Context UriInfo ui, @PathParam("stackName") String stackName, @PathParam("stackVersion") String stackVersion, @PathParam("serviceName") String serviceName) { return handleRequest(headers, body, new StackUriInfo(ui), Request.Type.GET, createStackServiceResource(stackName, stackVersion, serviceName)); }', 'output': 'public Response getStackService(@Context HttpHeaders headers, @Context UriInfo ui, @PathParam("stackName") String stackName, @PathParam("stackVersion") String stackVersion, @PathParam("serviceName") String serviceName, @QueryParam("format") String format) { return handleRequest(headers, null, new StackUriInfo(ui), Request.Type.GET, createStackServiceResource(stackName, stackVersion, serviceName, format)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public FileObject getFileObject(String relativePath) { if (relativePath.equals(".")) { return this; } if(relativePath.indexOf(\'\\\\\') != -1) { return null; } if (relativePath.startsWith("/")) { relativePath = relativePath.substring(1); } File file = new File(getFileName().getFile(), relativePath); if (relativePath.contains("..") || relativePath.contains("./") || relativePath.contains("/.") ) { file = FileUtil.normalizeFile(file); } FileObjectFactory factory = getFactory(); assert factory != null : "No factory for " + getPath() + " this: " + this; return factory.getValidFileObject(file, FileObjectFactory.Caller.GetFileObject); }', 'ground_truth': 'public FileObject getFileObject(String relativePath, boolean onlyExisting) { if (relativePath.equals(".")) { return this; } if(relativePath.indexOf(\'\\\\\') != -1) { return null; } if (relativePath.startsWith("/")) { relativePath = relativePath.substring(1); } File file = new File(getFileName().getFile(), relativePath); if (relativePath.contains("..") || relativePath.contains("./") || relativePath.contains("/.") ) { file = FileUtil.normalizeFile(file); } FileObjectFactory factory = getFactory(); assert factory != null : "No factory for " + getPath() + " this: " + this; return factory.getValidFileObject(file, FileObjectFactory.Caller.GetFileObject, onlyExisting); }', 'output': 'public FileObject getFileObject(String relativePath) { if (relativePath.equals(".")) { return this; } if(relativePath.indexOf(\'\\\\\') != -1) { return null; } if (relativePath.startsWith("/")) { relativePath = relativePath.substring(1); } File file = new File(getFileName().getFile(), relativePath); if (relativePath.contains("..") || relativePath.contains("./") || relativePath.contains("/.") ) { file = FileUtil.normalizeFile(file); } FileObjectFactory factory = getFactory(); assert factory != null : "No factory for " + getPath() + " this: " + this; return factory.getValidFileObject(file, FileObjectFactory.Caller.GetFileObject); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void doVerb(Message msg) { StorageService.instance.confirmReplication(msg.getFrom()); Message response = msg.getInternalReply(ArrayUtils.EMPTY_BYTE_ARRAY); if (logger.isDebugEnabled()) logger.debug("Replying to " + msg.getMessageId() + "@" + msg.getFrom()); MessagingService.instance().sendOneWay(response, msg.getFrom()); }', 'ground_truth': 'public void doVerb(Message msg, String id) { StorageService.instance.confirmReplication(msg.getFrom()); Message response = msg.getInternalReply(ArrayUtils.EMPTY_BYTE_ARRAY); if (logger.isDebugEnabled()) logger.debug("Replying to " + id + "@" + msg.getFrom()); MessagingService.instance().sendReply(response, id, msg.getFrom()); }', 'output': 'public void doVerb(Message msg) { StorageService.instance.confirmReplication(msg.getFrom(), false); Message response = msg.getInternalReply(ArrayUtils.EMPTY_BYTE_ARRAY); if (logger.isDebugEnabled()) logger.debug("Replying to " + msg.getMessageId() + "@" + msg.getFrom()); MessagingService.instance().sendOneWay(response, msg.getFrom()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void bootstrap(ByteBuffer buf, int startIdx) throws IgniteCheckedException { assert buf != null; row = -1; this.buf = buf; fillFromBuffer(startIdx); }', 'ground_truth': 'private void bootstrap(ByteBuffer buf, BPlusIO<L> io,  int startIdx) throws IgniteCheckedException { assert buf != null; assert io != null; assert rows == null; rows = new ArrayList<>(); row = -1; fillFromBuffer(buf, io, startIdx); }', 'output': 'public void bootstrap(ByteBuffer buf, int startIdx) throws IgniteCheckedException { assert buf != null; row = -1; this.buf = buf; fillFromBuffer(startIdx); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ListenableFuture<?> submitAntiCompaction(final ColumnFamilyStore cfs, final Collection<Range<Token>> ranges, final Refs<SSTableReader> sstables, final long repairedAt, final UUID parentRepairSession) { Runnable runnable = new WrappedRunnable() { @Override @SuppressWarnings("resource") public void runMayThrow() throws Exception { LifecycleTransaction modifier = null; while (modifier == null) { for (SSTableReader compactingSSTable : cfs.getTracker().getCompacting()) sstables.releaseIfHolds(compactingSSTable); Set<SSTableReader> compactedSSTables = new HashSet<>(); for (SSTableReader sstable : sstables) if (sstable.isMarkedCompacted()) compactedSSTables.add(sstable); sstables.release(compactedSSTables); modifier = cfs.getTracker().tryModify(sstables, OperationType.ANTICOMPACTION); } performAnticompaction(cfs, ranges, sstables, modifier, repairedAt, parentRepairSession); } }; ListenableFuture<?> ret = null; try { ret = executor.submitIfRunning(runnable, "anticompaction"); return ret; } finally { if (ret == null || ret.isCancelled()) sstables.release(); } }', 'ground_truth': 'public ListenableFuture<?> submitAntiCompaction(final ColumnFamilyStore cfs, final Collection<Range<Token>> ranges, final Refs<SSTableReader> sstables, final long repairedAt, final UUID pendingRepair, final UUID parentRepairSession) { Runnable runnable = new WrappedRunnable() { @Override @SuppressWarnings("resource") public void runMayThrow() throws Exception { LifecycleTransaction modifier = null; while (modifier == null) { for (SSTableReader compactingSSTable : cfs.getTracker().getCompacting()) sstables.releaseIfHolds(compactingSSTable); Set<SSTableReader> compactedSSTables = new HashSet<>(); for (SSTableReader sstable : sstables) if (sstable.isMarkedCompacted()) compactedSSTables.add(sstable); sstables.release(compactedSSTables); modifier = cfs.getTracker().tryModify(sstables, OperationType.ANTICOMPACTION); } performAnticompaction(cfs, ranges, sstables, modifier, repairedAt, pendingRepair, parentRepairSession); } }; ListenableFuture<?> ret = null; try { ret = executor.submitIfRunning(runnable, "anticompaction"); return ret; } finally { if (ret == null || ret.isCancelled()) sstables.release(); } }', 'output': 'public ListenableFuture<?> submitAntiCompaction(final ColumnFamilyStore cfs, final Collection<Range<Token>> ranges, final Refs<SSTableReader> sstables, final long repairedAt, final UUID parentRepairSession) { Runnable runnable = new WrappedRunnable() { @Override @SuppressWarnings("resource") public void runMayThrow() throws Exception { LifecycleTransaction modifier = null; while (modifier == null) { for (SSTableReader compactingSSTable : cfs.getTracker().getCompacting()) sstables.releaseIfHolds(compactingSSTable); Set<SSTableReader> compactedSSTables = new HashSet<>(); for (SSTableReader sstable : sstables) if (sstable.isMarkedCompacted()) compactedSSTables.add(sstable); sstables.release(compactedSSTables); modifier = cfs.getTracker().tryModify(sstables, OperationType.ANTICOMPACTION); } performAnticompaction(cfs, ranges, sstables, modifier, repairedAt, parentRepairSession); } }; ListenableFuture<?> ret = null; try { ret = executor.submitIfRunning(runnable, "anticompaction"); return ret; } finally { if (ret == null || ret.isCancelled()) sstables.release(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public int findBlockIndex(Slice.Bound bound) { if (bound == Slice.Bound.BOTTOM) return -1; if (bound == Slice.Bound.TOP) return blocksCount(); return IndexHelper.indexFor(bound, indexes, comparator, reversed, currentIndexIdx); }', 'ground_truth': 'public int findBlockIndex(Slice.Bound bound, int fromIdx) { if (bound == Slice.Bound.BOTTOM) return -1; if (bound == Slice.Bound.TOP) return blocksCount(); return IndexHelper.indexFor(bound, indexes, comparator, reversed, fromIdx); }', 'output': 'public int findBlockIndex(SliceBound bound) { if (bound == SliceBound.BOTTOM) return -1; if (bound == SliceBound.TOP) return blocksCount(); return IndexHelper.indexFor(bound, indexes, comparator, reversed, currentIndexIdx); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Object readValue(ByteBuf cb) { switch (this) { case CUSTOM: return CBUtil.readString(cb); case LIST: return DataType.toType(codec.decodeOne(cb)); case SET: return DataType.toType(codec.decodeOne(cb)); case MAP: List<AbstractType> l = new ArrayList<AbstractType>(2); l.add(DataType.toType(codec.decodeOne(cb))); l.add(DataType.toType(codec.decodeOne(cb))); return l; default: return null; } }', 'ground_truth': 'public Object readValue(ByteBuf cb, int version) { switch (this) { case CUSTOM: return CBUtil.readString(cb); case LIST: return DataType.toType(codec.decodeOne(cb, version)); case SET: return DataType.toType(codec.decodeOne(cb, version)); case MAP: List<AbstractType> l = new ArrayList<AbstractType>(2); l.add(DataType.toType(codec.decodeOne(cb, version))); l.add(DataType.toType(codec.decodeOne(cb, version))); return l; case UDT: String ks = CBUtil.readString(cb); ByteBuffer name = UTF8Type.instance.decompose(CBUtil.readString(cb)); int n = cb.readUnsignedShort(); List<ByteBuffer> fieldNames = new ArrayList<>(n); List<AbstractType<?>> fieldTypes = new ArrayList<>(n); for (int i = 0; i < n; i++) { fieldNames.add(UTF8Type.instance.decompose(CBUtil.readString(cb))); fieldTypes.add(DataType.toType(codec.decodeOne(cb, version))); } return new UserType(ks, name, fieldNames, fieldTypes); default: return null; } }', 'output': 'public Object readValue(ByteBuf cb) { switch (this) { case CUSTOM: return CBUtil.readString(cb); case LIST: return codec.decodeOne(cb); case SET: return codec.decodeOne(cb); case MAP: List<AbstractType> l = new ArrayList<AbstractType>(2); l.add(codec.decodeOne(cb)); l.add(codec.decodeOne(cb)); return l; default: return null; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': "protected void addMethods(Class theClass) { Method[] methodArray = theClass.getDeclaredMethods(); for (int i = 0; i < methodArray.length; i++) { Method reflectionMethod = methodArray[i]; if ( reflectionMethod.getName().indexOf('+') >= 0 ) { continue; } MetaMethod method = createMetaMethod(reflectionMethod); addMethod(method); } }", 'ground_truth': "protected void addMethods(Class theClass, boolean forceOverwrite) { Method[] methodArray = theClass.getDeclaredMethods(); for (int i = 0; i < methodArray.length; i++) { Method reflectionMethod = methodArray[i]; if ( reflectionMethod.getName().indexOf('+') >= 0 ) { continue; } MetaMethod method = createMetaMethod(reflectionMethod); addMethod(method,forceOverwrite); } }", 'output': "protected void addMethods(Class<?> theClass) { Method[] methodArray = theClass.getDeclaredMethods(); for (int i = 0; i < methodArray.length; i++) { Method reflectionMethod = methodArray[i]; if ( reflectionMethod.getName().indexOf('+') >= 0 ) { continue; } MetaMethod method = createMetaMethod(reflectionMethod); addMethod(method); } }</s>"}
{'instruction': 'Please refactor the following code:', 'input': 'public TableSnapshot(String keyspaceName, String tableName, UUID tableId, String tag, Instant createdAt, Instant expiresAt, Set<File> snapshotDirs) { this.keyspaceName = keyspaceName; this.tableName = tableName; this.tableId = tableId; this.tag = tag; this.createdAt = createdAt; this.expiresAt = expiresAt; this.snapshotDirs = snapshotDirs; }', 'ground_truth': 'public TableSnapshot(String keyspaceName, String tableName, UUID tableId, String tag, Instant createdAt, Instant expiresAt, Set<File> snapshotDirs, boolean ephemeral) { this.keyspaceName = keyspaceName; this.tableName = tableName; this.tableId = tableId; this.tag = tag; this.createdAt = createdAt; this.expiresAt = expiresAt; this.snapshotDirs = snapshotDirs; this.ephemeral = ephemeral; }', 'output': 'public TableSnapshot(String keyspaceName, String tableName, UUID tableId, String tag, Instant createdAt, Instant expiresAt, Collection<File> snapshotDirs) { this.keyspaceName = keyspaceName; this.tableName = tableName; this.tableId = tableId; this.tag = tag; this.createdAt = createdAt; this.expiresAt = expiresAt; this.snapshotDirs = ImmutableSet.copyOf(snapshotDirs); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static String query(final Command command) { String name; String path; String target; String ctxRoot; String hotDeploy; String force = Boolean.toString(FORCE_VALUE); CommandDeploy deploy; if (command instanceof CommandDeploy) { deploy = (CommandDeploy)command; if (deploy.path == null) { throw new CommandException(CommandException.ILLEGAL_NULL_VALUE); } name = Utils.sanitizeName(deploy.name); path = deploy.path.getAbsolutePath(); target =deploy.target; ctxRoot = deploy.contextRoot; hotDeploy = Boolean.toString(deploy.hotDeploy); } else { throw new CommandException( CommandException.ILLEGAL_COMAND_INSTANCE); } StringBuilder sb = new StringBuilder( DEFAULT_PARAM.length() + 1 + path.length() + 1 + FORCE_PARAM.length() + 1 + force.length() + queryPropertiesLength( deploy.properties, PROPERTIES_PARAM) + queryLibrariesLength( deploy.libraries, LIBRARIES_PARAM) + ( name != null && name.length() > 0 ? 1 + NAME_PARAM.length() + 1 + name.length() : 0 ) + ( target != null ? 1 + TARGET_PARAM.length() + 1 + target.length() : 0 ) + ( ctxRoot != null && ctxRoot.length() > 0 ? 1 + CTXROOT_PARAM.length() + 1 + ctxRoot.length() : 0 )); sb.append(DEFAULT_PARAM).append(PARAM_ASSIGN_VALUE).append(path); sb.append(PARAM_SEPARATOR); sb.append(FORCE_PARAM).append(PARAM_ASSIGN_VALUE).append(force); if (name != null && name.length() > 0) { sb.append(PARAM_SEPARATOR); sb.append(NAME_PARAM).append(PARAM_ASSIGN_VALUE).append(name); } if (target != null) { sb.append(PARAM_SEPARATOR); sb.append(TARGET_PARAM).append(PARAM_ASSIGN_VALUE).append(target); } if (ctxRoot != null && ctxRoot.length() > 0) { sb.append(PARAM_SEPARATOR); sb.append(CTXROOT_PARAM).append(PARAM_ASSIGN_VALUE).append(ctxRoot); } if (((CommandDeploy) command).hotDeploy) { sb.append(PARAM_SEPARATOR); sb.append(HOT_DEPLOY_PARAM); sb.append(PARAM_ASSIGN_VALUE).append(hotDeploy); } queryPropertiesAppend(sb, deploy.properties, PROPERTIES_PARAM, true); queryLibrariesAppend(sb, deploy.libraries, LIBRARIES_PARAM, true); return sb.toString(); }', 'ground_truth': 'private static String query(final PayaraServer server, final Command command) { String name; String path; String target; String ctxRoot; String hotDeploy; String force = Boolean.toString(FORCE_VALUE); CommandDeploy deploy; if (command instanceof CommandDeploy) { deploy = (CommandDeploy)command; if (deploy.path == null) { throw new CommandException(CommandException.ILLEGAL_NULL_VALUE); } name = Utils.sanitizeName(deploy.name); path = deploy.path.getAbsolutePath(); if (server.isDocker() && server.getHostPath() != null && !server.getHostPath().isEmpty() && server.getContainerPath() != null && !server.getContainerPath().isEmpty()) { Path relativePath = Paths.get(server.getHostPath()).relativize(deploy.path.toPath()); path = Paths.get(server.getContainerPath(), relativePath.toString()).toString(); if (server.getContainerPath().startsWith("/")) { path = path.replace("\\\\", "/"); } } target =deploy.target; ctxRoot = deploy.contextRoot; hotDeploy = Boolean.toString(deploy.hotDeploy); } else { throw new CommandException( CommandException.ILLEGAL_COMAND_INSTANCE); } StringBuilder sb = new StringBuilder( DEFAULT_PARAM.length() + 1 + path.length() + 1 + FORCE_PARAM.length() + 1 + force.length() + queryPropertiesLength( deploy.properties, PROPERTIES_PARAM) + queryLibrariesLength( deploy.libraries, LIBRARIES_PARAM) + ( name != null && name.length() > 0 ? 1 + NAME_PARAM.length() + 1 + name.length() : 0 ) + ( target != null ? 1 + TARGET_PARAM.length() + 1 + target.length() : 0 ) + ( ctxRoot != null && ctxRoot.length() > 0 ? 1 + CTXROOT_PARAM.length() + 1 + ctxRoot.length() : 0 )); sb.append(DEFAULT_PARAM).append(PARAM_ASSIGN_VALUE).append(path); sb.append(PARAM_SEPARATOR); sb.append(FORCE_PARAM).append(PARAM_ASSIGN_VALUE).append(force); if (name != null && name.length() > 0) { sb.append(PARAM_SEPARATOR); sb.append(NAME_PARAM).append(PARAM_ASSIGN_VALUE).append(name); } if (target != null) { sb.append(PARAM_SEPARATOR); sb.append(TARGET_PARAM).append(PARAM_ASSIGN_VALUE).append(target); } if (ctxRoot != null && ctxRoot.length() > 0) { sb.append(PARAM_SEPARATOR); sb.append(CTXROOT_PARAM).append(PARAM_ASSIGN_VALUE).append(ctxRoot); } if (((CommandDeploy) command).hotDeploy) { sb.append(PARAM_SEPARATOR); sb.append(HOT_DEPLOY_PARAM); sb.append(PARAM_ASSIGN_VALUE).append(hotDeploy); } queryPropertiesAppend(sb, deploy.properties, PROPERTIES_PARAM, true); queryLibrariesAppend(sb, deploy.libraries, LIBRARIES_PARAM, true); return sb.toString(); }', 'output': 'private static String query(final Command command) { String name; String path; String target; String ctxRoot; String hotDeploy; String force = Boolean.toString(FORCE_VALUE); CommandDeploy deploy; if (command instanceof CommandDeploy) { deploy = (CommandDeploy)command; if (deploy.path == null) { throw new CommandException(CommandException.ILLEGAL_NULL_VALUE); } name = Utils.sanitizeName(deploy.name); path = deploy.path.getAbsolutePath(); target =deploy.target; ctxRoot = deploy.contextRoot; hotDeploy = Boolean.toString(deploy.hotDeploy); } else { throw new CommandException( CommandException.ILLEGAL_COMAND_INSTANCE); } StringBuilder sb = new StringBuilder( DEFAULT_PARAM.length() + 1 + path.length() + 1 + FORCE_PARAM.length() + 1 + force.length() + queryPropertiesLength( deploy.properties, PROPERTIES_PARAM) + queryLibrariesLength( deploy.libraries, LIBRARIES_PARAM) + ( name != null && name.length() > 0 ? 1 + NAME_PARAM.length() + 1 + name.length() : 0 ) + ( target != null ? 1 + TARGET_PARAM.length() + 1 + target.length() : 0 ) + ( ctxRoot != null && ctxRoot.length() > 0 ? 1 + CTXROOT_PARAM.length() + 1 + ctxRoot.length() : 0 )); sb.append(DEFAULT_PARAM).append(PARAM_ASSIGN_VALUE).append(path); sb.append(PARAM_SEPARATOR); sb.append(FORCE_PARAM).append(PARAM_ASSIGN_VALUE).append(force); if (name != null && name.length() > 0) { sb.append(PARAM_SEPARATOR); sb.append(NAME_PARAM).append(PARAM_ASSIGN_VALUE).append(name); } if (target != null) { sb.append(PARAM_SEPARATOR); sb.append(TARGET_PARAM).append(PARAM_ASSIGN_VALUE).append(target); } if (ctx'}
{'instruction': 'Please refactor the following code:', 'input': 'public TimelineMetricMetadata getTimelineMetricMetadata(TimelineMetric timelineMetric) { return new TimelineMetricMetadata( timelineMetric.getMetricName(), timelineMetric.getAppId(), timelineMetric.getUnits(), timelineMetric.getType(), timelineMetric.getStartTime(), supportAggregates(timelineMetric) ); }', 'ground_truth': 'public TimelineMetricMetadata getTimelineMetricMetadata(TimelineMetric timelineMetric, boolean isWhitelisted) { return new TimelineMetricMetadata( timelineMetric.getMetricName(), timelineMetric.getAppId(), timelineMetric.getUnits(), timelineMetric.getType(), timelineMetric.getStartTime(), supportAggregates(timelineMetric), isWhitelisted ); }', 'output': 'public TimelineMetricMetadata getTimelineMetricMetadata(TimelineMetric timelineMetric, String metricName) { return new TimelineMetricMetadata( metricName, timelineMetric.getAppId(), timelineMetric.getUnits(), timelineMetric.getType(), timelineMetric.getStartTime(), supportAggregates(timelineMetric) ); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static QueryFilter getSliceFilter(DecoratedKey key, String cfName, ByteBuffer start, ByteBuffer finish, boolean reversed, int limit) { return new QueryFilter(key, cfName, new SliceQueryFilter(start, finish, reversed, limit)); }', 'ground_truth': 'public static QueryFilter getSliceFilter(DecoratedKey key, String cfName, ByteBuffer start, ByteBuffer finish, boolean reversed, int limit, long timestamp) { return new QueryFilter(key, cfName, new SliceQueryFilter(start, finish, reversed, limit), timestamp); }', 'output': 'public static QueryFilter getSliceFilter(DecoratedKey key, String cfName, byte[] start, byte[] finish, boolean reversed, int limit) { return new QueryFilter(key, cfName, new SliceQueryFilter(start, finish, reversed, limit)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public SSTableReader closeAndOpenReader() throws IOException { FileOutputStream fos = new FileOutputStream(filterFilename()); DataOutputStream stream = new DataOutputStream(fos); BloomFilter.serializer().serialize(bf, stream); stream.flush(); fos.getFD().sync(); stream.close(); indexRAF.getChannel().force(true); indexRAF.close(); dataWriter.close(); rename(indexFilename()); rename(filterFilename()); dataFile = rename(dataFile); return new SSTableReader(dataFile, partitioner, indexPositions, bf); }', 'ground_truth': 'public SSTableReader closeAndOpenReader(double cacheFraction) throws IOException { FileOutputStream fos = new FileOutputStream(filterFilename()); DataOutputStream stream = new DataOutputStream(fos); BloomFilter.serializer().serialize(bf, stream); stream.flush(); fos.getFD().sync(); stream.close(); indexRAF.getChannel().force(true); indexRAF.close(); dataWriter.close(); rename(indexFilename()); rename(filterFilename()); dataFile = rename(dataFile); ConcurrentLinkedHashMap<String,Long> keyCache = cacheFraction > 0 ? SSTableReader.createKeyCache((int) (cacheFraction * keysWritten)) : null; return new SSTableReader(dataFile, partitioner, indexPositions, bf, keyCache); }', 'output': 'public SSTableReader closeAndOpenReader() throws IOException { FileOutputStream fos = new FileOutputStream(indexFilename()); DataOutputStream stream = new DataOutputStream(fos); BloomFilter.serializer().serialize(bf, stream); stream.flush(); fos.getFD().sync(); stream.close(); indexRAF.getChannel().force(true); indexRAF.close(); dataWriter.close(); rename(indexFilename()); rename(filterFilename()); dataFile = rename(dataFile); return new SSTableReader(dataFile, partitioner, indexPositions, bf); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean hasOnlyTombstones() { for (Column column : this) { if (column.isLive()) return false; } return true; }', 'ground_truth': 'public boolean hasOnlyTombstones(long now) { for (Column column : this) if (column.isLive(now)) return false; return true; }', 'output': 'public boolean hasOnlyTombstones() { for (Column column : this) { if (column.isLive()) continue; } return true; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private T doRemove(L row, boolean ceil) throws IgniteCheckedException { Remove r = new Remove(row, ceil); try { for (;;) { r.init(); switch (removeDown(r, r.rootId, 0L, 0L, r.rootLvl)) { case Remove.RETRY: case Remove.RETRY_ROOT: checkInterrupted(); continue; default: if (!r.isFinished()) r.finishTail(); assert r.isFinished(); return r.removed; } } } finally { r.releaseTail(); r.releaseMeta(); r.reuseEmptyPages(); } }', 'ground_truth': 'private T doRemove(L row, boolean ceil, ReuseBag bag) throws IgniteCheckedException { Remove r = new Remove(row, ceil, bag); try { for (;;) { r.init(); switch (removeDown(r, r.rootId, 0L, 0L, r.rootLvl)) { case Remove.RETRY: case Remove.RETRY_ROOT: checkInterrupted(); continue; default: if (!r.isFinished()) r.finishTail(); assert r.isFinished(); return r.removed; } } } finally { r.releaseTail(); r.releaseMeta(); r.reuseFreePages(); } }', 'output': 'private T doRemove(L row, boolean ceil) throws IgniteCheckedException { Remove r = new Remove(row, ceil); try { for (;;) { r.init(); switch (removeDown(r, r.rootId, 0L, 0L, r.rootLvl)) { case Remove.RETRY: case Remove.RETRY_ROOT: checkInterrupted(); continue; default: if (!r.isFinished()) r.finishTail(); assert r.isFinished(); return r.removed; } } } finally { r.releaseTail(); r.releaseMeta(); r.reuseEmptyPages(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private GridQueryNextPageResponse prepareNextPage(MapNodeResults nodeRess, ClusterNode node, MapQueryResults qr, int qry, int segmentId, int pageSize, boolean removeMapping) throws IgniteCheckedException { MapQueryResult res = qr.result(qry); assert res != null; if (res.closed()) return null; int page = res.page(); List<Value[]> rows = new ArrayList<>(Math.min(64, pageSize)); boolean last = res.fetchNextPage(rows, pageSize); if (last) { res.close(); if (qr.isAllClosed()) { nodeRess.remove(qr.queryRequestId(), segmentId, qr); if (MapQueryLazyWorker.currentWorker() != null) releaseReservations(); } } boolean loc = node.isLocal(); int colsCnt = !qr.isForUpdate() ? res.columnCount() : res.columnCount() - 1; GridQueryNextPageResponse msg = new GridQueryNextPageResponse(qr.queryRequestId(), segmentId, qry, page, page == 0 ? res.rowCount() : -1, colsCnt, loc ? null : toMessages(rows, new ArrayList<>(res.columnCount()), colsCnt), loc ? rows : null, last); msg.removeMapping(removeMapping); return msg; }', 'ground_truth': 'private GridQueryNextPageResponse prepareNextPage(MapNodeResults nodeRess, ClusterNode node, MapQueryResults qr, int qry, int segmentId, int pageSize, boolean removeMapping, Boolean dataPageScanEnabled) throws IgniteCheckedException { MapQueryResult res = qr.result(qry); assert res != null; if (res.closed()) return null; int page = res.page(); List<Value[]> rows = new ArrayList<>(Math.min(64, pageSize)); boolean last = res.fetchNextPage(rows, pageSize, dataPageScanEnabled); if (last) { res.close(); if (qr.isAllClosed()) { nodeRess.remove(qr.queryRequestId(), segmentId, qr); if (MapQueryLazyWorker.currentWorker() != null) releaseReservations(); } } boolean loc = node.isLocal(); int colsCnt = !qr.isForUpdate() ? res.columnCount() : res.columnCount() - 1; GridQueryNextPageResponse msg = new GridQueryNextPageResponse(qr.queryRequestId(), segmentId, qry, page, page == 0 ? res.rowCount() : -1, colsCnt, loc ? null : toMessages(rows, new ArrayList<>(res.columnCount()), colsCnt), loc ? rows : null, last); msg.removeMapping(removeMapping); return msg; }', 'output': 'private GridQueryNextPageResponse prepareNextPage(MapNodeResults nodeRess, ClusterNode node, MapQueryResults qr, int qry, int segmentId, int pageSize, boolean removeMapping) throws IgniteCheckedException { MapQueryResult res = qr.result(qry); assert res != null; if (res.closed()) return null; int page = res.page(); List<Value[]> rows = new ArrayList<>(Math.min(64, pageSize)); boolean last = res.fetchNextPage(rows, pageSize); if (last) { res.close(); if (qr.isAllClosed()) { nodeRess.remove(qr.queryRequestId(), segmentId, qr); if (MapQueryLazyWorker.currentWorker() != null) releaseReservations(); } } boolean loc = node.isLocal(); int colsCnt = !qr.isForUpdate() ? res.columnCount() : res.columnCount() - 1; GridQueryNextPageResponse msg = new GridQueryNextPageResponse(qr.queryRequestId(), segmentId, qry, page, page == 0 ? res.rowCount() : -1, colsCnt, toMessages(rows, new ArrayList<>(res.columnCount()), colsCnt), loc ? rows : null, last); msg.removeMapping(removeMapping); return msg; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ColumnFamily(String cfName, String columnType) { name_ = cfName; type_ = columnType; createColumnFactoryAndColumnSerializer(columnType); }', 'ground_truth': 'public ColumnFamily(String cfName, String columnType, Comparator<IColumn> comparator) { name_ = cfName; type_ = columnType; columnFactory_ = AbstractColumnFactory.getColumnFactory(columnType); columnSerializer_ = columnFactory_.createColumnSerializer(); if(columns_ == null) columns_ = new EfficientBidiMap(comparator); }', 'output': 'public ColumnFamily(String cfName, String columnType, int version) { name_ = cfName; type_ = columnType; version_ = version; createColumnFactoryAndColumnSerializer(columnType); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void addColumnToSchemaMutation(CFMetaData table, ColumnDefinition column, long timestamp, Mutation mutation) { RowUpdateBuilder adder = new RowUpdateBuilder(Columns, timestamp, mutation).clustering(table.cfName, column.name.toString()); AbstractType<?> type = column.type; if (type instanceof ReversedType) type = ((ReversedType) type).baseType; adder.add("column_name_bytes", column.name.bytes) .add("kind", column.kind.toString().toLowerCase()) .add("position", column.position()) .add("clustering_order", column.clusteringOrder().toString().toLowerCase()) .add("type", type.asCQL3Type().toString()) .build(); }', 'ground_truth': 'private static void addColumnToSchemaMutation(CFMetaData table, ColumnDefinition column, Mutation.SimpleBuilder builder) { AbstractType<?> type = column.type; if (type instanceof ReversedType) type = ((ReversedType) type).baseType; builder.update(Columns) .row(table.cfName, column.name.toString()) .add("column_name_bytes", column.name.bytes) .add("kind", column.kind.toString().toLowerCase()) .add("position", column.position()) .add("clustering_order", column.clusteringOrder().toString().toLowerCase()) .add("type", type.asCQL3Type().toString()); }', 'output': 'private static void addColumnToSchemaMutation(CFMetaData table, ColumnMetadata column, long timestamp, Mutation mutation) { RowUpdateBuilder adder = new RowUpdateBuilder(Columns, timestamp, mutation).clustering(table.cfName, column.name.toString()); AbstractType<?> type = column.type; if (type instanceof ReversedType) type = ((ReversedType) type).baseType; adder.add("column_name_bytes", column.name.bytes) .add("kind", column.kind.toString().toLowerCase()) .add("position", column.position()) .add("clustering_order", column.clusteringOrder().toString().toLowerCase()) .add("type", type.asCQL3Type().toString()) .build(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void apply(DecoratedKey key, ColumnFamily columnFamily) { long start = System.nanoTime(); Memtable mt = getMemtableThreadSafe(); mt.put(key, columnFamily); updateRowCache(key, columnFamily); metric.writeLatency.addNano(System.nanoTime() - start); while (true) { long last = liveRatioComputedAt.get(); long operations = metric.writeLatency.latency.count(); if (operations < 2 * last) break; if (liveRatioComputedAt.compareAndSet(last, operations)) { logger.debug("computing liveRatio of {} at {} ops", this, operations); mt.updateLiveRatio(); } } }', 'ground_truth': 'public void apply(DecoratedKey key, ColumnFamily columnFamily, SecondaryIndexManager.Updater indexer) { long start = System.nanoTime(); Memtable mt = getMemtableThreadSafe(); mt.put(key, columnFamily, indexer); updateRowCache(key, columnFamily); metric.writeLatency.addNano(System.nanoTime() - start); while (true) { long last = liveRatioComputedAt.get(); long operations = metric.writeLatency.latency.count(); if (operations < 2 * last) break; if (liveRatioComputedAt.compareAndSet(last, operations)) { logger.debug("computing liveRatio of {} at {} ops", this, operations); mt.updateLiveRatio(); } } }', 'output': 'public void apply(DecoratedKey key, ColumnFamily columnFamily) { long start = System.nanoTime(); Memtable mt = getMemtableThreadSafe(); mt.put(key, columnFamily); updateRowCache(key, columnFamily); metric.writeLatency.addNano(System.nanoTime() - start); while (true) { long last = liveRatioComputedAt.get(); long operations = metric.writeLatency.latency.count(); if (operations < 2 * last) break; if (liveRatioComputedAt.compareAndSet(last, operations)) { logger.debug("computing liveRatio of {} at {} ops", this, operations); mt.updateLiveRatio(); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void checkErrorState(final ConnClosure clo, String expState) throws SQLException { checkErrorState(new IgniteCallable<Void>() { @Override public Void call() throws Exception { try (final Connection conn = getConnection()) { clo.run(conn); fail(); return null; } } }, expState); }', 'ground_truth': 'protected void checkErrorState(final IgniteCallable<Void> clo, String expState, String expMsg) throws SQLException { SQLException ex = (SQLException)GridTestUtils.assertThrows(null, clo, SQLException.class, expMsg); assertEquals(expState, ex.getSQLState()); }', 'output': 'protected void checkErrorState(final ConnClosure clo, String expState) throws SQLException { checkErrorState(new IgniteCallable<Void>() { @Override public Void call() throws Exception { try (final Connection conn = getConnection()) { clo.run(conn); fail(); return null; } } }, expState); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private PushProjectPastFilterRule() { super( some( ProjectRel.class, any(FilterRel.class))); this.preserveExprCondition = PushProjector.ExprCondition.FALSE; }', 'ground_truth': 'private PushProjectPastFilterRule( PushProjector.ExprCondition preserveExprCondition) { super( operand( ProjectRel.class, operand(FilterRel.class, any()))); this.preserveExprCondition = preserveExprCondition; }', 'output': 'private PushProjectPastFilterRule( boolean preserveExprCondition) { super( some( ProjectRel.class, any(FilterRel.class))); this.preserveExprCondition = preserveExprCondition; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private PortableClassDescriptor registerUserClassDescriptor(Class<?> cls) { boolean registered; String typeName = typeName(cls.getName()); BinaryIdMapper idMapper = userTypeIdMapper(typeName); int typeId = idMapper.typeId(typeName); try { registered = marshCtx.registerClass(typeId, cls); } catch (IgniteCheckedException e) { throw new BinaryObjectException("Failed to register class.", e); } String affFieldName = affinityFieldName(cls); PortableClassDescriptor desc = new PortableClassDescriptor(this, cls, true, typeId, typeName, affFieldName, idMapper, null, true, registered, false /* predefined */ ); Collection<PortableSchema> schemas = desc.schema() != null ? Collections.singleton(desc.schema()) : null; metaHnd.addMeta(typeId, new BinaryMetadata(typeId, typeName, desc.fieldsMeta(), affFieldName, schemas, desc.isEnum()).wrap(this)); if (IgniteUtils.detectClassLoader(cls).equals(dfltLdr)) userTypes.put(typeId, desc); descByCls.put(cls, desc); mappers.putIfAbsent(typeId, idMapper); return desc; }', 'ground_truth': 'private PortableClassDescriptor registerUserClassDescriptor(Class<?> cls, boolean deserialize) { boolean registered; String typeName = typeName(cls.getName()); BinaryIdMapper idMapper = userTypeIdMapper(typeName); int typeId = idMapper.typeId(typeName); try { registered = marshCtx.registerClass(typeId, cls); } catch (IgniteCheckedException e) { throw new BinaryObjectException("Failed to register class.", e); } String affFieldName = affinityFieldName(cls); PortableClassDescriptor desc = new PortableClassDescriptor(this, cls, true, typeId, typeName, affFieldName, idMapper, null, true, registered, false /* predefined */ ); if (!deserialize) { Collection<PortableSchema> schemas = desc.schema() != null ? Collections.singleton(desc.schema()) : null; metaHnd.addMeta(typeId, new BinaryMetadata(typeId, typeName, desc.fieldsMeta(), affFieldName, schemas, desc.isEnum()).wrap(this)); } if (IgniteUtils.detectClassLoader(cls).equals(dfltLdr)) userTypes.put(typeId, desc); descByCls.put(cls, desc); mappers.putIfAbsent(typeId, idMapper); return desc; }', 'output': 'private PortableClassDescriptor registerUserClassDescriptor(Class<?> cls) { boolean registered; String typeName = typeName(cls.getCanonicalName()); BinaryIdMapper idMapper = userTypeIdMapper(typeName); int typeId = idMapper.typeId(typeName); try { registered = marshCtx.registerClass(typeId, cls); } catch (IgniteCheckedException e) { throw new BinaryObjectException("Failed to register class.", e); } String affFieldName = affinityFieldName(cls); PortableClassDescriptor desc = new PortableClassDescriptor(this, cls, true, typeId, typeName, affFieldName, idMapper, null, true, registered, false /* predefined */ ); Collection<PortableSchema> schemas = desc.schema() != null ? Collections.singleton(desc.schema()) : null; metaHnd.addMeta(typeId, new BinaryMetadata(typeId, typeName, desc.fieldsMeta(), affFieldName, schemas, desc.isEnum()).wrap(this)); if (IgniteUtils.detectClassLoader(cls).equals(dfltLdr)) userTypes.put(typeId, desc); descByCls.put(cls, desc); mappers.putIfAbsent(typeId, idMapper); return desc; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Object[] doReadObjectArray(BinaryInputStream in, BinaryContext ctx, ClassLoader ldr, BinaryReaderHandlesHolder handles, boolean deserialize) throws BinaryObjectException { int hPos = positionForHandle(in); Class compType = doReadClass(in, ctx, ldr, deserialize); int len = in.readInt(); Object[] arr = deserialize ? (Object[])Array.newInstance(compType, len) : new Object[len]; handles.setHandle(arr, hPos); for (int i = 0; i < len; i++) arr[i] = deserializeOrUnmarshal(in, ctx, ldr, handles, deserialize); return arr; }', 'ground_truth': 'public static Object[] doReadObjectArray(BinaryInputStream in, BinaryContext ctx, ClassLoader ldr, BinaryReaderHandlesHolder handles, boolean detach, boolean deserialize) throws BinaryObjectException { int hPos = positionForHandle(in); Class compType = doReadClass(in, ctx, ldr, deserialize); int len = in.readInt(); Object[] arr = deserialize ? (Object[])Array.newInstance(compType, len) : new Object[len]; handles.setHandle(arr, hPos); for (int i = 0; i < len; i++) arr[i] = deserializeOrUnmarshal(in, ctx, ldr, handles, detach, deserialize); return arr; }', 'output': 'public static Object[] doReadObjectArray(BinaryInputStream in, BinaryContext ctx, ClassLoader ldr, BinaryReaderHandlesHolder handles, boolean deserialize) throws BinaryObjectException { int hPos = positionForHandle(in); Class compType = doReadClass(in, ctx, ldr, deserialize); int len = in.readInt(); Object[] arr = deserialize ? (Object[])Array.newInstance(compType, len) : new Object[len]; handles.setHandle(arr, hPos); for (int i = 0; i < len; i++) arr[i] = deserializeOrUnmarshal(in, ctx, ldr, handles, deserialize); return arr; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void pushDataset(String pCollectionId, Dataset dataset) { dataset.setName(pCollectionId); datasets.put(pCollectionId, dataset); leaves.add(dataset); }', 'ground_truth': 'public void pushDataset(String pCollectionId, Dataset dataset, Coder coder) { dataset.setName(pCollectionId); SparkPipelineOptions sparkOptions = serializablePipelineOptions.get().as(SparkPipelineOptions.class); if (!sparkOptions.isCacheDisabled() && consumptionCount.getOrDefault(pCollectionId, 0) > 1) { String storageLevel = sparkOptions.getStorageLevel(); dataset.cache(storageLevel, coder); } datasets.put(pCollectionId, dataset); leaves.add(dataset); }', 'output': 'public void pushDataset(String pCollectionId, Dataset dataset) { dataset.setName(pCollectionId); collections.put(pCollectionId, dataset); leaves.add(dataset); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Map<ByteBuffer, List<ColumnOrSuperColumn>> getSlice(List<SinglePartitionReadCommand> commands, boolean subColumnsOnly, int cellLimit, org.apache.cassandra.db.ConsistencyLevel consistency_level, ClientState cState) throws org.apache.cassandra.exceptions.InvalidRequestException, UnavailableException, TimedOutException { try (PartitionIterator results = read(commands, consistency_level, cState)) { Map<ByteBuffer, List<ColumnOrSuperColumn>> columnFamiliesMap = new HashMap<>(); while (results.hasNext()) { try (RowIterator iter = results.next()) { List<ColumnOrSuperColumn> thriftifiedColumns = thriftifyPartition(iter, subColumnsOnly, iter.isReverseOrder(), cellLimit); columnFamiliesMap.put(iter.partitionKey().getKey(), thriftifiedColumns); } } return columnFamiliesMap; } }', 'ground_truth': 'private Map<ByteBuffer, List<ColumnOrSuperColumn>> getSlice(List<SinglePartitionReadCommand> commands, boolean subColumnsOnly, int cellLimit, org.apache.cassandra.db.ConsistencyLevel consistency_level, ClientState cState, long queryStartNanoTime) throws org.apache.cassandra.exceptions.InvalidRequestException, UnavailableException, TimedOutException { try (PartitionIterator results = read(commands, consistency_level, cState, queryStartNanoTime)) { Map<ByteBuffer, List<ColumnOrSuperColumn>> columnFamiliesMap = new HashMap<>(); while (results.hasNext()) { try (RowIterator iter = results.next()) { List<ColumnOrSuperColumn> thriftifiedColumns = thriftifyPartition(iter, subColumnsOnly, iter.isReverseOrder(), cellLimit); columnFamiliesMap.put(iter.partitionKey().getKey(), thriftifiedColumns); } } return columnFamiliesMap; } }', 'output': 'private Map<ByteBuffer, Collection<ColumnOrSuperColumn>> getSlice(List<SinglePartitionReadCommand> commands, boolean subColumnsOnly, int cellLimit, org.apache.cassandra.db.ConsistencyLevel consistency_level, ClientState cState) throws org.apache.cassandra.exceptions.InvalidRequestException, UnavailableException, TimedOutException { try (PartitionIterator results = read(commands, consistency_level, cState)) { Map<ByteBuffer, Collection<ColumnOrSuperColumn>> columnFamiliesMap = new HashMap<>(); while (results.hasNext()) { try (RowIterator iter = results.next()) { Collection<ColumnOrSuperColumn> thriftifiedColumns = thriftifyPartition(iter, subColumnsOnly, iter.isReverseOrder(), cellLimit); columnFamiliesMap.put(iter.partitionKey().getKey(), thriftifiedColumns); } } return columnFamiliesMap; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private List<ColumnOrSuperColumn> thriftifySuperColumns(Collection<org.apache.cassandra.db.Column> columns, boolean reverseOrder, boolean subcolumnsOnly, boolean isCounterCF) { if (subcolumnsOnly) { ArrayList<ColumnOrSuperColumn> thriftSuperColumns = new ArrayList<ColumnOrSuperColumn>(columns.size()); for (org.apache.cassandra.db.Column column : columns) { if (column.isMarkedForDelete()) continue; thriftSuperColumns.add(thriftifyColumnWithName(column, SuperColumns.subName(column.name()))); } if (reverseOrder) Collections.reverse(thriftSuperColumns); return thriftSuperColumns; } else { if (isCounterCF) return thriftifyCounterSuperColumns(columns, reverseOrder); else return thriftifySuperColumns(columns, reverseOrder); } }', 'ground_truth': 'private List<ColumnOrSuperColumn> thriftifySuperColumns(Collection<org.apache.cassandra.db.Column> columns, boolean reverseOrder, long now, boolean subcolumnsOnly, boolean isCounterCF) { if (subcolumnsOnly) { ArrayList<ColumnOrSuperColumn> thriftSuperColumns = new ArrayList<ColumnOrSuperColumn>(columns.size()); for (org.apache.cassandra.db.Column column : columns) { if (column.isMarkedForDelete(now)) continue; thriftSuperColumns.add(thriftifyColumnWithName(column, SuperColumns.subName(column.name()))); } if (reverseOrder) Collections.reverse(thriftSuperColumns); return thriftSuperColumns; } else { if (isCounterCF) return thriftifyCounterSuperColumns(columns, reverseOrder, now); else return thriftifySuperColumns(columns, reverseOrder, now); } }', 'output': 'private List<ColumnOrSuperColumn> thriftifySuperColumns(Collection<Column> columns, boolean reverseOrder, boolean subcolumnsOnly, boolean isCounterCF) { if (subcolumnsOnly) { ArrayList<ColumnOrSuperColumn> thriftSuperColumns = new ArrayList<ColumnOrSuperColumn>(columns.size()); for (Column column : columns) { if (column.isMarkedForDelete()) continue; thriftSuperColumns.add(thriftifyColumnWithName(column, SuperColumns.subName(column.name()))); } if (reverseOrder) Collections.reverse(thriftSuperColumns); return thriftSuperColumns; } else { if (isCounterCF) return thriftifyCounterSuperColumns(columns, reverseOrder); else return thriftifySuperColumns(columns, reverseOrder); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public MemberOrderFacetXml( final String name, final String sequence, final TranslationService translationService, final FacetHolder holder) { super(name, sequence, translationService, holder); }', 'ground_truth': 'public MemberOrderFacetXml( TranslationContext context, final String name, final String sequence, final TranslationService translationService, final FacetHolder holder) { super(context, name, sequence, translationService, holder); }', 'output': 'public MemberOrderFacetXml( final String name, final String sequence, final TranslationService translationService, final FacetHolder holder, final int order ) { super(name, sequence, translationService, holder, order); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static String stringifyUserTypeParameters(String keysace, ByteBuffer typeName, List<ByteBuffer> columnNames, List<AbstractType<?>> columnTypes) { StringBuilder sb = new StringBuilder(); sb.append(\'(\').append(keysace).append(",").append(ByteBufferUtil.bytesToHex(typeName)); for (int i = 0; i < columnNames.size(); i++) { sb.append(\',\'); sb.append(ByteBufferUtil.bytesToHex(columnNames.get(i))).append(":"); sb.append(columnTypes.get(i).toString(true)); } sb.append(\')\'); return sb.toString(); }', 'ground_truth': 'public static String stringifyUserTypeParameters(String keysace, ByteBuffer typeName, List<ByteBuffer> columnNames, List<AbstractType<?>> columnTypes, boolean ignoreFreezing) { StringBuilder sb = new StringBuilder(); sb.append(\'(\').append(keysace).append(",").append(ByteBufferUtil.bytesToHex(typeName)); for (int i = 0; i < columnNames.size(); i++) { sb.append(\',\'); sb.append(ByteBufferUtil.bytesToHex(columnNames.get(i))).append(":"); sb.append(columnTypes.get(i).toString(ignoreFreezing)); } sb.append(\')\'); return sb.toString(); }', 'output': 'public static String stringifyUserTypeParameters(String keysace, ByteBuffer typeName, List<ByteBuffer> columnNames, List<ColumnType<?>> columnTypes) { StringBuilder sb = new StringBuilder(); sb.append(\'(\').append(keysace).append(",").append(ByteBufferUtil.bytesToHex(typeName)); for (int i = 0; i < columnNames.size(); i++) { sb.append(\',\'); sb.append(ByteBufferUtil.bytesToHex(columnNames.get(i))).append(":"); sb.append(columnTypes.get(i).toString(true)); } sb.append(\')\'); return sb.toString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static List wrap(List list, RelDataType type) { if (type.isNullable()) { switch (type.getSqlTypeName()) { case TIMESTAMP: return Functions.adapt(list, TIMESTAMP_TO_LONG); case TIME: return Functions.adapt(list, TIME_TO_INT); case DATE: return Functions.adapt(list, DATE_TO_INT); } } return list; }', 'ground_truth': 'private static List wrap(ColumnMetaData.Rep rep, List list, RelDataType type) { switch (type.getSqlTypeName()) { case TIMESTAMP: switch (rep) { case OBJECT: case JAVA_SQL_TIMESTAMP: return Functions.adapt(list, TIMESTAMP_TO_LONG); } break; case TIME: switch (rep) { case OBJECT: case JAVA_SQL_TIME: return Functions.adapt(list, TIME_TO_INT); } break; case DATE: switch (rep) { case OBJECT: case JAVA_SQL_DATE: return Functions.adapt(list, DATE_TO_INT); } break; } return list; }', 'output': 'private static List<Object> wrap(List<Object> list, RelDataType type) { if (type.isNullable()) { switch (type.getSqlTypeName()) { case TIMESTAMP: return Functions.adapt(list, TIMESTAMP_TO_LONG); case TIME: return Functions.adapt(list, TIME_TO_INT); case DATE: return Functions.adapt(list, DATE_TO_INT); } } return list; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private int checkLogin(String url, String username, String password) throws IOException { Map<String, List<String>> headers = getHeaders(username, password); HttpURLConnection conn = m_streamProvider.processURL(url, "GET", (InputStream) null, headers); int result = conn.getResponseCode(); if (result == 200) { Gson gson = new Gson(); try { gson.fromJson(new InputStreamReader(conn.getInputStream()), Object.class); } catch (Exception e) { result = 401; } } return result; }', 'ground_truth': 'private int checkLogin(URLStreamProvider streamProvider, String url, String username, String password) throws IOException { Map<String, List<String>> headers = getHeaders(username, password); HttpURLConnection conn = streamProvider.processURL(url, "GET", (InputStream) null, headers); int result = conn.getResponseCode(); if (result == 200) { Gson gson = new Gson(); try { gson.fromJson(new InputStreamReader(conn.getInputStream()), Object.class); } catch (Exception e) { result = 401; } } return result; }', 'output': 'private int checkLogin(String url, String username, String password) throws IOException { Map<String, List<String>> headers = getHeaders(username, password); HttpURLConnection conn = m_streamProvider.processURL(url, "GET", (InputStream) null, headers); int result = conn.getResponseCode(); if (result == 200) { Gson gson = new Gson(); try { gson.fromJson(new InputStreamReader(conn.getInputStream()), Object.class); } catch (Exception e) { result = 401; } } return result; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Component createCollectionComponent( final ObjectAssociation objectAssociation, final ManagedObject assocObject) { val labelLiteral = "Collection: " + objectAssociation.getName(); val pojo = assocObject.getPojo(); if (pojo instanceof Collection) { return TableView.fromObjectAssociation(objectAssociation, assocObject); } if (pojo == null) { val textField = new TextField(); textField.setLabel(labelLiteral); textField.setValue(NULL_LITERAL); return textField; } val textField = new TextField(); textField.setLabel(labelLiteral); textField.setValue("Unknown collection type: " + pojo.getClass()); return textField; }', 'ground_truth': 'private Component createCollectionComponent( final ManagedCollection managedCollection) { val labelLiteral = "Collection: " + managedCollection.getName(); val pojo = managedCollection.getCollectionValue().getPojo(); if (pojo instanceof Collection) { return TableView.fromManagedCollection(managedCollection); } if (pojo == null) { val textField = new TextField(); textField.setLabel(labelLiteral); textField.setValue(NULL_LITERAL); return textField; } val textField = new TextField(); textField.setLabel(labelLiteral); textField.setValue("Unknown collection type: " + pojo.getClass()); return textField; }', 'output': 'private Component createCollectionComponent( final ObjectAssociation objectAssociation, final ManagedObject assocObject) { val labelLiteral = "Collection: " + objectAssociation.getName(); val pojo = assocObject.getPojo(); if (pojo instanceof Collection) { return TableView.fromObjectAssociation(objectAssociation, assocObject); } if (pojo == null) { val textField = new TextField(); textField.setLabel(labelLiteral); textField.setValue(NULL_LITERAL); return textField; } val textField = new TextField(); textField.setLabel(labelLiteral); textField.setValue("Unknown collection type: " + pojo.getClass()); return textField; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void messages(@Nullable Collection<TcpDiscoveryAbstractMessage> msgs, @Nullable IgniteUuid discardMsgId) { this.msgs = msgs; this.discardMsgId = discardMsgId; }', 'ground_truth': 'public void messages( @Nullable Collection<TcpDiscoveryAbstractMessage> msgs, @Nullable IgniteUuid discardMsgId, @Nullable IgniteUuid discardCustomMsgId ) { this.msgs = msgs; this.discardMsgId = discardMsgId; this.discardCustomMsgId = discardCustomMsgId; }', 'output': 'public void messages(Collection<TcpDiscoveryAbstractMessage> msgs, @Nullable IgniteUuid discardMsgId) { this.msgs = msgs; this.discardMsgId = discardMsgId; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private ProcessingState( BlockingQueue output, TriggerScheduler triggering, UnaryFunction stateFactory, CombinableReduceFunction stateCombiner, boolean isBounded) { this.stateOutput = QueueCollector.wrap(requireNonNull(output)); this.rawOutput = output; this.triggering = requireNonNull(triggering); this.stateFactory = requireNonNull(stateFactory); this.stateCombiner = requireNonNull(stateCombiner); this.isBounded = isBounded; }', 'ground_truth': 'private ProcessingState( BlockingQueue output, TriggerScheduler triggering, UnaryFunction stateFactory, CombinableReduceFunction stateCombiner, boolean isBounded, int maxKeyStatesPerWindow) { this.stateOutput = QueueCollector.wrap(requireNonNull(output)); this.rawOutput = output; this.triggering = requireNonNull(triggering); this.stateFactory = requireNonNull(stateFactory); this.stateCombiner = requireNonNull(stateCombiner); this.isBounded = isBounded; this.maxKeyStatesPerWindow = maxKeyStatesPerWindow; }', 'output': 'private ProcessingState( BlockingQueue output, TriggerScheduler triggering, UnaryFunction stateFactory, CombinableReduceFunction stateCombiner, boolean isBounded, int parallelism) { this.stateOutput = QueueCollector.wrap(requireNonNull(output)); this.rawOutput = output; this.triggering = requireNonNull(triggering); this.stateFactory = requireNonNull(stateFactory); this.stateCombiner = requireNonNull(stateCombiner); this.isBounded = isBounded; this.parallelism = parallelism; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void clientServerOrderedMessage() throws Exception { Ignite ignite = grid(CLIENT_NODE_IDX); ClusterGroup grp = ignite.cluster().forServers(); assert grp.nodes().size() > 0; registerListenerAndSendOrderedMessages(ignite, grp); }', 'ground_truth': 'private void clientServerOrderedMessage(boolean async) throws Exception { Ignite ignite = grid(CLIENT_NODE_IDX); ClusterGroup grp = ignite.cluster().forServers(); assert grp.nodes().size() > 0; registerListenerAndSendOrderedMessages(ignite, grp, async); }', 'output': 'private void clientServerOrderedMessage() throws Exception { IgniteEx ignite = grid(CLIENT_NODE_IDX); ClusterGroup grp = ignite.cluster().forServers(); assert grp.nodes().size() > 0; registerListenerAndSendOrderedMessages(ignite, grp); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RepairCompleteCallback(UUID parentSession, Collection<Range<Token>> successfulRanges, long startTime, TraceState traceState, AtomicBoolean hasFailure, ExecutorService executor) { this.parentSession = parentSession; this.successfulRanges = successfulRanges; this.startTime = startTime; this.traceState = traceState; this.hasFailure = hasFailure; this.executor = executor; }', 'ground_truth': 'public RepairCompleteCallback(UUID parentSession, Collection<Range<Token>> successfulRanges, Set<InetAddressAndPort> preparedEndpoints, long startTime, TraceState traceState, AtomicBoolean hasFailure, ExecutorService executor) { this.parentSession = parentSession; this.successfulRanges = successfulRanges; this.preparedEndpoints = preparedEndpoints; this.startTime = startTime; this.traceState = traceState; this.hasFailure = hasFailure; this.executor = executor; }', 'output': 'public RepairCompleteCallback(String parentSession, Collection<Range<Token>> successfulRanges, long startTime, TraceState traceState, AtomicBoolean hasFailure, ExecutorService executor) { this.parentSession = parentSession; this.successfulRanges = successfulRanges; this.startTime = startTime; this.traceState = traceState; this.hasFailure = hasFailure; this.executor = executor; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private AbstractStrategyHolder getHolder(long repairedAt, UUID pendingRepair) { return getHolder(repairedAt != ActiveRepairService.UNREPAIRED_SSTABLE, pendingRepair != ActiveRepairService.NO_PENDING_REPAIR); }', 'ground_truth': 'private AbstractStrategyHolder getHolder(long repairedAt, UUID pendingRepair, boolean isTransient) { return getHolder(repairedAt != ActiveRepairService.UNREPAIRED_SSTABLE, pendingRepair != ActiveRepairService.NO_PENDING_REPAIR, isTransient); }', 'output': 'private AbstractStrategyHolder getHolder(boolean isUnrepaired, UUID pendingRepair) { return getHolder(isUnrepaired, pendingRepair != ActiveRepairService.NO_PENDING_REPAIR); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void addActionArgs( final ObjectAction objectAction, final ActionDto actionDto, final Can<ManagedObject> argAdapters) { actionDto.setLogicalMemberIdentifier(IdentifierUtil.logicalMemberIdentifierForDeclaredMember(objectAction)); actionDto.setMemberIdentifier(IdentifierUtil.memberIdentifierFor(objectAction)); val actionParameters = objectAction.getParameters(); for (int paramNum = 0; paramNum < actionParameters.size(); paramNum++) { final ObjectActionParameter actionParameter = actionParameters.getElseFail(paramNum); val argAdapter = argAdapters.getElseFail(paramNum); val elementType = actionParameter.getElementType(); final ParamDto paramDto = new ParamDto(); paramDto.setName(actionParameter.getStaticFriendlyName() .orElseThrow(_Exceptions::unexpectedCodeReach)); actionParameter.getFeatureIdentifier(); if(actionParameter.getFeatureType() != FeatureType.ACTION_PARAMETER_COLLECTION) { valueMarshaller.recordParamScalar(paramDto, actionParameter, argAdapter); } else { val values = ManagedObjects.unpack(elementType, argAdapter); valueMarshaller.recordParamNonScalar(paramDto, actionParameter, values); } CommandDtoUtils.parametersFor(actionDto) .getParameter() .add(paramDto); } }', 'ground_truth': 'public void addActionArgs( final InteractionHead head, final ObjectAction objectAction, final ActionDto actionDto, final Can<ManagedObject> argAdapters) { actionDto.setLogicalMemberIdentifier(IdentifierUtil.logicalMemberIdentifierFor(head, objectAction)); actionDto.setMemberIdentifier(IdentifierUtil.memberIdentifierFor(objectAction)); val actionParameters = objectAction.getParameters(); for (int paramNum = 0; paramNum < actionParameters.size(); paramNum++) { final ObjectActionParameter actionParameter = actionParameters.getElseFail(paramNum); val argAdapter = argAdapters.getElseFail(paramNum); val elementType = actionParameter.getElementType(); final ParamDto paramDto = new ParamDto(); paramDto.setName(actionParameter.getStaticFriendlyName() .orElseThrow(_Exceptions::unexpectedCodeReach)); actionParameter.getFeatureIdentifier(); if(actionParameter.getFeatureType() != FeatureType.ACTION_PARAMETER_COLLECTION) { valueMarshaller.recordParamScalar(paramDto, actionParameter, argAdapter); } else { val values = ManagedObjects.unpack(elementType, argAdapter); valueMarshaller.recordParamNonScalar(paramDto, actionParameter, values); } CommandDtoUtils.parametersFor(actionDto) .getParameter() .add(paramDto); } }', 'output': 'public void addActionArgs( final ObjectAction objectAction, final ActionDto actionDto, final Can<ManagedObject> argAdapters) { actionDto.setLogicalMemberIdentifier(IdentifierUtil.logicalMemberIdentifierForDeclaredMember(objectAction)); actionDto.setMemberIdentifier(IdentifierUtil.memberIdentifierFor(objectAction)); val actionParameters = objectAction.getParameters(); for (int paramNum = 0; paramNum < actionParameters.size(); paramNum++) { final ObjectActionParameter actionParameter = actionParameters.getElseFail(paramNum); val argAdapter = argAdapters.getElseFail(paramNum); val elementType = actionParameter.getElementType(); final ParamDto paramDto = new ParamDto(); paramDto.setName(actionParameter.getStaticFriendlyName() .orElseThrow(_Exceptions::unexpectedCodeReach)); actionParameter.getFeatureIdentifier(); if(actionParameter.getFeatureType() != FeatureType.ACTION_PARAMETER_COLLECTION) { valueMarshaller.recordParamScalar(paramDto, actionParameter, argAdapter); } else { val values = ManagedObjects.unpack(elementType, argAdapter); valueMarshaller.recordParamNonScalar(paramDto, actionParameter, values); } CommandDtoUtils.parametersFor(actionDto) .getParameter() .add(paramDto); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private StatementRestrictions prepareRestrictions(CFMetaData cfm, VariableSpecifications boundNames, Selection selection) throws InvalidRequestException { try { return new StatementRestrictions(StatementType.SELECT, cfm, whereClause, boundNames, selection.containsOnlyStaticColumns(), selection.containsACollection(), parameters.allowFiltering); } catch (UnrecognizedEntityException e) { if (containsAlias(e.entity)) throw invalidRequest("Aliases aren\'t allowed in the where clause (\'%s\')", e.relation); throw e; } }', 'ground_truth': 'private StatementRestrictions prepareRestrictions(CFMetaData cfm, VariableSpecifications boundNames, Selection selection, boolean forView) throws InvalidRequestException { try { return new StatementRestrictions(StatementType.SELECT, cfm, whereClause, boundNames, selection.containsOnlyStaticColumns(), selection.containsACollection(), parameters.allowFiltering, forView); } catch (UnrecognizedEntityException e) { if (containsAlias(e.entity)) throw invalidRequest("Aliases aren\'t allowed in the where clause (\'%s\')", e.relation); throw e; } }', 'output': 'private StatementRestrictions prepareRestrictions(CFMetaData cfm, Collection<String> boundNames, Selection selection) throws InvalidRequestException { try { return new StatementRestrictions(StatementType.SELECT, cfm, whereClause, boundNames, selection.containsOnlyStaticColumns(), selection.containsACollection(), parameters.allowFiltering); } catch (UnrecognizedEntityException e) { if (containsAlias(e.entity)) throw invalidRequest("Aliases aren\'t allowed in the where clause (\'%s\')", e.relation); throw e; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Map<T, Object> decode(ByteBuf body) { EnumMap<T, Object> options = new EnumMap<T, Object>(klass); int n = body.readUnsignedShort(); for (int i = 0; i < n; i++) { T opt = fromId(body.readUnsignedShort()); Object value = opt.readValue(body); if (options.containsKey(opt)) throw new ProtocolException(String.format("Duplicate option %s in message", opt.name())); options.put(opt, value); } return options; } public Pair<T, Object> decodeOne(ByteBuf body)\n    { { T opt = fromId(body.readUnsignedShort()); Object value = opt.readValue(body); return Pair.create(opt, value); }', 'ground_truth': 'public Map<T, Object> decode(ByteBuf body, int version) { EnumMap<T, Object> options = new EnumMap<T, Object>(klass); int n = body.readUnsignedShort(); for (int i = 0; i < n; i++) { T opt = fromId(body.readUnsignedShort()); Object value = opt.readValue(body, version); if (options.containsKey(opt)) throw new ProtocolException(String.format("Duplicate option %s in message", opt.name())); options.put(opt, value); } return options; }', 'output': 'public Map<T, Object> decode(ByteBuf body) { EnumMap<T, Object> options = new EnumMap<T, Object>(klass); int n = body.readUnsignedShort(); for (int i = 0; i < n; i++) { T opt = fromId(body.readUnsignedShort()); Object value = opt.readValue(body); if (options.containsKey(opt)) throw new ProtocolException(String.format("Duplicate option %s in message", opt.name())); options.put(opt, value); } return options; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Service addService(String serviceName) throws AmbariException { if (services.containsKey(serviceName)) { throw new AmbariException("Service already exists" + ", clusterName=" + getClusterName() + ", clusterId=" + getClusterId() + ", serviceName=" + serviceName); } Service service = serviceFactory.createNew(this, serviceName); addService(service); return service; }', 'ground_truth': 'public Service addService(String serviceName, RepositoryVersionEntity repositoryVersion) throws AmbariException { if (services.containsKey(serviceName)) { String message = MessageFormat.format("The {0} service already exists in {1}", serviceName, getClusterName()); throw new AmbariException(message); } @Experimental(feature = ExperimentalFeature.PATCH_UPGRADES) Service service = serviceFactory.createNew(this, serviceName, repositoryVersion); addService(service); return service; }', 'output': 'public Service addService(String serviceName, String description) throws AmbariException { if (services.containsKey(serviceName)) { throw new AmbariException("Service already exists" + ", clusterName=" + getClusterName() + ", clusterId=" + getClusterId() + ", serviceName=" + serviceName); } Service service = serviceFactory.createNew(this, serviceName, description); addService(service); return service; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public IntervalSqlType( SqlIntervalQualifier intervalQualifier, boolean isNullable) { super( intervalQualifier.isYearMonth() ? SqlTypeName.INTERVAL_YEAR_MONTH : SqlTypeName.INTERVAL_DAY_TIME, isNullable, null); this.intervalQualifier = intervalQualifier; computeDigest(); }', 'ground_truth': 'public IntervalSqlType(RelDataTypeSystem typeSystem, SqlIntervalQualifier intervalQualifier, boolean isNullable) { super(intervalQualifier.isYearMonth() ? SqlTypeName.INTERVAL_YEAR_MONTH : SqlTypeName.INTERVAL_DAY_TIME, isNullable, null); this.typeSystem = typeSystem; this.intervalQualifier = intervalQualifier; computeDigest(); }', 'output': 'public IntervalSqlType( SqlIntervalQualifier intervalQualifier, boolean isNullable, int precision) { super( intervalQualifier.isYearMonth() ? SqlTypeName.INTERVAL_YEAR_MONTH : SqlTypeName.INTERVAL_DAY_TIME, isNullable, precision); this.intervalQualifier = intervalQualifier; computeDigest(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ResourceId windowedFilename( ResourceId outputDirectory, WindowedContext context, String extension) { IntervalWindow window = (IntervalWindow) context.getWindow(); String filename = String.format( "%s-%s-of-%s%s", filenamePrefixForWindow(window), context.getShardNumber(), context.getNumShards(), extension); return outputDirectory.resolve(filename, StandardResolveOptions.RESOLVE_FILE); }', 'ground_truth': 'public ResourceId windowedFilename(WindowedContext context, OutputFileHints outputFileHints) { IntervalWindow window = (IntervalWindow) context.getWindow(); String filename = String.format( "%s-%s-of-%s%s", filenamePrefixForWindow(window), context.getShardNumber(), context.getNumShards(), outputFileHints.getSuggestedFilenameSuffix()); return baseFilename .getCurrentDirectory() .resolve(filename, StandardResolveOptions.RESOLVE_FILE); }', 'output': 'public ResourceId windowedFilename( ResourceIdentifier outputDirectory, WindowedContext context, String extension) { IntervalWindow window = (IntervalWindow) context.getWindow(); String filename = String.format( "%s-%s-of-%s%s", filenamePrefixForWindow(window), context.getShardNumber(), context.getNumShards(), extension); return outputDirectory.resolve(filename, StandardResolveOptions.RESOLVE_FILE); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public <K, V> Iterator<Map.Entry<K, V>> lazyOffHeapIterator() { if (!offheapEnabled) return new GridEmptyCloseableIterator<>(); return lazyIterator(offheap.iterator(spaceName)); }', 'ground_truth': 'public <K, V> Iterator<Map.Entry<K, V>> lazyOffHeapIterator(boolean keepBinary) { if (!offheapEnabled) return new GridEmptyCloseableIterator<>(); return lazyIterator(offheap.iterator(spaceName), keepBinary); }', 'output': 'public <K, V> Iterator<Map.Entry<K, V>> lazyOffHeapIterator() { if (!offheapEnabled) return new GridEmptyCloseableIterator<>(); return lazyIterator(offheap.iterator(spaceName)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void addEvent(int part, K key, UUID evtNodeId, @Nullable GridCacheMvccCandidate<K> owner, int type, @Nullable V newVal, boolean hasNewVal, V oldVal, boolean hasOldVal, UUID subjId) { GridCacheTx tx = owner == null ? null : cctx.tm().tx(owner.version()); addEvent(part, key, evtNodeId, tx == null ? null : tx.xid(), owner == null ? null : owner.version(), type, newVal, hasNewVal, oldVal, hasOldVal, subjId); }', 'ground_truth': 'public void addEvent(int part, K key, UUID evtNodeId, @Nullable GridCacheMvccCandidate<K> owner, int type, @Nullable V newVal, boolean hasNewVal, V oldVal, boolean hasOldVal, UUID subjId, String cloClsName) { GridCacheTx tx = owner == null ? null : cctx.tm().tx(owner.version()); addEvent(part, key, evtNodeId, tx == null ? null : tx.xid(), owner == null ? null : owner.version(), type, newVal, hasNewVal, oldVal, hasOldVal, subjId, cloClsName); }', 'output': 'public void addEvent(int part, K key, UUID evtNodeId, @Nullable IgniteTx tx, @Nullable CacheMvccCandidate<K> owner, int type, @Nullable V newVal, boolean hasNewVal, V oldVal, boolean hasOldVal, UUID subjId) { GridCacheTx tx0 = owner == null ? null : cctx.tm().tx(owner.version()); addEvent(part, key, evtNodeId, tx0 == null ? null : tx0.xid(), owner == null ? null : owner.version(), type, newVal, hasNewVal, oldVal, hasOldVal, subjId); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean deleteRecursive(File f) { BasicFileAttributes attrs; try { attrs = Files.readAttributes(f.toPath(), BasicFileAttributes.class, LinkOption.NOFOLLOW_LINKS); } catch (IOException ignore) { return false; } if (!attrs.isDirectory() || attrs.isSymbolicLink()) return f.delete(); File[] entries = f.listFiles(); if (entries != null) { for (File entry : entries) { boolean res = deleteRecursive(entry); if (!res) return false; } } return f.delete(); }', 'ground_truth': 'private boolean deleteRecursive(File f, boolean deleteIfExists) { BasicFileAttributes attrs; try { attrs = Files.readAttributes(f.toPath(), BasicFileAttributes.class, LinkOption.NOFOLLOW_LINKS); } catch (IOException ignore) { return deleteIfExists && !f.exists(); } if (!attrs.isDirectory() || attrs.isSymbolicLink()) return f.delete() || (deleteIfExists && !f.exists()); File[] entries = f.listFiles(); if (entries != null) { for (File entry : entries) { boolean res = deleteRecursive(entry, true); if (!res) return false; } } return f.delete() || (deleteIfExists && !f.exists()); }', 'output': 'private boolean deleteRecursive(File f) { BasicFileAttributes attrs; try { attrs = Files.readAttributes(f.toPath(), BasicFileAttributes.class, LinkOption.NOFOLLOW_LINKS); } catch (IOException ignore) { return false; } if (!attrs.isDirectory() || attrs.isSymbolicLink()) return f.delete(); File[] entries = f.listFiles(); if (entries != null) { for (File entry : entries) { boolean res = deleteRecursive(entry); if (!res) return false; } } return f.delete(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private GridCacheTx txStartGroupLock(GridCacheContext ctx, Object grpLockKey, GridCacheTxConcurrency concurrency, GridCacheTxIsolation isolation, boolean partLock, long timeout, int txSize) throws IllegalStateException, IgniteCheckedException { GridCacheTx tx = cctx.tm().userTx(); if (tx != null) throw new IllegalStateException("Failed to start new transaction " + "(current thread already has a transaction): " + tx); GridCacheTxLocalAdapter<K, V> tx0 = cctx.tm().newTx( false, false, concurrency, isolation, timeout, ctx.hasFlag(INVALIDATE), !ctx.hasFlag(SKIP_STORE), txSize, ctx.txKey(grpLockKey), partLock ); assert tx0 != null; if (ctx.hasFlag(SYNC_COMMIT)) tx0.syncCommit(true); IgniteFuture<?> lockFut = tx0.groupLockAsync(ctx, (Collection)F.asList(grpLockKey)); try { lockFut.get(); } catch (IgniteCheckedException e) { tx0.rollback(); throw e; } return new GridCacheTxProxyImpl<>(tx0, cctx); }', 'ground_truth': 'private GridCacheTx txStartGroupLock(GridCacheContext ctx, Object grpLockKey, GridCacheTxConcurrency concurrency, GridCacheTxIsolation isolation, boolean partLock, long timeout, int txSize, boolean sys) throws IllegalStateException, IgniteCheckedException { GridCacheTx tx = cctx.tm().userTx(); if (tx != null) throw new IllegalStateException("Failed to start new transaction " + "(current thread already has a transaction): " + tx); GridCacheTxLocalAdapter<K, V> tx0 = cctx.tm().newTx( false, false, sys, concurrency, isolation, timeout, ctx.hasFlag(INVALIDATE), !ctx.hasFlag(SKIP_STORE), txSize, ctx.txKey(grpLockKey), partLock ); assert tx0 != null; if (ctx.hasFlag(SYNC_COMMIT)) tx0.syncCommit(true); IgniteFuture<?> lockFut = tx0.groupLockAsync(ctx, (Collection)F.asList(grpLockKey)); try { lockFut.get(); } catch (IgniteCheckedException e) { tx0.rollback(); throw e; } return new GridCacheTxProxyImpl<>(tx0, cctx); }', 'output': 'private GridCacheTx txStartGroupLock(GridCacheContext ctx, Object grpLockKey, GridCacheTxConcurrency concurrency, GridCacheTxIsolation isolation, boolean partLock, long timeout, int txSize) throws IllegalStateException, IgniteCheckedException { GridCacheTx tx = cctx.tm().userTx(); if (tx != null) throw new IllegalStateException("Failed to start new transaction " + "(current thread already has a transaction): " + tx); GridCacheTxLocalAdapter<K, V> tx0 = cctx.tm().newTx( false, false, concurrency, isolation, timeout, ctx.hasFlag(INVALIDATE), !ctx.hasFlag(SKIP_STORE), txSize, ctx.txKey(grpLockKey), partLock ); assert tx0 != null; if (ctx.hasFlag(SYNC_COMMIT)) tx0.syncCommit(true); IgniteFuture<?> lockFut = tx0.groupLockAsync(ctx, (Collection)F.asList(grpLockKey)); try { lockFut.get(); } catch (IgniteCheckedException e) { tx0.rollback(); throw e; } return new GridCacheTxProxyImpl<>(tx0, cctx); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private QueryCursorImpl<List<?>> executeSelectForDml( String schema, SqlFieldsQuery selectQry, MvccQueryTracker mvccTracker, GridQueryCancel cancel, int timeout ) throws IgniteCheckedException { QueryParserResult parseRes = parser.parse(schema, selectQry, false); QueryParserResultSelect select = parseRes.select(); assert select != null; Iterable<List<?>> iter = executeSelect0( parseRes.queryDescriptor(), parseRes.queryParameters(), select, true, mvccTracker, cancel, false, timeout ); QueryCursorImpl<List<?>> cursor = new QueryCursorImpl<>(iter, cancel, true, parseRes.queryParameters().lazy()); cursor.fieldsMeta(select.meta()); cursor.partitionResult(select.twoStepQuery() != null ? select.twoStepQuery().derivedPartitions() : null); return cursor; }', 'ground_truth': 'private QueryCursorImpl<List<?>> executeSelectForDml( long qryId, String schema, SqlFieldsQuery selectQry, MvccQueryTracker mvccTracker, GridQueryCancel cancel, int timeout ) throws IgniteCheckedException { QueryParserResult parseRes = parser.parse(schema, selectQry, false); QueryParserResultSelect select = parseRes.select(); assert select != null; Iterable<List<?>> iter = executeSelect0( qryId, parseRes.queryDescriptor(), parseRes.queryParameters(), select, true, mvccTracker, cancel, false, timeout ); QueryCursorImpl<List<?>> cursor = new QueryCursorImpl<>(iter, cancel, true, parseRes.queryParameters().lazy()); cursor.fieldsMeta(select.meta()); cursor.partitionResult(select.twoStepQuery() != null ? select.twoStepQuery().derivedPartitions() : null); return cursor; }', 'output': 'private QueryCursorImpl<List<?>> executeSelectForDml( String schema, SqlFieldsQuery selectQry, MvccQueryTracker mvccTracker, GridQueryCancel cancel, int timeout ) throws IgniteCheckedException { QueryParserResult parseRes = parser.parse(schema, selectQry, false); QueryParserResultSelect select = parseRes.select(); assert select != null; Iterable<List<?>> iter = executeSelect0( parseRes.queryDescriptor(), parseRes.queryParameters(), select, true, mvccTracker, cancel, false, timeout ); QueryCursorImpl<List<?>> cursor = new QueryCursorImpl<>(iter, cancel, true, parseRes.queryParameters().lazy()); cursor.fieldsMeta(select.meta()); cursor.partitionResult(select.twoStepQuery() != null ? select.twoStepQuery().derivedPartitions() : null); return cursor; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GridDistributedLockResponse(int cacheId, GridCacheVersion lockVer, IgniteUuid futId, int cnt) { super(lockVer, cnt); assert futId != null; this.cacheId = cacheId; this.futId = futId; vals = new ArrayList<>(cnt); } public GridDistributedLockResponse(int cacheId,\n        GridCacheVersion lockVer,\n        IgniteUuid futId,\n        int cnt,\n        Throwable err) { GridCacheVersion lockVer, IgniteUuid futId, int cnt, Throwable err) { super(lockVer, cnt); assert futId != null; this.cacheId = cacheId; this.futId = futId; this.err = err; vals = new ArrayList<>(cnt); }', 'ground_truth': 'public GridDistributedLockResponse(int cacheId, GridCacheVersion lockVer, IgniteUuid futId, int cnt, boolean addDepInfo) { super(lockVer, cnt, addDepInfo); assert futId != null; this.cacheId = cacheId; this.futId = futId; vals = new ArrayList<>(cnt); }', 'output': 'public GridDistributedLockResponse(int cacheId, GridCacheVersion lockVer, IgniteUuid futId, int cnt) { super(lockVer, cnt); assert futId != null; this.cacheId = cacheId; this.futId = futId; vals = new ArrayList<>(cnt); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public UseOfSynchronizedWithWait(TypeCodec<Object> returnDataType, TypeCodec<Object>[] argDataTypes) { super(returnDataType, argDataTypes); }', 'ground_truth': 'public UseOfSynchronizedWithWait(TypeCodec<Object> returnDataType, TypeCodec<Object>[] argDataTypes, UDFContext udfContext) { super(returnDataType, argDataTypes, udfContext); }', 'output': 'public UseOfSynchronizedWithWait(TypeCodec<Object> returnDataType, TypeCodec<Object>[] argDataTypes, String name) { super(returnDataType, argDataTypes, name); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ReuseListImpl( int cacheId, String name, PageMemory pageMem, IgniteWriteAheadLogManager wal, long metaPageId, boolean initNew, PageLockListener lockLsnr, GridKernalContext ctx ) throws IgniteCheckedException { super( cacheId, name, pageMem, 1, wal, metaPageId, lockLsnr, ctx ); reuseList = this; init(metaPageId, initNew); }', 'ground_truth': 'public ReuseListImpl( int cacheId, String name, PageMemory pageMem, IgniteWriteAheadLogManager wal, long metaPageId, boolean initNew, PageLockListener lockLsnr, GridKernalContext ctx, AtomicLong pageListCacheLimit ) throws IgniteCheckedException { super( cacheId, name, pageMem, 1, wal, metaPageId, lockLsnr, ctx ); bucketCache = new PagesCache(pageListCacheLimit); reuseList = this; init(metaPageId, initNew); }', 'output': 'public ReuseListImpl( int cacheId, String name, PageMemory pageMem, IgniteWriteAheadLogManager wal, long metaPageId, boolean initNew, PageLockListener lockLsnr, GridKernalContext ctx ) throws IgniteCheckedException { super( cacheId, name, pageMem, 1, wal, metaPageId, lockLsnr, ctx ); reuseList = this; init(metaPageId, initNew); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GridDhtFuture<Boolean> lockAllAsyncInternal(@Nullable Collection<KeyCacheObject> keys, long timeout, IgniteTxLocalEx txx, boolean isInvalidate, boolean isRead, boolean retval, TransactionIsolation isolation, long accessTtl, CacheEntryPredicate[] filter, boolean skipStore, boolean keepBinary) { if (keys == null || keys.isEmpty()) return new GridDhtFinishedFuture<>(true); GridDhtTxLocalAdapter tx = (GridDhtTxLocalAdapter)txx; assert tx != null; GridDhtLockFuture fut = new GridDhtLockFuture( ctx, tx.nearNodeId(), tx.nearXidVersion(), tx.topologyVersion(), keys.size(), isRead, retval, timeout, tx, tx.threadId(), accessTtl, filter, skipStore, keepBinary); for (KeyCacheObject key : keys) { try { while (true) { GridDhtCacheEntry entry = entryExx(key, tx.topologyVersion()); try { fut.addEntry(entry); if (fut.isDone()) return fut; break; } catch (GridCacheEntryRemovedException ignore) { if (log.isDebugEnabled()) log.debug("Got removed entry when adding lock (will retry): " + entry); } catch (GridDistributedLockCancelledException e) { if (log.isDebugEnabled()) log.debug("Failed to add entry [err=" + e + ", entry=" + entry + \']\'); return new GridDhtFinishedFuture<>(e); } } } catch (GridDhtInvalidPartitionException e) { fut.addInvalidPartition(ctx, e.partition()); if (log.isDebugEnabled()) log.debug("Added invalid partition to DHT lock future [part=" + e.partition() + ", fut=" + fut + \']\'); } } ctx.mvcc().addFuture(fut); fut.map(); return fut; }', 'ground_truth': 'public GridDhtFuture<Boolean> lockAllAsyncInternal(@Nullable Collection<KeyCacheObject> keys, long timeout, IgniteTxLocalEx txx, boolean isInvalidate, boolean isRead, boolean retval, TransactionIsolation isolation, long createTtl, long accessTtl, CacheEntryPredicate[] filter, boolean skipStore, boolean keepBinary) { if (keys == null || keys.isEmpty()) return new GridDhtFinishedFuture<>(true); GridDhtTxLocalAdapter tx = (GridDhtTxLocalAdapter)txx; assert tx != null; GridDhtLockFuture fut = new GridDhtLockFuture( ctx, tx.nearNodeId(), tx.nearXidVersion(), tx.topologyVersion(), keys.size(), isRead, retval, timeout, tx, tx.threadId(), createTtl, accessTtl, filter, skipStore, keepBinary); for (KeyCacheObject key : keys) { try { while (true) { GridDhtCacheEntry entry = entryExx(key, tx.topologyVersion()); try { fut.addEntry(entry); if (fut.isDone()) return fut; break; } catch (GridCacheEntryRemovedException ignore) { if (log.isDebugEnabled()) log.debug("Got removed entry when adding lock (will retry): " + entry); } catch (GridDistributedLockCancelledException e) { if (log.isDebugEnabled()) log.debug("Failed to add entry [err=" + e + ", entry=" + entry + \']\'); return new GridDhtFinishedFuture<>(e); } } } catch (GridDhtInvalidPartitionException e) { fut.addInvalidPartition(ctx, e.partition()); if (log.isDebugEnabled()) log.debug("Added invalid partition to DHT lock future [part=" + e.partition() + ", fut=" + fut + \']\'); } } ctx.mvcc().addFuture(fut); fut.map(); return fut; }', 'output': 'public GridDhtFuture<Boolean> lockAllAsyncInternal(@Nullable Collection<KeyCacheObject> keys, long timeout, IgniteTxLocalEx txx, boolean isInvalidate, boolean isRead, boolean retval, TransactionIsolation isolation, long accessTtl, CacheEntryPredicate[] filter, boolean skipStore, boolean keepBinary) { if (keys == null || keys.isEmpty()) return new GridDhtFinishedFuture<>(true); GridDhtTxLocalAdapter tx = (GridDhtTxLocalAdapter)txx; assert tx != null; GridDhtLockFuture fut = new GridDhtLockFuture( ctx, tx.nearNodeId(), tx.nearXidVersion(), tx.topologyVersion(), keys.size(), isRead, retval, timeout, tx, tx.threadId(), accessTtl, filter, skipStore, keepBinary); for (KeyCacheObject key : keys) { try { while (true) { GridDhtCacheEntry entry = entryExx(key, tx.topologyVersion()); try { fut.addEntry(entry); if (fut.isDone()) return fut; break; } catch (GridCacheEntryRemovedException ignore) { if (log.isDebugEnabled()) log.debug("Got removed entry when adding lock (will retry): " + entry); } catch (GridDistributedLockCancelledException e) { if (log.isDebugEnabled()) log.debug("Failed to add entry [err=" + e + ", entry=" + entry + \']\'); return new GridDhtFinishedFuture<>(e); } } } catch (GridDhtInvalidPartitionException e) { fut.addInvalidPartition(ctx, e.partition()); if (log.isDebugEnabled()) log.debug("Added invalid partition to DHT lock future [part=" + e.partition() + ", fut=" + fut + \']\'); } } ctx.mvcc().addFuture(fut); fut.map(); return fut; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public CacheDataStoreImpl( int partId, CacheDataRowStore rowStore, CacheDataTree dataTree, Supplier<PendingEntriesTree> pendingEntries, CacheGroupContext grp, GridSpinBusyLock busyLock, IgniteLogger log ) { this.partId = partId; this.rowStore = rowStore; this.dataTree = dataTree; this.pendingEntries = pendingEntries; this.grp = grp; this.busyLock = busyLock; this.log = log; PartitionUpdateCounter delegate = grp.mvccEnabled() ? new PartitionUpdateCounterMvccImpl(grp) : !grp.persistenceEnabled() || grp.hasAtomicCaches() ? new PartitionUpdateCounterVolatileImpl(grp) : new PartitionUpdateCounterTrackingImpl(grp); pCntr = grp.shared().logger(PartitionUpdateCounterDebugWrapper.class).isDebugEnabled() ? new PartitionUpdateCounterDebugWrapper(partId, delegate) : new PartitionUpdateCounterErrorWrapper(partId, delegate); updateValSizeThreshold = grp.shared().database().pageSize() / 2; mvccUpdateMarker = new MvccMarkUpdatedHandler(grp); mvccUpdateTxStateHint = new MvccUpdateTxStateHintHandler(grp); mvccApplyChanges = new MvccApplyChangesHandler(grp); }', 'ground_truth': 'public CacheDataStoreImpl( int partId, CacheDataRowStore rowStore, CacheDataTree dataTree, Supplier<PendingEntriesTree> pendingEntries, CacheGroupContext grp, GridSpinBusyLock busyLock, IgniteLogger log, @Nullable Supplier<GridQueryRowCacheCleaner> cleaner ) { this.partId = partId; this.rowStore = rowStore; this.dataTree = dataTree; this.pendingEntries = pendingEntries; this.grp = grp; this.busyLock = busyLock; this.log = log; PartitionUpdateCounter delegate = grp.mvccEnabled() ? new PartitionUpdateCounterMvccImpl(grp) : !grp.persistenceEnabled() || grp.hasAtomicCaches() ? new PartitionUpdateCounterVolatileImpl(grp) : new PartitionUpdateCounterTrackingImpl(grp); pCntr = grp.shared().logger(PartitionUpdateCounterDebugWrapper.class).isDebugEnabled() ? new PartitionUpdateCounterDebugWrapper(partId, delegate) : new PartitionUpdateCounterErrorWrapper(partId, delegate); updateValSizeThreshold = grp.shared().database().pageSize() / 2; mvccUpdateMarker = new MvccMarkUpdatedHandler(grp); mvccUpdateTxStateHint = new MvccUpdateTxStateHintHandler(grp); mvccApplyChanges = new MvccApplyChangesHandler(grp); if (cleaner == null) rowStore.setRowCacheCleaner(() -> rowCacheCleaner); else rowStore.setRowCacheCleaner(cleaner); }', 'output': 'public CacheDataStoreImpl( int partId, CacheDataRowStore rowStore, CacheDataTree dataTree, Supplier<PendingEntriesTree> pendingEntries, CacheGroupContext grp, GridSpinBusyLock busyLock, IgniteLogger log ) { this.partId = partId; this.rowStore = rowStore; this.dataTree = dataTree; this.pendingEntries = pendingEntries; this.grp = grp; this.busyLock = busyLock; this.log = log; PartitionUpdateCounter delegate = grp.mvccEnabled() ? new PartitionUpdateCounterMvccImpl(grp) : !grp.persistenceEnabled() || grp.hasAtomicCaches() ? new PartitionUpdateCounterVolatileImpl(grp) : new PartitionUpdateCounterTrackingImpl(grp); pCntr = grp.shared().logger(PartitionUpdateCounterDebugWrapper.class).isDebugEnabled() ? new PartitionUpdateCounterDebugWrapper(partId, delegate) : new PartitionUpdateCounterErrorWrapper(partId, delegate); updateValSizeThreshold = grp.shared().database().pageSize() / 2; mvccUpdateMarker = new MvccMarkUpdatedHandler(); mvccUpdateTxStateHint = new MvccUpdateTxStateHintHandler(); mvccApplyChanges = new MvccApplyChangesHandler(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Object readSideInput( PipelineOptions options, SideInputInfo sideInputInfo, Observer observer, ExecutionContext executionContext) throws Exception { Iterable<Object> elements = readSideInputSources(options, sideInputInfo.getSources(), observer, executionContext); return readSideInputValue(sideInputInfo.getKind(), elements); } public static Object readSideInput(\n      PipelineOptions options,\n      SideInputInfo sideInputInfo,\n      ExecutionContext executionContext)\n      throws Exception { PipelineOptions options, SideInputInfo sideInputInfo, ExecutionContext executionContext) throws Exception { Iterable<Object> elements = readSideInputSources(options, sideInputInfo.getSources(), null, executionContext); return readSideInputValue(sideInputInfo.getKind(), elements); }', 'ground_truth': 'public static Object readSideInput( PipelineOptions options, SideInputInfo sideInputInfo, ReaderFactory readerFactory, ExecutionContext executionContext) throws Exception { Iterable<Object> elements = readSideInputSources( options, sideInputInfo.getSources(), readerFactory, null, executionContext); return readSideInputValue(sideInputInfo.getKind(), elements); }', 'output': 'public static Object readSideInput( PipelineOptions options, SideInputInfo sideInputInfo, Observer observer, ExecutionContext executionContext) throws Exception { Iterable<Object> elements = readSideInputSources(options, sideInputInfo.getSources(), observer, executionContext); return readSideInputValue(sideInputInfo.getKind(), elements); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Map<ByteBuffer, ColumnDefinition> fromThrift(List<ColumnDef> thriftDefs) throws SyntaxException, ConfigurationException { if (thriftDefs == null) return new HashMap<ByteBuffer,ColumnDefinition>(); Map<ByteBuffer, ColumnDefinition> cds = new TreeMap<ByteBuffer, ColumnDefinition>(); for (ColumnDef thriftColumnDef : thriftDefs) cds.put(ByteBufferUtil.clone(thriftColumnDef.name), fromThrift(thriftColumnDef)); return cds; }', 'ground_truth': 'public static Map<ByteBuffer, ColumnDefinition> fromThrift(List<ColumnDef> thriftDefs, boolean isSuper) throws SyntaxException, ConfigurationException { if (thriftDefs == null) return new HashMap<ByteBuffer,ColumnDefinition>(); Map<ByteBuffer, ColumnDefinition> cds = new TreeMap<ByteBuffer, ColumnDefinition>(); for (ColumnDef thriftColumnDef : thriftDefs) cds.put(ByteBufferUtil.clone(thriftColumnDef.name), fromThrift(thriftColumnDef, isSuper)); return cds; }', 'output': 'public static Map<ByteBuffer, ColumnMetadata> fromThrift(List<ColumnDef> thriftDefs) throws SyntaxException, ConfigurationException { if (thriftDefs == null) return new HashMap<ByteBuffer,ColumnMetadata>(); Map<ByteBuffer, ColumnMetadata> cds = new TreeMap<ByteBuffer, ColumnMetadata>(); for (ColumnDef thriftColumnDef : thriftDefs) cds.put(ByteBufferUtil.clone(thriftColumnDef.name), fromThrift(thriftColumnDef)); return cds; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean onComplete() { Throwable err0 = err; if (err0 == null || tx.needCheckBackup()) tx.state(PREPARED); if (super.onDone(tx, err0)) { cctx.mvcc().removeMvccFuture(this); return true; } return false; }', 'ground_truth': 'private boolean onComplete(boolean discoThread) { Throwable err0 = err; if (err0 == null || tx.needCheckBackup()) tx.state(PREPARED); if (super.onDone(tx, err0)) { cctx.mvcc().removeMvccFuture(this); return true; } return false; }', 'output': 'private boolean onComplete() { Throwable err0 = err; if (err0 == null || !tx.needCheckBackup()) tx.state(PREPARED); if (super.onDone(tx, err0)) { cctx.mvcc().removeMvccFuture(this); return true; } return false; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void finishUpdate(CacheDataRow newRow, @Nullable CacheDataRow oldRow) throws IgniteCheckedException { if (oldRow == null) storageSize.incrementAndGet(); KeyCacheObject key = newRow.key(); long expireTime = newRow.expireTime(); GridCacheQueryManager qryMgr = cctx.queries(); if (qryMgr.enabled()) { if (oldRow != null) { qryMgr.store(key, partId, oldRow.value(), oldRow.version(), newRow.value(), newRow.version(), expireTime, newRow.link()); } else { qryMgr.store(key, partId, null, null, newRow.value(), newRow.version(), expireTime, newRow.link()); } } if (oldRow != null) { assert oldRow.link() != 0 : oldRow; if (pendingEntries != null && oldRow.expireTime() != 0) pendingEntries.removex(new PendingRow(oldRow.expireTime(), oldRow.link())); if (newRow.link() != oldRow.link()) rowStore.removeRow(oldRow.link()); } if (pendingEntries != null && expireTime != 0) { pendingEntries.putx(new PendingRow(expireTime, newRow.link())); hasPendingEntries = true; } updateIgfsMetrics(key, (oldRow != null ? oldRow.value() : null), newRow.value()); }', 'ground_truth': 'private void finishUpdate(GridCacheContext cctx, CacheDataRow newRow, @Nullable CacheDataRow oldRow) throws IgniteCheckedException { if (oldRow == null) incrementSize(cctx.cacheId()); KeyCacheObject key = newRow.key(); long expireTime = newRow.expireTime(); GridCacheQueryManager qryMgr = cctx.queries(); int cacheId = grp.sharedGroup() ? cctx.cacheId() : UNDEFINED_CACHE_ID; if (qryMgr.enabled()) { if (oldRow != null) { qryMgr.store(key, partId, oldRow.value(), oldRow.version(), newRow.value(), newRow.version(), expireTime, newRow.link()); } else { qryMgr.store(key, partId, null, null, newRow.value(), newRow.version(), expireTime, newRow.link()); } } if (oldRow != null) { assert oldRow.link() != 0 : oldRow; if (pendingEntries != null && oldRow.expireTime() != 0) pendingEntries.removex(new PendingRow(cacheId, oldRow.expireTime(), oldRow.link())); if (newRow.link() != oldRow.link()) rowStore.removeRow(oldRow.link()); } if (pendingEntries != null && expireTime != 0) { pendingEntries.putx(new PendingRow(cacheId, expireTime, newRow.link())); hasPendingEntries = true; } updateIgfsMetrics(cctx, key, (oldRow != null ? oldRow.value() : null), newRow.value()); }', 'output': 'private void finishUpdate(CacheDataRow newRow, @Nullable CacheDataRow oldRow) throws IgniteCheckedException { if (oldRow == null) storageSize.incrementAndGet(); KeyCacheObject key = newRow.key(); long expireTime = newRow.expireTime(); GridCacheQueryManager qryMgr = cctx.queries(); if (qryMgr.enabled()) { if (oldRow != null) { qryMgr.store(key, partId, oldRow.value(), oldRow.version(), newRow.value(), newRow.version(), expireTime, newRow.link()); } else { qryMgr.store(key, partId, null, null, newRow.value(), newRow.version(), expireTime, newRow.link()); } } if (oldRow != null) { assert oldRow.link() != 0 : oldRow; if (pendingEntries != null && oldRow.expireTime() != 0) pendingEntries.removex(new PendingRow(oldRow.expireTime(), oldRow.link())); if (newRow.link() != oldRow.link()) rowStore.removeRow(oldRow.link()); } if (pendingEntries != null && expireTime != 0) { pendingEntries.putx(new PendingRow(expireTime, newRow.link())); hasPendingEntries = true; } updateIgfsMetrics(key, (oldRow != null ? oldRow.value() : null), newRow.value()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static PatchLocation createPatchLocation( @NonNull final String modName, @NonNull final List<? extends URL> roots) throws IOException { Collection<URL> bin = new ArrayList<>(roots.size()); Collection<URL> src = new ArrayList<>(roots.size()); for (URL root : roots) { if (JavaIndex.hasSourceCache(root, false)) { src.add(root); bin.add(FileUtil.urlForArchiveOrDir(JavaIndex.getClassFolder(root))); } else { bin.add(root); } } return new PatchLocation( StandardLocation.PATCH_MODULE_PATH, bin, src, modName); }', 'ground_truth': 'private static PatchLocation createPatchLocation( @NonNull final String modName, @NonNull final List<? extends URL> roots, final boolean sourceOverride) throws IOException { Collection<URL> bin = new ArrayList<>(roots.size()); Collection<URL> src = new ArrayList<>(roots.size()); for (URL root : roots) { if (JavaIndex.hasSourceCache(root, false)) { src.add(root); bin.add(FileUtil.urlForArchiveOrDir(JavaIndex.getClassFolder(root))); } else { if (sourceOverride) { bin.addAll(Arrays.asList(BinaryForSourceQuery.findBinaryRoots(root).getRoots())); } else { bin.add(root); } } } return new PatchLocation( StandardLocation.PATCH_MODULE_PATH, bin, src, modName); }', 'output': 'private static PatchLocation createPatchLocation( @NonNull final String modName, @NonNull final List<? extends URL> roots) throws IOException { Collection<URL> bin = new ArrayList<>(roots.size()); Collection<URL> src = new ArrayList<>(roots.size()); for (URL root : roots) { if (JavaIndex.hasSourceCache(root, false)) { src.add(root); bin.add(FileUtil.urlForArchiveOrDir(JavaIndex.getClassFolder(root))); } else { bin.add(root); } } return new PatchLocation( StandardLocation.PATCH_MODULE_PATH, bin, src, modName); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void processAffinityAssignmentResponse(UUID nodeId, GridDhtAffinityAssignmentResponse res) { if (log.isDebugEnabled()) log.debug("Processing affinity assignment response [node=" + nodeId + ", res=" + res + \']\'); GridDhtAssignmentFetchFuture fut = pendingAssignmentFetchFuts.get(res.futureId()); if (fut != null) fut.onResponse(nodeId, res); }', 'ground_truth': 'private void processAffinityAssignmentResponse(Integer grpId, UUID nodeId, GridDhtAffinityAssignmentResponse res) { if (log.isDebugEnabled()) log.debug("Processing affinity assignment response [node=" + nodeId + ", res=" + res + \']\'); GridDhtAssignmentFetchFuture fut = pendingAssignmentFetchFuts.get(res.futureId()); if (fut != null) fut.onResponse(nodeId, res); }', 'output': 'private void processAffinityAssignmentResponse(UUID nodeId, AffinityAssignmentResponse res) { if (log.isDebugEnabled()) log.debug("Processing affinity assignment response [node=" + nodeId + ", res=" + res + \']\'); GridDhtAssignmentFetchFuture fut = pendingAssignmentFetchFuts.get(res.futureId()); if (fut != null) fut.onResponse(nodeId, res); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ActionManager(@Named("schedulerSleeptime") long schedulerSleepTime, @Named("actionTimeout") long actionTimeout, ActionQueue aq, Clusters fsm, ActionDBAccessor db, HostsMap hostsMap, ServerActionManager serverActionManager, UnitOfWork unitOfWork, CustomActionDBAccessor cdb) { this.actionQueue = aq; this.db = db; scheduler = new ActionScheduler(schedulerSleepTime, actionTimeout, db, actionQueue, fsm, 2, hostsMap, serverActionManager, unitOfWork); requestCounter = new AtomicLong( db.getLastPersistedRequestIdWhenInitialized()); this.cdb = cdb; }', 'ground_truth': 'public ActionManager(@Named("schedulerSleeptime") long schedulerSleepTime, @Named("actionTimeout") long actionTimeout, ActionQueue aq, Clusters fsm, ActionDBAccessor db, HostsMap hostsMap, ServerActionManager serverActionManager, UnitOfWork unitOfWork, CustomActionDBAccessor cdb, RequestFactory requestFactory) { this.actionQueue = aq; this.db = db; scheduler = new ActionScheduler(schedulerSleepTime, actionTimeout, db, actionQueue, fsm, 2, hostsMap, serverActionManager, unitOfWork); requestCounter = new AtomicLong( db.getLastPersistedRequestIdWhenInitialized()); this.cdb = cdb; this.requestFactory = requestFactory; }', 'output': 'public ActionManager(long schedulerSleepTime, long actionTimeout, ActionQueue aq, Clusters fsm, ActionDBAccessor db, HostsMap hostsMap, ServerActionManager serverActionManager, UnitOfWork unitOfWork, CustomActionDBAccessor cdb) { this.actionQueue = aq; this.db = db; scheduler = new ActionScheduler(schedulerSleepTime, actionTimeout, db, actionQueue, fsm, 2, hostsMap, serverActionManager, unitOfWork); requestCounter = new AtomicLong( db.getLastPersistedRequestIdWhenInitialized()); this.cdb = cdb; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static boolean splitAggregates( final GridSqlElement parentExpr, final int childIdx, final List<GridSqlElement> mapSelect, final int exprIdx, boolean first) { GridSqlElement el = parentExpr.child(childIdx); if (el instanceof GridSqlAggregateFunction) { splitAggregate(parentExpr, childIdx, mapSelect, exprIdx, first); return true; } for (int i = 0; i < el.size(); i++) { if (splitAggregates(el, i, mapSelect, exprIdx, first)) first = false; } return !first; }', 'ground_truth': 'private static boolean splitAggregates( final GridSqlElement parentExpr, final int childIdx, final List<GridSqlElement> mapSelect, final int exprIdx, boolean hasDistinctAggregate, boolean first) { GridSqlElement el = parentExpr.child(childIdx); if (el instanceof GridSqlAggregateFunction) { splitAggregate(parentExpr, childIdx, mapSelect, exprIdx, hasDistinctAggregate, first); return true; } for (int i = 0; i < el.size(); i++) { if (splitAggregates(el, i, mapSelect, exprIdx, hasDistinctAggregate, first)) first = false; } return !first; }', 'output': 'private static boolean splitAggregates( final GridSqlElement parentExpr, final int childIdx, final List<GridSqlElement> mapSelect, final int exprIdx, boolean first) { GridSqlElement el = parentExpr.child(childIdx); if (el instanceof GridSqlAggregateFunction) { splitAggregate(parentExpr, childIdx, mapSelect, exprIdx, first); return true; } for (int i = 0; i < el.size(); i++) { if (splitAggregates(el, i, mapSelect, exprIdx, first)) first = false; } return !first; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ColumnFamily getColumnFamilyWithColumns() throws IOException { assert inputWithTracker.getBytesRead() == headerSize(); ColumnFamily cf = columnFamily.cloneMeShallow(ArrayBackedSortedColumns.factory(), false); columnFamily.serializer.deserializeColumnsFromSSTable(inputWithTracker, cf, columnCount, flag, expireBefore, dataVersion); if (validateColumns) { try { cf.validateColumnFields(); } catch (MarshalException e) { throw new RuntimeException("Error validating row " + key, e); } } return cf; }', 'ground_truth': 'public ColumnFamily getColumnFamilyWithColumns(ISortedColumns.Factory containerFactory) throws IOException { assert inputWithTracker.getBytesRead() == headerSize(); ColumnFamily cf = columnFamily.cloneMeShallow(containerFactory, false); columnFamily.serializer.deserializeColumnsFromSSTable(inputWithTracker, cf, columnCount, flag, expireBefore, dataVersion); if (validateColumns) { try { cf.validateColumnFields(); } catch (MarshalException e) { throw new RuntimeException("Error validating row " + key, e); } } return cf; }', 'output': 'public ColumnFamily getColumnFamilyWithColumns(boolean validateColumns) throws IOException { assert inputWithTracker.getBytesRead() == headerSize(); ColumnFamily cf = columnFamily.cloneMeShallow(ArrayBackedSortedColumns.factory(), false); columnFamily.serializer.deserializeColumnsFromSSTable(inputWithTracker, cf, columnCount, flag, expireBefore, dataVersion); if (validateColumns) { try { cf.validateColumnFields(); } catch (MarshalException e) { throw new RuntimeException("Error validating row " + key, e); } } return cf; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static Collection<InetAddress> getBatchlogEndpoints(String localDataCenter) throws UnavailableException { TokenMetadata.Topology topology = StorageService.instance.getTokenMetadata().cloneOnlyTokenMap().getTopology(); Collection<InetAddress> localMembers = topology.getDatacenterEndpoints().get(localDataCenter); if (localMembers.size() == 1) return localMembers; localMembers.remove(FBUtilities.getBroadcastAddress()); List<InetAddress> candidates = new ArrayList<InetAddress>(localMembers.size()); for (InetAddress member : localMembers) { if (FailureDetector.instance.isAlive(member)) candidates.add(member); } if (candidates.isEmpty()) throw new UnavailableException(ConsistencyLevel.ONE, 1, 0); if (candidates.size() > 2) { IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch(); snitch.sortByProximity(FBUtilities.getBroadcastAddress(), candidates); candidates = candidates.subList(0, 2); } return candidates; }', 'ground_truth': 'private static Collection<InetAddress> getBatchlogEndpoints(String localDataCenter, ConsistencyLevel consistencyLevel) throws UnavailableException { TokenMetadata.Topology topology = StorageService.instance.getTokenMetadata().cloneOnlyTokenMap().getTopology(); Collection<InetAddress> localMembers = topology.getDatacenterEndpoints().get(localDataCenter); if (localMembers.size() == 1) return localMembers; localMembers.remove(FBUtilities.getBroadcastAddress()); List<InetAddress> candidates = new ArrayList<InetAddress>(localMembers.size()); for (InetAddress member : localMembers) { if (FailureDetector.instance.isAlive(member)) candidates.add(member); } if (candidates.isEmpty()) { if (consistencyLevel == ConsistencyLevel.ANY) return Collections.singleton(FBUtilities.getBroadcastAddress()); throw new UnavailableException(ConsistencyLevel.ONE, 1, 0); } if (candidates.size() > 2) { IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch(); snitch.sortByProximity(FBUtilities.getBroadcastAddress(), candidates); candidates = candidates.subList(0, 2); } return candidates; }', 'output': 'private static Collection<InetAddress> getBatchlogEndpoints(String localDataCenter) throws UnavailableException { TokenMetadata.Topology topology = StorageService.instance.getTokenMetadata().cloneOnlyTokenMap().getTopology(); Collection<InetAddress> localMembers = topology.getDatacenterEndpoints().get(localDataCenter); if (localMembers.size() == 1) return localMembers; localMembers.remove(FBUtilities.getBroadcastAddress()); List<InetAddress> candidates = new ArrayList<InetAddress>(localMembers.size()); for (InetAddress member : localMembers) { if (FailureDetector.instance.isAlive(member)) candidates.add(member); } if (candidates.isEmpty()) throw new UnavailableException(ConsistencyLevel.ONE, 1, 0); if (candidates.size() > 2) { IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch(); snitch.sortByProximity(FBUtilities.getBroadcastAddress(), candidates); candidates = candidates.subList(0, 2); } return candidates; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void readBinary(BinaryReaderExImpl reader) throws BinaryObjectException { super.readBinary(reader); int size = reader.readInt(); if (size == 0) meta = Collections.emptyList(); else { meta = new ArrayList<>(size); for (int i = 0; i < size; ++i) { JdbcParameterMeta m = new JdbcParameterMeta(); m.readBinary(reader); meta.add(m); } } } /** * @return SQL query parameters metadata. */ public List<JdbcParameterMeta> meta() { return meta; } /** {@inheritDoc} */ @Override public String toString() { return S.toString(JdbcMetaParamsResult.class, this); } } }', 'ground_truth': '@Override public void readBinary(BinaryReaderExImpl reader, ClientListenerProtocolVersion ver) throws BinaryObjectException { super.readBinary(reader, ver); int size = reader.readInt(); if (size == 0) meta = Collections.emptyList(); else { meta = new ArrayList<>(size); for (int i = 0; i < size; ++i) { JdbcParameterMeta m = new JdbcParameterMeta(); m.readBinary(reader, ver); meta.add(m); } } }', 'output': '@Override public void readBinary(BinaryReaderExImpl reader) throws BinaryObjectException { super.readBinary(reader); int size = reader.readInt(); if (size == 0) meta = Collections.emptyList(); else { meta = new ArrayList<>(size); for (int i = 0; i < size; ++i) { JdbcParameterMeta m = new JdbcParameterMeta(); m.readBinary(reader); meta.add(m); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void receive(Message message) { message = SinkManager.processServerMessage(message); if (message == null) return; Runnable runnable = new MessageDeliveryTask(message); ExecutorService stage = StageManager.getStage(message.getMessageType()); assert stage != null : "No stage for message type " + message.getMessageType(); stage.execute(runnable); }', 'ground_truth': 'public void receive(Message message, String id) { message = SinkManager.processServerMessage(message); if (message == null) return; Runnable runnable = new MessageDeliveryTask(message, id); ExecutorService stage = StageManager.getStage(message.getMessageType()); assert stage != null : "No stage for message type " + message.getMessageType(); stage.execute(runnable); }', 'output': 'public void receive(Message message) { message = SinkManager.processServerMessage(message); if (message == null) return; Runnable runnable = new MessageDeliveryTask(message); Stage stage = StageManager.getStage(message.getMessageType()); assert stage != null : "No stage for message type " + message.getMessageType(); stage.execute(runnable); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static PartitionIterator read(SinglePartitionReadCommand.Group group, ConsistencyLevel consistencyLevel) throws UnavailableException, IsBootstrappingException, ReadFailureException, ReadTimeoutException, InvalidRequestException { assert !consistencyLevel.isSerialConsistency(); return read(group, consistencyLevel, null); } public static PartitionIterator read(SinglePartitionReadCommand.Group group, ConsistencyLevel consistencyLevel, ClientState state)\n    throws UnavailableException, IsBootstrappingException, ReadFailureException, ReadTimeoutException, InvalidRequestException\n    { throws UnavailableException, IsBootstrappingException, ReadFailureException, ReadTimeoutException, InvalidRequestException { if (StorageService.instance.isBootstrapMode() && !systemKeyspaceQuery(group.commands)) { readMetrics.unavailables.mark(); readMetricsMap.get(consistencyLevel).unavailables.mark(); throw new IsBootstrappingException(); } return consistencyLevel.isSerialConsistency() ? readWithPaxos(group, consistencyLevel, state) : readRegular(group, consistencyLevel); }', 'ground_truth': 'public static PartitionIterator read(SinglePartitionReadCommand.Group group, ConsistencyLevel consistencyLevel, long queryStartNanoTime) throws UnavailableException, IsBootstrappingException, ReadFailureException, ReadTimeoutException, InvalidRequestException { assert !consistencyLevel.isSerialConsistency(); return read(group, consistencyLevel, null, queryStartNanoTime); }', 'output': 'public static PartitionIterator read(SinglePartitionReadCommand.Group group, ConsistencyLevel consistencyLevel, ClientState state)\n    throws UnavailableException, IsBootstrappingException, ReadFailureException, ReadTimeoutException, InvalidRequestException\n    { throws UnavailableException, IsBootstrappingException, ReadFailureException, ReadTimeoutException, InvalidRequestException { if (StorageService.instance.isBootstrapMode() && !systemKeyspaceQuery(group.commands)) { readMetrics.unavailables.mark(); readMetricsMap.get(consistencyLevel).unavailables.mark(); throw new IsBootstrappingException(); } return consistencyLevel.isSerialConsistency() ? readWithPaxos(group, consistencyLevel, state) : readRegular(group, consistencyLevel); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public AlertGroupEntity findDefaultServiceGroup(String serviceName) { TypedQuery<AlertGroupEntity> query = entityManagerProvider.get().createNamedQuery( "AlertGroupEntity.findServiceDefaultGroup", AlertGroupEntity.class); query.setParameter("serviceName", serviceName); return daoUtils.selectSingle(query); }', 'ground_truth': 'public AlertGroupEntity findDefaultServiceGroup(long clusterId, String serviceName) { TypedQuery<AlertGroupEntity> query = entityManagerProvider.get().createNamedQuery( "AlertGroupEntity.findServiceDefaultGroup", AlertGroupEntity.class); query.setParameter("clusterId", clusterId); query.setParameter("serviceName", serviceName); return daoUtils.selectSingle(query); }', 'output': 'public List<AlertGroupEntity> findDefaultServiceGroups(String serviceName) { TypedQuery<AlertGroupEntity> query = entityManagerProvider.get().createNamedQuery( "AlertGroupEntity.findServiceDefaultGroup", AlertGroupEntity.class); query.setParameter("serviceName", serviceName); return daoUtils.selectList(query); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public AsyncOpRetryFuture(AsyncOp<T> op, boolean skipStore, int retries) { assert retries > 1 : retries; this.op = op; this.tx = null; this.skipStore = skipStore; this.retries = retries; }', 'ground_truth': 'public AsyncOpRetryFuture( AsyncOp<T> op, int retries, CacheOperationContext opCtx ) { assert retries > 1 : retries; tx = null; this.op = op; this.retries = retries; this.opCtx = opCtx; }', 'output': 'public AsyncOpRetryFuture(AsyncOp<?> op, boolean skipStore, int retries) { assert retries > 1 : retries; this.op = op; this.tx = null; this.skipStore = skipStore; this.retries = retries; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected Tuple columnToTuple(Cell col, CfDef cfDef, AbstractType comparator) throws IOException { Tuple pair = TupleFactory.getInstance().newTuple(2); ByteBuffer colName = col.name().toByteBuffer(); if(comparator instanceof AbstractCompositeType) StorageHelper.setTupleValue(pair, 0, composeComposite((AbstractCompositeType) comparator, colName)); else StorageHelper.setTupleValue(pair, 0, StorageHelper.cassandraToObj(comparator, colName, nativeProtocolVersion)); Map<ByteBuffer,AbstractType> validators = getValidatorMap(cfDef); if (validators.get(colName) == null) { Map<MarshallerType, AbstractType> marshallers = getDefaultMarshallers(cfDef); StorageHelper.setTupleValue(pair, 1, StorageHelper.cassandraToObj(marshallers.get(MarshallerType.DEFAULT_VALIDATOR), col.value(), nativeProtocolVersion)); } else StorageHelper.setTupleValue(pair, 1, StorageHelper.cassandraToObj(validators.get(colName), col.value(), nativeProtocolVersion)); return pair; }', 'ground_truth': 'protected Tuple columnToTuple(ColumnFamilyRecordReader.Column column, CfDef cfDef, AbstractType comparator) throws IOException { Tuple pair = TupleFactory.getInstance().newTuple(2); if(comparator instanceof AbstractCompositeType) StorageHelper.setTupleValue(pair, 0, composeComposite((AbstractCompositeType) comparator, column.name)); else StorageHelper.setTupleValue(pair, 0, StorageHelper.cassandraToObj(comparator, column.name, nativeProtocolVersion)); Map<ByteBuffer,AbstractType> validators = getValidatorMap(cfDef); if (validators.get(column.name) == null) { Map<MarshallerType, AbstractType> marshallers = getDefaultMarshallers(cfDef); StorageHelper.setTupleValue(pair, 1, StorageHelper.cassandraToObj(marshallers.get(MarshallerType.DEFAULT_VALIDATOR), column.value, nativeProtocolVersion)); } else StorageHelper.setTupleValue(pair, 1, StorageHelper.cassandraToObj(validators.get(column.name), column.value, nativeProtocolVersion)); return pair; }', 'output': 'protected Tuple columnToTuple(Cell col, CfDefinition cfDef, AbstractType comparator) throws IOException { Tuple pair = TupleFactory.getInstance().newTuple(2); ByteBuffer colName = col.name().toByteBuffer(); if(comparator instanceof AbstractCompositeType) StorageHelper.setTupleValue(pair, 0, composeComposite((AbstractCompositeType) comparator, colName)); else StorageHelper.setTupleValue(pair, 0, StorageHelper.cassandraToObj(comparator, colName, nativeProtocolVersion)); Map<ByteBuffer,AbstractType> validators = getValidatorMap(cfDef); if (validators.get(colName) == null) { Map<MarshallerType, AbstractType> marshallers = getDefaultMarshallers(cfDef); StorageHelper.setTupleValue(pair, 1, StorageHelper.cassandraToObj(marshallers.get(MarshallerType.DEFAULT_VALIDATOR), col.value(), nativeProtocolVersion)); } else StorageHelper.setTupleValue(pair, 1, StorageHelper.cassandraToObj(validators.get(colName), col.value(), nativeProtocolVersion)); return pair; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Response getService(@Context HttpHeaders headers, @Context UriInfo ui, @PathParam("serviceName") String serviceName) { return handleRequest(headers, null, ui, Request.Type.GET, createServiceResource(serviceName)); } public Response getServiceComponents(@Context HttpHeaders headers, @Context UriInfo ui,\n      @PathParam("serviceName") String serviceName) { @PathParam("serviceName") String serviceName) { return handleRequest(headers, null, ui, Request.Type.GET, createServiceComponentResource(serviceName, null)); } public Response getServiceComponent(@Context HttpHeaders headers, @Context UriInfo ui,\n      @PathParam("serviceName") String serviceName,\n      @PathParam("componentName") String componentName) { @PathParam("serviceName") String serviceName, @PathParam("componentName") String componentName) { return handleRequest(headers, null, ui, Request.Type.GET, createServiceComponentResource(serviceName, componentName)); }', 'ground_truth': 'public Response getService(String body, @Context HttpHeaders headers, @Context UriInfo ui, @PathParam("serviceName") String serviceName) { return handleRequest(headers, body, ui, Request.Type.GET, createServiceResource(serviceName)); }', 'output': 'public Response getService(@Context HttpHeaders headers, @Context UriInfo ui, @PathParam("serviceName") String serviceName) { return handleRequest(headers, null, ui, Request.Type.GET, createServiceResource(serviceName)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void reduce(List<RexNode> constExps, List<RexNode> reducedValues) { Object[] values = compiledFunction.apply(dataContext); assert values.length == constExps.size(); final List<Object> valueList = Arrays.asList(values); for (Pair<RexNode, Object> value : Pair.zip(constExps, valueList)) { reducedValues.add(rexBuilder.makeLiteral(value.right, value.left.getType(), true)); } Hook.EXPRESSION_REDUCER.run(Pair.of(generatedCode, values)); }', 'ground_truth': 'public void reduce(RexBuilder rexBuilder, List<RexNode> constExps, List<RexNode> reducedValues) { Object[] values = compiledFunction.apply(dataContext); assert values.length == constExps.size(); final List<Object> valueList = Arrays.asList(values); for (Pair<RexNode, Object> value : Pair.zip(constExps, valueList)) { reducedValues.add( rexBuilder.makeLiteral(value.right, value.left.getType(), true)); } Hook.EXPRESSION_REDUCER.run(Pair.of(code, values)); }', 'output': 'public void reduce(List<RexNode> constExps, List<RexNode> reducedValues) { Object[] values = compiledFunction.apply(dataContext); assert values.length == constExps.size(); final List<Object> valueList = Arrays.asList(values); for (Pair<RexNode, Object> value : Pair.zip(constExps, valueList)) { reducedValues.add(rexBuilder.makeLiteral(value.right, value.left.getType(), true)); } Hook.EXPRESSION_REDUCER.run(Pair.of(generatedCode, values)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static RunnerApi.TimerSpec translateTimerSpec(TimerSpec timer) { return RunnerApi.TimerSpec.newBuilder() .setTimeDomain(translateTimeDomain(timer.getTimeDomain())) .build(); }', 'ground_truth': 'public static RunnerApi.TimerSpec translateTimerSpec(TimerSpec timer, SdkComponents components) { return RunnerApi.TimerSpec.newBuilder() .setTimeDomain(translateTimeDomain(timer.getTimeDomain())) .setTimerCoderId(registerCoderOrThrow(components, Timer.Coder.of(VoidCoder.of()))) .build(); }', 'output': 'public static TimerSpec translateTimerSpec(TimerSpec timer) { return TimerSpec.newBuilder() .setTimeDomain(translateTimeDomain(timer.getTimeDomain())) .build(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void replayBatch(UUID id) { logger.debug("Replaying batch {}", id); UntypedResultSet result = process("SELECT written_at, data FROM %s.%s WHERE id = %s", Table.SYSTEM_KS, SystemTable.BATCHLOG_CF, id); if (result.isEmpty()) return; try { replaySerializedMutations(result.one().getBytes("data"), result.one().getLong("written_at")); } catch (IOException e) { logger.warn("Skipped batch replay of {} due to {}", id, e); } process("DELETE FROM %s.%s WHERE id = %s", Table.SYSTEM_KS, SystemTable.BATCHLOG_CF, id); totalBatchesReplayed.incrementAndGet(); }', 'ground_truth': 'private void replayBatch(UUID id, RateLimiter rateLimiter) { logger.debug("Replaying batch {}", id); UntypedResultSet result = process("SELECT written_at, data FROM %s.%s WHERE id = %s", Table.SYSTEM_KS, SystemTable.BATCHLOG_CF, id); if (result.isEmpty()) return; try { replaySerializedMutations(result.one().getBytes("data"), result.one().getLong("written_at"), rateLimiter); } catch (IOException e) { logger.warn("Skipped batch replay of {} due to {}", id, e); } process("DELETE FROM %s.%s WHERE id = %s", Table.SYSTEM_KS, SystemTable.BATCHLOG_CF, id); totalBatchesReplayed.incrementAndGet(); }', 'output': 'private void replayBatch(UUID id) { logger.debug("Replaying batch {}", id); UntypedResultSet result = process("SELECT written_at, data FROM %s.%s WHERE id = %s", Table.SYSTEM_KS, SystemTable.BATCHLOG_CF, id); if (result.isEmpty()) return; try { replaySerializedMutations(result.one().getBytes("data"), result.one().getLong("written_at")); } catch (IOException e) { logger.warn("Skipped batch replay of {} due to {}", id, e); } process("DELETE FROM %s.%s WHERE id = %s", Table.SYSTEM_KS, SystemTable.BATCHLOG_CF, id); totalBatchesReplayed.incrementAndGet(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ServiceComponentUninstalledEvent(long clusterId, String stackName, String stackVersion, String serviceName, String componentName, String hostName, boolean recoveryEnabled) { super(AmbariEventType.SERVICE_COMPONENT_UNINSTALLED_SUCCESS, clusterId, stackName, stackVersion, serviceName); m_componentName = componentName; m_hostName = hostName; m_recoveryEnabled = recoveryEnabled; }', 'ground_truth': 'public ServiceComponentUninstalledEvent(long clusterId, String stackName, String stackVersion, String serviceName, String componentName, String hostName, boolean recoveryEnabled, Long hostId) { super(AmbariEventType.SERVICE_COMPONENT_UNINSTALLED_SUCCESS, clusterId, stackName, stackVersion, serviceName); m_componentName = componentName; m_hostName = hostName; m_recoveryEnabled = recoveryEnabled; m_hostId = hostId; }', 'output': 'public ServiceComponentUninstalledEvent(long clusterId, String stackName, String stackVersion, String serviceName, String componentName, String hostName, boolean recoveryEnabled, long timestamp) { super(AmbariEventType.SERVICE_COMPONENT_UNINSTALLED_SUCCESS, clusterId, stackName, stackVersion, serviceName, timestamp); m_componentName = componentName; m_hostName = hostName; m_recoveryEnabled = recoveryEnabled; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static Pair<List<InetAddress>, Integer> getPaxosParticipants(String keyspaceName, ByteBuffer key) throws UnavailableException { Token tk = StorageService.getPartitioner().getToken(key); List<InetAddress> naturalEndpoints = StorageService.instance.getNaturalEndpoints(keyspaceName, tk); Collection<InetAddress> pendingEndpoints = StorageService.instance.getTokenMetadata().pendingEndpointsFor(tk, keyspaceName); int requiredParticipants = pendingEndpoints.size() + 1 + naturalEndpoints.size() / 2; List<InetAddress> liveEndpoints = ImmutableList.copyOf(Iterables.filter(Iterables.concat(naturalEndpoints, pendingEndpoints), IAsyncCallback.isAlive)); if (liveEndpoints.size() < requiredParticipants) throw new UnavailableException(ConsistencyLevel.SERIAL, requiredParticipants, liveEndpoints.size()); return Pair.create(liveEndpoints, requiredParticipants); }', 'ground_truth': 'private static Pair<List<InetAddress>, Integer> getPaxosParticipants(String keyspaceName, ByteBuffer key, ConsistencyLevel consistencyForPaxos) throws UnavailableException { Token tk = StorageService.getPartitioner().getToken(key); List<InetAddress> naturalEndpoints = StorageService.instance.getNaturalEndpoints(keyspaceName, tk); Collection<InetAddress> pendingEndpoints = StorageService.instance.getTokenMetadata().pendingEndpointsFor(tk, keyspaceName); if (consistencyForPaxos == ConsistencyLevel.LOCAL_SERIAL) { String localDc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(FBUtilities.getBroadcastAddress()); Predicate<InetAddress> isLocalDc = sameDCPredicateFor(localDc); naturalEndpoints = ImmutableList.copyOf(Iterables.filter(naturalEndpoints, isLocalDc)); pendingEndpoints = ImmutableList.copyOf(Iterables.filter(pendingEndpoints, isLocalDc)); } int requiredParticipants = pendingEndpoints.size() + 1 + naturalEndpoints.size() / 2; List<InetAddress> liveEndpoints = ImmutableList.copyOf(Iterables.filter(Iterables.concat(naturalEndpoints, pendingEndpoints), IAsyncCallback.isAlive)); if (liveEndpoints.size() < requiredParticipants) throw new UnavailableException(consistencyForPaxos, requiredParticipants, liveEndpoints.size()); return Pair.create(liveEndpoints, requiredParticipants); }', 'output': 'private static Pair<Set<InetAddress>, Integer> getPaxosParticipants(String keyspaceName, ByteBuffer key) throws UnavailableException { Token tk = StorageService.getPartitioner().getToken(key); Set<InetAddress> naturalEndpoints = StorageService.instance.getNaturalEndpoints(keyspaceName, tk); Collection<InetAddress> pendingEndpoints = StorageService.instance.getTokenMetadata().pendingEndpointsFor(tk, keyspaceName); int requiredParticipants = pendingEndpoints.size() + 1 + naturalEndpoints.size() / 2; Set<InetAddress> liveEndpoints = ImmutableSet.copyOf(Iterables.filter(Iterables.concat(naturalEndpoints, pendingEndpoints), IAsyncCallback.isAlive)); if (liveEndpoints.size() < requiredParticipants) throw new UnavailableException(ConsistencyLevel.SERIAL, requiredParticipants, liveEndpoints.size()); return Pair.create(liveEndpoints, requiredParticipants); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void send_get_slice_super_by_names(String table, String key, String column_family, List<byte[]> super_column_names) throws TException { oprot_.writeMessageBegin(new TMessage("get_slice_super_by_names", TMessageType.CALL, seqid_)); get_slice_super_by_names_args args = new get_slice_super_by_names_args(); args.table = table; args.key = key; args.column_family = column_family; args.super_column_names = super_column_names; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'ground_truth': 'public void send_get_slice_super_by_names(String table, String key, String column_family, List<byte[]> super_column_names, int consistency_level) throws TException { oprot_.writeMessageBegin(new TMessage("get_slice_super_by_names", TMessageType.CALL, seqid_)); get_slice_super_by_names_args args = new get_slice_super_by_names_args(); args.table = table; args.key = key; args.column_family = column_family; args.super_column_names = super_column_names; args.consistency_level = consistency_level; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'output': 'public void send_get_slice_super_by_names(String table, String key, String column_family, List<ByteBuffer> super_column_names) throws TException { oprot_.writeMessageBegin(new TMessage("get_slice_super_by_names", TMessageType.CALL, seqid_)); get_slice_super_by_names_args args = new get_slice_super_by_names_args(); args.table = table; args.key = key; args.column_family = column_family; args.super_column_names = super_column_names; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public final String format(TemplateDateModel dateModel) throws TemplateModelException { final Date date = dateModel.getAsDate(); return format( date, dateType != TemplateDateModel.TIME, dateType != TemplateDateModel.DATE, showZoneOffset == null ? !DateUtil.isSQLDateOrTimeClass(date.getClass()) : showZoneOffset.booleanValue(), DateUtil.ACCURACY_MILLISECONDS, timeZone, env.getISOBuiltInCalendar()); }', 'ground_truth': 'public final String format(TemplateDateModel dateModel, boolean zonelessInput) throws TemplateModelException { final Date date = dateModel.getAsDate(); return format( date, dateType != TemplateDateModel.TIME, dateType != TemplateDateModel.DATE, showZoneOffset == null ? !zonelessInput : showZoneOffset.booleanValue(), accuracy, zonelessInput || !useUTC ? timeZone : DateUtil.UTC, factory.getISOBuiltInCalendar()); }', 'output': 'public final String format(TemplateDateModel dateModel) throws TemplateModelException { final Date date = dateModel.getAsDate(); return format( date, dateType != TemplateDateModel.TIME, dateType != TemplateDateModel.DATE, showZoneOffset == null ? !DateUtil.isSQLDateOrTimeClass(date.getClass()) : showZoneOffset.booleanValue(), DateUtil.ACCURACY_MILLISECONDS, timeZone, env.getISOBuiltInCalendar()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean isStale(IndexedEntry entry, ColumnFamily data) { ByteBuffer bb = entry.indexedEntryNameBuilder.copy().add(columnDef.name).build(); Column liveColumn = data.getColumn(bb); if (liveColumn == null || liveColumn.isMarkedForDelete()) return true; ByteBuffer liveValue = liveColumn.value(); return columnDef.getValidator().compare(entry.indexValue.key, liveValue) != 0; }', 'ground_truth': 'public boolean isStale(IndexedEntry entry, ColumnFamily data, long now) { ByteBuffer bb = entry.indexedEntryNameBuilder.copy().add(columnDef.name).build(); Column liveColumn = data.getColumn(bb); if (liveColumn == null || liveColumn.isMarkedForDelete(now)) return true; ByteBuffer liveValue = liveColumn.value(); return columnDef.getValidator().compare(entry.indexValue.key, liveValue) != 0; }', 'output': 'public boolean isStale(IndexedEntry entry, ColumnFamily data) { ByteBuffer bb = entry.indexedEntryNameBuilder.copy().add(columnDef.name).build(); Column liveColumn = data.getColumn(bb); if (liveColumn == null || liveColumn.isMarkedForDelete()) return true; ByteBuffer liveValue = liveColumn.value(); return columnDef.getValidator().compare(entry.indexValue.key, liveValue) != 0; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void writeBinary(BinaryWriterExImpl writer) { super.writeBinary(writer); writer.writeString(dfltValue); } /** {@inheritDoc} */ @Override public void readBinary(BinaryReaderExImpl reader) { super.readBinary(reader); dfltValue = reader.readString(); } /** {@inheritDoc} */ @Override public String toString() { return S.toString(JdbcColumnMetaV3.class, this); } } }', 'ground_truth': '@Override public void writeBinary(BinaryWriterExImpl writer, ClientListenerProtocolVersion ver) { super.writeBinary(writer, ver); writer.writeString(dfltValue); }', 'output': '@Override public void writeBinary(BinaryWriterExImpl writer) { super.writeBinary(writer); writer.writeString(defaultValue); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void scrubOne(ColumnFamilyStore cfs, SSTableReader sstable) throws IOException { Scrubber scrubber = new Scrubber(cfs, sstable); CompactionInfo.Holder scrubInfo = scrubber.getScrubInfo(); metrics.beginCompaction(scrubInfo); try { scrubber.scrub(); } finally { scrubber.close(); metrics.finishCompaction(scrubInfo); } if (scrubber.getNewInOrderSSTable() != null) cfs.addSSTable(scrubber.getNewInOrderSSTable()); if (scrubber.getNewSSTable() == null) cfs.markObsolete(Collections.singletonList(sstable), OperationType.SCRUB); else cfs.replaceCompactedSSTables(Collections.singletonList(sstable), Collections.singletonList(scrubber.getNewSSTable()), OperationType.SCRUB); }', 'ground_truth': 'private void scrubOne(ColumnFamilyStore cfs, SSTableReader sstable, boolean skipCorrupted) throws IOException { Scrubber scrubber = new Scrubber(cfs, sstable, skipCorrupted); CompactionInfo.Holder scrubInfo = scrubber.getScrubInfo(); metrics.beginCompaction(scrubInfo); try { scrubber.scrub(); } finally { scrubber.close(); metrics.finishCompaction(scrubInfo); } if (scrubber.getNewInOrderSSTable() != null) cfs.addSSTable(scrubber.getNewInOrderSSTable()); if (scrubber.getNewSSTable() == null) cfs.markObsolete(Collections.singletonList(sstable), OperationType.SCRUB); else cfs.replaceCompactedSSTables(Collections.singletonList(sstable), Collections.singletonList(scrubber.getNewSSTable()), OperationType.SCRUB); }', 'output': 'private void scrubOne(ColumnFamilyStore cfs, SSTableReader sstable) throws IOException { Scrubmer scrubmer = new Scrubmer(cfs, sstable); CompactionInfo.Holder scrubInfo = scrubmer.getScrubInfo(); metrics.beginCompaction(scrubInfo); try { scrubmer.scrub(); } finally { scrubmer.close(); metrics.finishCompaction(scrubInfo); } if (scrubmer.getNewInOrderSSTable() != null) cfs.addSSTable(scrubmer.getNewInOrderSSTable()); if (scrubmer.getNewSSTable() == null) cfs.markObsolete(Collections.singletonList(sstable), OperationType.SCRUB); else cfs.replaceCompactedSSTables(Collections.singletonList(sstable), Collections.singletonList(scrubmer.getNewSSTable()), OperationType.SCRUB); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Response getJob(@Context HttpHeaders headers, @Context UriInfo ui, @PathParam("jobId") String jobId) { return handleRequest(headers, null, ui, Request.Type.GET, createJobResource(clusterName, workflowId, jobId)); }', 'ground_truth': 'public Response getJob(String body, @Context HttpHeaders headers, @Context UriInfo ui, @PathParam("jobId") String jobId) { return handleRequest(headers, body, ui, Request.Type.GET, createJobResource(clusterName, workflowId, jobId)); }', 'output': 'public Response getJob(@Context HttpHeaders headers, @Context UriInfo ui, @PathParam("jobId") String jobId, @QueryParam("status") String status) { return handleRequest(headers, null, ui, Request.Type.GET, createJobResource(clusterName, workflowId, jobId, status)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static UpdatePlan forMerge(GridH2Table tbl, String[] colNames, KeyValueSupplier keySupplier, KeyValueSupplier valSupplier, int keyColIdx, int valColIdx, String selectQry, boolean isLocSubqry, int rowsNum) { assert !F.isEmpty(colNames); return new UpdatePlan(UpdateMode.MERGE, tbl, colNames, keySupplier, valSupplier, keyColIdx, valColIdx, selectQry, isLocSubqry, rowsNum, null); }', 'ground_truth': 'public static UpdatePlan forMerge(GridH2Table tbl, String[] colNames, int[] colTypes, KeyValueSupplier keySupplier, KeyValueSupplier valSupplier, int keyColIdx, int valColIdx, String selectQry, boolean isLocSubqry, int rowsNum) { assert !F.isEmpty(colNames); return new UpdatePlan(UpdateMode.MERGE, tbl, colNames, colTypes, keySupplier, valSupplier, keyColIdx, valColIdx, selectQry, isLocSubqry, rowsNum, null); }', 'output': 'public static UpdatePlan forMerge(IgniteTable tbl, String[] colNames, KeyValueSupplier keySupplier, KeyValueSupplier valSupplier, int keyColIdx, int valColIdx, String selectQry, boolean isLocSubqry, int rowsNum) { assert !F.isEmpty(colNames); return new UpdatePlan(UpdateMode.MERGE, tbl, colNames, keySupplier, valSupplier, keyColIdx, valColIdx, selectQry, isLocSubqry, rowsNum, null); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void transferRanges(InetAddress target, Collection<Range> ranges) throws IOException { assert ranges.size() > 0; if (logger.isDebugEnabled()) logger.debug("Beginning transfer process to " + target + " for ranges " + StringUtils.join(ranges, ", ")); /* * (1) dump all the memtables to disk. * (2) anticompaction -- split out the keys in the range specified * (3) transfer the data. */ List<String> tables = DatabaseDescriptor.getTables(); for (String tName : tables) { Table table = Table.open(tName); if (logger.isDebugEnabled()) logger.debug("Flushing memtables ..."); table.flush(false); if (logger.isDebugEnabled()) logger.debug("Performing anticompaction ..."); /* Get the list of files that need to be streamed */ List<String> fileList = new ArrayList<String>(); for (SSTableReader sstable : table.forceAntiCompaction(ranges, target)) { fileList.addAll(sstable.getAllFilenames()); } transferOneTable(target, fileList, tName); } }', 'ground_truth': 'public static void transferRanges(InetAddress target, Collection<Range> ranges, Runnable callback) { assert ranges.size() > 0; if (logger.isDebugEnabled()) logger.debug("Beginning transfer process to " + target + " for ranges " + StringUtils.join(ranges, ", ")); /* * (1) dump all the memtables to disk. * (2) anticompaction -- split out the keys in the range specified * (3) transfer the data. */ List<String> tables = DatabaseDescriptor.getTables(); for (String tName : tables) { try { Table table = Table.open(tName); if (logger.isDebugEnabled()) logger.debug("Flushing memtables ..."); table.flush(false); if (logger.isDebugEnabled()) logger.debug("Performing anticompaction ..."); /* Get the list of files that need to be streamed */ List<String> fileList = new ArrayList<String>(); for (SSTableReader sstable : table.forceAntiCompaction(ranges, target)) { fileList.addAll(sstable.getAllFilenames()); } transferOneTable(target, fileList, tName); } catch (IOException e) { throw new IOError(e); } } if (callback != null) callback.run(); }', 'output': 'public static void transferRanges(InetAddress target, Collection<Range> ranges) throws IOException { assert ranges.size() > 0; if (logger.isDebugEnabled()) logger.debug("Beginning transfer process to " + target + " for ranges " + StringUtils.join(ranges, ", ")); /* * (1) dump all the memtables to disk. * (2) anticompaction -- split out the keys in the range specified * (3) transfer the data. */ List<String> tables = DatabaseDescriptor.getTables(); for (String tName : tables) { Table table = Table.open(tName); if (logger.isDebugEnabled()) logger.debug("Flushing memtables ..."); table.flush(false); if (logger.isDebugEnabled()) logger.debug("Performing anticompaction ..."); /* Get the list of files that need to be streamed */ List<String> fileList = new ArrayList<String>(); for (SSTableReader sstable : table.forceAntiCompaction(ranges, target)) { fileList.addAll(sstable.getAllFilenames()); } transferOneTable(target, fileList, tName); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ResultMessage executeInternal(QueryState queryState) throws RequestValidationException, RequestExecutionException { if (hasConditions()) throw new UnsupportedOperationException(); for (IMutation mutation : getMutations(QueryOptions.DEFAULT, true, queryState.getTimestamp())) { assert mutation instanceof Mutation; ((Mutation) mutation).apply(); } return null; }', 'ground_truth': 'public ResultMessage executeInternal(QueryState queryState, QueryOptions options) throws RequestValidationException, RequestExecutionException { if (hasConditions()) throw new UnsupportedOperationException(); for (IMutation mutation : getMutations(options, true, queryState.getTimestamp())) { assert mutation instanceof Mutation; ((Mutation) mutation).apply(); } return null; }', 'output': 'public ResultMessage executeInternal(QueryState queryState, QueryOptions options) throws RequestValidationException, RequestExecutionException { if (hasConditions()) throw new UnsupportedOperationException(); for (IMutation mutation : getMutations(options, true, queryState.getTimestamp())) { assert mutation instanceof Mutation; ((Mutation) mutation).apply(); } return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public get_result( ColumnOrSuperColumn success, InvalidRequestException ire, NotFoundException nfe, UnavailableException ue) { this(); this.success = success; this.ire = ire; this.nfe = nfe; this.ue = ue; }', 'ground_truth': 'public get_result( ColumnOrSuperColumn success, InvalidRequestException ire, NotFoundException nfe, UnavailableException ue, TimedOutException te) { this(); this.success = success; this.ire = ire; this.nfe = nfe; this.ue = ue; this.te = te; }', 'output': 'public get_result( ColumnOrSuperColumn success, InvalidRequestException ire, NotFoundException nfe, UnavailableException ue, TimedOutException te) { this(); this.success = success; this.ire = ire; this.nfe = nfe; this.ue = ue; this.te = te; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ActorRef getOperationController(ViewContext context) { String instanceName = context.getInstanceName(); ActorRef ref = operationControllerMap.get(instanceName); if (ref == null) { synchronized (lock) { ref = operationControllerMap.get(instanceName); if (ref == null) { ref = createOperationController(context); operationControllerMap.put(instanceName, ref); } } } return ref; }', 'ground_truth': 'public synchronized ActorRef getOperationController(ViewContext viewContext) { SafeViewContext context = new SafeViewContext(viewContext); String instanceName = context.getInstanceName(); ActorRef ref = null; Map<String, ActorRef> stringActorRefMap = operationControllerMap.get(instanceName); if(stringActorRefMap != null) { ref = stringActorRefMap.get(context.getUsername()); } if (ref == null) { ref = createOperationController(context); if(stringActorRefMap == null) { stringActorRefMap = new HashMap<>(); stringActorRefMap.put(context.getUsername(), ref); operationControllerMap.put(instanceName, stringActorRefMap); } else { stringActorRefMap.put(context.getUsername(), ref); } } return ref; }', 'output': 'public ActorRef getOperationController(final ViewContext context) { String instanceName = context.getInstanceName(); ActorRef ref = operationControllerMap.get(instanceName); if (ref == null) { synchronized (lock) { ref = operationControllerMap.get(instanceName); if (ref == null) { ref = createOperationController(context); operationControllerMap.put(instanceName, ref); } } } return ref; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ColumnDef toThrift(ColumnDefinition column) { ColumnDef cd = new ColumnDef(); cd.setName(ByteBufferUtil.clone(column.name.bytes)); cd.setValidation_class(column.type.toString()); cd.setIndex_type(column.getIndexType() == null ? null : org.apache.cassandra.thrift.IndexType.valueOf(column.getIndexType().name())); cd.setIndex_name(column.getIndexName()); cd.setIndex_options(column.getIndexOptions() == null ? null : Maps.newHashMap(column.getIndexOptions())); return cd; }', 'ground_truth': 'public static ColumnDef toThrift(CFMetaData cfMetaData, ColumnDefinition column) { ColumnDef cd = new ColumnDef(); cd.setName(ByteBufferUtil.clone(column.name.bytes)); cd.setValidation_class(column.type.toString()); Optional<IndexMetadata> index = cfMetaData.getIndexes().get(column); index.ifPresent(def -> { cd.setIndex_type(org.apache.cassandra.thrift.IndexType.valueOf(def.indexType.name())); cd.setIndex_name(def.name); cd.setIndex_options(def.options == null || def.options.isEmpty() ? null : Maps.newHashMap(def.options)); }); return cd; }', 'output': 'public static ColumnDef toThrift(ColumnMetadata column) { ColumnDef cd = new ColumnDef(); cd.setName(ByteBufferUtil.clone(column.name.bytes)); cd.setValidation_class(column.type.toString()); cd.setIndex_type(column.getIndexType() == null ? null : org.apache.cassandra.thrift.IndexType.valueOf(column.getIndexType().name())); cd.setIndex_name(column.getIndexName()); cd.setIndex_options(column.getIndexOptions() == null ? null : Maps.newHashMap(column.getIndexOptions())); return cd; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GridTcpNioCommunicationClient(GridNioSession ses) { super(null); assert ses != null; this.ses = ses; }', 'ground_truth': 'public GridTcpNioCommunicationClient(GridNioSession ses, IgniteLogger log) { super(null); assert ses != null; assert log != null; this.ses = ses; this.log = log; }', 'output': 'public GridTcpNioCommunicationClient(GridNioSession ses, int port) { super(null); assert ses != null; this.ses = ses; this.port = port; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Schema load(Collection<KSMetaData> tableDefs, UUID version) { for (KSMetaData def : tableDefs) { if (!Migration.isLegalName(def.name)) throw new RuntimeException("invalid keyspace name: " + def.name); for (CFMetaData cfm : def.cfMetaData().values()) { if (!Migration.isLegalName(cfm.cfName)) throw new RuntimeException("invalid column family name: " + cfm.cfName); try { load(cfm); } catch (ConfigurationException ex) { throw new IOError(ex); } } setTableDefinition(def, version); } setVersion(version); return this; } public void load(CFMetaData cfm) throws ConfigurationException\n    { { Pair<String, String> key = new Pair<String, String>(cfm.ksName, cfm.cfName); if (cfIdMap.containsKey(key)) throw new ConfigurationException("Attempt to assign id to existing column family."); logger.debug("Adding {} to cfIdMap", cfm); cfIdMap.put(key, cfm.cfId); }', 'ground_truth': 'public Schema load(KSMetaData keyspaceDef) { if (!Migration.isLegalName(keyspaceDef.name)) throw new RuntimeException("invalid keyspace name: " + keyspaceDef.name); for (CFMetaData cfm : keyspaceDef.cfMetaData().values()) { if (!Migration.isLegalName(cfm.cfName)) throw new RuntimeException("invalid column family name: " + cfm.cfName); try { load(cfm); } catch (ConfigurationException ex) { throw new IOError(ex); } } setTableDefinition(keyspaceDef); fixCFMaxId(); return this; }', 'output': 'public Schema load(List<KSMetaData> tableDefs, UUID version) { for (KSMetaData def : tableDefs) { if (!Migration.isLegalName(def.name)) throw new RuntimeException("invalid keyspace name: " + def.name); for (CFMetaData cfm : def.cfMetaData().values()) { if (!Migration.isLegalName(cfm.cfName)) throw new RuntimeException("invalid column family name: " + cfm.cfName); try { load(cfm); } catch (ConfigurationException ex) { throw new IOError(ex); } } setTableDefinition(def, version); } setVersion(version); return this; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public CompactionAwareWriter getCompactionAwareWriter(ColumnFamilyStore cfs, LifecycleTransaction txn, Set<SSTableReader> nonExpiredSSTables) { if (majorCompaction) return new MajorLeveledCompactionWriter(cfs, txn, nonExpiredSSTables, maxSSTableBytes, false, false); return new MaxSSTableSizeWriter(cfs, txn, nonExpiredSSTables, maxSSTableBytes, getLevel(), false, false); }', 'ground_truth': 'public CompactionAwareWriter getCompactionAwareWriter(ColumnFamilyStore cfs, Directories directories, LifecycleTransaction txn, Set<SSTableReader> nonExpiredSSTables) { if (majorCompaction) return new MajorLeveledCompactionWriter(cfs, directories, txn, nonExpiredSSTables, maxSSTableBytes, false, false); return new MaxSSTableSizeWriter(cfs, directories, txn, nonExpiredSSTables, maxSSTableBytes, getLevel(), false, false); }', 'output': 'public CompactionAwareWriter getCompactionAwareWriter(ColumnFamilyStore cfs, LifecycleTransaction txn, Set<SSTableReader> nonExpiredSSTables, int level) { if (majorCompaction) return new MajorLeveledCompactionWriter(cfs, txn, nonExpiredSSTables, maxSSTableBytes, false, false); return new MaxSSTableSizeWriter(cfs, txn, nonExpiredSSTables, maxSSTableBytes, level, false, false); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void internal_remove(ByteBuffer key, ColumnPath column_path, long timestamp, ConsistencyLevel consistency_level, boolean isCommutativeOp) throws RequestValidationException, UnavailableException, TimedOutException { ThriftClientState cState = state(); String keyspace = cState.getKeyspace(); cState.hasColumnFamilyAccess(keyspace, column_path.column_family, Permission.MODIFY); CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_path.column_family, isCommutativeOp); if (metadata.isView()) throw new org.apache.cassandra.exceptions.InvalidRequestException("Cannot modify Materialized Views directly"); ThriftValidation.validateKey(metadata, key); ThriftValidation.validateColumnPathOrParent(metadata, column_path); if (isCommutativeOp) ThriftConversion.fromThrift(consistency_level).validateCounterForWrite(metadata); DecoratedKey dk = metadata.decorateKey(key); int nowInSec = FBUtilities.nowInSeconds(); PartitionUpdate update; if (column_path.super_column == null && column_path.column == null) { update = PartitionUpdate.fullPartitionDelete(metadata, dk, timestamp, nowInSec); } else if (column_path.super_column != null && column_path.column == null) { Row row = BTreeRow.emptyDeletedRow(Clustering.make(column_path.super_column), Row.Deletion.regular(new DeletionTime(timestamp, nowInSec))); update = PartitionUpdate.singleRowUpdate(metadata, dk, row); } else { try { LegacyLayout.LegacyCellName name = LegacyLayout.decodeCellName(metadata, column_path.super_column, column_path.column); CellPath path = name.collectionElement == null ? null : CellPath.create(name.collectionElement); Cell cell = BufferCell.tombstone(name.column, timestamp, nowInSec, path); update = PartitionUpdate.singleRowUpdate(metadata, dk, BTreeRow.singleCellRow(name.clustering, cell)); } catch (UnknownColumnException e) { throw new org.apache.cassandra.exceptions.InvalidRequestException(e.getMessage()); } } org.apache.cassandra.db.Mutation mutation = new org.apache.cassandra.db.Mutation(update); if (isCommutativeOp) doInsert(consistency_level, Collections.singletonList(new CounterMutation(mutation, ThriftConversion.fromThrift(consistency_level)))); else doInsert(consistency_level, Collections.singletonList(mutation)); }', 'ground_truth': 'private void internal_remove(ByteBuffer key, ColumnPath column_path, long timestamp, ConsistencyLevel consistency_level, boolean isCommutativeOp, long queryStartNanoTime) throws RequestValidationException, UnavailableException, TimedOutException { ThriftClientState cState = state(); String keyspace = cState.getKeyspace(); cState.hasColumnFamilyAccess(keyspace, column_path.column_family, Permission.MODIFY); CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_path.column_family, isCommutativeOp); if (metadata.isView()) throw new org.apache.cassandra.exceptions.InvalidRequestException("Cannot modify Materialized Views directly"); ThriftValidation.validateKey(metadata, key); ThriftValidation.validateColumnPathOrParent(metadata, column_path); if (isCommutativeOp) ThriftConversion.fromThrift(consistency_level).validateCounterForWrite(metadata); DecoratedKey dk = metadata.decorateKey(key); int nowInSec = FBUtilities.nowInSeconds(); PartitionUpdate update; if (column_path.super_column == null && column_path.column == null) { update = PartitionUpdate.fullPartitionDelete(metadata, dk, timestamp, nowInSec); } else if (column_path.super_column != null && column_path.column == null) { Row row = BTreeRow.emptyDeletedRow(Clustering.make(column_path.super_column), Row.Deletion.regular(new DeletionTime(timestamp, nowInSec))); update = PartitionUpdate.singleRowUpdate(metadata, dk, row); } else { try { LegacyLayout.LegacyCellName name = LegacyLayout.decodeCellName(metadata, column_path.super_column, column_path.column); CellPath path = name.collectionElement == null ? null : CellPath.create(name.collectionElement); Cell cell = BufferCell.tombstone(name.column, timestamp, nowInSec, path); update = PartitionUpdate.singleRowUpdate(metadata, dk, BTreeRow.singleCellRow(name.clustering, cell)); } catch (UnknownColumnException e) { throw new org.apache.cassandra.exceptions.InvalidRequestException(e.getMessage()); } } org.apache.cassandra.db.Mutation mutation = new org.apache.cassandra.db.Mutation(update); if (isCommutativeOp) doInsert(consistency_level, Collections.singletonList(new CounterMutation(mutation, ThriftConversion.fromThrift(consistency_level))), queryStartNanoTime); else doInsert(consistency_level, Collections.singletonList(mutation), queryStartNanoTime); }', 'output': 'private void internal_remove(ByteBuffer key, ColumnPath column_path, long timestamp, ConsistencyLevel consistency_level, boolean isCommutativeOp) throws RequestValidationException, UnavailableException, TimedOutException { ThriftClientState cState = state(); String keyspace = cState.getKeyspace(); cState.hasColumnFamilyAccess(keyspace, column_path.column_family, Permission.MODIFY); CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_path.column_family, isCommutativeOp); if (metadata.isView()) throw new org.apache.cassandra.exceptions.InvalidRequestException("Cannot modify Materialized Views directly"); ThriftValidation.validateKey(metadata, key); ThriftValidation.validateColumnPathOrParent(metadata, column_path); if (isCommutativeOp) ThriftConversion.fromThrift(consistency_level).validateCounterForWrite(metadata); DecoratedKey dk = metadata.decorateKey(key); int nowInSec = FBUtilities.nowInSeconds(); PartitionUpdate update; if (column_path.super_column == null && column_path.column == null) { update = PartitionUpdate.fullPartitionDelete(metadata, dk, timestamp, nowInSec); } else if (column_path.super_column != null && column_path.column == null) { Row row = BTreeRow.emptyDeletedRow(Clustering.make(column_path.super_column), Row.Deletion.regular(new DeletionTime(timestamp, nowInSec))); update = PartitionUpdate.singleRowUpdate(metadata, dk, row); } else { try { LegacyLayout.LegacyCellName name = LegacyLayout.decodeCellName(metadata, column_path.super_column, column_path.column); CellPath path = name.collectionElement == null ? null : CellPath.create(name.collectionElement); Cell cell = BufferCell.tombstone(name.column, timestamp, nowInSec, path); update = PartitionUpdate.singleRowUpdate(metadata, dk, BTreeRow.singleCellRow(name.clustering, cell)); } catch (UnknownColumnException e) { throw new org.apache.cassandra.'}
{'instruction': 'Please refactor the following code:', 'input': 'private synchronized TreeViewProvider registerManager(ExplorerManager em, String id, Lookup ctxLookup) { TreeViewProvider p = providers.get(id); if (p != null) { return p; } em.addVetoableChangeListener(e -> { if (ExplorerManager.PROP_ROOT_CONTEXT.equals(e.getPropertyName())) { throw new PropertyVetoException("Root change not allowed", e); } }); ActionMap map = new ActionMap(); map.put(DefaultEditorKit.copyAction, ExplorerUtils.actionCopy(em)); map.put(DefaultEditorKit.cutAction, ExplorerUtils.actionCut(em)); map.put(DefaultEditorKit.pasteAction, ExplorerUtils.actionPaste(em)); map.put("delete", ExplorerUtils.actionDelete(em, false)); Lookup expLookup = ExplorerUtils.createLookup (em, map); final TreeViewProvider tvp = new TreeViewProvider(id, em, this, new ProxyLookup(expLookup, ctxLookup)) { @Override protected void onDidChangeTreeData(Node n, int id) { int rootId = findId(em.getRootContext()); if (n == null) { notifyItemChanged(new NodeChangedParams(rootId)); } else { notifyItemChanged(new NodeChangedParams(rootId, id)); } } }; providers.put(id, tvp); return tvp; }', 'ground_truth': 'private synchronized TreeViewProvider registerManager(ExplorerManager em, String id, Lookup ctxLookup, boolean confirmDelete) { TreeViewProvider p = providers.get(id); if (p != null) { return p; } em.addVetoableChangeListener(e -> { if (ExplorerManager.PROP_ROOT_CONTEXT.equals(e.getPropertyName())) { throw new PropertyVetoException("Root change not allowed", e); } }); ActionMap map = new ActionMap(); map.put(DefaultEditorKit.copyAction, ExplorerUtils.actionCopy(em)); map.put(DefaultEditorKit.cutAction, ExplorerUtils.actionCut(em)); map.put(DefaultEditorKit.pasteAction, ExplorerUtils.actionPaste(em)); map.put("delete", ExplorerUtils.actionDelete(em, confirmDelete)); Lookup expLookup = ExplorerUtils.createLookup (em, map); final TreeViewProvider tvp = new TreeViewProvider(id, em, this, new ProxyLookup(expLookup, ctxLookup)) { @Override protected void onDidChangeTreeData(Node n, int id) { int rootId = findId(em.getRootContext()); if (n == null) { notifyItemChanged(new NodeChangedParams(rootId)); } else { notifyItemChanged(new NodeChangedParams(rootId, id)); } } }; providers.put(id, tvp); return tvp; }', 'output': 'private synchronized TreeViewProvider registerManager(ExplorerManager em, String id, Lookup ctxLookup) { TreeViewProvider p = providers.get(id); if (p != null) { return p; } em.addVetoableChangeListener(e -> { if (ExplorerManager.PROP_ROOT_CONTEXT.equals(e.getPropertyName())) { throw new PropertyVetoException("Root change not allowed", e); } }); ActionMap map = new ActionMap(); map.put(DefaultEditorKit.copyAction, ExplorerUtils.actionCopy(em)); map.put(DefaultEditorKit.cutAction, ExplorerUtils.actionCut(em)); map.put(DefaultEditorKit.pasteAction, ExplorerUtils.actionPaste(em)); map.put("delete", ExplorerUtils.actionDelete(em, false)); Lookup expLookup = ExplorerUtils.createLookup (em, map); final TreeViewProvider tvp = new TreeViewProvider(id, em, this, new ProxyLookup(expLookup, ctxLookup)) { @Override protected void onDidChangeTreeData(Node n, int id) { int rootId = findId(em.getRootContext()); if (n == null) { notifyItemChanged(new NodeChangedParams(rootId)); } else { notifyItemChanged(new NodeChangedParams(rootId, id)); } } }; providers.put(id, tvp); return tvp; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void replaySerializedMutation(RowMutation mutation, long writtenAt, RateLimiter rateLimiter) { int ttl = calculateHintTTL(mutation, writtenAt); if (ttl <= 0) return; Set<InetAddress> liveEndpoints = new HashSet<>(); String ks = mutation.getKeyspaceName(); Token<?> tk = StorageService.getPartitioner().getToken(mutation.key()); int mutationSize = (int) RowMutation.serializer.serializedSize(mutation, VERSION); for (InetAddress endpoint : Iterables.concat(StorageService.instance.getNaturalEndpoints(ks, tk), StorageService.instance.getTokenMetadata().pendingEndpointsFor(tk, ks))) { rateLimiter.acquire(mutationSize); if (endpoint.equals(FBUtilities.getBroadcastAddress())) mutation.apply(); else if (FailureDetector.instance.isAlive(endpoint)) liveEndpoints.add(endpoint); else StorageProxy.writeHintForMutation(mutation, ttl, endpoint); } if (!liveEndpoints.isEmpty()) attemptDirectDelivery(mutation, writtenAt, liveEndpoints); }', 'ground_truth': 'private void replaySerializedMutation(RowMutation mutation, long writtenAt, int version, RateLimiter rateLimiter) { int ttl = calculateHintTTL(mutation, writtenAt); if (ttl <= 0) return; Set<InetAddress> liveEndpoints = new HashSet<>(); String ks = mutation.getKeyspaceName(); Token<?> tk = StorageService.getPartitioner().getToken(mutation.key()); int mutationSize = (int) RowMutation.serializer.serializedSize(mutation, version); for (InetAddress endpoint : Iterables.concat(StorageService.instance.getNaturalEndpoints(ks, tk), StorageService.instance.getTokenMetadata().pendingEndpointsFor(tk, ks))) { rateLimiter.acquire(mutationSize); if (endpoint.equals(FBUtilities.getBroadcastAddress())) mutation.apply(); else if (FailureDetector.instance.isAlive(endpoint)) liveEndpoints.add(endpoint); else StorageProxy.writeHintForMutation(mutation, ttl, endpoint); } if (!liveEndpoints.isEmpty()) attemptDirectDelivery(mutation, writtenAt, liveEndpoints); }', 'output': 'private void replaySerializedMutation(RowMutation mutation, long writtenAt, RateLimiter rateLimiter) { int ttl = calculateHintTTL(mutation, writtenAt); if (ttl <= 0) return; Set<InetAddress> liveEndpoints = new HashSet<>(); String ks = mutation.getKeyspaceName(); Token<?> tk = StorageService.getPartitioner().getToken(mutation.key()); int mutationSize = (int) RowMutation.serializer.serializedSize(mutation, VERSION); for (InetAddress endpoint : Iterables.concat(StorageService.instance.getNaturalEndpoints(ks, tk), StorageService.instance.getTokenMetadata().pendingEndpointsFor(tk, ks))) { rateLimiter.acquire(mutationSize); if (endpoint.equals(FBUtilities.getBroadcastAddress())) mutation.apply(); else if (FailureDetector.instance.isAlive(endpoint)) liveEndpoints.add(endpoint); else StorageProxy.writeHintForMutation(mutation, ttl, endpoint); } if (!liveEndpoints.isEmpty()) attemptDirectDelivery(mutation, writtenAt, liveEndpoints); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static List<Composite> buildMultiColumnEQBound(Bound bound, MultiColumnRestriction.EQ restriction, boolean isReversed, CBuilder builder, QueryOptions options) throws InvalidRequestException { Bound eocBound = isReversed ? Bound.reverse(bound) : bound; for (ByteBuffer component : restriction.values(options)) builder.add(component); Composite prefix = builder.build(); return Collections.singletonList(builder.remainingCount() > 0 && eocBound == Bound.END ? prefix.end() : prefix); }', 'ground_truth': 'private static List<Composite> buildMultiColumnEQBound(Bound bound, List<ColumnDefinition> defs, MultiColumnRestriction.EQ restriction, boolean isReversed, CBuilder builder, QueryOptions options) throws InvalidRequestException { Bound eocBound = isReversed ? Bound.reverse(bound) : bound; List<ByteBuffer> values = restriction.values(options); for (int i = 0; i < values.size(); i++) { ByteBuffer component = values.get(i); if (component == null) throw new InvalidRequestException("Invalid null value in condition for column " + defs.get(i)); builder.add(component); } Composite prefix = builder.build(); return Collections.singletonList(builder.remainingCount() > 0 && eocBound == Bound.END ? prefix.end() : prefix); }', 'output': 'private static List<Composite> buildMultiColumnBound(Bound bound, MultiColumnRestriction.EQ restriction, boolean isReversed, CBuilder builder, QueryOptions options) throws InvalidRequestException { Bound eocBound = isReversed ? Bound.reverse(bound) : bound; for (ByteBuffer component : restriction.values(options)) builder.add(component); Composite prefix = builder.build(); return Collections.singletonList(builder.remainingCount() > 0 && eocBound == Bound.END ? prefix.end() : prefix); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public remove_result( InvalidRequestException ire, UnavailableException ue) { this(); this.ire = ire; this.ue = ue; }', 'ground_truth': 'public remove_result( InvalidRequestException ire, UnavailableException ue, TimedOutException te) { this(); this.ire = ire; this.ue = ue; this.te = te; }', 'output': 'public remove_result( InvalidRequestException ire, UnavailableException ue, TimeoutException te) { this(); this.ire = ire; this.ue = ue; this.te = te; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Collection<Token> getBootstrapTokens(final TokenMetadata metadata, InetAddress address) throws ConfigurationException { String allocationKeyspace = DatabaseDescriptor.getAllocateTokensForKeyspace(); Collection<String> initialTokens = DatabaseDescriptor.getInitialTokens(); if (initialTokens.size() > 0 && allocationKeyspace != null) logger.warn("manually specified tokens override automatic allocation"); if (initialTokens.size() > 0) return getSpecifiedTokens(metadata, initialTokens); int numTokens = DatabaseDescriptor.getNumTokens(); if (numTokens < 1) throw new ConfigurationException("num_tokens must be >= 1"); if (allocationKeyspace != null) return allocateTokens(metadata, address, allocationKeyspace, numTokens); if (numTokens == 1) logger.warn("Picking random token for a single vnode.  You should probably add more vnodes and/or use the automatic token allocation mechanism."); return getRandomTokens(metadata, numTokens); }', 'ground_truth': 'public static Collection<Token> getBootstrapTokens(final TokenMetadata metadata, InetAddress address, int schemaWaitDelay) throws ConfigurationException { String allocationKeyspace = DatabaseDescriptor.getAllocateTokensForKeyspace(); Collection<String> initialTokens = DatabaseDescriptor.getInitialTokens(); if (initialTokens.size() > 0 && allocationKeyspace != null) logger.warn("manually specified tokens override automatic allocation"); if (initialTokens.size() > 0) return getSpecifiedTokens(metadata, initialTokens); int numTokens = DatabaseDescriptor.getNumTokens(); if (numTokens < 1) throw new ConfigurationException("num_tokens must be >= 1"); if (allocationKeyspace != null) return allocateTokens(metadata, address, allocationKeyspace, numTokens, schemaWaitDelay); if (numTokens == 1) logger.warn("Picking random token for a single vnode.  You should probably add more vnodes and/or use the automatic token allocation mechanism."); return getRandomTokens(metadata, numTokens); }', 'output': 'public static Collection<Token> getBootstrapTokens(final TokenMetadata metadata, final InetAddress address) throws ConfigurationException { String allocationKeyspace = DatabaseDescriptor.getAllocateTokensForKeyspace(); Collection<String> initialTokens = DatabaseDescriptor.getInitialTokens(); if (initialTokens.size() > 0 && allocationKeyspace != null) logger.warn("manually specified tokens override automatic allocation"); if (initialTokens.size() > 0) return getSpecifiedTokens(metadata, initialTokens); int numTokens = DatabaseDescriptor.getNumTokens(); if (numTokens < 1) throw new ConfigurationException("num_tokens must be >= 1"); if (allocationKeyspace != null) return allocateTokens(metadata, address, allocationKeyspace, numTokens); if (numTokens == 1) logger.warn("Picking random token for a single vnode.  You should probably add more vnodes and/or use the automatic token allocation mechanism."); return getRandomTokens(metadata, numTokens); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void doAuthSetup() { if (!authSetupCalled.getAndSet(true)) { maybeAddOrUpdateKeyspace(AuthKeyspace.metadata()); DatabaseDescriptor.getRoleManager().setup(); DatabaseDescriptor.getAuthenticator().setup(); DatabaseDescriptor.getAuthorizer().setup(); MigrationManager.instance.register(new AuthMigrationListener()); } }', 'ground_truth': 'private void doAuthSetup(boolean setUpSchema) { if (!authSetupCalled.getAndSet(true)) { if (setUpSchema) evolveSystemKeyspace(AuthKeyspace.metadata(), AuthKeyspace.GENERATION).ifPresent(MigrationManager::announceGlobally); DatabaseDescriptor.getRoleManager().setup(); DatabaseDescriptor.getAuthenticator().setup(); DatabaseDescriptor.getAuthorizer().setup(); MigrationManager.instance.register(new AuthMigrationListener()); } }', 'output': 'private void doAuthSetup() { if (!authSetupCalled.compareAndSet(false, true)) { maybeAddOrUpdateKeyspace(AuthKeyspace.metadata()); DatabaseDescriptor.getRoleManager().setup(); DatabaseDescriptor.getAuthenticator().setup(); DatabaseDescriptor.getAuthorizer().setup(); MigrationManager.instance.register(new AuthMigrationListener()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean reduceScopeForLimitedSpace() { if (partialCompactionsAcceptable() && transaction.originals().size() > 1) { logger.warn("insufficient space to compact all requested files {}", StringUtils.join(transaction.originals(), ", ")); SSTableReader removedSSTable = cfs.getMaxSizeFile(transaction.originals()); transaction.cancel(removedSSTable); return true; } return false; }', 'ground_truth': 'public boolean reduceScopeForLimitedSpace(long expectedSize) { if (partialCompactionsAcceptable() && transaction.originals().size() > 1) { logger.warn("insufficient space to compact all requested files. {}MB required, {}", (float) expectedSize / 1024 / 1024, StringUtils.join(transaction.originals(), ", ")); SSTableReader removedSSTable = cfs.getMaxSizeFile(transaction.originals()); transaction.cancel(removedSSTable); return true; } return false; }', 'output': 'public boolean reduceScopeForLimitedSpace() { if (partialCompactionsAcceptable() && transaction.originals().size() > 1) { logger.info("insufficient space to compact all requested files {}", StringUtils.join(transaction.originals(), ", ")); SSTableReader removedSSTable = cfs.getMaxSizeFile(transaction.originals()); transaction.cancel(removedSSTable); return true; } return false; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void addMethod(FieldNode fieldNode, BlockStatement body, ClassNode type) { int visibility = ACC_PUBLIC; if (fieldNode.isStatic()) visibility |= ACC_STATIC; String propName = capitalize(fieldNode.getName().substring(1)); ClassNode declaringClass = fieldNode.getDeclaringClass(); addGeneratedMethod(declaringClass, "get" + propName, visibility, type, Parameter.EMPTY_ARRAY, ClassNode.EMPTY_ARRAY, body); if (ClassHelper.boolean_TYPE.equals(type)) { addGeneratedMethod(declaringClass, "is" + propName, visibility, type, Parameter.EMPTY_ARRAY, ClassNode.EMPTY_ARRAY, stmt(callThisX("get" + propName))); } }', 'ground_truth': 'private static void addMethod(FieldNode fieldNode, BlockStatement body, ClassNode type, ErrorCollecting xform) { int visibility = ACC_PUBLIC; if (fieldNode.isStatic()) visibility |= ACC_STATIC; String propName = capitalize(fieldNode.getName().substring(1)); ClassNode declaringClass = fieldNode.getDeclaringClass(); addGeneratedMethodOrError(declaringClass, "get" + propName, visibility, type, body, xform, fieldNode); if (ClassHelper.boolean_TYPE.equals(type)) { addGeneratedMethodOrError(declaringClass, "is" + propName, visibility, type, stmt(callThisX("get" + propName)), xform, fieldNode); } MethodNode existing = declaringClass.getDeclaredMethod("set" + propName, params(param(fieldNode.getType(), "value"))); if (existing != null && (existing.getModifiers() & Opcodes.ACC_SYNTHETIC) == 0) { xform.addError("Error during @Lazy processing: invalid explicit setter \'set" + propName + "\' found", fieldNode); } }', 'output': 'private static void addMethod(FieldNode fieldNode, BlockStatement body, Class<?> type) { int visibility = ACC_PUBLIC; if (fieldNode.isStatic()) visibility |= ACC_STATIC; String propName = capitalize(fieldNode.getName().substring(1)); Class<?> declaringClass = fieldNode.getDeclaringClass(); addGeneratedMethod(declaringClass, "get" + propName, visibility, type, Parameter.EMPTY_ARRAY, ClassNode.EMPTY_ARRAY, body); if (ClassHelper.boolean_TYPE.equals(type)) { addGeneratedMethod(declaringClass, "is" + propName, visibility, type, Parameter.EMPTY_ARRAY, ClassNode.EMPTY_ARRAY, stmt(callThisX("get" + propName))); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void addColumn(KeyspaceMetadata keyspace, TableMetadata table, Column column, TableMetadata.Builder tableBuilder, Views.Builder viewsBuilder) { ColumnIdentifier name = column.name; AbstractType<?> type = column.type.prepare(keyspaceName, keyspace.types).getType(); boolean isStatic = column.isStatic; if (null != tableBuilder.getColumn(name)) throw ire("Column with name \'%s\' already exists", name); if (table.isCompactTable()) throw ire("Cannot add new column to a COMPACT STORAGE table"); if (isStatic && table.clusteringColumns().isEmpty()) throw ire("Static columns are only useful (and thus allowed) if the table has at least one clustering column"); ColumnMetadata droppedColumn = table.getDroppedColumn(name.bytes); if (null != droppedColumn) { if (!type.isSerializationCompatibleWith(droppedColumn.type)) { throw ire("Cannot re-add previously dropped column \'%s\' of type %s, incompatible with previous type %s", name, type.asCQL3Type(), droppedColumn.type.asCQL3Type()); } if (droppedColumn.isStatic() != isStatic) { throw ire("Cannot re-add previously dropped column \'%s\' of kind %s, incompatible with previous kind %s", name, isStatic ? ColumnMetadata.Kind.STATIC : ColumnMetadata.Kind.REGULAR, droppedColumn.kind); } if (table.isCounter()) throw ire("Cannot re-add previously dropped counter column %s", name); } if (isStatic) tableBuilder.addStaticColumn(name, type); else tableBuilder.addRegularColumn(name, type); if (!isStatic) { for (ViewMetadata view : keyspace.views.forTable(table.id)) { if (view.includeAllColumns) { ColumnMetadata viewColumn = ColumnMetadata.regularColumn(view.metadata, name.bytes, type); viewsBuilder.put(viewsBuilder.get(view.name()).withAddedRegularColumn(viewColumn)); } } } }', 'ground_truth': 'private void addColumn(KeyspaceMetadata keyspace, TableMetadata table, Column column, boolean ifColumnNotExists, TableMetadata.Builder tableBuilder, Views.Builder viewsBuilder) { ColumnIdentifier name = column.name; AbstractType<?> type = column.type.prepare(keyspaceName, keyspace.types).getType(); boolean isStatic = column.isStatic; if (null != tableBuilder.getColumn(name)) { if (!ifColumnNotExists) throw ire("Column with name \'%s\' already exists", name); return; } if (table.isCompactTable()) throw ire("Cannot add new column to a COMPACT STORAGE table"); if (isStatic && table.clusteringColumns().isEmpty()) throw ire("Static columns are only useful (and thus allowed) if the table has at least one clustering column"); ColumnMetadata droppedColumn = table.getDroppedColumn(name.bytes); if (null != droppedColumn) { if (!type.isSerializationCompatibleWith(droppedColumn.type)) { throw ire("Cannot re-add previously dropped column \'%s\' of type %s, incompatible with previous type %s", name, type.asCQL3Type(), droppedColumn.type.asCQL3Type()); } if (droppedColumn.isStatic() != isStatic) { throw ire("Cannot re-add previously dropped column \'%s\' of kind %s, incompatible with previous kind %s", name, isStatic ? ColumnMetadata.Kind.STATIC : ColumnMetadata.Kind.REGULAR, droppedColumn.kind); } if (table.isCounter()) throw ire("Cannot re-add previously dropped counter column %s", name); } if (isStatic) tableBuilder.addStaticColumn(name, type); else tableBuilder.addRegularColumn(name, type); if (!isStatic) { for (ViewMetadata view : keyspace.views.forTable(table.id)) { if (view.includeAllColumns) { ColumnMetadata viewColumn = ColumnMetadata.regularColumn(view.metadata, name.bytes, type); viewsBuilder.put(viewsBuilder.get(view.name()).withAddedRegularColumn(viewColumn)); } } } }', 'output': 'private void addColumn(KeyspaceMetadata keyspace, TableMetadata table, Column column, TableMetadata.Builder tableBuilder, Views.Builder viewsBuilder) { ColumnIdentifier name = column.name; AbstractType<?> type = column.type.prepare(keyspaceName, keyspace.types).getType(); boolean isStatic = column.isStatic; if (null != tableBuilder.getColumn(name)) throw ire("Column with name \'%s\' already exists", name); if (table.isCompactTable()) throw ire("Cannot add new column to a COMPACT STORAGE table"); if (isStatic && table.clusteringColumns().isEmpty()) throw ire("Static columns are only useful (and thus allowed) if the table has at least one clustering column"); ColumnMetadata droppedColumn = table.getDroppedColumn(name.bytes); if (null != droppedColumn) { if (!type.isSerializationCompatibleWith(droppedColumn.type)) { throw ire("Cannot re-add previously dropped column \'%s\' of type %s, incompatible with previous type %s", name, type.asCQL3Type(), droppedColumn.type.asCQL3Type()); } if (droppedColumn.isStatic() != isStatic) { throw ire("Cannot re-add previously dropped column \'%s\' of kind %s, incompatible with previous kind %s", name, isStatic ? ColumnMetadata.Kind.STATIC : ColumnMetadata.Kind.REGULAR, droppedColumn.kind); } if (table.isCounter()) throw ire("Cannot re-add previously dropped counter column %s", name); } if (isStatic) tableBuilder.addStaticColumn(name, type); else tableBuilder.addRegularColumn(name, type); if (!isStatic) { for (ViewMetadata view : keyspace.views.forTable(table.id)) { if (view.includeAllColumns) { ColumnMetadata viewColumn = ColumnMetadata.regularColumn(view.metadata, name.bytes, type); viewsBuilder.put(viewsBuilder.get(view.name()).withAddedRegularColumn(viewColumn)); } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected ExtendedFilter(ColumnFamilyStore cfs, IFilter filter, int maxResults, boolean maxIsColumns) { assert cfs != null; assert filter != null; this.cfs = cfs; this.originalFilter = filter; this.maxResults = maxResults; this.maxIsColumns = maxIsColumns; if (maxIsColumns) originalFilter.updateColumnsLimit(maxResults); }', 'ground_truth': 'protected ExtendedFilter(ColumnFamilyStore cfs, IFilter filter, int maxResults, boolean maxIsColumns, boolean isPaging) { assert cfs != null; assert filter != null; this.cfs = cfs; this.originalFilter = filter; this.maxResults = maxResults; this.maxIsColumns = maxIsColumns; this.isPaging = isPaging; if (maxIsColumns) originalFilter.updateColumnsLimit(maxResults); if (isPaging && (!(originalFilter instanceof SliceQueryFilter) || ((SliceQueryFilter)originalFilter).finish.remaining() != 0)) throw new IllegalArgumentException("Cross-row paging is only supported for SliceQueryFilter having an empty finish column"); }', 'output': 'public ExtendedFilter(ColumnFamilyStore cfs, IFilter filter, int maxResults, boolean maxIsColumns) { assert cfs != null; assert filter != null; this.cfs = cfs; this.originalFilter = filter; this.maxResults = maxResults; this.maxIsColumns = maxIsColumns; if (maxIsColumns) originalFilter.updateColumnsLimit(maxResults); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public long getPosition(DecoratedKey decoratedKey) { if (!bf.isPresent(partitioner.convertToDiskFormat(decoratedKey))) return -1; Pair<Descriptor, DecoratedKey> unifiedKey = new Pair<Descriptor, DecoratedKey>(desc, decoratedKey); if (keyCache != null && keyCache.getCapacity() > 0) { Long cachedPosition = keyCache.get(unifiedKey); if (cachedPosition != null) { return cachedPosition; } } IndexSummary.KeyPosition sampledPosition = getIndexScanPosition(decoratedKey); if (sampledPosition == null) return -1; int i = 0; Iterator<FileDataInput> segments = ifile.iterator(sampledPosition.indexPosition, INDEX_FILE_BUFFER_BYTES); while (segments.hasNext()) { FileDataInput input = segments.next(); try { while (!input.isEOF() && i++ < IndexSummary.INDEX_INTERVAL) { DecoratedKey indexDecoratedKey = partitioner.convertFromDiskFormat(FBUtilities.readShortByteArray(input)); long dataPosition = input.readLong(); int v = indexDecoratedKey.compareTo(decoratedKey); if (v == 0) { if (keyCache != null && keyCache.getCapacity() > 0) keyCache.put(unifiedKey, Long.valueOf(dataPosition)); return dataPosition; } if (v > 0) return -1; } } catch (IOException e) { throw new IOError(e); } finally { try { input.close(); } catch (IOException e) { logger.error("error closing file", e); } } } return -1; }', 'ground_truth': 'public long getPosition(DecoratedKey decoratedKey, Operator op) { if (op == Operator.EQ && !bf.isPresent(partitioner.convertToDiskFormat(decoratedKey))) return -1; Pair<Descriptor, DecoratedKey> unifiedKey = new Pair<Descriptor, DecoratedKey>(desc, decoratedKey); if (keyCache != null && keyCache.getCapacity() > 0) { Long cachedPosition = keyCache.get(unifiedKey); if (cachedPosition != null) { return cachedPosition; } } IndexSummary.KeyPosition sampledPosition = getIndexScanPosition(decoratedKey); if (sampledPosition == null) return op.apply(1) >= 0 ? 0 : -1; Iterator<FileDataInput> segments = ifile.iterator(sampledPosition.indexPosition, INDEX_FILE_BUFFER_BYTES); while (segments.hasNext()) { FileDataInput input = segments.next(); try { while (!input.isEOF()) { DecoratedKey indexDecoratedKey = partitioner.convertFromDiskFormat(FBUtilities.readShortByteArray(input)); long dataPosition = input.readLong(); int comparison = indexDecoratedKey.compareTo(decoratedKey); int v = op.apply(comparison); if (v == 0) { if (comparison == 0 && keyCache != null && keyCache.getCapacity() > 0) keyCache.put(unifiedKey, Long.valueOf(dataPosition)); return dataPosition; } if (v < 0) return -1; } } catch (IOException e) { throw new IOError(e); } finally { try { input.close(); } catch (IOException e) { logger.error("error closing file", e); } } } return -1; }', 'output': 'public long getPosition(DecoratedKey decoratedKey) { if (!bf.isPresent(partitioner.convertToDiskFormat(decoratedKey))) return -1; Pair<Descriptor, DecoratedKey> unifiedKey = new Pair<Descriptor, DecoratedKey>(desc, decoratedKey); if (keyCache != null && keyCache.getCapacity() > 0) { Long cachedPosition = keyCache.get(unifiedKey); if (cachedPosition != null) { return cachedPosition; } } IndexSummary.KeyPosition sampledPosition = getIndexScanPosition(decoratedKey); if (sampledPosition == null) return -1; int i = 0; Iterator<FileDataInput> segments = ifile.iterator(sampledPosition.indexPosition, INDEX_FILE_BUFFER_BYTES); while (segments.hasNext()) { FileDataInput input = segments.next(); try { while (!input.isEOF() && i++ < IndexSummary.INDEX_INTERVAL) { DecoratedKey indexDecoratedKey = partitioner.convertFromDiskFormat(FBUtilities.readShortByteArray(input)); long dataPosition = input.readLong(); int v = indexDecoratedKey.compareTo(decoratedKey); if (v == 0) { if (keyCache != null && keyCache.getCapacity() > 0) keyCache.put(unifiedKey, Long.valueOf(dataPosition)); return dataPosition; } if (v > 0) return -1; } } catch (IOException e) { throw new IOError(e); } finally { try { input.close(); } catch (IOException e) { logger.error("error closing file", e); } } } return -1; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void loadCache0(BinaryRawReaderEx reader, boolean loc) { PlatformCacheEntryFilter filter = null; Object pred = reader.readObjectDetached(); if (pred != null) filter = platformCtx.createCacheEntryFilter(pred, 0); Object[] args = reader.readObjectArray(); if (loc) cache.localLoadCache(filter, args); else cache.loadCache(filter, args); }', 'ground_truth': 'private void loadCache0(BinaryRawReaderEx reader, boolean loc, IgniteCache cache) { PlatformCacheEntryFilter filter = null; Object pred = reader.readObjectDetached(); if (pred != null) filter = platformCtx.createCacheEntryFilter(pred, 0); Object[] args = reader.readObjectArray(); if (loc) cache.localLoadCache(filter, args); else cache.loadCache(filter, args); }', 'output': 'private void loadCache0(BinaryRawReaderEx reader, boolean loc) { CacheEntryFilter filter = null; Object pred = reader.readObjectDetached(); if (pred != null) filter = platformCtx.createCacheEntryFilter(pred, 0); Object[] args = reader.readObjectArray(); if (loc) cache.localLoadCache(filter, args); else cache.loadCache(filter, args); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static QueryFilter getNamesFilter(DecoratedKey key, String cfName, SortedSet<ByteBuffer> columns) { return new QueryFilter(key, cfName, new NamesQueryFilter(columns)); } public static QueryFilter getNamesFilter(DecoratedKey key, String cfName, ByteBuffer column)\n    { { return new QueryFilter(key, cfName, new NamesQueryFilter(column)); }', 'ground_truth': 'public static QueryFilter getNamesFilter(DecoratedKey key, String cfName, ByteBuffer column, long timestamp) { return new QueryFilter(key, cfName, new NamesQueryFilter(column), timestamp); }', 'output': 'public static QueryFilter getNamesFilter(DecoratedKey key, String cfName, Collection<ByteBuffer> columns) { return new QueryFilter(key, cfName, new NamesQueryFilter(columns)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ResultMessage execute(ConsistencyLevel cl, QueryState state, List<ByteBuffer> variables, int pageSize) throws RequestExecutionException, RequestValidationException { return execute(state.getClientState()); }', 'ground_truth': 'public ResultMessage execute(ConsistencyLevel cl, QueryState state, List<ByteBuffer> variables, int pageSize, PagingState pagingState) throws RequestExecutionException, RequestValidationException { return execute(state.getClientState()); }', 'output': 'public ResultMessage execute(QueryState state, List<ByteBuffer> variables, int pageSize) throws RequestExecutionException, RequestValidationException { return execute(state.getClientState()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void localLoad(Collection<? extends K> keys, @Nullable ExpiryPolicy plc) throws IgniteCheckedException { final boolean replicate = ctx.isDrEnabled(); final AffinityTopologyVersion topVer = ctx.affinity().affinityTopologyVersion(); final ExpiryPolicy plc0 = plc != null ? plc : ctx.expiry(); Collection<KeyCacheObject> keys0 = ctx.cacheKeysView(keys); if (ctx.store().isLocal()) { DataStreamerImpl ldr = ctx.kernalContext().dataStream().dataStreamer(ctx.namex()); try { ldr.skipStore(true); ldr.receiver(new IgniteDrDataStreamerCacheUpdater()); LocalStoreLoadClosure c = new LocalStoreLoadClosure(null, ldr, plc0); ctx.store().localStoreLoadAll(null, keys0, c); c.onDone(); } finally { ldr.closeEx(false); } } else { final GridCacheVersion ver0 = ctx.versions().nextForLoad(); ctx.store().loadAll(null, keys0, new CI2<KeyCacheObject, Object>() { @Override public void apply(KeyCacheObject key, Object val) { long ttl = CU.ttlForLoad(plc0); if (ttl == CU.TTL_ZERO) return; loadEntry(key, val, ver0, null, topVer, replicate, ttl); } }); } }', 'ground_truth': 'public void localLoad(Collection<? extends K> keys, @Nullable ExpiryPolicy plc, final boolean keepBinary) throws IgniteCheckedException { final boolean replicate = ctx.isDrEnabled(); final AffinityTopologyVersion topVer = ctx.affinity().affinityTopologyVersion(); final ExpiryPolicy plc0 = plc != null ? plc : ctx.expiry(); Collection<KeyCacheObject> keys0 = ctx.cacheKeysView(keys); if (ctx.store().isLocal()) { DataStreamerImpl ldr = ctx.kernalContext().dataStream().dataStreamer(ctx.namex()); try { ldr.skipStore(true); ldr.keepBinary(keepBinary); ldr.receiver(new IgniteDrDataStreamerCacheUpdater()); LocalStoreLoadClosure c = new LocalStoreLoadClosure(null, ldr, plc0); ctx.store().localStoreLoadAll(null, keys0, c); c.onDone(); } finally { ldr.closeEx(false); } } else { final GridCacheVersion ver0 = ctx.versions().nextForLoad(); ctx.store().loadAll(null, keys0, new CI2<KeyCacheObject, Object>() { @Override public void apply(KeyCacheObject key, Object val) { long ttl = CU.ttlForLoad(plc0); if (ttl == CU.TTL_ZERO) return; loadEntry(key, val, ver0, null, topVer, replicate, ttl); } }); } }', 'output': 'public void localLoad(Collection<? extends K> keys, @Nullable ExpiryPolicy plc) throws IgniteCheckedException { final boolean replicate = ctx.isDrEnabled(); final AffinityTopologyVersion topVer = ctx.affinity().affinityTopologyVersion(); final ExpiryPolicy plc0 = plc != null ? plc : ctx.expiry(); Collection<KeyCacheObject> keys0 = ctx.cacheKeysView(keys); if (ctx.store().isLocal()) { DataStreamerImpl ldr = ctx.kernalContext().dataStream().dataStreamer(ctx.namex()); try { ldr.skipStore(true); ldr.receiver(new IgniteDrDataStreamerCacheUpdater()); LocalStoreLoadClosure c = new LocalStoreLoadClosure(null, ldr, plc0); ctx.store().localStoreLoadAll(null, keys0, c); c.onDone(); } finally { ldr.closeEx(false); } } else { final GridCacheVersion ver0 = ctx.versions().nextForLoad(); ctx.store().loadAll(null, keys0, new CI2<KeyCacheObject, Object>() { @Override public void apply(KeyCacheObject key, Object val) { long ttl = CU.ttlForLoad(plc0); if (ttl == CU.TTL_ZERO) return; loadEntry(key, val, ver0, null, topVer, replicate, ttl); } }); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ConfigurationDependenciesRecommendationCommand(File recommendationsDir, String recommendationsArtifactsLifetime, ServiceInfo.ServiceAdvisorType serviceAdvisorType, int requestId, StackAdvisorRunner saRunner, AmbariMetaInfo metaInfo) { super(recommendationsDir, recommendationsArtifactsLifetime, serviceAdvisorType, requestId, saRunner, metaInfo); }', 'ground_truth': 'public ConfigurationDependenciesRecommendationCommand(File recommendationsDir, String recommendationsArtifactsLifetime, ServiceInfo.ServiceAdvisorType serviceAdvisorType, int requestId, StackAdvisorRunner saRunner, AmbariMetaInfo metaInfo, AmbariServerConfigurationHandler ambariServerConfigurationHandler) { super(recommendationsDir, recommendationsArtifactsLifetime, serviceAdvisorType, requestId, saRunner, metaInfo, ambariServerConfigurationHandler); }', 'output': 'public ConfigurationDependenciesRecommendationCommand(String recommendationsDir, String recommendationsArtifactsLifetime, ServiceInfo.ServiceAdvisorType serviceAdvisorType, int requestId, StackAdvisorRunner saRunner, AmbariMetaInfo metaInfo) { super(recommendationsDir, recommendationsArtifactsLifetime, serviceAdvisorType, requestId, saRunner, metaInfo); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ContextAction( ContextAction.Performer<? super T> performer, ContextSelection selectMode, Lookup actionContext, Class<T> type, boolean surviveFocusChange ) { if (performer == null) { throw new NullPointerException("Has to provide a key!"); } this.type = type; this.selectMode = selectMode; this.performer = performer; this.global = ContextManager.findManager(actionContext, surviveFocusChange); }', 'ground_truth': 'public ContextAction( ContextAction.Performer<? super T> performer, ContextSelection selectMode, Lookup actionContext, Class<T> type, boolean surviveFocusChange, StatefulMonitor enableMonitor ) { if (performer == null) { throw new NullPointerException("Has to provide a key!"); } this.type = type; this.selectMode = selectMode; this.performer = performer; this.global = ContextManager.findManager(actionContext, surviveFocusChange); this.enableMonitor = enableMonitor; if (enableMonitor != null) { LOG.log(Level.FINE, "Setting enable monitor {0}: {1}", new Object[] { this, enableMonitor} ); } }', 'output': 'public ContextAction( Performer<? super T> performer, ContextSelection selectMode, Lookup actionContext, Class<T> type, boolean surviveFocusChange ) { if (performer == null) { throw new NullPointerException("Has to provide a key!"); } this.type = type; this.selectMode = selectMode; this.performer = performer; this.global = ContextManager.findManager(actionContext, surviveFocusChange); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static List<SSTableReader> intersecting(Collection<SSTableReader> sstables, Range<Token> range) { ArrayList<SSTableReader> filtered = new ArrayList<>(); for (SSTableReader sstable : sstables) { Range<Token> sstableRange = new Range<>(sstable.first.getToken(), sstable.last.getToken()); if (range == null || sstableRange.intersects(range)) filtered.add(sstable); } return filtered; }', 'ground_truth': 'public static Collection<SSTableReader> intersecting(Collection<SSTableReader> sstables, Collection<Range<Token>> ranges) { if (ranges == null) return Lists.newArrayList(sstables); Set<SSTableReader> filtered = new HashSet<>(); for (Range<Token> range : ranges) { for (SSTableReader sstable : sstables) { Range<Token> sstableRange = new Range<>(sstable.first.getToken(), sstable.last.getToken()); if (range == null || sstableRange.intersects(range)) filtered.add(sstable); } } return filtered; }', 'output': 'public static List<SSTableReader> intersecting(List<SSTableReader> sstables, Range<Token> range) { ArrayList<SSTableReader> filtered = new ArrayList<>(); for (SSTableReader sstable : sstables) { Range<Token> sstableRange = new Range<>(sstable.first.getToken(), sstable.last.getToken()); if (range == null || sstableRange.intersects(range)) filtered.add(sstable); } return filtered; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GetComponentLayoutValidationCommand(File recommendationsDir, String stackAdvisorScript, int requestId, StackAdvisorRunner saRunner) { super(recommendationsDir, stackAdvisorScript, requestId, saRunner); }', 'ground_truth': 'public GetComponentLayoutValidationCommand(File recommendationsDir, String stackAdvisorScript, int requestId, StackAdvisorRunner saRunner, AmbariMetaInfo metaInfo) { super(recommendationsDir, stackAdvisorScript, requestId, saRunner, metaInfo); }', 'output': 'public GetComponentLayoutValidationCommand(String recommendationsDir, String stackAdvisorScript, int requestId, StackAdvisorRunner saRunner) { super(recommendationsDir, stackAdvisorScript, requestId, saRunner); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static ColumnFamily makeCf(CFMetaData metadata, String columnValue1, String columnValue2) { ColumnFamily cf = ArrayBackedSortedColumns.factory.create(metadata); if (columnValue1 != null) cf.addColumn(new BufferCell(getColumnName(metadata, "c1"), bytes(columnValue1))); if (columnValue2 != null) cf.addColumn(new BufferCell(getColumnName(metadata, "c2"), bytes(columnValue2))); return cf; }', 'ground_truth': 'private static PartitionUpdate makeCf(CFMetaData metadata, String key, String columnValue1, String columnValue2) { PartitionUpdate update = new PartitionUpdate(metadata, Util.dk(key), metadata.partitionColumns(), 1); LivenessInfo info = SimpleLivenessInfo.forUpdate(FBUtilities.timestampMicros(), LivenessInfo.NO_TTL, FBUtilities.nowInSeconds(), metadata); if (columnValue1 != null) { update.writer().writeCell(metadata.getColumnDefinition(bytes("c1")), false, bytes(columnValue1), info, null); update.writer().endOfRow(); } if (columnValue2 != null) { update.writer().writeCell(metadata.getColumnDefinition(bytes("c2")), false, bytes(columnValue1), info, null); update.writer().endOfRow(); } return update; }', 'output': 'private static ColumnFamily makeCf(CFMetaData metadata, String columnValue1, String columnValue2, String columnValue3) { ColumnFamily cf = ArrayBackedSortedColumns.factory.create(metadata); if (columnValue1 != null) cf.addColumn(new BufferCell(getColumnName(metadata, "c1"), bytes(columnValue1))); if (columnValue2 != null) cf.addColumn(new BufferCell(getColumnName(metadata, "c2"), bytes(columnValue2))); if (columnValue3 != null) cf.addColumn(new BufferCell(getColumnName(metadata, "c3"), bytes(columnValue3))); return cf; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected SSTable(Descriptor descriptor, Set<Component> components, CFMetaData metadata) { assert descriptor != null; assert components != null; assert metadata != null; this.descriptor = descriptor; Set<Component> dataComponents = new HashSet<>(components); this.compression = dataComponents.contains(Component.COMPRESSION_INFO); this.components = new CopyOnWriteArraySet<>(dataComponents); this.metadata = metadata; }', 'ground_truth': 'protected SSTable(Descriptor descriptor, Set<Component> components, CFMetaData metadata, DiskOptimizationStrategy optimizationStrategy) { assert descriptor != null; assert components != null; assert metadata != null; this.descriptor = descriptor; Set<Component> dataComponents = new HashSet<>(components); this.compression = dataComponents.contains(Component.COMPRESSION_INFO); this.components = new CopyOnWriteArraySet<>(dataComponents); this.metadata = metadata; this.optimizationStrategy = Objects.requireNonNull(optimizationStrategy); }', 'output': 'protected SSTable(Descriptor descriptor, Collection<Component> components, CFMetaData metadata) { assert descriptor != null; assert components != null; assert metadata != null; this.descriptor = descriptor; Set<Component> dataComponents = new HashSet<>(components); this.compression = dataComponents.contains(Component.COMPRESSION_INFO); this.components = new CopyOnWriteArraySet<>(dataComponents); this.metadata = metadata; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static SSTableReader open(String dataFileName, IPartitioner partitioner) throws IOException { assert partitioner != null; assert openedFiles.get(dataFileName) == null; long start = System.currentTimeMillis(); SSTableReader sstable = new SSTableReader(dataFileName, partitioner); sstable.loadIndexFile(); sstable.loadBloomFilter(); if (logger.isDebugEnabled()) logger.debug("INDEX LOAD TIME for "  + dataFileName + ": " + (System.currentTimeMillis() - start) + " ms."); return sstable; }', 'ground_truth': 'public static SSTableReader open(String dataFileName, IPartitioner partitioner, double cacheFraction) throws IOException { assert partitioner != null; assert openedFiles.get(dataFileName) == null; long start = System.currentTimeMillis(); SSTableReader sstable = new SSTableReader(dataFileName, partitioner); sstable.loadIndexFile(); sstable.loadBloomFilter(); if (cacheFraction > 0) { sstable.keyCache = createKeyCache((int)((sstable.getIndexPositions().size() + 1) * INDEX_INTERVAL * cacheFraction)); } if (logger.isDebugEnabled()) logger.debug("INDEX LOAD TIME for "  + dataFileName + ": " + (System.currentTimeMillis() - start) + " ms."); return sstable; }', 'output': 'public static SSTableReader open(String dataFileName, IPartitioner partitioner, int replicationFactor) throws IOException { assert partitioner != null; assert openedFiles.get(dataFileName) == null; long start = System.currentTimeMillis(); SSTableReader sstable = new SSTableReader(dataFileName, partitioner, replicationFactor); sstable.loadIndexFile(); sstable.loadBloomFilter(); if (logger.isDebugEnabled()) logger.debug("INDEX LOAD TIME for "  + dataFileName + ": " + (System.currentTimeMillis() - start) + " ms."); return sstable; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public StackDefinedPropertyProvider(Resource.Type type, MetricsService metricsService, JMXHostProvider jmxHostProvider, MetricHostProvider metricHostProvider, StreamProvider streamProvider, String clusterPropertyId, String hostPropertyId, String componentPropertyId, String jmxStatePropertyId, PropertyProvider defaultJmxPropertyProvider, PropertyProvider defaultGangliaPropertyProvider) { this(type, jmxHostProvider, metricHostProvider, streamProvider, clusterPropertyId, hostPropertyId, componentPropertyId, jmxStatePropertyId, defaultJmxPropertyProvider, defaultGangliaPropertyProvider); this.metricsService = metricsService; }', 'ground_truth': 'public StackDefinedPropertyProvider(Resource.Type type, MetricsService metricsService, JMXHostProvider jmxHostProvider, MetricHostProvider metricHostProvider, MetricsServiceProvider serviceProvider, StreamProvider streamProvider, String clusterPropertyId, String hostPropertyId, String componentPropertyId, String jmxStatePropertyId, PropertyProvider defaultJmxPropertyProvider, PropertyProvider defaultGangliaPropertyProvider) { this(type, jmxHostProvider, metricHostProvider, serviceProvider, streamProvider, clusterPropertyId, hostPropertyId, componentPropertyId, jmxStatePropertyId, defaultJmxPropertyProvider, defaultGangliaPropertyProvider); this.metricsService = metricsService; }', 'output': 'public StackDefinedPropertyProvider(Resource.Type type, JMXHostProvider jmxHostProvider, MetricHostProvider metricHostProvider, StreamProvider streamProvider, String clusterPropertyId, String hostPropertyId, String componentPropertyId, String jmxStatePropertyId, PropertyProvider defaultJmxPropertyProvider, PropertyProvider defaultGangliaPropertyProvider) { this(type, jmxHostProvider, metricHostProvider, streamProvider, clusterPropertyId, hostPropertyId, componentPropertyId, jmxStatePropertyId, defaultJmxPropertyProvider, defaultGangliaPropertyProvider); this.metricsService = null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ReuseListNew(int cacheId, PageMemory pageMem, IgniteWriteAheadLogManager wal, long metaPageId) throws IgniteCheckedException { super(cacheId, pageMem, wal, metaPageId); reuseList = this; }', 'ground_truth': 'public ReuseListNew(int cacheId, PageMemory pageMem, IgniteWriteAheadLogManager wal, long metaPageId, boolean initNew) throws IgniteCheckedException { super(cacheId, pageMem, 1, wal, metaPageId, initNew); reuseList = this; }', 'output': 'public ReuseListNew(int cacheId, PageMemory pageMem, IgniteWriteAheadLogManager wal, long metaPageId, int grpId) throws IgniteCheckedException { super(cacheId, pageMem, wal, metaPageId, grpId); reuseList = this; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Tail(Page page, long buf, BPlusIO<L> io, byte type, int lvl) { assert type == BACK || type == EXACT || type == FORWARD : type; assert lvl >= 0 && lvl <= Byte.MAX_VALUE : lvl; assert page != null; this.page = page; this.buf = buf; this.io = io; this.type = type; this.lvl = (byte)lvl; }', 'ground_truth': 'private Tail(long pageId, long page, long buf, BPlusIO<L> io, byte type, int lvl) { assert type == BACK || type == EXACT || type == FORWARD : type; assert lvl >= 0 && lvl <= Byte.MAX_VALUE : lvl; assert pageId != 0L; assert page != 0L; assert buf != 0L; this.id = pageId; this.page = page; this.buf = buf; this.io = io; this.type = type; this.lvl = (byte)lvl; }', 'output': 'private Tail(Page page, long pos, BPlusIO<L> io, byte type, int lvl) { assert type == BACK || type == EXACT || type == FORWARD : type; assert lvl >= 0 && lvl <= Byte.MAX_VALUE : lvl; assert page != null; this.page = page; this.pos = pos; this.io = io; this.type = type; this.lvl = (byte)lvl; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Set<File> createFolders() throws IOException { File folder = tempFolder.newFolder(); Set<File> folders = new HashSet<>(); for (String folderName : Arrays.asList("foo", "bar", "buzz")) { File subfolder = new File(folder, folderName); subfolder.mkdir(); assertThat(subfolder).exists(); folders.add(subfolder); } return folders; }', 'ground_truth': 'public static Set<File> createFolders(TemporaryFolder temp) throws IOException { File folder = new File(temp.newFolder()); Set<File> folders = new HashSet<>(); for (String folderName : Arrays.asList("foo", "bar", "buzz")) { File subfolder = new File(folder, folderName); subfolder.tryCreateDirectories(); assertThat(subfolder.exists()); folders.add(subfolder); }; return folders; }', 'output': 'public List<File> createFolders() throws IOException { File folder = tempFolder.newFolder(); List<File> folders = new ArrayList<>(); for (String folderName : Arrays.asList("foo", "bar", "buzz")) { File subfolder = new File(folder, folderName); subfolder.mkdir(); assertThat(subfolder).exists(); folders.add(subfolder); } return folders; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ResultMessage process(String queryString, ConsistencyLevel cl, QueryState queryState) throws RequestExecutionException, RequestValidationException { return instance.process(queryString, queryState, QueryOptions.forInternalCalls(cl, Collections.<ByteBuffer>emptyList())); }', 'ground_truth': 'public static ResultMessage process(String queryString, ConsistencyLevel cl, QueryState queryState, long queryStartNanoTime) throws RequestExecutionException, RequestValidationException { return instance.process(queryString, queryState, QueryOptions.forInternalCalls(cl, Collections.<ByteBuffer>emptyList()), queryStartNanoTime); }', 'output': 'public static ResultMessage process(String queryString, ConsistencyLevel cl, QueryState queryState, QueryOptions options) throws RequestExecutionException, RequestValidationException { return instance.process(queryString, queryState, options); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private int[] evaluateIntervalLiteralAsHourToSecond( int sign, String value, String originalValue, SqlParserPos pos) { BigDecimal hour; BigDecimal minute; BigDecimal second; BigDecimal secondFrac; boolean hasFractionalSecond; String intervalPatternWithFracSec = "(\\\\d+):(\\\\d{1,2}):(\\\\d{1,2})\\\\.(\\\\d{1," + fractionalSecondPrecision + "})"; String intervalPatternWithoutFracSec = "(\\\\d+):(\\\\d{1,2}):(\\\\d{1,2})"; Matcher m = Pattern.compile(intervalPatternWithFracSec).matcher(value); if (m.matches()) { hasFractionalSecond = true; } else { m = Pattern.compile(intervalPatternWithoutFracSec).matcher(value); hasFractionalSecond = false; } if (m.matches()) { try { hour = parseField(m, 1); minute = parseField(m, 2); second = parseField(m, 3); } catch (NumberFormatException e) { throw invalidValueException(pos, originalValue); } if (hasFractionalSecond) { secondFrac = normalizeSecondFraction(m.group(4)); } else { secondFrac = ZERO; } checkLeadFieldInRange(sign, hour, TimeUnit.HOUR, pos); if (!(isSecondaryFieldInRange(minute, TimeUnit.MINUTE)) || !(isSecondaryFieldInRange(second, TimeUnit.SECOND)) || !(isFractionalSecondFieldInRange(secondFrac))) { throw invalidValueException(pos, originalValue); } return fillIntervalValueArray( sign, ZERO, hour, minute, second, secondFrac); } else { throw invalidValueException(pos, originalValue); } }', 'ground_truth': 'private int[] evaluateIntervalLiteralAsHourToSecond( RelDataTypeSystem typeSystem, int sign, String value, String originalValue, SqlParserPos pos) { BigDecimal hour; BigDecimal minute; BigDecimal second; BigDecimal secondFrac; boolean hasFractionalSecond; final int fractionalSecondPrecision = getFractionalSecondPrecision(typeSystem); String intervalPatternWithFracSec = "(\\\\d+):(\\\\d{1,2}):(\\\\d{1,2})\\\\.(\\\\d{1," + fractionalSecondPrecision + "})"; String intervalPatternWithoutFracSec = "(\\\\d+):(\\\\d{1,2}):(\\\\d{1,2})"; Matcher m = Pattern.compile(intervalPatternWithFracSec).matcher(value); if (m.matches()) { hasFractionalSecond = true; } else { m = Pattern.compile(intervalPatternWithoutFracSec).matcher(value); hasFractionalSecond = false; } if (m.matches()) { try { hour = parseField(m, 1); minute = parseField(m, 2); second = parseField(m, 3); } catch (NumberFormatException e) { throw invalidValueException(pos, originalValue); } if (hasFractionalSecond) { secondFrac = normalizeSecondFraction(m.group(4)); } else { secondFrac = ZERO; } checkLeadFieldInRange(typeSystem, sign, hour, TimeUnit.HOUR, pos); if (!(isSecondaryFieldInRange(minute, TimeUnit.MINUTE)) || !(isSecondaryFieldInRange(second, TimeUnit.SECOND)) || !(isFractionalSecondFieldInRange(secondFrac))) { throw invalidValueException(pos, originalValue); } return fillIntervalValueArray( sign, ZERO, hour, minute, second, secondFrac); } else { throw invalidValueException(pos, originalValue); } }', 'output': 'private int[] evaluateIntervalLiteralAsHourToSecond( int sign, String value, String originalValue, SqlParserPos pos) { BigDecimal hour; BigDecimal minute; BigDecimal second; BigDecimal secondFrac; boolean hasFractionalSecond; String intervalPatternWithFracSec = "(\\\\d+):(\\\\d{1,2}):(\\\\d{1,2})\\\\.(\\\\d{1," + fractionalSecondPrecision + "})"; String intervalPatternWithoutFracSec = "(\\\\d+):(\\\\d{1,2}):(\\\\d{1,2})"; Matcher m = Pattern.compile(intervalPatternWithFracSec).matcher(value); if (m.matches()) { hasFractionalSecond = true; } else { m = Pattern.compile(intervalPatternWithoutFracSec).matcher(value); hasFractionalSecond = false; } if (m.matches()) { try { hour = parseField(m, 1); minute = parseField(m, 2); second = parseField(m, 3); } catch (NumberFormatException e) { throw invalidValueException(pos, originalValue); } if (hasFractionalSecond) { secondFrac = normalizeSecondFraction(m.group(4)); } else { secondFrac = ZERO; } checkLeadFieldInRange(sign, hour, TimeUnit.HOUR, pos); if (!(isSecondaryFieldInRange(minute, TimeUnit.MINUTE)) || !(isSecondaryFieldInRange(second, TimeUnit.SECOND)) || !(isFractionalSecondFieldInRange(secondFrac))) { throw invalidValueException(pos, originalValue); } return fillIntervalValueArray( sign, ZERO, hour, minute, second, secondFrac); } else { throw invalidValueException(pos, originalValue); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public KerberosIdentityDescriptor(String name, KerberosPrincipalDescriptor principal, KerberosKeytabDescriptor keytab) { setName(name); setPrincipalDescriptor(principal); setKeytabDescriptor(keytab); }', 'ground_truth': 'public KerberosIdentityDescriptor(String name, KerberosPrincipalDescriptor principal, KerberosKeytabDescriptor keytab, Predicate when) { setName(name); setPrincipalDescriptor(principal); setKeytabDescriptor(keytab); setWhen(when); }', 'output': 'public KerberosIdentityDescriptor(String name, KerberosPrincipalDescriptor principal, KerberosKeytabDescriptor keytab, String realm) { setName(name); setPrincipalDescriptor(principal); setKeytabDescriptor(keytab); setRealm(realm); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public int addRow( CacheObjectContext coctx, ByteBuffer buf, CacheObject key, CacheObject val, GridCacheVersion ver, int entrySizeWithItem ) throws IgniteCheckedException { if (entrySizeWithItem > buf.capacity() - ITEMS_OFF) throw new IgniteException("Too big entry [key=" + key + ", val=" + val + ", entrySizeWithItem=" + entrySizeWithItem + ", activeCap=" + (buf.capacity() - ITEMS_OFF) + \']\'); int directCnt = getDirectCount(buf); int indirectCnt = getIndirectCount(buf); int dataOff = getFirstEntryOffset(buf); if (!isEnoughSpace(entrySizeWithItem, dataOff, directCnt, indirectCnt)) { dataOff = compactDataEntries(buf, directCnt); assert isEnoughSpace(entrySizeWithItem, dataOff, directCnt, indirectCnt); } dataOff -= entrySizeWithItem - ITEM_SIZE; writeRowData(coctx, buf, dataOff, entrySizeWithItem, key, val, ver); setFirstEntryOffset(buf, dataOff); int itemId = insertItem(buf, dataOff, directCnt, indirectCnt); assert check(itemId): itemId; assert getIndirectCount(buf) <= getDirectCount(buf); setFreeSpace(buf, getFreeSpace(buf) - entrySizeWithItem + (getIndirectCount(buf) != indirectCnt ? ITEM_SIZE : 0)); assert getFreeSpace(buf) >= 0; assert (itemId & ~0xFF) == 0; return itemId; }', 'ground_truth': 'public int addRow( CacheObjectContext coctx, ByteBuffer buf, CacheObject key, CacheObject val, GridCacheVersion ver, long expireTime, int entrySizeWithItem ) throws IgniteCheckedException { if (entrySizeWithItem > buf.capacity() - ITEMS_OFF) throw new IgniteException("Too big entry [key=" + key + ", val=" + val + ", entrySizeWithItem=" + entrySizeWithItem + ", activeCap=" + (buf.capacity() - ITEMS_OFF) + \']\'); int directCnt = getDirectCount(buf); int indirectCnt = getIndirectCount(buf); int dataOff = getFirstEntryOffset(buf); if (!isEnoughSpace(entrySizeWithItem, dataOff, directCnt, indirectCnt)) { dataOff = compactDataEntries(buf, directCnt); assert isEnoughSpace(entrySizeWithItem, dataOff, directCnt, indirectCnt); } dataOff -= entrySizeWithItem - ITEM_SIZE; writeRowData(coctx, buf, dataOff, entrySizeWithItem, key, val, ver, expireTime); setFirstEntryOffset(buf, dataOff); int itemId = insertItem(buf, dataOff, directCnt, indirectCnt); assert check(itemId): itemId; assert getIndirectCount(buf) <= getDirectCount(buf); setFreeSpace(buf, getFreeSpace(buf) - entrySizeWithItem + (getIndirectCount(buf) != indirectCnt ? ITEM_SIZE : 0)); assert getFreeSpace(buf) >= 0; assert (itemId & ~0xFF) == 0; return itemId; }', 'output': 'public int addRow( CacheObjectContext coctx, ByteBuffer buf, CacheObject key, CacheObject val, GridCacheVersion ver, int entrySizeWithItem ) throws IgniteCheckedException { if (entrySizeWithItem > buf.capacity() - ITEMS_OFF) throw new IgniteException("Too big entry [key=" + key + ", val=" + val + ", entrySizeWithItem=" + entrySizeWithItem + ", activeCap=" + (buf.capacity() - ITEMS_OFF) + \']\'); int directCnt = getDirectCount(buf); int indirectCnt = getIndirectCount(buf); int dataOff = getFirstEntryOffset(buf); if (!isEnoughSpace(entrySizeWithItem, dataOff, directCnt, indirectCnt)) { dataOff = compactDataEntries(buf, directCnt); assert isEnoughSpace(entrySizeWithItem, dataOff, directCnt, indirectCnt); } dataOff -= entrySizeWithItem - ITEM_SIZE; writeRowData(coctx, buf, dataOff, entrySizeWithItem, key, val, ver); setFirstEntryOffset(buf, dataOff); int itemId = insertItem(buf, dataOff, directCnt, indirectCnt); assert check(itemId): itemId; assert getIndirectCount(buf) <= getDirectCount(buf); setFreeSpace(buf, getFreeSpace(buf) - entrySizeWithItem + (getIndirectCount(buf) != indirectCnt ? ITEM_SIZE : 0)); assert getFreeSpace(buf) >= 0; assert (itemId & ~0xFF) == 0; return itemId; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void addRowFragment( long pageAddr, byte[] payload, long lastLink, int pageSize ) throws IgniteCheckedException { addRowFragment(null, pageAddr, 0, 0, lastLink, null, payload, pageSize); }', 'ground_truth': 'public void addRowFragment( long pageId, long pageAddr, byte[] payload, long lastLink, int pageSize ) throws IgniteCheckedException { addRowFragment(null, pageId, pageAddr, 0, 0, lastLink, null, payload, pageSize); }', 'output': 'public void addRowFragment( ClusterNode node, long pageAddr, byte[] payload, long lastLink, int pageSize ) throws IgniteCheckedException { addRowFragment(node, pageAddr, 0, 0, lastLink, null, payload, pageSize); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<String> generateDestinationFilenames(FilenamePolicy policy, int numFiles) { List<String> filenames = new ArrayList<>(); for (int i = 0; i < numFiles; i++) { filenames.add(policy.unwindowedFilename(new Context(i, numFiles))); } return filenames; }', 'ground_truth': 'public List<ResourceId> generateDestinationFilenames( ResourceId outputDirectory, FilenamePolicy policy, int numFiles) { List<ResourceId> filenames = new ArrayList<>(); for (int i = 0; i < numFiles; i++) { filenames.add(policy.unwindowedFilename(outputDirectory, new Context(i, numFiles), "")); } return filenames; }', 'output': 'public List<String> generateDestinationFilenames(FilenamePolicy policy, int numFiles, int windowSize) { List<String> filenames = new ArrayList<>(); for (int i = 0; i < numFiles; i++) { filenames.add(policy.unwindowedFilename(new Context(i, numFiles, windowSize))); } return filenames; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void send_get_slice_super(String table, String key, String column_family, byte[] start, byte[] finish, boolean is_ascending, int count) throws TException { oprot_.writeMessageBegin(new TMessage("get_slice_super", TMessageType.CALL, seqid_)); get_slice_super_args args = new get_slice_super_args(); args.table = table; args.key = key; args.column_family = column_family; args.start = start; args.finish = finish; args.is_ascending = is_ascending; args.count = count; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'ground_truth': 'public void send_get_slice_super(String table, String key, String column_family, byte[] start, byte[] finish, boolean is_ascending, int count, int consistency_level) throws TException { oprot_.writeMessageBegin(new TMessage("get_slice_super", TMessageType.CALL, seqid_)); get_slice_super_args args = new get_slice_super_args(); args.table = table; args.key = key; args.column_family = column_family; args.start = start; args.finish = finish; args.is_ascending = is_ascending; args.count = count; args.consistency_level = consistency_level; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'output': 'public void send_get_slice_super(String table, String key, String column_family, long start, long finish, boolean is_ascending, int count) throws TException { oprot_.writeMessageBegin(new TMessage("get_slice_super", TMessageType.CALL, seqid_)); get_slice_super_args args = new get_slice_super_args(); args.table = table; args.key = key; args.column_family = column_family; args.start = start; args.finish = finish; args.is_ascending = is_ascending; args.count = count; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public MainViewVaa( final MetaModelContext metaModelContext, final UiActionHandler uiActionHandler, final HeaderUiModelProvider headerUiModelProvider) { this.commonContext = IsisAppCommonContext.of(metaModelContext); this.uiActionHandler = uiActionHandler; this.headerUiModelProvider = headerUiModelProvider; }', 'ground_truth': 'public MainViewVaa( final MetaModelContext metaModelContext, final IsisInteractionFactory isisInteractionFactory, final UiActionHandler uiActionHandler, final HeaderUiModelProvider headerUiModelProvider) { this.commonContext = IsisAppCommonContext.of(metaModelContext); this.uiActionHandler = uiActionHandler; this.isisInteractionFactory = isisInteractionFactory; this.headerUiModelProvider = headerUiModelProvider; }', 'output': 'public MainViewVaa( final MetaModelContext metaModelContext, final UiActionHandler uiActionHandler, final HeaderUiModelProvider headerUiModelProvider, final EventBus eventBus) { this.commonContext = IsisAppCommonContext.of(metaModelContext); this.uiActionHandler = uiActionHandler; this.headerUiModelProvider = headerUiModelProvider; this.eventBus = eventBus; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Expression translateCondition( List<Expression> inputs, RexProgram program, List<Expression> list) { List<Expression> x = new RexToLixTranslator(program, inputs) .translate(list, Collections.singletonList(program.getCondition())); assert x.size() == 1; return x.get(0); }', 'ground_truth': 'public static Expression translateCondition( List<Expression> inputs, RexProgram program, JavaTypeFactory typeFactory, List<Expression> list) { List<Expression> x = new RexToLixTranslator(program, typeFactory, inputs) .translate( list, Collections.singletonList(program.getCondition())); assert x.size() == 1; return x.get(0); }', 'output': 'public static Expression translateCondition( List<Expression> inputs, RexProgram program, List<Expression> list, RelDataType rowType) { List<Expression> x = new RexToLixTranslator(program, inputs) .translate(list, Collections.singletonList(program.getCondition()), rowType); assert x.size() == 1; return x.get(0); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void init(BlueprintDAO blueprintDAO, Gson gson) { dao            = blueprintDAO; jsonSerializer = gson; }', 'ground_truth': 'public static void init(BlueprintDAO blueprintDAO, Gson gson, AmbariMetaInfo metaInfo) { dao            = blueprintDAO; jsonSerializer = gson; stackInfo      = metaInfo; }', 'output': 'public static void init(BlueprintDAO blueprintDAO, Gson gson, BlueprintService blueprintService) { dao            = blueprintDAO; jsonSerializer = gson; blueprintService = blueprintService; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GridNearLockRequest( int cacheId, @NotNull AffinityTopologyVersion topVer, UUID nodeId, long threadId, IgniteUuid futId, GridCacheVersion lockVer, boolean isInTx, boolean isRead, boolean retVal, TransactionIsolation isolation, boolean isInvalidate, long timeout, int keyCnt, int txSize, boolean syncCommit, @Nullable UUID subjId, int taskNameHash, long createTtl, long accessTtl, boolean skipStore, boolean keepBinary, boolean firstClientReq, boolean addDepInfo ) { super( cacheId, nodeId, lockVer, threadId, futId, lockVer, isInTx, isRead, isolation, isInvalidate, timeout, keyCnt, txSize, skipStore, keepBinary, addDepInfo); assert topVer.compareTo(AffinityTopologyVersion.ZERO) > 0; this.topVer = topVer; this.subjId = subjId; this.taskNameHash = taskNameHash; this.createTtl = createTtl; this.accessTtl = accessTtl; dhtVers = new GridCacheVersion[keyCnt]; setFlag(syncCommit, SYNC_COMMIT_FLAG_MASK); setFlag(firstClientReq, FIRST_CLIENT_REQ_FLAG_MASK); setFlag(retVal, NEED_RETURN_VALUE_FLAG_MASK); }', 'ground_truth': 'public GridNearLockRequest( int cacheId, @NotNull AffinityTopologyVersion topVer, UUID nodeId, long threadId, IgniteUuid futId, GridCacheVersion lockVer, boolean isInTx, boolean isRead, boolean retVal, TransactionIsolation isolation, boolean isInvalidate, long timeout, int keyCnt, int txSize, boolean syncCommit, @Nullable UUID subjId, int taskNameHash, long createTtl, long accessTtl, boolean skipStore, boolean keepBinary, boolean firstClientReq, boolean nearCache, boolean addDepInfo ) { super( cacheId, nodeId, lockVer, threadId, futId, lockVer, isInTx, isRead, isolation, isInvalidate, timeout, keyCnt, txSize, skipStore, keepBinary, addDepInfo); assert topVer.compareTo(AffinityTopologyVersion.ZERO) > 0; this.topVer = topVer; this.subjId = subjId; this.taskNameHash = taskNameHash; this.createTtl = createTtl; this.accessTtl = accessTtl; dhtVers = new GridCacheVersion[keyCnt]; setFlag(syncCommit, SYNC_COMMIT_FLAG_MASK); setFlag(firstClientReq, FIRST_CLIENT_REQ_FLAG_MASK); setFlag(retVal, NEED_RETURN_VALUE_FLAG_MASK); setFlag(nearCache, NEAR_CACHE_FLAG_MASK); }', 'output': 'public GridNearLockRequest( int cacheId, @NotNull AffinityTopologyVersion topVer, UUID nodeId, long threadId, IgniteUuid futId, GridCacheVersion lockVer, boolean isInTx, boolean isRead, boolean retVal, TransactionIsolation isolation, boolean isInvalidate, long timeout, int keyCnt, int txSize, boolean syncCommit, @Nullable UUID subjId, int taskNameHash, long createTtl, long accessTtl, boolean skipStore, boolean keepBinary, boolean firstClientReq, boolean addDepInfo ) { super( cacheId, nodeId, lockVer, threadId, futId, lockVer, isInTx, isRead, isolation, isInvalidate, timeout, keyCnt, txSize, skipStore, keepBinary, addDepInfo); assert topVer.compareTo(AffinityTopologyVersion.ZERO) > 0; this.topVer = topVer; this.subjId = subjId; this.taskNameHash = taskNameHash; this.createTtl = createTtl; this.accessTtl = accessTtl; dhtVers = new GridCacheVersion[keyCnt]; setFlag(syncCommit, SYNC_COMMIT_FLAG_MASK); setFlag(firstClientReq, FIRST_CLIENT_REQ_FLAG_MASK); setFlag(retVal, NEED_RETURN_VALUE_FLAG_MASK); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected AbstractTable( Type elementType, Schema schema, String tableName) { this.elementType = elementType; this.schema = schema; this.tableName = tableName; assert elementType != null; assert schema != null; assert tableName != null; }', 'ground_truth': 'protected AbstractTable( Schema schema, Type elementType, RelDataType relDataType, String tableName) { this.schema = schema; this.elementType = elementType; this.relDataType = relDataType; this.tableName = tableName; assert schema != null; assert relDataType != null; assert elementType != null; assert tableName != null; }', 'output': 'protected AbstractTable( Class<?> elementType, Schema schema, String tableName) { this.elementType = elementType; this.schema = schema; this.tableName = tableName; assert elementType != null; assert schema != null; assert tableName != null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static boolean cas(String table, String cfName, ByteBuffer key, ColumnFamily expected, ColumnFamily updates, ConsistencyLevel consistencyLevel) throws UnavailableException, IsBootstrappingException, ReadTimeoutException, WriteTimeoutException, InvalidRequestException { consistencyLevel.validateForCas(table); CFMetaData metadata = Schema.instance.getCFMetaData(table, cfName); long start = System.nanoTime(); long timeout = TimeUnit.MILLISECONDS.toNanos(DatabaseDescriptor.getCasContentionTimeout()); while (System.nanoTime() - start < timeout) { Pair<List<InetAddress>, Integer> p = getPaxosParticipants(table, key); List<InetAddress> liveEndpoints = p.left; int requiredParticipants = p.right; UUID ballot = beginAndRepairPaxos(key, metadata, liveEndpoints, requiredParticipants); if (ballot == null) continue; Tracing.trace("Reading existing values for CAS precondition"); long timestamp = System.currentTimeMillis(); IDiskAtomFilter filter = expected == null ? new SliceQueryFilter(ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1) : new NamesQueryFilter(ImmutableSortedSet.copyOf(expected.getColumnNames())); ReadCommand readCommand = filter instanceof SliceQueryFilter ? new SliceFromReadCommand(table, key, cfName, timestamp, (SliceQueryFilter) filter) : new SliceByNamesReadCommand(table, key, cfName, timestamp, (NamesQueryFilter) filter); List<Row> rows = read(Arrays.asList(readCommand), ConsistencyLevel.QUORUM); ColumnFamily current = rows.get(0).cf; if (!casApplies(expected, current)) { Tracing.trace("CAS precondition {} does not match current values {}", expected, current); return false; } Commit proposal = Commit.newProposal(key, ballot, updates); Tracing.trace("CAS precondition is met; proposing client-requested updates for {}", ballot); if (proposePaxos(proposal, liveEndpoints, requiredParticipants)) { if (consistencyLevel == ConsistencyLevel.SERIAL) sendCommit(proposal, liveEndpoints); else commitPaxos(proposal, consistencyLevel); Tracing.trace("CAS successful"); return true; } Tracing.trace("Paxos proposal not accepted (pre-empted by a higher ballot)"); Uninterruptibles.sleepUninterruptibly(FBUtilities.threadLocalRandom().nextInt(100), TimeUnit.MILLISECONDS); } throw new WriteTimeoutException(WriteType.CAS, ConsistencyLevel.SERIAL, -1, -1); }', 'ground_truth': 'public static ColumnFamily cas(String table, String cfName, ByteBuffer key, ColumnNameBuilder prefix, ColumnFamily expected, ColumnFamily updates, ConsistencyLevel consistencyLevel) throws UnavailableException, IsBootstrappingException, ReadTimeoutException, WriteTimeoutException, InvalidRequestException { consistencyLevel.validateForCas(table); CFMetaData metadata = Schema.instance.getCFMetaData(table, cfName); long start = System.nanoTime(); long timeout = TimeUnit.MILLISECONDS.toNanos(DatabaseDescriptor.getCasContentionTimeout()); while (System.nanoTime() - start < timeout) { Pair<List<InetAddress>, Integer> p = getPaxosParticipants(table, key); List<InetAddress> liveEndpoints = p.left; int requiredParticipants = p.right; UUID ballot = beginAndRepairPaxos(key, metadata, liveEndpoints, requiredParticipants); if (ballot == null) continue; Tracing.trace("Reading existing values for CAS precondition"); long timestamp = System.currentTimeMillis(); ReadCommand readCommand; if (expected == null) { SliceQueryFilter filter = prefix == null ? new SliceQueryFilter(ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1) : new SliceQueryFilter(prefix.build(), prefix.buildAsEndOfRange(), false, 1, prefix.componentCount()); readCommand = new SliceFromReadCommand(table, key, cfName, timestamp, filter); } else { readCommand = new SliceByNamesReadCommand(table, key, cfName, timestamp, new NamesQueryFilter(ImmutableSortedSet.copyOf(expected.getColumnNames()))); } List<Row> rows = read(Arrays.asList(readCommand), ConsistencyLevel.QUORUM); ColumnFamily current = rows.get(0).cf; if (!casApplies(expected, current)) { Tracing.trace("CAS precondition {} does not match current values {}", expected, current); return current; } Commit proposal = Commit.newProposal(key, ballot, updates); Tracing.trace("CAS precondition is met; proposing client-requested updates for {}", ballot); if (proposePaxos(proposal, liveEndpoints, requiredParticipants)) { if (consistencyLevel == ConsistencyLevel.SERIAL) sendCommit(proposal, liveEndpoints); else commitPaxos(proposal, consistencyLevel); Tracing.trace("CAS successful"); return null; } Tracing.trace("Paxos proposal not accepted (pre-empted by a higher ballot)"); Uninterruptibles.sleepUninterruptibly(FBUtilities.threadLocalRandom().nextInt(100), TimeUnit.MILLISECONDS); } throw new WriteTimeoutException(WriteType.CAS, ConsistencyLevel.SERIAL, -1, -1); }', 'output': 'public static boolean cas(String table, String cfName, ByteBuffer key, ColumnFamily expected, ColumnFamily updates, ConsistencyLevel consistencyLevel) throws UnavailableException, IsBootstrappingException, ReadTimeoutException, WriteTimeoutException, InvalidRequestException { consistencyLevel.validateForCas(table); CFMetaData metadata = Schema.instance.getCFMetaData(table, cfName); long start = System.nanoTime(); long timeout = TimeUnit.MILLISECONDS.toNanos(DatabaseDescriptor.getCasContentionTimeout()); while (System.nanoTime() - start < timeout) { Pair<List<InetAddress>, Integer> p = getPaxosParticipants(table, key); List<InetAddress> liveEndpoints = p.left; int requiredParticipants = p.right; UUID ballot = beginAndRepairPaxos(key, metadata, liveEndpoints, requiredParticipants); if (ballot == null) continue; Tracing.trace("Reading existing values for CAS precondition"); long timestamp = System.currentTimeMillis(); IDiskAtomFilter filter = expected == null ? new SliceQueryFilter(ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1) : new NamesQueryFilter(ImmutableSortedSet.copyOf(expected.getColumnNames())); ReadCommand readCommand = filter instanceof SliceQueryFilter ? new SliceFromReadCommand(table, key, cfName, timestamp, (SliceQueryFilter) filter) : new SliceByNamesReadCommand(table, key, cfName, timestamp, (NamesQueryFilter) filter); List<Row> rows = read(Arrays.asList(readCommand), consistencyLevel); ColumnFamily current = rows.get(0).cf; if (!casApplies(expected, current)) { Tracing.trace("CAS precondition {} does not match current values {}", expected, current); return false; } Commit proposal = Commit.newProposal(key, ballot, updates); Tracing.trace("CAS precondition is met; proposing client-requested updates for {}", ballot); if (proposePaxos(proposal, liveEndpoints'}
{'instruction': 'Please refactor the following code:', 'input': 'public void submitChangedArtifacts(J2eeModuleProvider provider, Iterable<Artifact> artifacts) { assert provider != null; assert artifacts != null; synchronized (this) { Set<Artifact> preparedArtifacts = toDeploy.get(provider); if (preparedArtifacts == null) { preparedArtifacts = new HashSet<>(); toDeploy.put(provider, preparedArtifacts); } for (Artifact artifact : artifacts) { preparedArtifacts.add(artifact); } boolean delayed = true; if (current != null && !current.isDone()) { current.cancel(false); delayed = false; } current = EXECUTOR.submit(new DeployTask(delayed)); } }', 'ground_truth': 'public void submitChangedArtifacts(Project project, J2eeModuleProvider provider, Iterable<Artifact> artifacts) { assert provider != null; assert artifacts != null; synchronized (this) { DeployArtifact preparedArtifacts = toDeploy.get(provider); if (preparedArtifacts == null) { preparedArtifacts = new DeployArtifact(project, artifacts); toDeploy.put(provider, preparedArtifacts); } boolean delayed = true; if (current != null && !current.isDone()) { current.cancel(false); delayed = false; } current = EXECUTOR.submit(new DeployTask(delayed)); } }', 'output': 'public void submitChangedArtifacts(J2eeModuleProvider provider, Collection<Artifact> artifacts) { assert provider != null; assert artifacts != null; synchronized (this) { Set<Artifact> preparedArtifacts = toDeploy.get(provider); if (preparedArtifacts == null) { preparedArtifacts = new HashSet<>(); toDeploy.put(provider, preparedArtifacts); } for (Artifact artifact : artifacts) { preparedArtifacts.add(artifact); } boolean delayed = true; if (current != null && !current.isDone()) { current.cancel(false); delayed = false; } current = EXECUTOR.submit(new DeployTask(delayed)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public KSMetaData cloneWith(Iterable<CFMetaData> tables, UTMetaData types) { return new KSMetaData(name, strategyClass, strategyOptions, durableWrites, tables, types); }', 'ground_truth': 'public KSMetaData cloneWith(Iterable<CFMetaData> tables, UTMetaData types, Functions functions) { return new KSMetaData(name, strategyClass, strategyOptions, durableWrites, tables, types, functions); }', 'output': 'public KSMetaData cloneWith(Iterable<CFMetaData> tables, UTMetaData types, boolean isDurable) { return new KSMetaData(name, strategyClass, strategyOptions, isDurable, tables, types); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void readBinary(BinaryReaderExImpl reader) throws BinaryObjectException { super.readBinary(reader); schemaName = reader.readString(); sql = reader.readString(); } /** {@inheritDoc} */ @Override public String toString() { return S.toString(JdbcMetaParamsRequest.class, this); } } }', 'ground_truth': '@Override public void readBinary(BinaryReaderExImpl reader, ClientListenerProtocolVersion ver) throws BinaryObjectException { super.readBinary(reader, ver); schemaName = reader.readString(); sql = reader.readString(); }', 'output': '@Override public void writeBinary(BinaryWriterExImpl writer) throws BinaryObjectException { super.writeBinary(writer); schemaName = writer.writeString(); sql = writer.writeString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAllocate(int threadCount, long duration, int poolSize) throws InterruptedException, ExecutionException { System.out.println(String.format("%s - testing %d threads for %dm", DATE_FORMAT.format(new Date()), threadCount, TimeUnit.NANOSECONDS.toMinutes(duration))); long prevPoolSize = BufferPool.getMemoryUsageThreshold(); logger.info("Overriding configured BufferPool.MEMORY_USAGE_THRESHOLD={} and enabling BufferPool.DEBUG", poolSize); BufferPool.setMemoryUsageThreshold(poolSize); Debug debug = new Debug(); BufferPool.debug(debug); TestEnvironment testEnv = new TestEnvironment(threadCount, duration, poolSize); startBurnerThreads(testEnv); for (int threadIdx = 0; threadIdx < threadCount; threadIdx++) testEnv.addCheckedFuture(startWorkerThread(testEnv, threadIdx)); while (!testEnv.latch.await(10L, TimeUnit.SECONDS)) { int stalledThreads = testEnv.countStalledThreads(); int doneThreads = testEnv.countDoneThreads(); if (doneThreads == 0) { assert stalledThreads == 0; boolean allFreed = testEnv.burnFreed.getAndSet(false); for (AtomicBoolean freedMemory : testEnv.freedAllMemory) allFreed = allFreed && freedMemory.getAndSet(false); if (allFreed) debug.check(); else logger.info("All threads did not free all memory in this time slot - skipping buffer recycle check"); } } for (SPSCQueue<BufferCheck> queue : testEnv.sharedRecycle) { BufferCheck check; while ( null != (check = queue.poll()) ) { check.validate(); BufferPool.put(check.buffer); } } assertEquals(0, testEnv.executorService.shutdownNow().size()); logger.info("Reverting BufferPool.MEMORY_USAGE_THRESHOLD={}", prevPoolSize); BufferPool.setMemoryUsageThreshold(prevPoolSize); BufferPool.debug(null); testEnv.assertCheckedThreadsSucceeded(); System.out.println(String.format("%s - finished.", DATE_FORMAT.format(new Date()))); }', 'ground_truth': 'public void testAllocate(BufferPool bufferPool, int threadCount, long duration) throws InterruptedException, ExecutionException { logger.info("{} - testing {} threads for {}m", DATE_FORMAT.format(new Date()), threadCount, TimeUnit.NANOSECONDS.toMinutes(duration)); logger.info("Testing BufferPool with memoryUsageThreshold={} and enabling BufferPool.DEBUG", bufferPool.memoryUsageThreshold()); Debug debug = new Debug(); bufferPool.debug(debug); TestEnvironment testEnv = new TestEnvironment(threadCount, duration, bufferPool.memoryUsageThreshold()); startBurnerThreads(bufferPool, testEnv); for (int threadIdx = 0; threadIdx < threadCount; threadIdx++) testEnv.addCheckedFuture(startWorkerThread(bufferPool, testEnv, threadIdx)); while (!testEnv.latch.await(10L, TimeUnit.SECONDS)) { int stalledThreads = testEnv.countStalledThreads(); int doneThreads = testEnv.countDoneThreads(); if (doneThreads == 0) { assert stalledThreads == 0; boolean allFreed = testEnv.burnFreed.getAndSet(false); for (AtomicBoolean freedMemory : testEnv.freedAllMemory) allFreed = allFreed && freedMemory.getAndSet(false); if (allFreed) debug.check(); else logger.info("All threads did not free all memory in this time slot - skipping buffer recycle check"); } } for (SPSCQueue<BufferCheck> queue : testEnv.sharedRecycle) { BufferCheck check; while ( null != (check = queue.poll()) ) { check.validate(); bufferPool.put(check.buffer); } } assertEquals(0, testEnv.executorService.shutdownNow().size()); logger.info("Reverting BufferPool DEBUG config"); bufferPool.debug(BufferPool.Debug.NO_OP); testEnv.assertCheckedThreadsSucceeded(); logger.info("{} - finished.", DATE_FORMAT.format(new Date())); }', 'output': 'public void testAllocate(int threadCount, long duration, int poolSize) throws InterruptedException, ExecutionException { System.out.println(String.format("%s - testing %d threads for %dm", DATE_FORMAT.format(new Date()), threadCount, TimeUnit.NANOSECONDS.toMinutes(duration))); long prevPoolSize = BufferPool.getMemoryUsageThreshold(); logger.info("Overriding configured BufferPool.MEMORY_USAGE_THRESHOLD={} and enabling BufferPool.DEBUG", poolSize); BufferPool.setMemoryUsageThreshold(poolSize); Debug debug = new Debug(); BufferPool.debug(debug); TestEnvironment testEnv = new TestEnvironment(threadCount, duration, poolSize); startBurnerThreads(testEnv); for (int threadIdx = 0; threadIdx < threadCount; threadIdx++) testEnv.addCheckedFuture(startWorkerThread(testEnv, threadIdx)); while (!testEnv.latch.await(10L, TimeUnit.SECONDS)) { int stalledThreads = testEnv.countStalledThreads(); int doneThreads = testEnv.countDoneThreads(); if (doneThreads == 0) { assert stalledThreads == 0; boolean allFreed = testEnv.burnFreed.getAndSet(false); for (AtomicBoolean freedMemory : testEnv.freedAllMemory) allFreed = allFreed && freedMemory.getAndSet(false); if (allFreed) debug.check(); else logger.info("All threads did not free all memory in this time slot - skipping buffer recycle check"); } } for (SPSCQueue<BufferCheck> queue : testEnv.sharedRecycle) { BufferCheck check; while ( null != (check = queue.poll()) ) { check.validate(); BufferPool.put(check.buffer); } } assertEquals(0, testEnv.executorService.shutdownNow().size()); logger.info("Reverting BufferPool.MEMORY_USAGE_THRESHOLD={}", prevPoolSize); BufferPool.setMemoryUsageThreshold(prevPoolSize); BufferPool.debug(null); testEnv.assertCheckedThreadsSucceeded(); System.out.println('}
{'instruction': 'Please refactor the following code:', 'input': 'private long initReusedPage(long reusedPageId, int partId) throws IgniteCheckedException { long reusedPage = acquirePage(reusedPageId); try { long reusedPageAddr = writeLock(reusedPageId, reusedPage); assert reusedPageAddr != 0; try { return initReusedPage(reusedPageId, reusedPage, reusedPageAddr, partId, PageIdAllocator.FLAG_DATA, ioVersions().latest()); } finally { writeUnlock(reusedPageId, reusedPage, reusedPageAddr, true); } } finally { releasePage(reusedPageId, reusedPage); } }', 'ground_truth': 'private long initReusedPage(long reusedPageId, int partId, IoStatisticsHolder statHolder) throws IgniteCheckedException { long reusedPage = acquirePage(reusedPageId, statHolder); try { long reusedPageAddr = writeLock(reusedPageId, reusedPage); assert reusedPageAddr != 0; try { return initReusedPage(reusedPageId, reusedPage, reusedPageAddr, partId, PageIdAllocator.FLAG_DATA, ioVersions().latest()); } finally { writeUnlock(reusedPageId, reusedPage, reusedPageAddr, true); } } finally { releasePage(reusedPageId, reusedPage); } }', 'output': 'private long initReusedPage(long reusedPageId, int partId) throws IgniteCheckedException { long reusedPage = acquirePage(reusedPageId); try { long reusedPageAddr = writeLock(reusedPageId, reusedPage); assert reusedPageAddr != 0; try { return initReusedPage(reusedPageId, reusedPage, reusedPageAddr, partId, PageIdAllocator.FLAG_DATA, ioVersions().latest()); } finally { writeUnlock(reusedPageId, reusedPage, reusedPageAddr, true); } } finally { releasePage(reusedPageId, reusedPage); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private RelDataType sqlType(RelDataTypeFactory typeFactory, int dataType, int precision, int scale) { SqlTypeName sqlTypeName = SqlTypeName.getNameForJdbcType(dataType); if (precision >= 0 && scale >= 0 && sqlTypeName.allowsPrecScale(true, true)) { return typeFactory.createSqlType(sqlTypeName, precision, scale); } else if (precision >= 0 && sqlTypeName.allowsPrecNoScale()) { return typeFactory.createSqlType(sqlTypeName, precision); } else { assert sqlTypeName.allowsNoPrecNoScale(); return typeFactory.createSqlType(sqlTypeName); } }', 'ground_truth': 'private RelDataType sqlType(RelDataTypeFactory typeFactory, int dataType, int precision, int scale, String typeString) { SqlTypeName sqlTypeName = SqlTypeName.getNameForJdbcType(dataType); switch (sqlTypeName) { case ARRAY: RelDataType component = null; if (typeString != null && typeString.endsWith(" ARRAY")) { final String remaining = typeString.substring(0, typeString.length() - " ARRAY".length()); component = parseTypeString(typeFactory, remaining); } if (component == null) { component = typeFactory.createSqlType(SqlTypeName.ANY); } return typeFactory.createArrayType(component, -1); } if (precision >= 0 && scale >= 0 && sqlTypeName.allowsPrecScale(true, true)) { return typeFactory.createSqlType(sqlTypeName, precision, scale); } else if (precision >= 0 && sqlTypeName.allowsPrecNoScale()) { return typeFactory.createSqlType(sqlTypeName, precision); } else { assert sqlTypeName.allowsNoPrecNoScale(); return typeFactory.createSqlType(sqlTypeName); } }', 'output': 'private RelDataType sqlType(final RelDataTypeFactory typeFactory, final int dataType, final int precision, final int scale) { SqlTypeName sqlTypeName = SqlTypeName.getNameForJdbcType(dataType); if (precision >= 0 && scale >= 0 && sqlTypeName.allowsPrecScale(true, true)) { return typeFactory.createSqlType(sqlTypeName, precision, scale); } else if (precision >= 0 && sqlTypeName.allowsPrecNoScale()) { return typeFactory.createSqlType(sqlTypeName, precision); } else { assert sqlTypeName.allowsNoPrecNoScale(); return typeFactory.createSqlType(sqlTypeName); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ResultSet executeSqlQueryWithTimer(PreparedStatement stmt, Connection conn, String sql, @Nullable Collection<Object> params, int timeoutMillis, @Nullable GridQueryCancel cancel) throws IgniteCheckedException { long start = U.currentTimeMillis(); try { ResultSet rs = executeSqlQuery(conn, stmt, timeoutMillis, cancel); long time = U.currentTimeMillis() - start; long longQryExecTimeout = ctx.config().getLongQueryWarningTimeout(); if (time > longQryExecTimeout) { ResultSet plan = executeSqlQuery(conn, preparedStatementWithParams(conn, "EXPLAIN " + sql, params, false), 0, null); plan.next(); String msg = "Query execution is too long [time=" + time + " ms, sql=\'" + sql + \'\\\'\' + ", plan=" + U.nl() + plan.getString(1) + U.nl() + ", parameters=" + (params == null ? "[]" : Arrays.deepToString(params.toArray())) + "]"; LT.warn(log, msg); } return rs; } catch (SQLException e) { connMgr.onSqlException(conn); throw new IgniteCheckedException(e); } }', 'ground_truth': 'public ResultSet executeSqlQueryWithTimer( PreparedStatement stmt, Connection conn, String sql, @Nullable Collection<Object> params, int timeoutMillis, @Nullable GridQueryCancel cancel, Boolean dataPageScanEnabled ) throws IgniteCheckedException { long start = U.currentTimeMillis(); enableDataPageScan(dataPageScanEnabled); try { ResultSet rs = executeSqlQuery(conn, stmt, timeoutMillis, cancel); long time = U.currentTimeMillis() - start; long longQryExecTimeout = ctx.config().getLongQueryWarningTimeout(); if (time > longQryExecTimeout) { ResultSet plan = executeSqlQuery(conn, preparedStatementWithParams(conn, "EXPLAIN " + sql, params, false), 0, null); plan.next(); String msg = "Query execution is too long [time=" + time + " ms, sql=\'" + sql + \'\\\'\' + ", plan=" + U.nl() + plan.getString(1) + U.nl() + ", parameters=" + (params == null ? "[]" : Arrays.deepToString(params.toArray())) + "]"; LT.warn(log, msg); } return rs; } catch (SQLException e) { connMgr.onSqlException(conn); throw new IgniteCheckedException(e); } finally { CacheDataTree.setDataPageScanEnabled(false); } }', 'output': 'public ResultSet executeSqlQueryWithTimer(PreparedStatement stmt, Connection conn, String sql, @Nullable Collection<Object> params, int timeoutMillis, @Nullable GridQueryCancel cancel) throws IgniteCheckedException { long start = U.currentTimeMillis(); try { ResultSet rs = executeSqlQuery(conn, stmt, timeoutMillis, cancel); long time = U.currentTimeMillis() - start; long longQryExecTimeout = ctx.config().getLongQueryWarningTimeout(); if (time > longQryExecTimeout) { ResultSet plan = executeSqlQuery(conn, preparedStatementWithParams(conn, "EXPLAIN " + sql, params, false), 0, null); plan.next(); String msg = "Query execution is too long [time=" + time + " ms, sql=\'" + sql + \'\\\'\' + ", plan=" + U.nl() + plan.getString(1) + U.nl() + ", parameters=" + (params == null ? "[]" : Arrays.deepToString(params.toArray())) + "]"; LT.warn(log, msg); } return rs; } catch (SQLException e) { connMgr.onSqlException(conn); throw new IgniteCheckedException(e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void transferRanges(InetAddress target, String tableName, Collection<Range> ranges, Runnable callback) { assert ranges.size() > 0; StreamOutSession session = StreamOutSession.create(tableName, target, callback); logger.info("Beginning transfer to {}", target); logger.debug("Ranges are {}", StringUtils.join(ranges, ",")); try { Table table = flushSSTable(tableName); transferSSTables(session, table.getAllSSTables(), ranges); } catch (IOException e) { throw new IOError(e); } }', 'ground_truth': 'public static void transferRanges(InetAddress target, String tableName, Collection<Range> ranges, Runnable callback, OperationType type) { assert ranges.size() > 0; StreamOutSession session = StreamOutSession.create(tableName, target, callback); logger.info("Beginning transfer to {}", target); logger.debug("Ranges are {}", StringUtils.join(ranges, ",")); try { Table table = flushSSTable(tableName); transferSSTables(session, table.getAllSSTables(), ranges, type); } catch (IOException e) { throw new IOError(e); } }', 'output': 'public static void transferRanges(InetAddress target, String tableName, Collection<Range> ranges, Runnable callback) { assert ranges.size() > 0; StreamOutSession session = StreamOutSession.create(tableName, target, callback); logger.info("Beginning transfer to {}", target); logger.debug("Ranges are {}", StringUtils.join(ranges, ",")); try { Table table = flushSSTable(tableName); transferSSTables(session, table.getAllSSTables(), ranges); } catch (IOException e) { throw new IOError(e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public VisorBaselineTaskResult( boolean active, long topVer, Collection<? extends BaselineNode> baseline, Collection<? extends BaselineNode> servers) { this.active = active; this.topVer = topVer; this.baseline = toMap(baseline); this.servers = toMap(servers); }', 'ground_truth': 'public VisorBaselineTaskResult( boolean active, long topVer, Collection<? extends BaselineNode> baseline, Collection<? extends BaselineNode> servers, VisorBaselineAutoAdjustSettings autoAdjustSettings ) { this.active = active; this.topVer = topVer; this.baseline = toMap(baseline); this.servers = toMap(servers); this.autoAdjustSettings = autoAdjustSettings; }', 'output': 'public VisorBaselineTaskResult( boolean active, long topVer, Collection<? extends BaselineNode> baseline, Collection<? extends BaselineNode> servers, Map<String, String> tags) { this.active = active; this.topVer = topVer; this.baseline = toMap(baseline); this.servers = toMap(servers); this.tags = tags; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private IgniteBiTuple<Collection<GridMultiCollectionWrapper<FullPageId>>, Integer> beginAllCheckpoints() { Collection<GridMultiCollectionWrapper<FullPageId>> res = new ArrayList(dataRegions().size()); int pagesNum = 0; for (DataRegion memPlc : dataRegions()) { if (!memPlc.config().isPersistenceEnabled()) continue; GridMultiCollectionWrapper<FullPageId> nextCpPagesCol = ((PageMemoryEx)memPlc.pageMemory()).beginCheckpoint(); pagesNum += nextCpPagesCol.size(); res.add(nextCpPagesCol); } currCheckpointPagesCnt = pagesNum; return new IgniteBiTuple<>(res, pagesNum); }', 'ground_truth': 'private IgniteBiTuple<Collection<GridMultiCollectionWrapper<FullPageId>>, Integer> beginAllCheckpoints( IgniteInternalFuture allowToReplace ) { Collection<GridMultiCollectionWrapper<FullPageId>> res = new ArrayList(dataRegions().size()); int pagesNum = 0; for (DataRegion memPlc : dataRegions()) { if (!memPlc.config().isPersistenceEnabled()) continue; GridMultiCollectionWrapper<FullPageId> nextCpPagesCol = ((PageMemoryEx)memPlc.pageMemory()) .beginCheckpoint(allowToReplace); pagesNum += nextCpPagesCol.size(); res.add(nextCpPagesCol); } currCheckpointPagesCnt = pagesNum; return new IgniteBiTuple<>(res, pagesNum); }', 'output': 'private IgniteBiTuple<List<GridMultiCollectionWrapper<FullPageId>>, Integer> beginAllCheckpoints() { List<GridMultiCollectionWrapper<FullPageId>> res = new ArrayList(dataRegions().size()); int pagesNum = 0; for (DataRegion memPlc : dataRegions()) { if (!memPlc.config().isPersistenceEnabled()) continue; GridMultiCollectionWrapper<FullPageId> nextCpPagesCol = ((PageMemoryEx)memPlc.pageMemory()).beginCheckpoint(); pagesNum += nextCpPagesCol.size(); res.add(nextCpPagesCol); } currCheckpointPagesCnt = pagesNum; return new IgniteBiTuple<>(res, pagesNum); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void initializeTarget(Class targetClass, Object targetAnnotation) { Object[] elementTypeEnum = (Object[]) invoke(targetClass, "value", EMPTY_ARG_TYPES, targetAnnotation, EMPTY_ARGS); if (elementTypeEnum == null) { addError("Cannot read @Target on the @" + this.annotationClass.getName() + ExtendedVerifier.JVM_ERROR_MESSAGE); return; } int bitmap = 0; for (int i = 0; i < elementTypeEnum.length; i++) { String targetName = elementTypeEnum[i].toString(); if("TYPE".equals(targetName)) { bitmap |= AnnotationNode.TYPE_TARGET; } else if("CONSTRUCTOR".equals(targetName)) { bitmap |= AnnotationNode.CONSTRUCTOR_TARGET; } else if("METHOD".equals(targetName)) { bitmap |= AnnotationNode.METHOD_TARGET; } else if("FIELD".equals(targetName)) { bitmap |= AnnotationNode.FIELD_TARGET; } else if("PARAMETER".equals(targetName)) { bitmap |= AnnotationNode.PARAMETER_TARGET; } else if("LOCAL_VARIABLE".equals(targetName)) { bitmap |= AnnotationNode.LOCAL_VARIABLE_TARGET; } else if("ANNOTATION".equals(targetName)) { bitmap |= AnnotationNode.ANNOTATION_TARGET; } } this.annotation.setAllowedTargets(bitmap); }', 'ground_truth': 'private void initializeTarget(Class annotationClass, Class targetClass, Object targetAnnotation) { Object[] elementTypeEnum = (Object[]) invoke(targetClass, "value", EMPTY_ARG_TYPES, targetAnnotation, EMPTY_ARGS); if (elementTypeEnum == null) { addError("Cannot read @Target on the @" + annotationClass.getName() + ExtendedVerifier.JVM_ERROR_MESSAGE); return; } int bitmap = 0; for (int i = 0; i < elementTypeEnum.length; i++) { String targetName = elementTypeEnum[i].toString(); if("TYPE".equals(targetName)) { bitmap |= AnnotationNode.TYPE_TARGET; } else if("CONSTRUCTOR".equals(targetName)) { bitmap |= AnnotationNode.CONSTRUCTOR_TARGET; } else if("METHOD".equals(targetName)) { bitmap |= AnnotationNode.METHOD_TARGET; } else if("FIELD".equals(targetName)) { bitmap |= AnnotationNode.FIELD_TARGET; } else if("PARAMETER".equals(targetName)) { bitmap |= AnnotationNode.PARAMETER_TARGET; } else if("LOCAL_VARIABLE".equals(targetName)) { bitmap |= AnnotationNode.LOCAL_VARIABLE_TARGET; } else if("ANNOTATION".equals(targetName)) { bitmap |= AnnotationNode.ANNOTATION_TARGET; } } this.annotation.setAllowedTargets(bitmap); }', 'output': 'private void initializeTarget(Class<?> targetClass, Object targetAnnotation) { Object[] elementTypeEnum = (Object[]) invoke(targetClass, "value", EMPTY_ARG_TYPES, targetAnnotation, EMPTY_ARGS); if (elementTypeEnum == null) { addError("Cannot read @Target on the @" + this.annotationClass.getName() + ExtendedVerifier.JVM_ERROR_MESSAGE); return; } int bitmap = 0; for (int i = 0; i < elementTypeEnum.length; i++) { String targetName = elementTypeEnum[i].toString(); if("TYPE".equals(targetName)) { bitmap |= AnnotationNode.TYPE_TARGET; } else if("CONSTRUCTOR".equals(targetName)) { bitmap |= AnnotationNode.CONSTRUCTOR_TARGET; } else if("METHOD".equals(targetName)) { bitmap |= AnnotationNode.METHOD_TARGET; } else if("FIELD".equals(targetName)) { bitmap |= AnnotationNode.FIELD_TARGET; } else if("PARAMETER".equals(targetName)) { bitmap |= AnnotationNode.PARAMETER_TARGET; } else if("LOCAL_VARIABLE".equals(targetName)) { bitmap |= AnnotationNode.LOCAL_VARIABLE_TARGET; } else if("ANNOTATION".equals(targetName)) { bitmap |= AnnotationNode.ANNOTATION_TARGET; } } this.annotation.setAllowedTargets(bitmap); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public CompactionIterator(Iterable<SSTableReader> sstables, CompactionController controller) throws IOException { this(getCollatingIterator(sstables), controller); }', 'ground_truth': 'public CompactionIterator(String type, Iterable<SSTableReader> sstables, CompactionController controller) throws IOException { this(type, getCollatingIterator(sstables), controller); }', 'output': 'public CompactionIterator(List<SSTableReader> sstables, CompactionController controller) throws IOException { this(getCollatingIterator(sstables), controller); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GridNioRecoveryDescriptor(int queueLimit, ClusterNode node, IgniteLogger log) { assert !node.isLocal() : node; assert queueLimit > 0; msgFuts = new ArrayDeque<>(queueLimit); this.queueLimit = queueLimit; this.node = node; this.log = log; }', 'ground_truth': 'public GridNioRecoveryDescriptor( boolean pairedConnections, int queueLimit, ClusterNode node, IgniteLogger log ) { assert !node.isLocal() : node; assert queueLimit > 0; msgReqs = new ArrayDeque<>(queueLimit); this.pairedConnections = pairedConnections; this.queueLimit = queueLimit; this.node = node; this.log = log; }', 'output': 'public GridNioRecoveryDescriptor(int queueLimit, ClusterNode node, IgniteLogger log, int threadId) { assert !node.isLocal() : node; assert queueLimit > 0; msgFuts = new ArrayDeque<>(queueLimit); this.queueLimit = queueLimit; this.node = node; this.log = log; this.threadId = threadId; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void insertRow(String key) throws IOException { RowMutation rm = new RowMutation("Keyspace1", key.getBytes()); ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1"); cf.addColumn(column("col1", "val1", new TimestampClock(1L))); rm.add(cf); rm.apply(); }', 'ground_truth': 'private void insertRow(String cfname, String key) throws IOException { RowMutation rm = new RowMutation("Keyspace1", key.getBytes()); ColumnFamily cf = ColumnFamily.create("Keyspace1", cfname); cf.addColumn(column("col1", "val1", new TimestampClock(1L))); rm.add(cf); rm.apply(); }', 'output': 'private void insertRow(String key) throws IOException { Mutation rm = new Mutation("Keyspace1", key.getBytes()); ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1"); cf.addColumn(column("col1", "val1", new TimestampClock(1L))); rm.add(cf); rm.apply(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private List<List<?>> executeSql(IgniteEx node, String sqlText) throws Exception { GridQueryProcessor qryProc = node.context().query(); return qryProc.querySqlFields(new SqlFieldsQuery(sqlText), true).getAll(); }', 'ground_truth': 'private List<List<?>> executeSql(IgniteEx node, String sqlText, Object... args) throws Exception { GridQueryProcessor qryProc = node.context().query(); return qryProc.querySqlFields(new SqlFieldsQuery(sqlText).setArgs(args), true).getAll(); }', 'output': 'private List<List<?>> executeSql(IgniteEx node, String sqlText, Collection<String> fields) throws Exception { GridQueryProcessor qryProc = node.context().query(); return qryProc.querySqlFields(new SqlFieldsQuery(sqlText, fields), true).getAll(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public BinaryFieldImpl(int typeId, BinarySchemaRegistry schemas, String fieldName, int fieldId) { assert typeId != 0; assert schemas != null; assert fieldName != null; assert fieldId != 0; this.typeId = typeId; this.schemas = schemas; this.fieldName = fieldName; this.fieldId = fieldId; }', 'ground_truth': 'public BinaryFieldImpl( BinaryContext ctx, int typeId, BinarySchemaRegistry schemas, String fieldName, int fieldId ) { assert ctx != null; assert typeId != 0; assert schemas != null; assert fieldName != null; assert fieldId != 0; this.ctx = ctx; this.typeId = typeId; this.schemas = schemas; this.fieldName = fieldName; this.fieldId = fieldId; }', 'output': 'public BinaryFieldImpl(int typeId, BinarySchemaRegistry schemas, String fieldName, int fieldId, boolean isOptional) { assert typeId != 0; assert schemas != null; assert fieldName != null; assert fieldId != 0; this.typeId = typeId; this.schemas = schemas; this.fieldName = fieldName; this.fieldId = fieldId; this.isOptional = isOptional; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private GridIterator<IgniteBiTuple<K, V>> offheapIterator( GridCacheQueryAdapter<?> qry, AffinityTopologyVersion topVer, boolean backups, ExpiryPolicy expPlc ) { IgniteBiPredicate<K, V> filter = qry.scanFilter(); if (expPlc != null) { return scanExpiryIterator( cctx.swap().rawOffHeapIterator(qry.partition(), true, backups), topVer, filter, expPlc, qry.keepBinary()); } if (cctx.offheapTiered() && filter != null) { OffheapIteratorClosure c = new OffheapIteratorClosure(filter, qry.keepBinary()); return cctx.swap().rawOffHeapIterator(c, qry.partition(), true, backups); } else { Iterator<Map.Entry<byte[], byte[]>> it = cctx.swap().rawOffHeapIterator(qry.partition(), true, backups); return scanIterator(it, filter, qry.keepBinary()); } }', 'ground_truth': 'private GridIterator<IgniteBiTuple<K, V>> offheapIterator( GridCacheQueryAdapter<?> qry, AffinityTopologyVersion topVer, boolean backups, ExpiryPolicy expPlc, boolean locNode) { IgniteBiPredicate<K, V> filter = qry.scanFilter(); if (expPlc != null) { return scanExpiryIterator( cctx.swap().rawOffHeapIterator(qry.partition(), true, backups), topVer, filter, expPlc, qry.keepBinary(), locNode); } if (cctx.offheapTiered() && filter != null) { OffheapIteratorClosure c = new OffheapIteratorClosure(filter, qry.keepBinary(), locNode); return cctx.swap().rawOffHeapIterator(c, qry.partition(), true, backups); } else { Iterator<Map.Entry<byte[], byte[]>> it = cctx.swap().rawOffHeapIterator(qry.partition(), true, backups); return scanIterator(it, filter, qry.keepBinary(), locNode); } }', 'output': 'private GridIterator<IgniteTuple<K, V>> offheapIterator( GridCacheQueryAdapter<?> qry, AffinityTopologyVersion topVer, boolean backups, ExpiryPolicy expPlc ) { IgniteBiPredicate<K, V> filter = qry.scanFilter(); if (expPlc != null) { return scanExpiryIterator( cctx.swap().rawOffHeapIterator(qry.partition(), true, backups), topVer, filter, expPlc, qry.keepBinary()); } if (cctx.offheapTiered() && filter != null) { OffheapIteratorClosure c = new OffheapIteratorClosure(filter, qry.keepBinary()); return cctx.swap().rawOffHeapIterator(c, qry.partition(), true, backups); } else { Iterator<Map.Entry<byte[], byte[]>> it = cctx.swap().rawOffHeapIterator(qry.partition(), true, backups); return scanIterator(it, filter, qry.keepBinary()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static SSTableWriter create(Descriptor descriptor, Long keyCount, Long repairedAt, UUID pendingRepair, TableMetadataRef metadata, MetadataCollector metadataCollector, SerializationHeader header, Collection<Index> indexes, LifecycleTransaction txn) { Factory writerFactory = descriptor.getFormat().getWriterFactory(); return writerFactory.open(descriptor, keyCount, repairedAt, pendingRepair, metadata, metadataCollector, header, observers(descriptor, indexes, txn.opType()), txn); }', 'ground_truth': 'public static SSTableWriter create(Descriptor descriptor, Long keyCount, Long repairedAt, UUID pendingRepair, boolean isTransient, TableMetadataRef metadata, MetadataCollector metadataCollector, SerializationHeader header, Collection<Index> indexes, LifecycleTransaction txn) { Factory writerFactory = descriptor.getFormat().getWriterFactory(); return writerFactory.open(descriptor, keyCount, repairedAt, pendingRepair, isTransient, metadata, metadataCollector, header, observers(descriptor, indexes, txn.opType()), txn); }', 'output': 'public static SSTableWriter create(Descriptor descriptor, long keyCount, long repairedAt, UUID pendingRepair, TableMetadataRef metadata, MetadataCollector metadataCollector, SerializationHeader header, Collection<Index> indexes, LifecycleTransaction txn) { Factory writerFactory = descriptor.getFormat().getWriterFactory(); return writerFactory.open(descriptor, keyCount, repairedAt, pendingRepair, metadata, metadataCollector, header, observers(descriptor, indexes, txn.opType()), txn); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static UpdatePlan forMerge(GridH2Table tbl, String[] colNames, int[] colTypes, KeyValueSupplier keySupplier, KeyValueSupplier valSupplier, int keyColIdx, int valColIdx, String selectQry, boolean isLocSubqry, int rowsNum) { assert !F.isEmpty(colNames); return new UpdatePlan(UpdateMode.MERGE, tbl, colNames, colTypes, keySupplier, valSupplier, keyColIdx, valColIdx, selectQry, isLocSubqry, rowsNum, null); }', 'ground_truth': 'public static UpdatePlan forMerge(GridH2Table tbl, String[] colNames, int[] colTypes, KeyValueSupplier keySupplier, KeyValueSupplier valSupplier, int keyColIdx, int valColIdx, String selectQry, boolean isLocSubqry, int rowsNum, DistributedPlanInfo distributed) { assert !F.isEmpty(colNames); return new UpdatePlan(UpdateMode.MERGE, tbl, colNames, colTypes, keySupplier, valSupplier, keyColIdx, valColIdx, selectQry, isLocSubqry, rowsNum, null, distributed); }', 'output': 'public static UpdatePlan forMerge(IgniteTable tbl, String[] colNames, int[] colTypes, KeyValueSupplier keySupplier, KeyValueSupplier valSupplier, int keyColIdx, int valColIdx, String selectQry, boolean isLocSubqry, int rowsNum) { assert !F.isEmpty(colNames); return new UpdatePlan(UpdateMode.MERGE, tbl, colNames, colTypes, keySupplier, valSupplier, keyColIdx, valColIdx, selectQry, isLocSubqry, rowsNum, null); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private UpdateParameters makeUpdateParameters(Collection<ByteBuffer> keys, ClusteringIndexFilter filter, QueryOptions options, DataLimits limits, boolean local, long timestamp, int nowInSeconds, long queryStartNanoTime) { Map<DecoratedKey, Partition> lists = readRequiredLists(keys, filter, limits, local, options.getConsistency(), nowInSeconds, queryStartNanoTime); return new UpdateParameters(metadata(), updatedColumns(), options, getTimestamp(timestamp, options), nowInSeconds, getTimeToLive(options), lists); }', 'ground_truth': 'private UpdateParameters makeUpdateParameters(Collection<ByteBuffer> keys, ClusteringIndexFilter filter, ClientState state, QueryOptions options, DataLimits limits, boolean local, long timestamp, int nowInSeconds, long queryStartNanoTime) { Map<DecoratedKey, Partition> lists = readRequiredLists(keys, filter, limits, local, options.getConsistency(), nowInSeconds, queryStartNanoTime); return new UpdateParameters(metadata(), updatedColumns(), state, options, getTimestamp(timestamp, options), nowInSeconds, getTimeToLive(options), lists); }', 'output': 'private UpdateParameters makeUpdateParameters(Collection<ByteBuffer> keys, ClusteringIndexFilter filter, QueryOptions options, DataLimits limits, boolean local, long timestamp, int nowInSeconds, long queryStartNanoTime, TimeUUID timeUUID) { Map<DecoratedKey, Partition> lists = readRequiredLists(keys, filter, limits, local, options.getConsistency(), nowInSeconds, queryStartNanoTime, timeUUID); return new UpdateParameters(metadata(), updatedColumns(), options, getTimestamp(timestamp, options), nowInSeconds, getTimeToLive(options), lists); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public BaseFileObj getFileObject(FileInfo fInfo, Caller caller) { File file = fInfo.getFile(); FolderObj parent = BaseFileObj.getExistingParentFor(file, this); FileNaming child = null; boolean isInitializedCache = true; if (parent != null) { final ChildrenCache childrenCache = parent.getChildrenCache(); final Mutex.Privileged mutexPrivileged = childrenCache.getMutexPrivileged(); Runnable[] task = new Runnable[1]; for (int i = 0; i < 2; i++) { if (i == 1) { if (task[0] != null) { task[0].run(); } else { break; } } mutexPrivileged.enterReadAccess(); try { final String nameExt = BaseFileObj.getNameExt(file); isInitializedCache = childrenCache.isCacheInitialized(); child = childrenCache.getChild(nameExt, false, task); } finally { mutexPrivileged.exitReadAccess(); } } } int initTouch = (isInitializedCache) ? -1 : (child != null ? 1 : 0); if (initTouch == -1  && FileBasedFileSystem.isModificationInProgress()) { initTouch = file.exists() ? 1 : 0; } return issueIfExist(file, caller, parent, child, initTouch, caller.asynchFire()); }', 'ground_truth': 'public BaseFileObj getFileObject(FileInfo fInfo, Caller caller, boolean onlyExisting) { File file = fInfo.getFile(); FolderObj parent = BaseFileObj.getExistingParentFor(file, this); FileNaming child = null; boolean isInitializedCache = true; if (parent != null) { final ChildrenCache childrenCache = parent.getChildrenCache(); final Mutex.Privileged mutexPrivileged = childrenCache.getMutexPrivileged(); Runnable[] task = new Runnable[1]; for (int i = 0; i < 2; i++) { if (i == 1) { if (task[0] != null) { task[0].run(); } else { break; } } mutexPrivileged.enterReadAccess(); try { final String nameExt = BaseFileObj.getNameExt(file); isInitializedCache = childrenCache.isCacheInitialized(); child = childrenCache.getChild(nameExt, false, task); } finally { mutexPrivileged.exitReadAccess(); } } } int initTouch = (isInitializedCache) ? -1 : (child != null ? 1 : 0); if (initTouch == -1  && FileBasedFileSystem.isModificationInProgress()) { initTouch = file.exists() ? 1 : 0; } return issueIfExist(file, caller, parent, child, initTouch, caller.asynchFire(), onlyExisting); }', 'output': 'public FileObject getFileObject(FileInfo fInfo, Caller caller) { File file = fInfo.getFile(); FolderObject parent = BaseFileObject.getExistingParentFor(file, this); FileNaming child = null; boolean isInitializedCache = true; if (parent != null) { final ChildrenCache childrenCache = parent.getChildrenCache(); final Mutex.Privileged mutexPrivileged = childrenCache.getMutexPrivileged(); Runnable[] task = new Runnable[1]; for (int i = 0; i < 2; i++) { if (i == 1) { if (task[0] != null) { task[0].run(); } else { break; } } mutexPrivileged.enterReadAccess(); try { final String nameExt = BaseFileObject.getNameExt(file); isInitializedCache = childrenCache.isCacheInitialized(); child = childrenCache.getChild(nameExt, false, task); } finally { mutexPrivileged.exitReadAccess(); } } int initTouch = (isInitializedCache) ? -1 : (child != null ? 1 : 0); if (initTouch == -1  && FileBasedFileSystem.isModificationInProgress()) { initTouch = file.exists() ? 1 : 0; } return issueIfExist(file, caller, parent, child, initTouch, caller.asynchFire()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void validateMetadata(String pathToMetadata, SupportedSparkModels parsedSparkMdl) throws FileNotFoundException { File metadataFile = IgniteUtils.resolveIgnitePath(pathToMetadata + File.separator + "part-00000"); if (metadataFile != null) { Scanner sc = new Scanner(metadataFile); boolean isInvalid = true; while (sc.hasNextLine()) { final String line = sc.nextLine(); if (line.contains(parsedSparkMdl.getMdlClsNameInSpark())) isInvalid = false; } if (isInvalid) throw new IllegalArgumentException("The metadata file contains incorrect model metadata. " + "It should contain " + parsedSparkMdl.getMdlClsNameInSpark() + " model metadata."); } }', 'ground_truth': 'private static void validateMetadata(String pathToMetadata, SupportedSparkModels parsedSparkMdl, LearningEnvironment learningEnvironment) throws FileNotFoundException { File metadataFile = IgniteUtils.resolveIgnitePath(pathToMetadata + File.separator + "part-00000"); if (metadataFile != null) { Scanner sc = new Scanner(metadataFile); boolean isInvalid = true; while (sc.hasNextLine()) { final String line = sc.nextLine(); if (line.contains(parsedSparkMdl.getMdlClsNameInSpark())) isInvalid = false; } if (isInvalid) { String msg = "The metadata file contains incorrect model metadata. " + "It should contain " + parsedSparkMdl.getMdlClsNameInSpark() + " model metadata."; learningEnvironment.logger().log(MLLogger.VerboseLevel.HIGH, msg); throw new IllegalArgumentException(msg); } } }', 'output': 'public static void validateMetadata(String pathToMetadata, SupportedSparkModels parsedSparkMdl) throws FileNotFoundException { File metadataFile = IgniteUtils.resolveIgnitePath(pathToMetadata + File.separator + "part-00000"); if (metadataFile != null) { Scanner sc = new Scanner(metadataFile); boolean isInvalid = true; while (sc.hasNextLine()) { final String line = sc.nextLine(); if (line.contains(parsedSparkMdl.getMdlClsNameInSpark())) isInvalid = false; } if (isInvalid) throw new IllegalArgumentException("The metadata file contains incorrect model metadata. " + "It should contain " + parsedSparkMdl.getMdlClsNameInSpark() + " model metadata."); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void encode(BinaryRawWriterEx writer) { super.encode(writer); writer.writeString(val); } } }', 'ground_truth': '@Override public void encode(ClientConnectionContext ctx, BinaryRawWriterEx writer) { super.encode(ctx, writer); writer.writeString(val); }', 'output': '@Override public void encode(BinaryEncoderEx encoder) { super.encode(encoder); encoder.writeString(val); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private List<AlertNoticeEntity> getSnmpMockNotices() { AlertDefinitionEntity definition = new AlertDefinitionEntity(); definition.setDefinitionId(1L); definition.setDefinitionName("alert-definition-1"); definition.setLabel("Alert Definition 1"); AlertHistoryEntity history1 = new AlertHistoryEntity(); history1.setAlertDefinition(definition); history1.setServiceName("HDFS"); history1.setClusterId(1L); history1.setAlertLabel("Label"); history1.setAlertState(AlertState.OK); history1.setAlertText(ALERT_UNIQUE_TEXT); history1.setAlertTimestamp(System.currentTimeMillis()); AlertHistoryEntity history2 = new AlertHistoryEntity(); history2.setAlertDefinition(definition); history2.setServiceName("HDFS"); history2.setClusterId(1L); history2.setAlertLabel("Label"); history2.setAlertState(AlertState.CRITICAL); history2.setAlertText(ALERT_UNIQUE_TEXT + " CRITICAL"); history2.setAlertTimestamp(System.currentTimeMillis()); AlertTargetEntity target = new AlertTargetEntity(); target.setTargetId(1L); target.setAlertStates(EnumSet.allOf(AlertState.class)); target.setTargetName("Alert Target"); target.setDescription("Mock Target"); target.setNotificationType("SNMP"); String properties = "{ \\"foo\\" : \\"bar\\" }"; target.setProperties(properties); AlertNoticeEntity notice1 = new AlertNoticeEntity(); notice1.setUuid(ALERT_NOTICE_UUID_1); notice1.setAlertTarget(target); notice1.setAlertHistory(history1); notice1.setNotifyState(NotificationState.PENDING); AlertNoticeEntity notice2 = new AlertNoticeEntity(); notice2.setUuid(ALERT_NOTICE_UUID_2); notice2.setAlertTarget(target); notice2.setAlertHistory(history2); notice2.setNotifyState(NotificationState.PENDING); ArrayList<AlertNoticeEntity> notices = new ArrayList<AlertNoticeEntity>(); notices.add(notice1); notices.add(notice2); return notices; }', 'ground_truth': 'private List<AlertNoticeEntity> getSnmpMockNotices(String notificationType) { AlertDefinitionEntity definition = new AlertDefinitionEntity(); definition.setDefinitionId(1L); definition.setDefinitionName("alert-definition-1"); definition.setLabel("Alert Definition 1"); AlertHistoryEntity history1 = new AlertHistoryEntity(); history1.setAlertDefinition(definition); history1.setServiceName("HDFS"); history1.setClusterId(1L); history1.setAlertLabel("Label"); history1.setAlertState(AlertState.OK); history1.setAlertText(ALERT_UNIQUE_TEXT); history1.setAlertTimestamp(System.currentTimeMillis()); AlertHistoryEntity history2 = new AlertHistoryEntity(); history2.setAlertDefinition(definition); history2.setServiceName("HDFS"); history2.setClusterId(1L); history2.setAlertLabel("Label"); history2.setAlertState(AlertState.CRITICAL); history2.setAlertText(ALERT_UNIQUE_TEXT + " CRITICAL"); history2.setAlertTimestamp(System.currentTimeMillis()); AlertTargetEntity target = new AlertTargetEntity(); target.setTargetId(1L); target.setAlertStates(EnumSet.allOf(AlertState.class)); target.setTargetName("Alert Target"); target.setDescription("Mock Target"); target.setNotificationType(notificationType); String properties = "{ \\"ambari.dispatch.snmp.version\\": \\"SNMPv1\\", \\"ambari.dispatch.snmp.port\\": \\"8000\\"," + " \\"ambari.dispatch.recipients\\": [\\"127.0.0.1\\"],\\"ambari.dispatch.snmp.community\\":\\"\\" }"; target.setProperties(properties); AlertNoticeEntity notice1 = new AlertNoticeEntity(); notice1.setUuid(ALERT_NOTICE_UUID_1); notice1.setAlertTarget(target); notice1.setAlertHistory(history1); notice1.setNotifyState(NotificationState.PENDING); AlertNoticeEntity notice2 = new AlertNoticeEntity(); notice2.setUuid(ALERT_NOTICE_UUID_2); notice2.setAlertTarget(target); notice2.setAlertHistory(history2); notice2.setNotifyState(NotificationState.PENDING); ArrayList<AlertNoticeEntity> notices = new ArrayList<AlertNoticeEntity>(); notices.add(notice1); notices.add(notice2); return notices; }', 'output': 'private List<AlertNoticeEntity> getSnmpMockNotices() { AlertDefinitionEntity definition = new AlertDefinitionEntity(); definition.setDefinitionId(1L); definition.setDefinitionName("alert-definition-1"); definition.setLabel("Alert Definition 1"); AlertHistoryEntity history1 = new AlertHistoryEntity(); history1.setAlertDefinition(definition); history1.setServiceName("HDFS"); history1.setClusterId(1L); history1.setAlertLabel("Label"); history1.setAlertState(AlertState.OK); history1.setAlertText(ALERT_UNIQUE_TEXT); history1.setAlertTimestamp(System.currentTimeMillis()); AlertHistoryEntity history2 = new AlertHistoryEntity(); history2.setAlertDefinition(definition); history2.setServiceName("HDFS"); history2.setClusterId(1L); history2.setAlertLabel("Label"); history2.setAlertState(AlertState.CRITICAL); history2.setAlertText(ALERT_UNIQUE_TEXT + " CRITICAL"); history2.setAlertTimestamp(System.currentTimeMillis()); AlertTargetEntity target = new AlertTargetEntity(); target.setTargetId(1L); target.setAlertStates(EnumSet.allOf(AlertState.class)); target.setTargetName("Alert Target"); target.setDescription("Mock Target"); target.setNotificationType("SNMP"); String properties = "{ \\"foo\\" : \\"bar\\" }"; target.setProperties(properties); AlertNoticeEntity notice1 = new AlertNoticeEntity(); notice1.setUuid(ALERT_NOTICE_UUID_1); notice1.setAlertTarget(target); notice1.setAlertHistory(history1); notice1.setNotifyState(NotificationState.PENDING); AlertNoticeEntity notice2 = new AlertNoticeEntity(); notice2.setUuid(ALERT_NOTICE_UUID_2); notice2.setAlertTarget(target); notice2.setAlertHistory(history2); notice2.setNotifyState(NotificationState.PENDING); ArrayList<AlertNoticeEntity> notices = new ArrayList<AlertNoticeEntity>(); notices.add(notice1); notices.add(notice2); return notices; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RowMutation toHint(int ttl, UUID targetId) throws IOException { assert ttl > 0; RowMutation rm = new RowMutation(Table.SYSTEM_KS, UUIDType.instance.decompose(targetId)); UUID hintId = UUIDGen.getTimeUUID(); QueryPath path = new QueryPath(SystemTable.HINTS_CF, null, HintedHandOffManager.comparator.decompose(hintId, MessagingService.current_version)); rm.add(path, ByteBuffer.wrap(FBUtilities.serialize(this, serializer, MessagingService.current_version)), System.currentTimeMillis(), ttl); return rm; }', 'ground_truth': 'public RowMutation toHint(long now, int ttl, UUID targetId) throws IOException { assert ttl > 0; RowMutation rm = new RowMutation(Table.SYSTEM_KS, UUIDType.instance.decompose(targetId)); UUID hintId = UUIDGen.getTimeUUID(); QueryPath path = new QueryPath(SystemTable.HINTS_CF, null, HintedHandOffManager.comparator.decompose(hintId, MessagingService.current_version)); rm.add(path, ByteBuffer.wrap(FBUtilities.serialize(this, serializer, MessagingService.current_version)), now, ttl); return rm; }', 'output': 'public RowMutation toHint(int ttl, UUID targetId, int version) throws IOException { assert ttl > 0; RowMutation rm = new RowMutation(Table.SYSTEM_KS, UUIDType.instance.decompose(targetId)); UUID hintId = UUIDGen.getTimeUUID(); QueryPath path = new QueryPath(SystemTable.HINTS_CF, null, HintedHandOffManager.comparator.decompose(hintId, version)); rm.add(path, ByteBuffer.wrap(FBUtilities.serialize(this, serializer, version)), System.currentTimeMillis(), ttl); return rm; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean isLeadFieldInRange(BigDecimal value, TimeUnit unit) { assert value.compareTo(ZERO) >= 0; return startPrecision < POWERS10.length ? value.compareTo(POWERS10[startPrecision]) < 0 : value.compareTo(INT_MAX_VALUE_PLUS_ONE) < 0; }', 'ground_truth': 'private boolean isLeadFieldInRange(RelDataTypeSystem typeSystem, BigDecimal value, TimeUnit unit) { assert value.compareTo(ZERO) >= 0; final int startPrecision = getStartPrecision(typeSystem); return startPrecision < POWERS10.length ? value.compareTo(POWERS10[startPrecision]) < 0 : value.compareTo(INT_MAX_VALUE_PLUS_ONE) < 0; }', 'output': 'private boolean isLeadFieldInRange(BigDecimal value, TimeUnit unit) { assert value.compareTo(ZERO) >= 0; return startPrecision < POWERS10.length ? value.compareTo(POWERS10[startPrecision]) < 0 : value.compareTo(INT_MAX_VALUE_PLUS_ONE) < 0; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected final List<Integer> movingKeysAfterJoin(Ignite ign, String cacheName, int size, @Nullable IgniteInClosure<ClusterNode> nodeInitializer) { assertEquals("Expected consistentId is set to node name", ign.name(), ign.cluster().localNode().consistentId()); ArrayList<ClusterNode> nodes = new ArrayList<>(ign.cluster().nodes()); List<List<ClusterNode>> calcAff = calcAffinity(ign.cache(cacheName), nodes); GridTestNode fakeNode = new GridTestNode(UUID.randomUUID(), null); if (nodeInitializer != null) nodeInitializer.apply(fakeNode); fakeNode.consistentId(getTestIgniteInstanceName(nodes.size())); nodes.add(fakeNode); List<List<ClusterNode>> calcAff2 = calcAffinity(ign.cache(cacheName), nodes); Set<Integer> movedParts = new HashSet<>(); UUID locId = ign.cluster().localNode().id(); for (int i = 0; i < calcAff.size(); i++) { if (calcAff.get(i).get(0).id().equals(locId) && !calcAff2.get(i).get(0).id().equals(locId)) movedParts.add(i); } List<Integer> keys = new ArrayList<>(); Affinity<Integer> aff = ign.affinity(cacheName); for (int i = 0; i < 10_000; i++) { int keyPart = aff.partition(i); if (movedParts.contains(keyPart)) { keys.add(i); if (keys.size() == size) break; } } assertEquals("Failed to find moving keys [movedPats=" + movedParts + ", keys=" + keys + \']\', size, keys.size()); return keys; }', 'ground_truth': 'protected final List<Integer> movingKeysAfterJoin(Ignite ign, String cacheName, int size, @Nullable IgniteInClosure<ClusterNode> nodeInitializer, @Nullable String joiningNodeConsistentId) { if (joiningNodeConsistentId == null) assertEquals("Expected consistentId is set to node name", ign.name(), ign.cluster().localNode().consistentId()); ArrayList<ClusterNode> nodes = new ArrayList<>(ign.cluster().nodes()); List<List<ClusterNode>> calcAff = calcAffinity(ign.cache(cacheName), nodes); GridTestNode fakeNode = new GridTestNode(UUID.randomUUID(), null); if (nodeInitializer != null) nodeInitializer.apply(fakeNode); fakeNode.consistentId(joiningNodeConsistentId == null ? getTestIgniteInstanceName(nodes.size()) : joiningNodeConsistentId); nodes.add(fakeNode); List<List<ClusterNode>> calcAff2 = calcAffinity(ign.cache(cacheName), nodes); Set<Integer> movedParts = new HashSet<>(); UUID locId = ign.cluster().localNode().id(); for (int i = 0; i < calcAff.size(); i++) { if (calcAff.get(i).get(0).id().equals(locId) && !calcAff2.get(i).get(0).id().equals(locId)) movedParts.add(i); } List<Integer> keys = new ArrayList<>(); Affinity<Integer> aff = ign.affinity(cacheName); for (int i = 0; i < 10_000; i++) { int keyPart = aff.partition(i); if (movedParts.contains(keyPart)) { keys.add(i); if (keys.size() == size) break; } } assertEquals("Failed to find moving keys [movedPats=" + movedParts + ", keys=" + keys + \']\', size, keys.size()); return keys; }', 'output': 'protected final List<Integer> movingKeysAfterJoin(Ignite ign, String cacheName, int size, @Nullable IgniteInClosure<ClusterNode> nodeInitializer) { assertEquals("Expected consistentId is set to node name", ign.name(), ign.cluster().localNode().consistentId()); ArrayList<ClusterNode> nodes = new ArrayList<>(ign.cluster().nodes()); List<List<ClusterNode>> calcAff = calcAffinity(ign.cache(cacheName), nodes); GridTestNode fakeNode = new GridTestNode(UUID.randomUUID(), null); if (nodeInitializer != null) nodeInitializer.apply(fakeNode); fakeNode.consistentId(getTestIgniteInstanceName(nodes.size())); nodes.add(fakeNode); List<List<ClusterNode>> calcAff2 = calcAffinity(ign.cache(cacheName), nodes); Set<Integer> movedParts = new HashSet<>(); UUID locId = ign.cluster().localNode().id(); for (int i = 0; i < calcAff.size(); i++) { if (calcAff.get(i).get(0).id().equals(locId) && !calcAff2.get(i).get(0).id().equals(locId)) movedParts.add(i); } List<Integer> keys = new ArrayList<>(); Affinity<Integer> aff = ign.affinity(cacheName); for (int i = 0; i < 10_000; i++) { int keyPart = aff.partition(i); if (movedParts.contains(keyPart)) { keys.add(i); if (keys.size() == size) break; } } assertEquals("Failed to find moving keys [movedPats=" + movedParts + ", keys=" + keys + \']\', size, keys.size()); return keys; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean isAllowed(String jsonBlock, InputMarker inputMarker) { if (org.apache.commons.lang3.StringUtils.isEmpty(jsonBlock)) { return DEFAULT_VALUE; } Map<String, Object> jsonObj = LogFeederUtil.toJSONObject(jsonBlock); return isAllowed(jsonObj, inputMarker); }', 'ground_truth': 'public boolean isAllowed(String jsonBlock, InputMarker inputMarker, List<String> defaultLogLevels) { if (org.apache.commons.lang3.StringUtils.isEmpty(jsonBlock)) { return DEFAULT_VALUE; } Map<String, Object> jsonObj = LogFeederUtil.toJSONObject(jsonBlock); return isAllowed(jsonObj, inputMarker, defaultLogLevels); }', 'output': 'public boolean isAllowed(String jsonBlock, InputMarker inputMarker) { if (org.apache.commons.lang3.StringUtils.isEmpty(jsonBlock)) { return false; } Map<String, Object> jsonObj = LogFeederUtil.toJSONObject(jsonBlock); return isAllowed(jsonObj, inputMarker); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static DataflowSideInputReader of( Iterable<? extends SideInputInfo> sideInputInfos, PipelineOptions options, ExecutionContext context) throws Exception { return new DataflowSideInputReader(sideInputInfos, options, context); }', 'ground_truth': 'public static DataflowSideInputReader of( Iterable<? extends SideInputInfo> sideInputInfos, ReaderFactory readerFactory, PipelineOptions options, ExecutionContext context) throws Exception { return new DataflowSideInputReader(sideInputInfos, readerFactory, options, context); }', 'output': 'public static DataflowSideInputReader of( Iterable<? extends SideInputInfo> sideInputInfos, PipelineOptions options, ExecutionContext context, ClassLoader classLoader) throws Exception { return new DataflowSideInputReader(sideInputInfos, options, context, classLoader); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void defragment( CacheGroupContext grpCtx, CacheGroupContext newCtx, PageMemoryEx partPageMem, IntMap<LinkMap> mappingByPart, CheckpointTimeoutLock cpLock, Runnable cancellationChecker ) throws IgniteCheckedException { } } }', 'ground_truth': '@Override public void defragment( CacheGroupContext grpCtx, CacheGroupContext newCtx, PageMemoryEx partPageMem, IntMap<LinkMap> mappingByPart, CheckpointTimeoutLock cpLock, Runnable cancellationChecker, IgniteThreadPoolExecutor defragmentationThreadPool ) throws IgniteCheckedException { }', 'output': '@Override public void defragment( final CacheGroupContext grpCtx, final CacheGroupContext newCtx, final PageMemoryEx partPageMem, final IntMap<LinkMap> mappingByPart, final CheckpointTimeoutLock cpLock, final Runnable cancellationChecker ) throws IgniteCheckedException { }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void init() throws Exception { super.init(); initParams(); setupSecurity(); createOutgoingBuffer(); createSolrStateWatcher(); createSolrWorkers(); }', 'ground_truth': 'public void init(LogFeederProps logFeederProps) throws Exception { super.init(logFeederProps); initParams(); setupSecurity(); createOutgoingBuffer(); createSolrStateWatcher(); createSolrWorkers(); }', 'output': 'public void init() throws Exception { super.init(); initParams(); setupSecurity(); createIncomingBuffer(); createSolrStateWatcher(); createSolrWorkers(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void fireTimer(TimerInternals.TimerData timer) { LOG.debug("Firing timer {}", timer); final StateNamespace namespace = timer.getNamespace(); final BoundedWindow window = ((StateNamespaces.WindowNamespace) namespace).getWindow(); pushbackFnRunner.onTimer(timer.getTimerId(), window, timer.getTimestamp(), timer.getDomain()); }', 'ground_truth': 'private void fireTimer(KeyedTimerData<?> keyedTimerData) { final TimerInternals.TimerData timer = keyedTimerData.getTimerData(); LOG.debug("Firing timer {}", timer); final StateNamespace namespace = timer.getNamespace(); final BoundedWindow window = ((StateNamespaces.WindowNamespace) namespace).getWindow(); if (fnRunner instanceof DoFnRunnerWithKeyedInternals) { ((DoFnRunnerWithKeyedInternals) fnRunner).onTimer(keyedTimerData, window); } else { pushbackFnRunner.onTimer(timer.getTimerId(), window, timer.getTimestamp(), timer.getDomain()); } }', 'output': 'private void fireTimer(final TimerInternals.TimerData timer) { LOG.debug("Firing timer {}", timer); final StateNamespace namespace = timer.getNamespace(); final BoundedWindow window = ((StateNamespaces.WindowNamespace) namespace).getWindow(); pushbackFnRunner.onTimer(timer.getTimerId(), window, timer.getTimestamp(), timer.getDomain()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Bounded(SparkContext sc, BoundedSource<T> source, SparkRuntimeContext runtimeContext) { super(sc, NIL, JavaSparkContext$.MODULE$.<WindowedValue<T>>fakeClassTag()); this.source = source; this.runtimeContext = runtimeContext; this.numPartitions = sc.defaultParallelism(); checkArgument(this.numPartitions > 0, "Number of partitions must be greater than zero."); }', 'ground_truth': 'public Bounded( SparkContext sc, BoundedSource<T> source, SparkRuntimeContext runtimeContext, String stepName) { super(sc, NIL, JavaSparkContext$.MODULE$.<WindowedValue<T>>fakeClassTag()); this.source = source; this.runtimeContext = runtimeContext; this.numPartitions = sc.defaultParallelism(); checkArgument(this.numPartitions > 0, "Number of partitions must be greater than zero."); this.stepName = stepName; this.metricsAccum = MetricsAccumulator.getInstance(); }', 'output': 'public Bounded<T>(SparkContext sc, BoundedSource<T> source, SparkRuntimeContext runtimeContext) { super(sc, NIL, JavaSparkContext$.MODULE$.<WindowedValue<T>>fakeClassTag()); this.source = source; this.runtimeContext = runtimeContext; this.numPartitions = sc.defaultParallelism(); checkArgument(this.numPartitions > 0, "Number of partitions must be greater than zero."); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private IgniteInternalFuture<GridCacheReturn> obtainLockAsync( final GridCacheContext cacheCtx, GridCacheReturn ret, final Collection<KeyCacheObject> passedKeys, final boolean read, final boolean needRetVal, final long accessTtl, @Nullable final CacheEntryPredicate[] filter, boolean skipStore, boolean keepBinary) { if (log.isDebugEnabled()) log.debug("Before acquiring transaction lock on keys [keys=" + passedKeys + \']\'); if (passedKeys.isEmpty()) return new GridFinishedFuture<>(ret); GridDhtTransactionalCacheAdapter<?, ?> dhtCache = cacheCtx.isNear() ? cacheCtx.nearTx().dht() : cacheCtx.dhtTx(); long timeout = remainingTime(); if (timeout == -1) return new GridFinishedFuture<>(timeoutException()); IgniteInternalFuture<Boolean> fut = dhtCache.lockAllAsyncInternal(passedKeys, timeout, this, isInvalidate(), read, needRetVal, isolation, accessTtl, CU.empty0(), skipStore, keepBinary); return new GridEmbeddedFuture<>( fut, new PLC1<GridCacheReturn>(ret) { @Override protected GridCacheReturn postLock(GridCacheReturn ret) throws IgniteCheckedException { if (log.isDebugEnabled()) log.debug("Acquired transaction lock on keys: " + passedKeys); postLockWrite(cacheCtx, passedKeys, ret, /*remove*/false, /*retval*/false, /*read*/read, accessTtl, filter == null ? CU.empty0() : filter, /*computeInvoke*/false); return ret; } } ); }', 'ground_truth': 'private IgniteInternalFuture<GridCacheReturn> obtainLockAsync( final GridCacheContext cacheCtx, GridCacheReturn ret, final Collection<KeyCacheObject> passedKeys, final boolean read, final boolean needRetVal, final long createTtl, final long accessTtl, @Nullable final CacheEntryPredicate[] filter, boolean skipStore, boolean keepBinary) { if (log.isDebugEnabled()) log.debug("Before acquiring transaction lock on keys [keys=" + passedKeys + \']\'); if (passedKeys.isEmpty()) return new GridFinishedFuture<>(ret); GridDhtTransactionalCacheAdapter<?, ?> dhtCache = cacheCtx.isNear() ? cacheCtx.nearTx().dht() : cacheCtx.dhtTx(); long timeout = remainingTime(); if (timeout == -1) return new GridFinishedFuture<>(timeoutException()); IgniteInternalFuture<Boolean> fut = dhtCache.lockAllAsyncInternal(passedKeys, timeout, this, isInvalidate(), read, needRetVal, isolation, createTtl, accessTtl, CU.empty0(), skipStore, keepBinary); return new GridEmbeddedFuture<>( fut, new PLC1<GridCacheReturn>(ret) { @Override protected GridCacheReturn postLock(GridCacheReturn ret) throws IgniteCheckedException { if (log.isDebugEnabled()) log.debug("Acquired transaction lock on keys: " + passedKeys); postLockWrite(cacheCtx, passedKeys, ret, /*remove*/false, /*retval*/false, /*read*/read, accessTtl, filter == null ? CU.empty0() : filter, /*computeInvoke*/false); return ret; } } ); }', 'output': 'private IgniteInternalFuture<CacheReturn> obtainLockAsync( final GridCacheContext cacheCtx, CacheReturn ret, final Collection<KeyCacheObject> passedKeys, final boolean read, final boolean needRetVal, final long accessTtl, @Nullable final CacheEntryPredicate[] filter, boolean skipStore, boolean keepBinary) { if (log.isDebugEnabled()) log.debug("Before acquiring transaction lock on keys [keys=" + passedKeys + \']\'); if (passedKeys.isEmpty()) return new GridFinishedFuture<>(ret); GridDhtTransactionalCacheAdapter<?, ?> dhtCache = cacheCtx.isNear() ? cacheCtx.nearTx().dht() : cacheCtx.dhtTx(); long timeout = remainingTime(); if (timeout == -1) return new GridFinishedFuture<>(timeoutException()); IgniteInternalFuture<Boolean> fut = dhtCache.lockAllAsyncInternal(passedKeys, timeout, this, isInvalidate(), read, needRetVal, isolation, accessTtl, CU.empty0(), skipStore, keepBinary); return new GridEmbeddedFuture<>( fut, new PLC1<CacheReturn>(ret) { @Override protected CacheReturn postLock(CacheReturn ret) throws IgniteCheckedException { if (log.isDebugEnabled()) log.debug("Acquired transaction lock on keys: " + passedKeys); postLockWrite(cacheCtx, passedKeys, ret, /*remove*/false, /*retval*/false, /*read*/read, accessTtl, filter == null ? CU.empty0() : filter, /*computeInvoke*/false); return ret; } } ); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private <T> TableFunction<T> viewFunction( JavaTypeFactory typeFactory, final String name, String viewSql) { return new TableFunction<T>() { public List<Parameter> getParameters() { return Collections.emptyList(); } public Type getElementType() { throw new UnsupportedOperationException(); } public Table<T> apply(List<Object> arguments) { throw new UnsupportedOperationException(); } }; }', 'ground_truth': 'private <T> TableFunction<T> viewFunction( final Schema schema, final JavaTypeFactory typeFactory, final String name, final String viewSql) { return new TableFunction<T>() { public List<Parameter> getParameters() { return Collections.emptyList(); } public Table<T> apply(List<Object> arguments) { Queryable<T> queryable = Factory.implement().toQueryable( new OptiqPrepare.Context() { public JavaTypeFactory getTypeFactory() { return typeFactory; } public Schema getRootSchema() { return schema; } }, viewSql); Type elementType = queryable.getElementType(); return new ViewTable<T>(elementType, schema, name); } }; }', 'output': 'private <T> ViewFunction<T> viewFunction( JavaTypeFactory typeFactory, final String name, String viewSql) { return new ViewFunction<T>() { public List<Parameter> getParameters() { return Collections.emptyList(); } public Type getElementType() { throw new UnsupportedOperationException(); } public View<T> apply(List<Object> arguments) { throw new UnsupportedOperationException(); } }; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Indexer indexerFor(final DecoratedKey key, int nowInSec, OpOrder.Group opGroup, IndexTransaction.Type transactionType) { return new Indexer() { public void begin() { } public void partitionDelete(DeletionTime deletionTime) { partitionDeletions.add(deletionTime); } public void rangeTombstone(RangeTombstone tombstone) { rangeTombstones.add(tombstone); } public void insertRow(Row row) { rowsInserted.add(row); } public void removeRow(Row row) { rowsDeleted.add(row); } public void updateRow(Row oldRowData, Row newRowData) { rowsUpdated.add(Pair.create(oldRowData, newRowData)); } public void finish() { } }; }', 'ground_truth': 'public Indexer indexerFor(final DecoratedKey key, PartitionColumns columns, int nowInSec, OpOrder.Group opGroup, IndexTransaction.Type transactionType) { return new Indexer() { public void begin() { } public void partitionDelete(DeletionTime deletionTime) { partitionDeletions.add(deletionTime); } public void rangeTombstone(RangeTombstone tombstone) { rangeTombstones.add(tombstone); } public void insertRow(Row row) { rowsInserted.add(row); } public void removeRow(Row row) { rowsDeleted.add(row); } public void updateRow(Row oldRowData, Row newRowData) { rowsUpdated.add(Pair.create(oldRowData, newRowData)); } public void finish() { } }; }', 'output': 'public Indexer indexerFor(final DecoratedKey key, int nowInSec, OpOrder.Group opGroup, IndexTransaction.Type transactionType) { return new Indexer() { public void begin() { } public void partitionDelete(DeletionTime deletionTime) { partitionDeletions.add(deletionTime); } public void rangeTombstone(RangeTombstone tombstone) { rangeTombstones.add(tombstone); } public void insertRow(Row row) { rowsInserted.add(row); } public void removeRow(Row row) { rowsDeleted.add(row); } public void updateRow(Row oldRowData, Row newRowData) { rowsUpdated.add(Pair.create(oldRowData, newRowData)); } public void finish() { } }; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public TaskGroupNode(String group, GradleBaseProject project) { super(new TaskGroupChildren(group, project), Lookup.EMPTY); setName(group); String displayName = GradleBaseProject.PRIVATE_TASK_GROUP.equals(group) ? LBL_PrivateTasks() : Utils.capitalize(group); setDisplayName(displayName); }', 'ground_truth': 'public TaskGroupNode(String group, GradleBaseProject project, Project genericProject) { super(new TaskGroupChildren(group, project, genericProject), Lookup.EMPTY); setName(group); String displayName = GradleBaseProject.PRIVATE_TASK_GROUP.equals(group) ? LBL_PrivateTasks() : Utils.capitalize(group); setDisplayName(displayName); }', 'output': 'public TaskGroupNode(String name, GradleBaseProject project) { super(new TaskGroupChildren(name, project), Lookup.EMPTY); setName(name); String displayName = GradleBaseProject.PRIVATE_TASK_GROUP.equals(name) ? LBL_PrivateTasks() : Utils.capitalize(name); setDisplayName(displayName); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void doVerb(Message message) { logger.debug("Received schema check request."); Message response = message.getInternalReply(DatabaseDescriptor.getDefsVersion().toString().getBytes()); MessagingService.instance().sendOneWay(response, message.getFrom()); }', 'ground_truth': 'public void doVerb(Message message, String id) { logger.debug("Received schema check request."); Message response = message.getInternalReply(DatabaseDescriptor.getDefsVersion().toString().getBytes()); MessagingService.instance().sendReply(response, id, message.getFrom()); }', 'output': 'public void doVerb(Message message) { logger.debug("Received schema check request."); Message response = message.reply(DatabaseDescriptor.getDefsVersion().toString().getBytes()); MessagingService.instance().sendOneWay(response, message.getFrom()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public SSTableReader open(Descriptor descriptor, Set<Component> components, CFMetaData metadata, IPartitioner partitioner, Long maxDataAge, StatsMetadata sstableMetadata, SSTableReader.OpenReason openReason) { return new BigTableReader(descriptor, components, metadata, partitioner, maxDataAge, sstableMetadata, openReason); }', 'ground_truth': 'public SSTableReader open(Descriptor descriptor, Set<Component> components, CFMetaData metadata, IPartitioner partitioner, Long maxDataAge, StatsMetadata sstableMetadata, SSTableReader.OpenReason openReason, SerializationHeader header) { return new BigTableReader(descriptor, components, metadata, partitioner, maxDataAge, sstableMetadata, openReason, header); }', 'output': 'public SSTableReader open(Descriptor descriptor, Collection<Component> components, CFMetaData metadata, IPartitioner partitioner, Long maxDataAge, StatsMetadata sstableMetadata, SSTableReader.OpenReason openReason) { return new BigTableReader(descriptor, components, metadata, partitioner, maxDataAge, sstableMetadata, openReason); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GroupByPartitionIterator(int pageSize, ConsistencyLevel consistency, ClientState clientState) { this(pageSize, consistency, clientState, null); }', 'ground_truth': 'public GroupByPartitionIterator(int pageSize, ConsistencyLevel consistency, ClientState clientState, long queryStartNanoTime) { this(pageSize, consistency, clientState, null, queryStartNanoTime); }', 'output': 'public GroupByPartitionIterator(int pageSize, ConsistencyLevel consistency, ClientState clientState, String tableName) { this(pageSize, consistency, clientState, tableName, null); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public InetAddress findSuitableEndPoint(String key) throws IOException, UnavailableException { List<InetAddress> endpoints = getNaturalEndpoints(key); endPointSnitch_.sortByProximity(FBUtilities.getLocalAddress(), endpoints); for (InetAddress endpoint : endpoints) { if (FailureDetector.instance.isAlive(endpoint)) return endpoint; } throw new UnavailableException(); }', 'ground_truth': 'public InetAddress findSuitableEndPoint(String table, String key) throws IOException, UnavailableException { List<InetAddress> endpoints = getNaturalEndpoints(table, key); DatabaseDescriptor.getEndPointSnitch(table).sortByProximity(FBUtilities.getLocalAddress(), endpoints); for (InetAddress endpoint : endpoints) { if (FailureDetector.instance.isAlive(endpoint)) return endpoint; } throw new UnavailableException(); }', 'output': 'public InetAddress findSuitableEndPoint(String key) throws IOException, UnavailableException { List<InetAddress> endpoints = getNaturalEndpoints(key); endPointSnitch_.sortByProximity(FBUtilities.getLocalAddress(), endpoints); for (InetAddress endpoint : endpoints) { if (FailureDetector.instance.isAlive(endpoint)) return endpoint; } throw new UnavailableException(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private IInvokableInstance newInstance() { ClassLoader classLoader = new InstanceClassLoader(config.num(), version.classpath, sharedClassLoader); return Instance.transferAdhoc((SerializableBiFunction<IInstanceConfig, ClassLoader, Instance>)Instance::new, classLoader) .apply(config.forVersion(version.major), classLoader); }', 'ground_truth': 'private IInvokableInstance newInstance(int generation) { ClassLoader classLoader = new InstanceClassLoader(generation, version.classpath, sharedClassLoader); return Instance.transferAdhoc((SerializableBiFunction<IInstanceConfig, ClassLoader, Instance>)Instance::new, classLoader) .apply(config.forVersion(version.major), classLoader); }', 'output': 'private IInvokableInstance newInstance() { ClassLoader classLoader = new InstanceClassLoader(config.num(), version.classpath, sharedClassLoader); return Instance.transferAdhoc((SerializableBiFunction<IInstanceConfig, ClassLoader, Instance>)Instance::new, classLoader) .apply(config.forVersion(version.major), classLoader); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Object read(MessageCollectionItemType type) { switch (type) { case BYTE: return readByte(); case SHORT: return readShort(); case INT: return readInt(); case LONG: return readLong(); case FLOAT: return readFloat(); case DOUBLE: return readDouble(); case CHAR: return readChar(); case BOOLEAN: return readBoolean(); case BYTE_ARR: return readByteArray(); case SHORT_ARR: return readShortArray(); case INT_ARR: return readIntArray(); case LONG_ARR: return readLongArray(); case FLOAT_ARR: return readFloatArray(); case DOUBLE_ARR: return readDoubleArray(); case CHAR_ARR: return readCharArray(); case BOOLEAN_ARR: return readBooleanArray(); case STRING: return readString(); case BIT_SET: return readBitSet(); case UUID: return readUuid(); case IGNITE_UUID: return readIgniteUuid(); case MSG: return readMessage(); default: throw new IllegalArgumentException("Unknown type: " + type); } }', 'ground_truth': 'private Object read(MessageCollectionItemType type, MessageReader reader) { switch (type) { case BYTE: return readByte(); case SHORT: return readShort(); case INT: return readInt(); case LONG: return readLong(); case FLOAT: return readFloat(); case DOUBLE: return readDouble(); case CHAR: return readChar(); case BOOLEAN: return readBoolean(); case BYTE_ARR: return readByteArray(); case SHORT_ARR: return readShortArray(); case INT_ARR: return readIntArray(); case LONG_ARR: return readLongArray(); case FLOAT_ARR: return readFloatArray(); case DOUBLE_ARR: return readDoubleArray(); case CHAR_ARR: return readCharArray(); case BOOLEAN_ARR: return readBooleanArray(); case STRING: return readString(); case BIT_SET: return readBitSet(); case UUID: return readUuid(); case IGNITE_UUID: return readIgniteUuid(); case MSG: return readMessage(reader); default: throw new IllegalArgumentException("Unknown type: " + type); } }', 'output': 'private Object read(MessageType type) { switch (type) { case BYTE: return readByte(); case SHORT: return readShort(); case INT: return readInt(); case LONG: return readLong(); case FLOAT: return readFloat(); case DOUBLE: return readDouble(); case CHAR: return readChar(); case BOOLEAN: return readBoolean(); case BYTE_ARR: return readByteArray(); case SHORT_ARR: return readShortArray(); case INT_ARR: return readIntArray(); case LONG_ARR: return readLongArray(); case FLOAT_ARR: return readFloatArray(); case DOUBLE_ARR: return readDoubleArray(); case CHAR_ARR: return readCharArray(); case BOOLEAN_ARR: return readBooleanArray(); case STRING: return readString(); case BIT_SET: return readBitSet(); case UUID: return readUuid(); case IGNITE_UUID: return readIgniteUuid(); case MSG: return readMessage(); default: throw new IllegalArgumentException("Unknown type: " + type); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void guard(long value, String what, @Nullable ClientState state) { if (!enabled(state)) return; long failValue = failValue(state); if (value > failValue) { triggerFail(value, failValue, what, state); return; } long warnValue = warnValue(state); if (value > warnValue) triggerWarn(value, warnValue, what); }', 'ground_truth': 'public void guard(long value, String what, boolean containsUserData, @Nullable ClientState state) { if (!enabled(state)) return; long failValue = failValue(state); if (value > failValue) { triggerFail(value, failValue, what, containsUserData, state); return; } long warnValue = warnValue(state); if (value > warnValue) triggerWarn(value, warnValue, what, containsUserData); }', 'output': 'public void guard(long value, String what, @Nullable ClientState state, TimeUnit unit) { if (!enabled(state, unit)) return; long failValue = failValue(state, unit); if (value > failValue) { triggerFail(value, failValue, what, state, unit); return; } long warnValue = warnValue(state, unit); if (value > warnValue) triggerWarn(value, warnValue, what); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void add(HostsType hostsType, String service, boolean clientOnly, ProcessingComponent pc) { int count = Double.valueOf(Math.ceil( (double) batch.percent / 100 * hostsType.hosts.size())).intValue(); int i = 0; for (String host : hostsType.hosts) { HostsType singleHostsType = new HostsType(); singleHostsType.hosts = Collections.singleton(host); Map<String, List<TaskProxy>> targetMap = ((i++) < count) ? initialBatch : finalBatches; List<TaskProxy> targetList = targetMap.get(host); if (null == targetList) { targetList = new ArrayList<TaskProxy>(); targetMap.put(host, targetList); } TaskProxy proxy = null; if (null != pc.preTasks && pc.preTasks.size() > 0) { proxy = new TaskProxy(); proxy.clientOnly = clientOnly; proxy.message = getStageText("Preparing", pc.name, Collections.singleton(host)); proxy.tasks.addAll(TaskWrapperBuilder.getTaskList(service, pc.name, singleHostsType, pc.preTasks)); proxy.service = service; proxy.component = pc.name; targetList.add(proxy); } if (null != pc.tasks && 1 == pc.tasks.size()) { Task t = pc.tasks.get(0); if (RestartTask.class.isInstance(t)) { proxy = new TaskProxy(); proxy.clientOnly = clientOnly; proxy.tasks.add(new TaskWrapper(service, pc.name, Collections.singleton(host), t)); proxy.restart = true; proxy.service = service; proxy.component = pc.name; proxy.message = getStageText("Restarting ", pc.name, Collections.singleton(host)); targetList.add(proxy); } } if (null != pc.postTasks && pc.postTasks.size() > 0) { proxy = new TaskProxy(); proxy.clientOnly = clientOnly; proxy.component = pc.name; proxy.service = service; proxy.tasks.addAll(TaskWrapperBuilder.getTaskList(service, pc.name, singleHostsType, pc.postTasks)); proxy.message = getStageText("Completing", pc.name, Collections.singleton(host)); targetList.add(proxy); } } }', 'ground_truth': 'public void add(HostsType hostsType, String service, boolean forUpgrade, boolean clientOnly, ProcessingComponent pc) { int count = Double.valueOf(Math.ceil( (double) batch.percent / 100 * hostsType.hosts.size())).intValue(); int i = 0; for (String host : hostsType.hosts) { HostsType singleHostsType = new HostsType(); singleHostsType.hosts = Collections.singleton(host); Map<String, List<TaskProxy>> targetMap = ((i++) < count) ? initialBatch : finalBatches; List<TaskProxy> targetList = targetMap.get(host); if (null == targetList) { targetList = new ArrayList<TaskProxy>(); targetMap.put(host, targetList); } TaskProxy proxy = null; List<Task> tasks = resolveTasks(forUpgrade, true, pc); if (null != tasks && tasks.size() > 0) { proxy = new TaskProxy(); proxy.clientOnly = clientOnly; proxy.message = getStageText("Preparing", pc.name, Collections.singleton(host)); proxy.tasks.addAll(TaskWrapperBuilder.getTaskList(service, pc.name, singleHostsType, tasks)); proxy.service = service; proxy.component = pc.name; targetList.add(proxy); } if (null != pc.tasks && 1 == pc.tasks.size()) { Task t = pc.tasks.get(0); if (RestartTask.class.isInstance(t)) { proxy = new TaskProxy(); proxy.clientOnly = clientOnly; proxy.tasks.add(new TaskWrapper(service, pc.name, Collections.singleton(host), t)); proxy.restart = true; proxy.service = service; proxy.component = pc.name; proxy.message = getStageText("Restarting ", pc.name, Collections.singleton(host)); targetList.add(proxy); } } tasks = resolveTasks(forUpgrade, false, pc); if (null != tasks && tasks.size() > 0) { proxy = new TaskProxy(); proxy.clientOnly = clientOnly; proxy.component = pc.name; proxy.service = service; proxy.tasks.addAll(TaskWrapperBuilder.getTaskList(service, pc.name, singleHostsType, tasks)); proxy.message = getStageText("Completing", pc.name, Collections.singleton(host)); targetList.add(proxy); } } }', 'output': 'public void add(HostsType hostsType, String service, boolean clientOnly, ProcessingComponent pc) { int count = Double.valueOf(Math.ceil( (double) batch.percent / 100 * hostsType.hosts.size())).intValue(); int i = 0; for (String host : hostsType.hosts) { HostsType singleHostsType = new HostsType(); singleHostsType.hosts = Collections.singleton(host); Map<String, List<TaskProxy>> targetMap = ((i++) < count) ? initialBatch : finalBatches; List<TaskProxy> targetList = targetMap.get(host); if (null == targetList) { targetList = new ArrayList<TaskProxy>(); targetMap.put(host, targetList); } TaskProxy proxy = null; if (null != pc.preTasks && pc.preTasks.size() > 0) { proxy = new TaskProxy(); proxy.clientOnly = clientOnly; proxy.message = getStageText("Preparing", pc.name, Collections.singleton(host)); proxy.tasks.addAll(TaskWrapperBuilder.getTaskList(service, pc.name, singleHostsType, pc.preTasks)); proxy.service = service; proxy.component = pc.name; targetList.add(proxy); } if (null != pc.tasks && 1 == pc.tasks.size()) { Task t = pc.tasks.get(0); if (RestartTask.class.isInstance(t)) { proxy = new TaskProxy(); proxy.clientOnly = clientOnly; proxy.tasks.add(new TaskWrapper(service, pc.name, Collections.singleton(host), t)); proxy.restart = true; proxy.service = service; proxy.component = pc.name; proxy.message = getStageText("Restarting ", pc.name, Collections.singleton(host)); targetList.add(proxy); } } if (null != pc.postTasks && pc.postTasks.size() > 0) { proxy = new TaskProxy(); proxy.clientOnly = clientOnly; proxy.component = pc.name; proxy.service = service; proxy.tasks.addAll(TaskWrapperBuilder.getTaskList(service, pc.name, singleHostsType, pc.postTasks)); proxy.message = getStage'}
{'instruction': 'Please refactor the following code:', 'input': 'private void addAllProperties() { m_mapQueryProperties.putAll(m_mapAllProperties); for (Map.Entry<String, ResourceDefinition> entry : m_resourceDefinition.getSubResources().entrySet()) { String name = entry.getKey(); if (! m_mapSubResources.containsKey(name)) { m_mapSubResources.put(name, entry.getValue()); } } }', 'ground_truth': 'private void addAllProperties(TemporalInfo temporalInfo) { if (temporalInfo == null) { m_mapQueryProperties.putAll(m_mapAllProperties); } else { for (Map.Entry<String, Set<String>> entry : m_mapAllProperties.entrySet()) { String path = entry.getKey(); Set<String> setProps = entry.getValue(); m_mapQueryProperties.put(path, setProps); m_mapCategoryTemporalInfo.put(path, temporalInfo); } } for (Map.Entry<String, ResourceDefinition> entry : m_resourceDefinition.getSubResources().entrySet()) { String name = entry.getKey(); if (! m_mapSubResources.containsKey(name)) { m_mapSubResources.put(name, entry.getValue()); } } }', 'output': 'private void addAllProperties() { m_mapQueryProperties.putAll(m_mapAllProperties); for (Map.Entry<String, ResourceDefinition> entry : m_resourceDefinition.getSubResources().entrySet()) { String name = entry.getKey(); if (! m_mapSubResources.containsKey(name)) { m_mapSubResources.put(name, entry.getValue()); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ServiceComponentHostResponse(String clusterName, String serviceName, String componentName, String displayName, String hostname, String liveState, String stackVersion, String desiredState, String desiredStackVersion, HostComponentAdminState adminState) { this.clusterName = clusterName; this.serviceName = serviceName; this.componentName = componentName; this.displayName = displayName; this.hostname = hostname; this.liveState = liveState; this.stackVersion = stackVersion; this.desiredState = desiredState; this.desiredStackVersion = desiredStackVersion; if (adminState != null) { this.adminState = adminState.name(); } }', 'ground_truth': 'public ServiceComponentHostResponse(String clusterName, String serviceName, String componentName, String displayName, String hostname, String publicHostname, String liveState, String stackVersion, String desiredState, String desiredStackVersion, HostComponentAdminState adminState) { this.clusterName = clusterName; this.serviceName = serviceName; this.componentName = componentName; this.displayName = displayName; this.hostname = hostname; this.publicHostname = publicHostname; this.liveState = liveState; this.stackVersion = stackVersion; this.desiredState = desiredState; this.desiredStackVersion = desiredStackVersion; if (adminState != null) { this.adminState = adminState.name(); } }', 'output': 'public ServiceComponentHostResponse(String clusterName, String serviceName, String componentName, String displayName, String hostname, String liveState, String stackVersion, String desiredState, String desiredStackVersion, HostComponentAdminState adminState, String status) { this.clusterName = clusterName; this.serviceName = serviceName; this.componentName = componentName; this.displayName = displayName; this.hostname = hostname; this.liveState = liveState; this.stackVersion = stackVersion; this.desiredState = desiredState; this.desiredStackVersion = desiredStackVersion; this.adminState = adminState.name(); this.status = status; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String fieldDeclaration( final @NonNull FieldDeclaration fd, final @NonNull Can<ImportDeclaration> importDeclarations) { val isDeprecated = fd.getAnnotations().stream() .anyMatch(a->a.getNameAsString().equals("Deprecated")) || fd.getJavadoc() .map(Javadocs::hasDeprecated) .orElse(false); val memberNameFormat = isDeprecated ? fd.isStatic() ? j2aContext.getDeprecatedStaticMemberNameFormat() : j2aContext.getDeprecatedMemberNameFormat() : fd.isStatic() ? j2aContext.getStaticMemberNameFormat() : j2aContext.getMemberNameFormat(); val fieldFormat =  j2aContext.getFormatter().getFieldFormat(); return String.format(fieldFormat, type(fd.getCommonType(), importDeclarations), String.format(memberNameFormat, FieldDeclarations.asNormalizedName(fd))); }', 'ground_truth': 'public String fieldDeclaration( final @NonNull FieldDeclaration fd, final @NonNull J2AdocUnit unit) { val isDeprecated = fd.getAnnotations().stream() .anyMatch(a->a.getNameAsString().equals("Deprecated")) || fd.getJavadoc() .map(Javadocs::hasDeprecated) .orElse(false); val memberNameFormat = isDeprecated ? fd.isStatic() ? j2aContext.getDeprecatedStaticMemberNameFormat() : j2aContext.getDeprecatedMemberNameFormat() : fd.isStatic() ? j2aContext.getStaticMemberNameFormat() : j2aContext.getMemberNameFormat(); val fieldFormat =  j2aContext.getFormatter().getFieldFormat(); return String.format(fieldFormat, type(fd.getCommonType(), unit), String.format(memberNameFormat, FieldDeclarations.asNormalizedName(fd))); }', 'output': 'public String fieldDeclaration( final @NonNull FieldDeclaration fd, final @NonNull Can<ImportDeclaration> importDeclarations) { val isDeprecated = fd.getAnnotations().stream() .anyMatch(a->a.getNameAsString().equals("Deprecated")) || fd.getJavadoc() .map(Javadocs::hasDeprecated) .orElse(false); val memberNameFormat = isDeprecated ? fd.isStatic() ? j2aContext.getDeprecatedStaticMemberNameFormat() : j2aContext.getDeprecatedMemberNameFormat() : fd.isStatic() ? j2aContext.getStaticMemberNameFormat() : j2aContext.getMemberNameFormat(); val fieldFormat =  j2aContext.getFormatter().getFieldFormat(); return String.format(fieldFormat, type(fd.getCommonType(), importDeclarations), String.format(memberNameFormat, FieldDeclarations.asNormalizedName(fd))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected  PageResponse getResources(Resource.Type type, Request request, Predicate predicate, PageRequest pageRequest) throws UnsupportedPropertyException, SystemException, NoSuchResourceException, NoSuchParentResourceException { Set<Resource> providerResources = getResources(type, request, predicate); populateResources(type, providerResources, request, predicate); return getPage(type, providerResources, request, predicate, pageRequest); }', 'ground_truth': 'protected  PageResponse getResources(Type type, Request request, Predicate predicate, PageRequest pageRequest, SortRequest sortRequest) throws UnsupportedPropertyException, SystemException, NoSuchResourceException, NoSuchParentResourceException { Set<Resource> providerResources = getResources(type, request, predicate); populateResources(type, providerResources, request, predicate); return getPage(type, providerResources, request, predicate, pageRequest, sortRequest); }', 'output': 'protected  PageResponse getResources(ResourceType type, Request request, Predicate predicate, PageRequest pageRequest) throws UnsupportedPropertyException, SystemException, NoSuchResourceException, NoSuchParentResourceException { Set<Resource> providerResources = getResources(type, request, predicate); populateResources(type, providerResources, request, predicate); return getPage(type, providerResources, request, predicate, pageRequest); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public DynamicCacheDescriptor(GridKernalContext ctx, CacheConfiguration cacheCfg, CacheType cacheType, CacheGroupDescriptor grpDesc, boolean template, UUID rcvdFrom, boolean staticCfg, boolean sql, IgniteUuid deploymentId, QuerySchema schema) { assert cacheCfg != null; assert grpDesc != null || template; assert schema != null; if (cacheCfg.getCacheMode() == CacheMode.REPLICATED && cacheCfg.getNearConfiguration() != null) { cacheCfg = new CacheConfiguration(cacheCfg); cacheCfg.setNearConfiguration(null); } this.cacheCfg = cacheCfg; this.cacheType = cacheType; this.grpDesc = grpDesc; this.template = template; this.rcvdFrom = rcvdFrom; this.staticCfg = staticCfg; this.sql = sql; this.deploymentId = deploymentId; cacheId = CU.cacheId(cacheCfg.getName()); synchronized (schemaMux) { this.schema = schema.copy(); } }', 'ground_truth': 'public DynamicCacheDescriptor(GridKernalContext ctx, CacheConfiguration cacheCfg, CacheType cacheType, CacheGroupDescriptor grpDesc, boolean template, UUID rcvdFrom, boolean staticCfg, boolean sql, IgniteUuid deploymentId, QuerySchema schema, @Nullable CacheConfigurationEnrichment cacheCfgEnrichment ) { assert cacheCfg != null; assert grpDesc != null || template; assert schema != null; if (cacheCfg.getCacheMode() == CacheMode.REPLICATED && cacheCfg.getNearConfiguration() != null) { cacheCfg = new CacheConfiguration(cacheCfg); cacheCfg.setNearConfiguration(null); } this.cacheCfg = cacheCfg; this.cacheType = cacheType; this.grpDesc = grpDesc; this.template = template; this.rcvdFrom = rcvdFrom; this.staticCfg = staticCfg; this.sql = sql; this.deploymentId = deploymentId; cacheId = CU.cacheId(cacheCfg.getName()); synchronized (schemaMux) { this.schema = schema.copy(); } this.cacheCfgEnrichment = cacheCfgEnrichment; }', 'output': 'public DynamicCacheDescriptor(IgniteKernalContext ctx, CacheConfiguration cacheCfg, CacheType cacheType, CacheGroupDescriptor grpDesc, boolean template, UUID rcvdFrom, boolean staticCfg, boolean sql, IgniteUuid deploymentId, QuerySchema schema) { assert cacheCfg != null; assert grpDesc != null || template; assert schema != null; if (cacheCfg.getCacheMode() == CacheMode.REPLICATED && cacheCfg.getNearConfiguration() != null) { cacheCfg = new CacheConfiguration(cacheCfg); cacheCfg.setNearConfiguration(null); } this.cacheCfg = cacheCfg; this.cacheType = cacheType; this.grpDesc = grpDesc; this.template = template; this.rcvdFrom = rcvdFrom; this.staticCfg = staticCfg; this.sql = sql; this.deploymentId = deploymentId; cacheId = CU.cacheId(cacheCfg.getName()); synchronized (schemaMux) { this.schema = schema.copy(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void defragment( CacheGroupContext grpCtx, CacheGroupContext newCtx, PageMemoryEx partPageMem, IntMap<LinkMap> mappingByPart, CheckpointTimeoutLock cpLock, Runnable cancellationChecker ) throws IgniteCheckedException { defragmentation.defragment(grpCtx, newCtx, partPageMem, mappingByPart, cpLock, cancellationChecker, log); } } }', 'ground_truth': '@Override public void defragment( CacheGroupContext grpCtx, CacheGroupContext newCtx, PageMemoryEx partPageMem, IntMap<LinkMap> mappingByPart, CheckpointTimeoutLock cpLock, Runnable cancellationChecker, IgniteThreadPoolExecutor defragmentationThreadPool ) throws IgniteCheckedException { defragmentation.defragment( grpCtx, newCtx, partPageMem, mappingByPart, cpLock, cancellationChecker, log, defragmentationThreadPool ); }', 'output': '@Override public void defragment( CacheGroupContext grpCtx, CacheGroupContext newCtx, PageMemoryEx partPageMem, IntMap<LinkMap> mappingByPart, CheckpointTimeoutLock cpLock, Runnable cancellationChecker ) throws IgniteCheckedException { defragmentation.defragment(grpCtx, newCtx, partPageMem, mappingByPart, cpLock, cancellationChecker, log); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Collection<QueryIndex> indexes(PreparedStatement stmt, String owner, String tbl) throws SQLException { Map<String, QueryIndex> idxs = new LinkedHashMap<>(); stmt.setString(1, owner); stmt.setString(2, tbl); try (ResultSet idxsRs = stmt.executeQuery()) { while (idxsRs.next()) { String idxName = idxsRs.getString(IDX_NAME_IDX); QueryIndex idx = idxs.get(idxName); if (idx == null) { idx = index(idxName); idxs.put(idxName, idx); } String expr = idxsRs.getString(IDX_EXPR_IDX); String col = expr == null ? idxsRs.getString(IDX_COL_NAME_IDX) : expr.replaceAll("\\"", ""); idx.getFields().put(col, !"DESC".equals(idxsRs.getString(IDX_COL_DESCEND_IDX))); } } return idxs.values(); }', 'ground_truth': 'private Collection<QueryIndex> indexes(PreparedStatement stmt, String owner, String tbl, String uniqueIdxAsPk) throws SQLException { stmt.setString(1, owner); stmt.setString(2, tbl); Map<String, QueryIndex> idxs = new LinkedHashMap<>(); try (ResultSet idxsRs = stmt.executeQuery()) { while (idxsRs.next()) { String idxName = idxsRs.getString(IDX_NAME_IDX); if (idxName.equals(uniqueIdxAsPk)) continue; QueryIndex idx = idxs.get(idxName); if (idx == null) { idx = index(idxName); idxs.put(idxName, idx); } String expr = idxsRs.getString(IDX_EXPR_IDX); String col = expr == null ? idxsRs.getString(IDX_COL_NAME_IDX) : expr.replaceAll("\\"", ""); idx.getFields().put(col, !"DESC".equals(idxsRs.getString(IDX_COL_DESCEND_IDX))); } } return idxs.values(); }', 'output': 'private List<QueryIndex> indexes(PreparedStatement stmt, String owner, String tbl) throws SQLException { Map<String, QueryIndex> idxs = new LinkedHashMap<>(); stmt.setString(1, owner); stmt.setString(2, tbl); try (ResultSet idxsRs = stmt.executeQuery()) { while (idxsRs.next()) { String idxName = idxsRs.getString(IDX_NAME_IDX); QueryIndex idx = idxs.get(idxName); if (idx == null) { idx = index(idxName); idxs.put(idxName, idx); } String expr = idxsRs.getString(IDX_EXPR_IDX); String col = expr == null ? idxsRs.getString(IDX_COL_NAME_IDX) : expr.replaceAll("\\"", ""); idx.getFields().put(col, !"DESC".equals(idxsRs.getString(IDX_COL_DESCEND_IDX))); } } return idxs.values(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void replaySerializedMutations(ByteBuffer data, long writtenAt, RateLimiter rateLimiter) throws IOException { DataInputStream in = new DataInputStream(ByteBufferUtil.inputStream(data)); int size = in.readInt(); for (int i = 0; i < size; i++) replaySerializedMutation(RowMutation.serializer.deserialize(in, VERSION), writtenAt, rateLimiter); }', 'ground_truth': 'private void replaySerializedMutations(ByteBuffer data, long writtenAt, int version, RateLimiter rateLimiter) throws IOException { DataInputStream in = new DataInputStream(ByteBufferUtil.inputStream(data)); int size = in.readInt(); for (int i = 0; i < size; i++) replaySerializedMutation(RowMutation.serializer.deserialize(in, version), writtenAt, version, rateLimiter); }', 'output': 'private void replaySerializedMutations(byte[] data, long writtenAt, RateLimiter rateLimiter) throws IOException { DataInputStream in = new DataInputStream(new ByteArrayInputStream(data)); int size = in.readInt(); for (int i = 0; i < size; i++) replaySerializedMutation(RowMutation.serializer.deserialize(in, VERSION), writtenAt, rateLimiter); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<ByteBuffer> values(QueryOptions options) { MultiCBuilder builder = MultiCBuilder.create(comparator, hasIN()); for (SingleRestriction r : restrictions) { r.appendTo(builder, options); if (builder.hasMissingElements()) break; } return toByteBuffers(builder.build()); }', 'ground_truth': 'public List<ByteBuffer> values(QueryOptions options, ClientState state) { MultiCBuilder builder = MultiCBuilder.create(comparator, hasIN()); for (SingleRestriction r : restrictions) { r.appendTo(builder, options); if (hasIN() && Guardrails.inSelectCartesianProduct.enabled(state)) Guardrails.inSelectCartesianProduct.guard(builder.buildSize(), "partition key", state); if (builder.hasMissingElements()) break; } return toByteBuffers(builder.build()); }', 'output': 'public List<byte[]> values(QueryOptions options) { MultiCBuilder builder = MultiCBuilder.create(comparator, hasIN()); for (SingleRestriction r : restrictions) { r.appendTo(builder, options); if (builder.hasMissingElements()) break; } return toByteArrays(builder.build()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected ByteBuffer getValue(DecoratedKey partitionKey, Row row) { switch (column.kind) { case PARTITION_KEY: return column.isOnAllComponents() ? partitionKey.getKey() : CompositeType.extractComponent(partitionKey.getKey(), column.position()); case CLUSTERING: return row.clustering().get(column.position()); default: Cell cell = row.getCell(column); return cell == null ? null : cell.value(); } }', 'ground_truth': 'protected ByteBuffer getValue(CFMetaData metadata, DecoratedKey partitionKey, Row row) { switch (column.kind) { case PARTITION_KEY: return metadata.getKeyValidator() instanceof CompositeType ? CompositeType.extractComponent(partitionKey.getKey(), column.position()) : partitionKey.getKey(); case CLUSTERING: return row.clustering().get(column.position()); default: Cell cell = row.getCell(column); return cell == null ? null : cell.value(); } }', 'output': 'protected ByteBuffer getValue(DecoratedKey partitionKey, Row row) { switch (column.kind) { case PARTITION_KEY: return partitionKey.getKey(); case CLUSTERING: return row.clustering().get(column.position()); default: Cell cell = row.getCell(column); return cell == null ? null : cell.value(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void processAffinityAssignmentResponse(UUID nodeId, GridDhtAffinityAssignmentResponse res) { if (log.isDebugEnabled()) log.debug("Processing affinity assignment response [node=" + nodeId + ", res=" + res + \']\'); GridDhtAssignmentFetchFuture fut = pendingAssignmentFetchFuts.get(res.futureId()); if (fut != null) fut.onResponse(nodeId, res); }', 'ground_truth': 'private void processAffinityAssignmentResponse(Integer grpId, UUID nodeId, GridDhtAffinityAssignmentResponse res) { if (log.isDebugEnabled()) log.debug("Processing affinity assignment response [node=" + nodeId + ", res=" + res + \']\'); GridDhtAssignmentFetchFuture fut = pendingAssignmentFetchFuts.get(res.futureId()); if (fut != null) fut.onResponse(nodeId, res); }', 'output': 'private void processAffinityAssignmentResponse(UUID nodeId, AffinityAssignmentResponse res) { if (log.isDebugEnabled()) log.debug("Processing affinity assignment response [node=" + nodeId + ", res=" + res + \']\'); GridDhtAssignmentFetchFuture fut = pendingAssignmentFetchFuts.get(res.futureId()); if (fut != null) fut.onResponse(nodeId, res); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private PageNoStoreImpl acquirePage(int cacheId, long pageId) { long absPtr = absolute(pageId); long marker = GridUnsafe.getLong(absPtr); if (marker != PAGE_MARKER) throw new IllegalStateException("Page was not allocated [absPtr=" + U.hexLong(absPtr) + ", cacheId=" + cacheId + ", pageId=" + U.hexLong(pageId) + ", marker=" + U.hexLong(marker) + \']\'); while (true) { long pinCnt = GridUnsafe.getLong(absPtr + PIN_CNT_OFFSET); if (pinCnt < 0) throw new IllegalStateException("Page has been deallocated [absPtr=" + U.hexLong(absPtr) + ", cacheId=" + cacheId + ", pageId=" + U.hexLong(pageId) + ", pinCnt=" + pinCnt + \']\'); if (GridUnsafe.compareAndSwapLong(null, absPtr + PIN_CNT_OFFSET, pinCnt, pinCnt + 1)) break; } acquiredPages.incrementAndGet(); return new PageNoStoreImpl(PageMemoryNoStoreImpl.this, idx, absPtr, cacheId, pageId); }', 'ground_truth': 'private PageNoStoreImpl acquirePage(int cacheId, long pageId, boolean restore) { long absPtr = absolute(pageId); long marker = GridUnsafe.getLong(absPtr); if (marker != PAGE_MARKER) throw new IllegalStateException("Page was not allocated [absPtr=" + U.hexLong(absPtr) + ", cacheId=" + cacheId + ", pageId=" + U.hexLong(pageId) + ", marker=" + U.hexLong(marker) + \']\'); while (true) { long pinCnt = GridUnsafe.getLong(absPtr + PIN_CNT_OFFSET); if (pinCnt < 0) throw new IllegalStateException("Page has been deallocated [absPtr=" + U.hexLong(absPtr) + ", cacheId=" + cacheId + ", pageId=" + U.hexLong(pageId) + ", pinCnt=" + pinCnt + \']\'); if (GridUnsafe.compareAndSwapLong(null, absPtr + PIN_CNT_OFFSET, pinCnt, pinCnt + 1)) break; } acquiredPages.incrementAndGet(); return new PageNoStoreImpl(PageMemoryNoStoreImpl.this, idx, absPtr, cacheId, pageId, restore); }', 'output': 'private PageNoStoreImpl acquirePage(long pageId) { long absPtr = absolute(pageId); long marker = GridUnsafe.getLong(absPtr); if (marker != PAGE_MARKER) throw new IllegalStateException("Page was not allocated [absPtr=" + U.hexLong(absPtr) + ", pageId=" + U.hexLong(pageId) + ", marker=" + U.hexLong(marker) + \']\'); while (true) { long pinCnt = GridUnsafe.getLong(absPtr + PIN_CNT_OFFSET); if (pinCnt < 0) throw new IllegalStateException("Page has been deallocated [absPtr=" + U.hexLong(absPtr) + ", pageId=" + U.hexLong(pageId) + ", pinCnt=" + pinCnt + \']\'); if (GridUnsafe.compareAndSwapLong(null, absPtr + PIN_CNT_OFFSET, pinCnt, pinCnt + 1)) break; } acquiredPages.incrementAndGet(); return new PageNoStoreImpl(PageMemoryNoStoreImpl.this, idx, absPtr, pageId); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ResultMessage executeInternal(QueryState queryState) throws RequestValidationException, RequestExecutionException { assert !hasConditions; for (IMutation mutation : getMutations(BatchQueryOptions.DEFAULT, true, queryState.getTimestamp())) { assert mutation instanceof Mutation; ((Mutation) mutation).apply(); } return null; }', 'ground_truth': 'public ResultMessage executeInternal(QueryState queryState, QueryOptions options) throws RequestValidationException, RequestExecutionException { assert !hasConditions; for (IMutation mutation : getMutations(BatchQueryOptions.withoutPerStatementVariables(options), true, queryState.getTimestamp())) { assert mutation instanceof Mutation; ((Mutation) mutation).apply(); } return null; }', 'output': 'public ResultMessage executeInternal(QueryState queryState, BatchQueryOptions options) throws RequestValidationException, RequestExecutionException { assert !hasConditions; for (IMutation mutation : getMutations(options, true, queryState.getTimestamp())) { assert mutation instanceof Mutation; ((Mutation) mutation).apply(); } return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private List<ReadCommand> getSliceCommands(List<ByteBuffer> variables, int limit) throws RequestValidationException { Collection<ByteBuffer> keys = getKeys(variables); List<ReadCommand> commands = new ArrayList<ReadCommand>(keys.size()); IDiskAtomFilter filter = makeFilter(variables, limit); for (ByteBuffer key : keys) { QueryProcessor.validateKey(key); commands.add(ReadCommand.create(keyspace(), key, columnFamily(), filter.cloneShallow())); } return commands; }', 'ground_truth': 'private List<ReadCommand> getSliceCommands(List<ByteBuffer> variables, int limit, long now) throws RequestValidationException { Collection<ByteBuffer> keys = getKeys(variables); List<ReadCommand> commands = new ArrayList<ReadCommand>(keys.size()); IDiskAtomFilter filter = makeFilter(variables, limit); for (ByteBuffer key : keys) { QueryProcessor.validateKey(key); commands.add(ReadCommand.create(keyspace(), key, columnFamily(), now, filter.cloneShallow())); } return commands; }', 'output': 'private List<WriteCommand> getSliceCommands(List<ByteBuffer> variables, int limit) throws RequestValidationException { Collection<ByteBuffer> keys = getKeys(variables); List<WriteCommand> commands = new ArrayList<WriteCommand>(keys.size()); IDiskAtomFilter filter = makeFilter(variables, limit); for (ByteBuffer key : keys) { QueryProcessor.validateKey(key); commands.add(WriteCommand.create(keyspace(), key, columnFamily(), filter.cloneShallow())); } return commands; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void addTail(Page page, ByteBuffer buf, BPlusIO<L> io, int lvl, boolean back, int idx) { Tail<L> t = new Tail<>(page, buf, io, lvl, idx); if (back) { assert tail != null; assert tail.lvl == lvl : "must be on the same level as out forward"; t.fwd = tail; } else { assert tail == null || tail.lvl == lvl - 1: "must be on upper higher than current tail"; t.down = tail; } tail = t; } private Tail(Page page, ByteBuffer buf, BPlusIO<L> io, int lvl, int idx) { assert page != null; this.page = page; this.buf = buf; this.io = io; this.lvl = lvl; this.idx = idx; }', 'ground_truth': 'private Tail(Page page, ByteBuffer buf, BPlusIO<L> io, boolean primary, int lvl, int idx) { assert idx == Integer.MIN_VALUE || (idx >= 0 && idx <= Short.MAX_VALUE): idx ; assert lvl >= 0 && lvl <= Byte.MAX_VALUE: lvl; assert page != null; this.page = page; this.buf = buf; this.io = io; this.primary = primary; this.lvl = (byte)lvl; this.idx = (short)idx; }', 'output': 'private void addTail(Page page, ByteBuffer buf, BPlusIO<Long> io, int lvl, boolean back, int idx) { Tail<Long> t = new Tail<>(page, buf, io, lvl, idx); if (back) { assert tail != null; assert tail.lvl == lvl : "must be on the same level as out forward"; t.fwd = tail; } else { assert tail == null || tail.lvl == lvl - 1: "must be on upper higher than current tail"; t.down = tail; } tail = t; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void doVerb(Message message) { byte[] body = message.getMessageBody(); ByteArrayInputStream buffer = new ByteArrayInputStream(body); try { RowMutation rm = RowMutation.serializer().deserialize(new DataInputStream(buffer)); rm.apply(); } catch (IOException e) { throw new IOError(e); } }', 'ground_truth': 'public void doVerb(Message message, String id) { byte[] body = message.getMessageBody(); ByteArrayInputStream buffer = new ByteArrayInputStream(body); try { RowMutation rm = RowMutation.serializer().deserialize(new DataInputStream(buffer)); rm.apply(); } catch (IOException e) { throw new IOError(e); } }', 'output': 'public void doVerb(Message message) { byte[] body = message.getMessageBody(); ByteBuffer buffer = ByteBuffer.wrap(body); try { RowMutation rm = RowMutation.serializer().deserialize(buffer); rm.apply(); } catch (IOException e) { throw new IOError(e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void fail(QueryRun r, UUID nodeId, String msg) { if (r != null) r.state(new CacheException("Failed to execute map query on the node: " + nodeId + ", " + msg), nodeId); }', 'ground_truth': 'private void fail(QueryRun r, UUID nodeId, String msg, byte failCode) { if (r != null) { CacheException e = new CacheException("Failed to execute map query on the node: " + nodeId + ", " + msg); if (failCode == GridQueryFailResponse.CANCELLED_BY_ORIGINATOR) e.addSuppressed(new QueryCancelledException()); r.state(e, nodeId); } }', 'output': 'private void fail(QueryRun r, UUID nodeId, String msg, Throwable t) { if (r != null) r.state(new CacheException("Failed to execute map query on the node: " + nodeId + ", " + msg, t), nodeId); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private NbBreakpoint[] convertClientBreakpointsToDebugger(String sourceFile, SourceBreakpoint[] sourceBreakpoints, DebugAdapterContext context) { int n = sourceBreakpoints.length; int[] lines = new int[n]; for (int i = 0; i < n; i++) { lines[i] = context.getDebuggerLine(sourceBreakpoints[i].getLine()); } NbBreakpoint[] breakpoints = new NbBreakpoint[n]; for (int i = 0; i < n; i++) { int hitCount = 0; try { hitCount = Integer.parseInt(sourceBreakpoints[i].getHitCondition()); } catch (NumberFormatException e) { hitCount = 0; } breakpoints[i] = new NbBreakpoint(sourceFile, lines[i], hitCount, sourceBreakpoints[i].getCondition(), sourceBreakpoints[i].getLogMessage()); } return breakpoints; }', 'ground_truth': 'private NbBreakpoint[] convertClientBreakpointsToDebugger(Source source, String sourceFile, SourceBreakpoint[] sourceBreakpoints, DebugAdapterContext context) { int n = sourceBreakpoints.length; int[] lines = new int[n]; for (int i = 0; i < n; i++) { lines[i] = context.getDebuggerLine(sourceBreakpoints[i].getLine()); } NbBreakpoint[] breakpoints = new NbBreakpoint[n]; for (int i = 0; i < n; i++) { int hitCount = 0; try { hitCount = Integer.parseInt(sourceBreakpoints[i].getHitCondition()); } catch (NumberFormatException e) { hitCount = 0; } breakpoints[i] = new NbBreakpoint(source, sourceFile, lines[i], hitCount, sourceBreakpoints[i].getCondition(), sourceBreakpoints[i].getLogMessage(), context); } return breakpoints; }', 'output': 'private Breakpoint[] convertClientBreakpointsToDebugger(String sourceFile, Breakpoint[] sourceBreakpoints, DebugAdapterContext context) { int n = sourceBreakpoints.length; int[] lines = new int[n]; for (int i = 0; i < n; i++) { lines[i] = context.getDebuggerLine(sourceBreakpoints[i].getLine()); } Breakpoint[] breakpoints = new Breakpoint[n]; for (int i = 0; i < n; i++) { int hitCount = 0; try { hitCount = Integer.parseInt(sourceBreakpoints[i].getHitCondition()); } catch (NumberFormatException e) { hitCount = 0; } breakpoints[i] = new Breakpoint(sourceFile, lines[i], hitCount, sourceBreakpoints[i].getCondition(), sourceBreakpoints[i].getLogMessage()); } return breakpoints; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected ExtendedFilter(ColumnFamilyStore cfs, IDiskAtomFilter filter, int maxResults, long timestamp, boolean countCQL3Rows, boolean isPaging) { assert cfs != null; assert filter != null; this.cfs = cfs; this.originalFilter = filter; this.maxResults = maxResults; this.timestamp = timestamp; this.countCQL3Rows = countCQL3Rows; this.isPaging = isPaging; if (countCQL3Rows) originalFilter.updateColumnsLimit(maxResults); if (isPaging && (!(originalFilter instanceof SliceQueryFilter) || ((SliceQueryFilter)originalFilter).finish().remaining() != 0)) throw new IllegalArgumentException("Cross-row paging is only supported for SliceQueryFilter having an empty finish column"); }', 'ground_truth': 'protected ExtendedFilter(ColumnFamilyStore cfs, DataRange dataRange, int maxResults, boolean countCQL3Rows, long timestamp) { assert cfs != null; assert dataRange != null; this.cfs = cfs; this.dataRange = dataRange; this.maxResults = maxResults; this.timestamp = timestamp; this.countCQL3Rows = countCQL3Rows; this.currentLimit = maxResults; if (countCQL3Rows) dataRange.updateColumnsLimit(maxResults); }', 'output': 'public ExtendedFilter(ColumnFamilyStore cfs, IDiskAtomFilter filter, int maxResults, long timestamp, boolean countCQL3Rows, boolean isPaging) { assert cfs != null; assert filter != null; this.cfs = cfs; this.originalFilter = filter; this.maxResults = maxResults; this.timestamp = timestamp; this.countCQL3Rows = countCQL3Rows; this.isPaging = isPaging; if (countCQL3Rows) originalFilter.updateColumnsLimit(maxResults); if (isPaging && (!(originalFilter instanceof SliceQueryFilter) || ((SliceQueryFilter)originalFilter).finish().remaining() != 0)) throw new IllegalArgumentException("Cross-row paging is only supported for SliceQueryFilter having an empty finish column"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ColumnDefinition ascii(String name) { return new ColumnDefinition(ByteBufferUtil.bytes(name), AsciiType.instance, null, null, null); }', 'ground_truth': 'public static ColumnDefinition ascii(String name, Integer cidx) { return new ColumnDefinition(ByteBufferUtil.bytes(name), AsciiType.instance, null, null, null, cidx); }', 'output': 'public static ColumnDefinition ascii(String name, String comment) { return new ColumnDefinition(ByteBufferUtil.bytes(name), AsciiType.instance, comment, null, null); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected final void teraSort() throws Exception { System.out.println("TeraSort ==============================================================="); getFileSystem().delete(new Path(sortOutDir), true); final JobConf jobConf = new JobConf(); jobConf.setUser(getUser()); jobConf.set("fs.defaultFS", getFsBase()); log().info("Desired number of reduces: " + numReduces()); jobConf.set("mapreduce.job.reduces", String.valueOf(numReduces())); log().info("Desired number of maps: " + numMaps()); final long splitSize = dataSizeBytes() / numMaps(); log().info("Desired split size: " + splitSize); jobConf.set("mapred.min.split.size", String.valueOf(splitSize)); jobConf.set("mapred.max.split.size", String.valueOf(splitSize)); jobConf.setBoolean(HadoopJobProperty.SHUFFLE_MAPPER_STRIPED_OUTPUT.propertyName(), true); jobConf.set(HadoopJobProperty.JOB_PARTIALLY_RAW_COMPARATOR.propertyName(), TextPartiallyRawComparator.class.getName()); Job job = setupConfig(jobConf); HadoopJobId jobId = new HadoopJobId(UUID.randomUUID(), 1); IgniteInternalFuture<?> fut = grid(0).hadoop().submit(jobId, createJobInfo(job.getConfiguration())); fut.get(); }', 'ground_truth': 'protected final void teraSort(boolean gzip) throws Exception { System.out.println("TeraSort ==============================================================="); getFileSystem().delete(new Path(sortOutDir), true); final JobConf jobConf = new JobConf(); jobConf.setUser(getUser()); jobConf.set("fs.defaultFS", getFsBase()); log().info("Desired number of reduces: " + numReduces()); jobConf.set("mapreduce.job.reduces", String.valueOf(numReduces())); log().info("Desired number of maps: " + numMaps()); final long splitSize = dataSizeBytes() / numMaps(); log().info("Desired split size: " + splitSize); jobConf.set("mapred.min.split.size", String.valueOf(splitSize)); jobConf.set("mapred.max.split.size", String.valueOf(splitSize)); jobConf.setBoolean(HadoopJobProperty.SHUFFLE_MAPPER_STRIPED_OUTPUT.propertyName(), true); jobConf.setInt(HadoopJobProperty.SHUFFLE_MSG_SIZE.propertyName(), 4096); if (gzip) jobConf.setBoolean(HadoopJobProperty.SHUFFLE_MSG_GZIP.propertyName(), true); jobConf.set(HadoopJobProperty.JOB_PARTIALLY_RAW_COMPARATOR.propertyName(), TextPartiallyRawComparator.class.getName()); Job job = setupConfig(jobConf); HadoopJobId jobId = new HadoopJobId(UUID.randomUUID(), 1); IgniteInternalFuture<?> fut = grid(0).hadoop().submit(jobId, createJobInfo(job.getConfiguration())); fut.get(); }', 'output': 'protected final void teraSort() throws Exception { System.out.println("TeraSort ==============================================================="); getFileSystem().delete(new Path(sortOutDir), true); final JobConf jobConf = new JobConf(); jobConf.setUser(getUser()); jobConf.set("fs.defaultFS", getFsBase()); log().info("Desired number of reduces: " + numReduces()); jobConf.set("mapreduce.job.reduces", String.valueOf(numReduces())); log().info("Desired number of maps: " + numMaps()); final long splitSize = dataSizeBytes() / numMaps(); log().info("Desired split size: " + splitSize); jobConf.set("mapred.min.split.size", String.valueOf(splitSize)); jobConf.set("mapred.max.split.size", String.valueOf(splitSize)); jobConf.setBoolean(HadoopJobProperty.SHUFFLE_MAPPER_STRIPED_OUTPUT.propertyName(), true); jobConf.set(HadoopJobProperty.JOB_PARTIALLY_RAW_COMPARATOR.propertyName(), TextPartiallyRawComparator.class.getName()); Job job = setupConfig(jobConf); HadoopJobId jobId = new HadoopJobId(UUID.randomUUID(), 1); IgniteInternalFuture<?> fut = grid(0).hadoop().submit(jobId, createJobInfo(job.getConfiguration())); fut.get(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public int serializedSize() { return 2 + firstName.remaining() + 2 + lastName.remaining() + 8 + 8; }', 'ground_truth': 'public int serializedSize(DBTypeSizes typeSizes) { int firstNameSize = firstName.remaining(); int lastNameSize = lastName.remaining(); return typeSizes.sizeof((short) firstNameSize) + firstNameSize + typeSizes.sizeof((short) lastNameSize) + lastNameSize + typeSizes.sizeof(offset) + typeSizes.sizeof(width); }', 'output': 'public int serializedSize() { return 1 + firstName.remaining() + 1 + lastName.remaining() + 8 + 8; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void onCacheStop0(GridCacheContextInfo cacheInfo, boolean destroy) { if (idx == null || !cacheNames.contains(cacheInfo.name())) return; String cacheName = cacheInfo.name(); synchronized (stateMux) { Iterator<Map.Entry<QueryTypeIdKey, QueryTypeDescriptorImpl>> it = types.entrySet().iterator(); while (it.hasNext()) { Map.Entry<QueryTypeIdKey, QueryTypeDescriptorImpl> entry = it.next(); if (F.eq(cacheName, entry.getKey().cacheName())) { it.remove(); typesByName.remove(new QueryTypeNameKey(cacheName, entry.getValue().name())); entry.getValue().markObsolete(); } } Iterator<Map.Entry<QueryIndexKey, QueryIndexDescriptorImpl>> idxIt = idxs.entrySet().iterator(); while (idxIt.hasNext()) { Map.Entry<QueryIndexKey, QueryIndexDescriptorImpl> idxEntry = idxIt.next(); if (F.eq(cacheName, idxEntry.getValue().typeDescriptor().cacheName())) idxIt.remove(); } for (SchemaOperation op : schemaOps.values()) { if (op.started()) op.manager().worker().cancel(); } try { idx.unregisterCache(cacheInfo, destroy); } catch (Exception e) { U.error(log, "Failed to clear indexing on cache unregister (will ignore): " + cacheName, e); } cacheNames.remove(cacheName); Iterator<Long> missedCacheTypeIter = missedCacheTypes.iterator(); while (missedCacheTypeIter.hasNext()) { long key = missedCacheTypeIter.next(); if (missedCacheTypeKeyMatches(key, cacheName)) missedCacheTypeIter.remove(); } } }', 'ground_truth': 'public void onCacheStop0(GridCacheContextInfo cacheInfo, boolean destroy, boolean clearIdx) { if (idx == null || !cacheNames.contains(cacheInfo.name())) return; String cacheName = cacheInfo.name(); synchronized (stateMux) { Iterator<Map.Entry<QueryTypeIdKey, QueryTypeDescriptorImpl>> it = types.entrySet().iterator(); while (it.hasNext()) { Map.Entry<QueryTypeIdKey, QueryTypeDescriptorImpl> entry = it.next(); if (F.eq(cacheName, entry.getKey().cacheName())) { it.remove(); typesByName.remove(new QueryTypeNameKey(cacheName, entry.getValue().name())); entry.getValue().markObsolete(); } } Iterator<Map.Entry<QueryIndexKey, QueryIndexDescriptorImpl>> idxIt = idxs.entrySet().iterator(); while (idxIt.hasNext()) { Map.Entry<QueryIndexKey, QueryIndexDescriptorImpl> idxEntry = idxIt.next(); if (F.eq(cacheName, idxEntry.getValue().typeDescriptor().cacheName())) idxIt.remove(); } for (SchemaOperation op : schemaOps.values()) { if (op.started()) op.manager().worker().cancel(); } try { idx.unregisterCache(cacheInfo, destroy, clearIdx); } catch (Exception e) { U.error(log, "Failed to clear indexing on cache unregister (will ignore): " + cacheName, e); } cacheNames.remove(cacheName); Iterator<Long> missedCacheTypeIter = missedCacheTypes.iterator(); while (missedCacheTypeIter.hasNext()) { long key = missedCacheTypeIter.next(); if (missedCacheTypeKeyMatches(key, cacheName)) missedCacheTypeIter.remove(); } } }', 'output': 'public void onCacheStop0(GridCacheContextInfo cacheInfo, boolean destroy) { if (idx == null || !cacheNames.contains(cacheInfo.name())) return; String cacheName = cacheInfo.name(); synchronized (stateMux) { Iterator<Map.Entry<QueryTypeIdKey, QueryTypeDescriptorImpl>> it = types.entrySet().iterator(); while (it.hasNext()) { Map.Entry<QueryTypeIdKey, QueryTypeDescriptorImpl> entry = it.next(); if (F.eq(cacheName, entry.getKey().cacheName())) { it.remove(); typesByName.remove(new QueryTypeNameKey(cacheName, entry.getValue().name())); entry.getValue().markObsolete(); } } Iterator<Map.Entry<QueryIndexKey, QueryIndexDescriptorImpl>> idxIt = idxs.entrySet().iterator(); while (idxIt.hasNext()) { Map.Entry<QueryIndexKey, QueryIndexDescriptorImpl> idxEntry = idxIt.next(); if (F.eq(cacheName, idxEntry.getValue().typeDescriptor().cacheName())) idxIt.remove(); } for (SchemaOperation op : schemaOps.values()) { if (op.started()) op.manager().worker().cancel(); } try { idx.unregisterCache(cacheInfo, destroy); } catch (Exception e) { U.error(log, "Failed to clear indexing on cache unregister (will ignore): " + cacheName, e); } cacheNames.remove(cacheName); Iterator<Long> missedCacheTypeIter = missedCacheTypes.iterator(); while (missedCacheTypeIter.hasNext()) { long key = missedCacheTypeIter.next(); if (missedCacheTypeKeyMatches(key, cacheName)) missedCacheTypeIter.remove(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void send_batch_insert(batch_mutation_t batchMutation) throws TException { oprot_.writeMessageBegin(new TMessage("batch_insert", TMessageType.CALL, seqid_)); batch_insert_args args = new batch_insert_args(); args.batchMutation = batchMutation; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); } public void send_batch_insert_blocking(batch_mutation_t batchMutation) throws TException\n    { { oprot_.writeMessageBegin(new TMessage("batch_insert_blocking", TMessageType.CALL, seqid_)); batch_insert_blocking_args args = new batch_insert_blocking_args(); args.batchMutation = batchMutation; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'ground_truth': 'public void send_batch_insert(batch_mutation_t batchMutation, boolean block) throws TException { oprot_.writeMessageBegin(new TMessage("batch_insert", TMessageType.CALL, seqid_)); batch_insert_args args = new batch_insert_args(); args.batchMutation = batchMutation; args.block = block; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'output': 'public void send_batch_insert(batch_mutation_t batchMutation) throws TException { oprot_.writeMessageBegin(new TMessage("batch_insert", TMessageType.CALL, seqid_)); batch_insert_args args = new batch_insert_args(); args.batchMutation = batchMutation; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Response getHost(@Context HttpHeaders headers, @Context UriInfo ui, @PathParam("hostName") String hostName) { return handleRequest(headers, null, ui, Request.Type.GET, createHostResource(m_clusterName, hostName, ui)); }', 'ground_truth': 'public Response getHost(String body, @Context HttpHeaders headers, @Context UriInfo ui, @PathParam("hostName") String hostName) { return handleRequest(headers, body, ui, Request.Type.GET, createHostResource(m_clusterName, hostName, ui)); }', 'output': 'public Response getHost(@Context final HttpHeaders headers, @Context final UriInfo ui, @PathParam("hostName") final String hostName) { return handleRequest(headers, null, ui, Request.Type.GET, createHostResource(m_clusterName, hostName, ui)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ResultMessage executeInternal(QueryState state) { throw new UnsupportedOperationException(); }', 'ground_truth': 'public ResultMessage executeInternal(QueryState state, QueryOptions options) { throw new UnsupportedOperationException(); }', 'output': 'public ResultMessage executeInternal(QueryState state, int maxRows) { throw new UnsupportedOperationException(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private TemplateLookupResult lookupTemplate(String name, Locale locale) throws IOException { final TemplateLookupResult lookupResult = templateLookupStrategy.lookup(new TemplateCacheTemplateLookupContext(name, locale)); if (lookupResult == null) { throw new NullPointerException("Lookup result shouldn\'t be null"); } return lookupResult; }', 'ground_truth': 'private TemplateLookupResult lookupTemplate(String name, Locale locale, Object customLookupCondition) throws IOException { final TemplateLookupResult lookupResult = templateLookupStrategy.lookup( new TemplateCacheTemplateLookupContext(name, locale, customLookupCondition)); if (lookupResult == null) { throw new NullPointerException("Lookup result shouldn\'t be null"); } return lookupResult; }', 'output': 'private TemplateLookupResult lookupTemplate(final String name, final Locale locale) throws IOException { final TemplateLookupResult lookupResult = templateLookupStrategy.lookup(new TemplateCacheTemplateLookupContext(name, locale)); if (lookupResult == null) { throw new NullPointerException("Lookup result shouldn\'t be null"); } return lookupResult; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void writeHintForMutation(RowMutation mutation, int ttl, InetAddress target) throws IOException { assert ttl > 0; UUID hostId = StorageService.instance.getTokenMetadata().getHostId(target); if ((hostId == null) && (MessagingService.instance().getVersion(target) < MessagingService.VERSION_12)) { logger.warn("Unable to store hint for host with missing ID, {} (old node?)", target.toString()); return; } assert hostId != null : "Missing host ID for " + target.getHostAddress(); mutation.toHint(ttl, hostId).apply(); StorageMetrics.totalHints.inc(); }', 'ground_truth': 'public static void writeHintForMutation(RowMutation mutation, long now, int ttl, InetAddress target) throws IOException { assert ttl > 0; UUID hostId = StorageService.instance.getTokenMetadata().getHostId(target); if ((hostId == null) && (MessagingService.instance().getVersion(target) < MessagingService.VERSION_12)) { logger.warn("Unable to store hint for host with missing ID, {} (old node?)", target.toString()); return; } assert hostId != null : "Missing host ID for " + target.getHostAddress(); mutation.toHint(now, ttl, hostId).apply(); StorageMetrics.totalHints.inc(); }', 'output': 'public static void writeHintForMutation(RowMutation mutation, long ttl, InetAddress target) throws IOException { assert ttl > 0; UUID hostId = StorageService.instance.getTokenMetadata().getHostId(target); if ((hostId == null) && (MessagingService.instance().getVersion(target) < MessagingService.VERSION_12)) { logger.warn("Unable to store hint for host with missing ID, {} (old node?)", target.toString()); return; } assert hostId != null : "Missing host ID for " + target.getHostAddress(); mutation.toHint(ttl, hostId).apply(); StorageMetrics.totalHints.inc(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void readBinary(BinaryReaderExImpl reader) throws BinaryObjectException { super.readBinary(reader); int size = reader.readInt(); if (size == 0) meta = Collections.emptyList(); else { meta = new ArrayList<>(size); for (int i = 0; i < size; ++i) { JdbcTableMeta m = new JdbcTableMeta(); m.readBinary(reader); meta.add(m); } } } /** * @return Tables metadata. */ public List<JdbcTableMeta> meta() { return meta; } /** {@inheritDoc} */ @Override public String toString() { return S.toString(JdbcMetaTablesResult.class, this); } } }', 'ground_truth': '@Override public void readBinary(BinaryReaderExImpl reader, ClientListenerProtocolVersion ver) throws BinaryObjectException { super.readBinary(reader, ver); int size = reader.readInt(); if (size == 0) meta = Collections.emptyList(); else { meta = new ArrayList<>(size); for (int i = 0; i < size; ++i) { JdbcTableMeta m = new JdbcTableMeta(); m.readBinary(reader, ver); meta.add(m); } } }', 'output': '@Override public void readBinary(BinaryReaderExImpl reader) throws BinaryObjectException { super.readBinary(reader); int size = reader.readInt(); if (size == 0) meta = Collections.emptyList(); else { meta = new ArrayList<>(size); for (int i = 0; i < size; ++i) { JdbcTableMeta m = new JdbcTableMeta(); m.readBinary(reader); meta.add(m); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testNewCluster(int perUnitCount, TokenCount tc, TestReplicationStrategy rs) { System.out.println("Testing new cluster, target " + perUnitCount + " vnodes, replication " + rs); final int targetClusterSize = TARGET_CLUSTER_SIZE; NavigableMap<Token, Unit> tokenMap = Maps.newTreeMap(); ReplicationAwareTokenAllocator<Unit> t = new ReplicationAwareTokenAllocator<>(tokenMap, rs, partitioner); grow(t, targetClusterSize * 2 / 5, tc, perUnitCount, false); grow(t, targetClusterSize, tc, perUnitCount, true); loseAndReplace(t, targetClusterSize / 5, tc, perUnitCount); System.out.println(); }', 'ground_truth': 'public void testNewCluster(int perUnitCount, TokenCount tc, TestReplicationStrategy rs, IPartitioner partitioner) { System.out.println("Testing new cluster, target " + perUnitCount + " vnodes, replication " + rs); final int targetClusterSize = TARGET_CLUSTER_SIZE; NavigableMap<Token, Unit> tokenMap = Maps.newTreeMap(); ReplicationAwareTokenAllocator<Unit> t = new ReplicationAwareTokenAllocator<>(tokenMap, rs, partitioner); grow(t, targetClusterSize * 2 / 5, tc, perUnitCount, false); grow(t, targetClusterSize, tc, perUnitCount, true); loseAndReplace(t, targetClusterSize / 5, tc, perUnitCount, partitioner); System.out.println(); }', 'output': 'public void testNewCluster(int perUnitCount, TokenCount tc, TestReplicationStrategy rs) { System.out.println("Testing new cluster, target " + perUnitCount + " vnodes, replication " + rs); final int targetClusterSize = TARGET_CLUSTER_SIZE; NavigableMap<Token, Unit> tokenMap = Maps.newTreeMap(); ReplicationAwareTokenAllocator<Unit> t = new ReplicationAwareTokenAllocator<>(tokenMap, rs, partitioner); grow(t, targetClusterSize * 2 / 5, tc, perUnitCount, false); grow(t, targetClusterSize, tc, perUnitCount, true); loseAndReplace(t, targetClusterSize / 5, tc, perUnitCount); System.out.println(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static SSTableWriter create(Descriptor descriptor, long keyCount, long repairedAt, int sstableLevel, SerializationHeader header, Collection<Index> indexes, LifecycleTransaction txn) { TableMetadataRef metadata = Schema.instance.getTableMetadataRef(descriptor); return create(metadata, descriptor, keyCount, repairedAt, sstableLevel, header, indexes, txn); } public static SSTableWriter create(TableMetadataRef metadata,\n                                       Descriptor descriptor,\n                                       long keyCount,\n                                       long repairedAt,\n                                       int sstableLevel,\n                                       SerializationHeader header,\n                                       Collection<Index> indexes,\n                                       LifecycleTransaction txn)\n    { Descriptor descriptor, long keyCount, long repairedAt, int sstableLevel, SerializationHeader header, Collection<Index> indexes, LifecycleTransaction txn) { MetadataCollector collector = new MetadataCollector(metadata.get().comparator).sstableLevel(sstableLevel); return create(descriptor, keyCount, repairedAt, metadata, collector, header, indexes, txn); }', 'ground_truth': 'public static SSTableWriter create(Descriptor descriptor, long keyCount, long repairedAt, UUID pendingRepair, int sstableLevel, SerializationHeader header, Collection<Index> indexes, LifecycleTransaction txn) { TableMetadataRef metadata = Schema.instance.getTableMetadataRef(descriptor); return create(metadata, descriptor, keyCount, repairedAt, pendingRepair, sstableLevel, header, indexes, txn); }', 'output': 'public static SSTableWriter create(Descriptor descriptor, long keyCount, long repairedAt, int sstableLevel, SerializationHeader header, Collection<Index> indexes, LifecycleTransaction txn) { TableMetadataRef metadata = Schema.instance.getTableMetadataRef(descriptor); return create(metadata, descriptor, keyCount, repairedAt, sstableLevel, header, indexes, txn); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void writeQueryEntity(BinaryRawWriter writer, QueryEntity qryEntity) { assert qryEntity != null; writer.writeString(qryEntity.getKeyType()); writer.writeString(qryEntity.getValueType()); writer.writeString(qryEntity.getTableName()); writer.writeString(qryEntity.getKeyFieldName()); writer.writeString(qryEntity.getValueFieldName()); LinkedHashMap<String, String> fields = qryEntity.getFields(); if (fields != null) { Set<String> keyFields = qryEntity.getKeyFields(); Set<String> notNullFields = qryEntity.getNotNullFields(); Map<String, Object> defVals = qryEntity.getDefaultFieldValues(); Map<String, IgniteBiTuple<Integer, Integer>> decimalInfo = qryEntity.getDecimalInfo(); writer.writeInt(fields.size()); for (Map.Entry<String, String> field : fields.entrySet()) { writer.writeString(field.getKey()); writer.writeString(field.getValue()); writer.writeBoolean(keyFields != null && keyFields.contains(field.getKey())); writer.writeBoolean(notNullFields != null && notNullFields.contains(field.getKey())); writer.writeObject(defVals != null ? defVals.get(field.getKey()) : null); IgniteBiTuple<Integer, Integer> precisionAndScale = decimalInfo == null ? null : decimalInfo.get(field.getKey()); writer.writeInt(precisionAndScale == null ? -1 : precisionAndScale.get1()); writer.writeInt(precisionAndScale == null ? -1 : precisionAndScale.get2()); } } else writer.writeInt(0); Map<String, String> aliases = qryEntity.getAliases(); if (aliases != null) { writer.writeInt(aliases.size()); for (Map.Entry<String, String> alias : aliases.entrySet()) { writer.writeString(alias.getKey()); writer.writeString(alias.getValue()); } } else writer.writeInt(0); Collection<QueryIndex> indexes = qryEntity.getIndexes(); if (indexes != null) { writer.writeInt(indexes.size()); for (QueryIndex index : indexes) writeQueryIndex(writer, index); } else writer.writeInt(0); }', 'ground_truth': 'public static void writeQueryEntity(BinaryRawWriter writer, QueryEntity qryEntity, ClientListenerProtocolVersion ver) { assert qryEntity != null; writer.writeString(qryEntity.getKeyType()); writer.writeString(qryEntity.getValueType()); writer.writeString(qryEntity.getTableName()); writer.writeString(qryEntity.getKeyFieldName()); writer.writeString(qryEntity.getValueFieldName()); LinkedHashMap<String, String> fields = qryEntity.getFields(); if (fields != null) { Set<String> keyFields = qryEntity.getKeyFields(); Set<String> notNullFields = qryEntity.getNotNullFields(); Map<String, Object> defVals = qryEntity.getDefaultFieldValues(); Map<String, Integer> fieldsPrecision = qryEntity.getFieldsPrecision(); Map<String, Integer> fieldsScale = qryEntity.getFieldsScale(); writer.writeInt(fields.size()); for (Map.Entry<String, String> field : fields.entrySet()) { writer.writeString(field.getKey()); writer.writeString(field.getValue()); writer.writeBoolean(keyFields != null && keyFields.contains(field.getKey())); writer.writeBoolean(notNullFields != null && notNullFields.contains(field.getKey())); writer.writeObject(defVals != null ? defVals.get(field.getKey()) : null); if (ver.compareTo(VER_1_2_0) >= 0) { writer.writeInt(fieldsPrecision == null ? -1 : fieldsPrecision.getOrDefault(field.getKey(), -1)); writer.writeInt(fieldsScale == null ? -1 : fieldsScale.getOrDefault(field.getKey(), -1)); } } } else writer.writeInt(0); Map<String, String> aliases = qryEntity.getAliases(); if (aliases != null) { writer.writeInt(aliases.size()); for (Map.Entry<String, String> alias : aliases.entrySet()) { writer.writeString(alias.getKey()); writer.writeString(alias.getValue()); } } else writer.writeInt(0); Collection<QueryIndex> indexes = qryEntity.getIndexes(); if (indexes != null) { writer.writeInt(indexes.size()); for (QueryIndex index : indexes) writeQueryIndex(writer, index); } else writer.writeInt(0); }', 'output': 'public static void writeQueryEntity(BinaryRawWriter writer, QueryEntity qryEntity) { assert qryEntity != null; writer.writeString(qryEntity.getKeyType()); writer.writeString(qryEntity.getValueType()); writer.writeString(qryEntity.getTableName()); writer.writeString(qryEntity.getKeyFieldName()); writer.writeString(qryEntity.getValueFieldName()); LinkedHashMap<String, String> fields = qryEntity.getFields(); if (fields != null) { Set<String> keyFields = qryEntity.getKeyFields(); Set<String> notNullFields = qryEntity.getNotNullFields(); Map<String, Object> defVals = qryEntity.getDefaultFieldValues(); Map<String, IgniteBiTuple<Integer, Integer>> decimalInfo = qryEntity.getDecimalInfo(); writer.writeInt(fields.size()); for (Map.Entry<String, String> field : fields.entrySet()) { writer.writeString(field.getKey()); writer.writeString(field.getValue()); writer.writeBoolean(keyFields != null && keyFields.contains(field.getKey())); writer.writeBoolean(notNullFields != null && notNullFields.contains(field.getKey())); writer.writeObject(defVals != null ? defVals.get(field.getKey()) : null); IgniteBiTuple<Integer, Integer> precisionAndScale = decimalInfo == null ? null : decimalInfo.get(field.getKey()); writer.writeInt(precisionAndScale == null ? -1 : precisionAndScale.get1()); writer.writeInt(precisionAndScale == null ? -1 : precisionAndScale.get2()); } } else writer.writeInt(0); Map<String, String> aliases = qryEntity.getAliases(); if (aliases != null) { writer.writeInt(aliases.size()); for (Map.Entry<String, String> alias : aliases.entrySet()) { writer.writeString(alias.getKey()); writer.writeString(alias.getValue()); } } else writer.writeInt(0); Collection<QueryIndex> indexes = qryEntity.getIndexes(); if (indexes != null) { writer.writeInt(indexes.size()); for (QueryIndex index : indexes'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<DynamicTest> generateTests( final Runnable given, final Runnable thenHappyCase, final Runnable thenFailureCase) { return _Lists.of( dynamicTest("No initial Transaction with Test Execution", this::no_initial_tx_context), programmaticExecution(given, thenFailureCase), wrapperSyncExecution(given, thenHappyCase), wrapperSyncExecutionWithFailure(given, thenFailureCase), wrapperAsyncExecution(given, thenHappyCase), wrapperAsyncExecutionWithFailure(given, thenFailureCase) ); }', 'ground_truth': 'public List<DynamicTest> generateTests( final Runnable given, final Consumer<VerificationStage> verifier) { return _Lists.of( dynamicTest("No initial Transaction with Test Execution", this::no_initial_tx_context), programmaticExecution(given, verifier), interactionApiExecution(given, verifier), wrapperSyncExecution(given, verifier), wrapperSyncExecutionWithFailure(given, verifier), wrapperAsyncExecution(given, verifier), wrapperAsyncExecutionWithFailure(given, verifier) ); }', 'output': 'public List<DynamicTest> generateTests( final Runnable given, final Runnable thenHappyCase, final Runnable thenFailureCase) { return _Lists.of( dynamicTest("No initial Transaction with Test Execution", this::no_initial_tx_context), programmaticExecution(given, thenHappyCase), wrapperSyncExecution(given, thenHappyCase), wrapperSyncExecutionWithFailure(given, thenFailureCase), wrapperAsyncExecution(given, thenHappyCase), wrapperAsyncExecutionWithFailure(given, thenFailureCase) ); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static QueryMessage generateQueryMessage(int idx, SizeCaps sizeCaps) { Random rnd = new Random(idx); List<ByteBuffer> values = new ArrayList<>(); for (int i = 0; i < sizeCaps.columnCountCap * sizeCaps.rowsCountCap; i++) values.add(bytes(rnd, sizeCaps.valueMinSize, sizeCaps.valueMaxSize)); QueryOptions queryOptions = QueryOptions.create(ConsistencyLevel.ONE, values, true, 10, null, null, ProtocolVersion.V4, "KEYSPACE"); return new QueryMessage(Integer.toString(idx), queryOptions); }', 'ground_truth': 'public static QueryMessage generateQueryMessage(int idx, SizeCaps sizeCaps, ProtocolVersion version) { Random rnd = new Random(idx); List<ByteBuffer> values = new ArrayList<>(); for (int i = 0; i < sizeCaps.columnCountCap * sizeCaps.rowsCountCap; i++) values.add(bytes(rnd, sizeCaps.valueMinSize, sizeCaps.valueMaxSize)); QueryOptions queryOptions = QueryOptions.create(ConsistencyLevel.ONE, values, true, 10, null, null, version, "KEYSPACE"); return new QueryMessage(Integer.toString(idx), queryOptions); }', 'output': 'public static QueryMessage generateQueryMessage(int idx, SizeCaps sizeCaps, String keyspace) { Random rnd = new Random(idx); List<ByteBuffer> values = new ArrayList<>(); for (int i = 0; i < sizeCaps.columnCountCap * sizeCaps.rowsCountCap; i++) values.add(bytes(rnd, sizeCaps.valueMinSize, sizeCaps.valueMaxSize)); QueryOptions queryOptions = QueryOptions.create(ConsistencyLevel.ONE, values, true, 10, null, null, ProtocolVersion.V4, keyspace); return new QueryMessage(Integer.toString(idx), queryOptions); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void doTestTransferAmount(FailoverScenario failoverScenario) throws Exception { failoverScenario.beforeNodesStarted(); startGrids(nodesCount()); IgniteEx igniteClient = startGrid(getClientConfiguration(nodesCount())); igniteClient.cluster().active(true); int[] initAmount = new int[txThreadsCount()]; completedTxs = new ConcurrentLinkedHashMap[txThreadsCount()]; for (int cachePrefixIdx = 0; cachePrefixIdx < txThreadsCount(); cachePrefixIdx++) { IgniteCache<Integer, AccountState> cache = igniteClient.getOrCreateCache(cacheName(cachePrefixIdx)); AtomicInteger coinsCounter = new AtomicInteger(); try (Transaction tx = igniteClient.transactions().txStart(PESSIMISTIC, REPEATABLE_READ)) { for (int accountId = 0; accountId < accountsCount(); accountId++) { Set<Integer> initialAmount = generateCoins(coinsCounter, 5); cache.put(accountId, new AccountState(accountId, tx.xid(), initialAmount)); } tx.commit(); } initAmount[cachePrefixIdx] = coinsCounter.get(); completedTxs[cachePrefixIdx] = new ConcurrentLinkedHashMap(); } CountDownLatch firstTransactionDone = new CountDownLatch(txThreadsCount()); ArrayList<Thread> transferThreads = new ArrayList<>(); for (int i = 0; i < txThreadsCount(); i++) { transferThreads.add(new TransferAmountTxThread(firstTransactionDone, igniteClient, cacheName(i), i)); transferThreads.get(i).start(); } firstTransactionDone.await(10, TimeUnit.SECONDS); failoverScenario.afterFirstTransaction(); for (Thread thread : transferThreads) { thread.join(); } failoverScenario.afterTransactionsFinished(); consistencyCheck(initAmount); }', 'ground_truth': 'public void doTestTransferAmount(FailoverScenario failoverScenario, boolean colocatedAccounts) throws Exception { failoverScenario.beforeNodesStarted(); startGrids(nodesCount()); IgniteEx igniteClient = startGrid(getClientConfiguration(nodesCount())); igniteClient.cluster().active(true); int[] initAmounts = new int[txThreadsCount()]; completedTxs = new ConcurrentLinkedHashMap[txThreadsCount()]; for (int cachePrefixIdx = 0; cachePrefixIdx < txThreadsCount(); cachePrefixIdx++) { IgniteCache<Integer, AccountState> cache = igniteClient.getOrCreateCache(cacheName(cachePrefixIdx)); AtomicInteger coinsCntr = new AtomicInteger(); try (Transaction tx = igniteClient.transactions().txStart(PESSIMISTIC, REPEATABLE_READ)) { for (int accountId = 0; accountId < accountsCount(); accountId++) { Set<Integer> initAmount = generateCoins(coinsCntr, 5); cache.put(accountId, new AccountState(accountId, tx.xid(), initAmount)); } tx.commit(); } initAmounts[cachePrefixIdx] = coinsCntr.get(); completedTxs[cachePrefixIdx] = new ConcurrentLinkedHashMap(); } CountDownLatch firstTransactionDone = new CountDownLatch(txThreadsCount()); ArrayList<Thread> transferThreads = new ArrayList<>(); for (int i = 0; i < txThreadsCount(); i++) { transferThreads.add(new TransferAmountTxThread(firstTransactionDone, igniteClient, cacheName(i), i, colocatedAccounts)); transferThreads.get(i).start(); } firstTransactionDone.await(10, TimeUnit.SECONDS); failoverScenario.afterFirstTransaction(); for (Thread thread : transferThreads) thread.join(); failoverScenario.afterTransactionsFinished(); consistencyCheck(initAmounts); }', 'output': 'public void doTestTransferAmount(FailoverScenario failoverScenario) throws Exception { failoverScenario.beforeNodesStarted(); startGrids(nodesCount()); IgniteEx igniteClient = startGrid(getClientConfiguration(nodesCount())); igniteClient.cluster().active(true); int[] initAmount = new int[txThreadsCount()]; completedTxs = new ConcurrentLinkedHashMap[txThreadsCount()]; for (int cachePrefixIdx = 0; cachePrefixIdx < txThreadsCount(); cachePrefixIdx++) { IgniteCache<Integer, AccountState> cache = igniteClient.getOrCreateCache(cacheName(cachePrefixIdx)); AtomicInteger coinsCounter = new AtomicInteger(); try (Transaction tx = igniteClient.transactions().txStart(PESSIMISTIC, REPEATABLE_READ)) { for (int accountId = 0; accountId < accountsCount(); accountId++) { Set<Integer> initialAmount = generateCoins(coinsCounter, 5); cache.put(accountId, new AccountState(accountId, tx.xid(), initialAmount)); } tx.commit(); } initAmount[cachePrefixIdx] = coinsCounter.get(); completedTxs[cachePrefixIdx] = new ConcurrentLinkedHashMap(); } CountDownLatch firstTransactionDone = new CountDownLatch(txThreadsCount()); ArrayList<Thread> transferThreads = new ArrayList<>(); for (int i = 0; i < txThreadsCount(); i++) { transferThreads.add(new TransferAmountTxThread(firstTransactionDone, igniteClient, cacheName(i), i)); transferThreads.get(i).start(); } firstTransactionDone.await(10, TimeUnit.SECONDS); failoverScenario.afterFirstTransaction(); for (Thread thread : transferThreads) { thread.join(); } failoverScenario.afterTransactionsFinished(); consistencyCheck(initAmount); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void upgradeSSTables(String tableName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException { ssProxy.upgradeSSTables(tableName, columnFamilies); }', 'ground_truth': 'public void upgradeSSTables(String tableName, boolean excludeCurrentVersion, String... columnFamilies) throws IOException, ExecutionException, InterruptedException { ssProxy.upgradeSSTables(tableName, excludeCurrentVersion, columnFamilies); }', 'output': 'public void upgradeSSTables(String tableName, Collection<String> columnFamilies) throws IOException, ExecutionException, InterruptedException { ssProxy.upgradeSSTables(tableName, columnFamilies); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public StripeConcurrentBlockingQueue( String igniteInstanceName, String poolName, int idx, IgniteLogger log ) { super(igniteInstanceName, poolName, idx, log); }', 'ground_truth': 'public StripeConcurrentBlockingQueue( String igniteInstanceName, String poolName, int idx, IgniteLogger log, Thread.UncaughtExceptionHandler errHnd ) { super(igniteInstanceName, poolName, idx, log, errHnd); }', 'output': 'public StripeConcurrentBlockingQueue( String igniteInstanceName, String poolName, int idx, IgniteLogger log, int maxSize ) { super(igniteInstanceName, poolName, idx, log, maxSize); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static PTransform createFlattenOfPartials( String outputId, Collection<PCollectionNode> generatedInputs) { PTransform.Builder newFlattenBuilder = PTransform.newBuilder(); int i = 0; for (PCollectionNode generatedInput : generatedInputs) { String localInputId = String.format("input_%s", i); i++; newFlattenBuilder.putInputs(localInputId, generatedInput.getId()); } return newFlattenBuilder .putOutputs("output", outputId) .setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.FLATTEN_TRANSFORM_URN)) .build(); }', 'ground_truth': 'private static PTransform createFlattenOfPartials( String transformId, String outputId, Collection<PCollectionNode> generatedInputs) { PTransform.Builder newFlattenBuilder = PTransform.newBuilder(); int i = 0; for (PCollectionNode generatedInput : generatedInputs) { String localInputId = String.format("input_%s", i); i++; newFlattenBuilder.putInputs(localInputId, generatedInput.getId()); } return newFlattenBuilder .setUniqueName(transformId) .putOutputs("output", outputId) .setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.FLATTEN_TRANSFORM_URN)) .build(); }', 'output': 'private static PTransform createFlattenOfPartials( String outputId, List<PCollectionNode> generatedInputs) { PTransform.Builder newFlattenBuilder = PTransform.newBuilder(); int i = 0; for (PCollectionNode generatedInput : generatedInputs) { String localInputId = String.format("input_%s", i); i++; newFlattenBuilder.putInputs(localInputId, generatedInput.getId()); } return newFlattenBuilder .putOutputs("output", outputId) .setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.FLATTEN_TRANSFORM_URN)) .build(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void notifyDropFunction(UFMetaData uf) { for (IMigrationListener listener : listeners) listener.onDropFunction(uf.namespace, uf.functionName); }', 'ground_truth': 'public void notifyDropFunction(UDFunction udf) { for (IMigrationListener listener : listeners) listener.onDropFunction(udf.name().namespace, udf.name().name); }', 'output': 'public void notifyDropFunction(String namespace, String functionName) { for (IMigrationListener listener : listeners) listener.onDropFunction(namespace, functionName); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Event.SchemaChange announceMigration(boolean isLocalOnly) throws RequestValidationException { Function old = Schema.instance.findFunction(functionName, argTypes).orElse(null); boolean replaced = old != null; if (replaced) { if (ifNotExists) return null; if (!orReplace) throw new InvalidRequestException(String.format("Function %s already exists", old)); if (!(old instanceof AggregateFunction)) throw new InvalidRequestException(String.format("Aggregate %s can only replace an aggregate", old)); if (old.isNative()) throw new InvalidRequestException(String.format("Cannot replace native aggregate %s", old)); if (!old.returnType().isValueCompatibleWith(returnType)) throw new InvalidRequestException(String.format("Cannot replace aggregate %s, the new return type %s is not compatible with the return type %s of existing function", functionName, returnType.asCQL3Type(), old.returnType().asCQL3Type())); } if (!stateFunction.isCalledOnNullInput() && initcond == null) throw new InvalidRequestException(String.format("Cannot create aggregate %s without INITCOND because state function %s does not accept \'null\' arguments", functionName, stateFunc)); UDAggregate udAggregate = new UDAggregate(functionName, argTypes, returnType, stateFunction, finalFunction, initcond); MigrationManager.announceNewAggregate(udAggregate, isLocalOnly); return new Event.SchemaChange(replaced ? Event.SchemaChange.Change.UPDATED : Event.SchemaChange.Change.CREATED, Event.SchemaChange.Target.AGGREGATE, udAggregate.name().keyspace, udAggregate.name().name, AbstractType.asCQLTypeStringList(udAggregate.argTypes())); }', 'ground_truth': 'public Event.SchemaChange announceMigration(QueryState queryState, boolean isLocalOnly) throws RequestValidationException { Function old = Schema.instance.findFunction(functionName, argTypes).orElse(null); boolean replaced = old != null; if (replaced) { if (ifNotExists) return null; if (!orReplace) throw new InvalidRequestException(String.format("Function %s already exists", old)); if (!(old instanceof AggregateFunction)) throw new InvalidRequestException(String.format("Aggregate %s can only replace an aggregate", old)); if (old.isNative()) throw new InvalidRequestException(String.format("Cannot replace native aggregate %s", old)); if (!old.returnType().isValueCompatibleWith(returnType)) throw new InvalidRequestException(String.format("Cannot replace aggregate %s, the new return type %s is not compatible with the return type %s of existing function", functionName, returnType.asCQL3Type(), old.returnType().asCQL3Type())); } if (!stateFunction.isCalledOnNullInput() && initcond == null) throw new InvalidRequestException(String.format("Cannot create aggregate %s without INITCOND because state function %s does not accept \'null\' arguments", functionName, stateFunc)); UDAggregate udAggregate = new UDAggregate(functionName, argTypes, returnType, stateFunction, finalFunction, initcond); MigrationManager.announceNewAggregate(udAggregate, isLocalOnly); return new Event.SchemaChange(replaced ? Event.SchemaChange.Change.UPDATED : Event.SchemaChange.Change.CREATED, Event.SchemaChange.Target.AGGREGATE, udAggregate.name().keyspace, udAggregate.name().name, AbstractType.asCQLTypeStringList(udAggregate.argTypes())); }', 'output': 'public Event.SchemaChange announceMigration(boolean isLocalOnly) throws RequestValidationException { Function old = Schema.instance.findFunction(functionName, argTypes).orElse(null); boolean replaced = old != null; if (replaced) { if (ifNotExists) return null; if (!orReplace) throw new InvalidRequestException(String.format("Function %s already exists", old)); if (!(old instanceof AggregateFunction)) throw new InvalidRequestException(String.format("Aggregate %s can only replace an aggregate", old)); if (old.isNative()) throw new InvalidRequestException(String.format("Cannot replace native aggregate %s", old)); if (!old.returnType().isValueCompatibleWith(returnType)) throw new InvalidRequestException(String.format("Cannot replace aggregate %s, the new return type %s is not compatible with the return type %s of existing function", functionName, returnType.asCQL3Type(), old.returnType().asCQL3Type())); } if (!stateFunction.isCalledOnNullInput() && initcond == null) throw new InvalidRequestException(String.format("Cannot create aggregate %s without INITCOND because state function %s does not accept \'null\' arguments", functionName, stateFunc)); UDAggregate udAggregate = new UDAggregate(functionName, argTypes, returnType, stateFunction, finalFunction, initcond); MigrationManager.announceNewAggregate(udAggregate, isLocalOnly); return new Event.SchemaChange(replaced ? Event.SchemaChange.Change.UPDATED : Event.SchemaChange.Change.CREATED, Event.SchemaChange.Target.AGGREGATE, udAggregate.name().keyspace, udAggregate.name().name, AbstractType.asCQLTypeStringList(udAggregate.argTypes())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private ClassLoader getArchiveClassLoader(ViewConfig viewConfig, File archiveDir) throws IOException { String    archivePath = archiveDir.getAbsolutePath(); List<URL> urlList     = new LinkedList<>(); String classesPath = archivePath + File.separator + ARCHIVE_CLASSES_DIR; File   classesDir  = archiveUtility.getFile(classesPath); if (classesDir.exists()) { urlList.add(classesDir.toURI().toURL()); } String libPath = archivePath + File.separator + ARCHIVE_LIB_DIR; File   libDir  = archiveUtility.getFile(libPath); if (libDir.exists()) { File[] files = libDir.listFiles(); if (files != null) { for (final File fileEntry : files) { if (!fileEntry.isDirectory()) { urlList.add(fileEntry.toURI().toURL()); } } } } urlList.add(archiveDir.toURI().toURL()); return new ViewClassLoader(viewConfig, urlList.toArray(new URL[urlList.size()])); }', 'ground_truth': 'private ClassLoader getArchiveClassLoader(ViewConfig viewConfig, File archiveDir, List<File> viewsAdditionalClasspath) throws IOException { String    archivePath = archiveDir.getAbsolutePath(); List<URL> urlList     = new LinkedList<>(); String classesPath = archivePath + File.separator + ARCHIVE_CLASSES_DIR; File   classesDir  = archiveUtility.getFile(classesPath); if (classesDir.exists()) { urlList.add(classesDir.toURI().toURL()); } for (File file : viewsAdditionalClasspath) { if (file.isDirectory()) { addDirToClasspath(urlList, file); } else if (file.isFile()) { urlList.add(file.toURI().toURL()); } } String libPath = archivePath + File.separator + ARCHIVE_LIB_DIR; File libDir = archiveUtility.getFile(libPath); addDirToClasspath(urlList, libDir); urlList.add(archiveDir.toURI().toURL()); LOG.trace("classpath for view {} is : {}", viewConfig.getName(), urlList); return new ViewClassLoader(viewConfig, urlList.toArray(new URL[urlList.size()])); }', 'output': 'private ClassLoader getArchiveClassLoader(ViewConfig viewConfig, File archiveDir) throws IOException { String    archivePath = archiveDir.getAbsolutePath(); List<URL> urlList     = new LinkedList<>(); String classesPath = archivePath + File.separator + ARCHIVE_CLASSES_DIR; File   classesDir  = archiveUtility.getFile(classesPath); if (classesDir.exists()) { urlList.add(classesDir.toURI().toURL()); } String libPath = archivePath + File.separator + ARCHIVE_LIB_DIR; File   libDir  = archiveUtility.getFile(libPath); if (libDir.exists()) { File[] files = libDir.listFiles(); if (files != null) { for (final File fileEntry : files) { if (!fileEntry.isDirectory()) { urlList.add(fileEntry.toURI().toURL()); } } } } urlList.add(archiveDir.toURI().toURL()); return new ViewClassLoader(viewConfig, urlList.toArray(new URL[urlList.size()])); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Event.SchemaChange announceMigration(boolean isLocalOnly) throws RequestValidationException { Function old = findFunction(); if (old == null) { if (ifExists) return null; else throw new InvalidRequestException(getMissingFunctionError()); } KeyspaceMetadata ksm = Schema.instance.getKSMetaData(old.name().keyspace); Collection<UDAggregate> referrers = ksm.functions.aggregatesUsingFunction(old); if (!referrers.isEmpty()) throw new InvalidRequestException(String.format("Function \'%s\' still referenced by %s", old, referrers)); MigrationManager.announceFunctionDrop((UDFunction) old, isLocalOnly); return new Event.SchemaChange(Event.SchemaChange.Change.DROPPED, Event.SchemaChange.Target.FUNCTION, old.name().keyspace, old.name().name, AbstractType.asCQLTypeStringList(old.argTypes())); }', 'ground_truth': 'public Event.SchemaChange announceMigration(QueryState queryState, boolean isLocalOnly) throws RequestValidationException { Function old = findFunction(); if (old == null) { if (ifExists) return null; else throw new InvalidRequestException(getMissingFunctionError()); } KeyspaceMetadata ksm = Schema.instance.getKSMetaData(old.name().keyspace); Collection<UDAggregate> referrers = ksm.functions.aggregatesUsingFunction(old); if (!referrers.isEmpty()) throw new InvalidRequestException(String.format("Function \'%s\' still referenced by %s", old, referrers)); MigrationManager.announceFunctionDrop((UDFunction) old, isLocalOnly); return new Event.SchemaChange(Event.SchemaChange.Change.DROPPED, Event.SchemaChange.Target.FUNCTION, old.name().keyspace, old.name().name, AbstractType.asCQLTypeStringList(old.argTypes())); }', 'output': 'public Event.SchemaChange announceMigration(boolean isLocalOnly) throws RequestValidationException { UDFunction old = findFunction(); if (old == null) { if (ifExists) return null; else throw new InvalidRequestException(getMissingFunctionError()); } KeyspaceMetadata ksm = Schema.instance.getKSMetaData(old.name().keyspace); Collection<UDAggregate> referrers = ksm.functions.aggregatesUsingFunction(old); if (!referrers.isEmpty()) throw new InvalidRequestException(String.format("Function \'%s\' still referenced by %s", old, referrers)); MigrationManager.announceFunctionDrop((UDFunction) old, isLocalOnly); return new Event.SchemaChange(Event.SchemaChange.Change.DROPPED, Event.SchemaChange.Target.FUNCTION, old.name().keyspace, old.name().name, AbstractType.asCQLTypeStringList(old.argTypes())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Service installService(Cluster cluster, String serviceName) throws AmbariException { Service service = null; try { service = cluster.getService(serviceName); } catch (ServiceNotFoundException e) { service = serviceFactory.createNew(cluster, serviceName); cluster.addService(service); } return service; }', 'ground_truth': 'private Service installService(Cluster cluster, String serviceName, RepositoryVersionEntity repositoryVersion) throws AmbariException { Service service = null; try { service = cluster.getService(serviceName); } catch (ServiceNotFoundException e) { service = serviceFactory.createNew(cluster, serviceName, repositoryVersion); cluster.addService(service); } return service; }', 'output': 'private Service installService(Cluster cluster, String serviceName, Map<String, String> properties) throws AmbariException { Service service = null; try { service = cluster.getService(serviceName); } catch (ServiceNotFoundException e) { service = serviceFactory.createNew(cluster, serviceName, properties); cluster.addService(service); } return service; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void serializeEvent(ByteBuf dest) { CBUtil.writeEnumValue(change, dest); CBUtil.writeInet(node, dest); } protected void serializeEvent(ByteBuf dest)\n        { { CBUtil.writeEnumValue(status, dest); CBUtil.writeInet(node, dest); } protected void serializeEvent(ByteBuf dest)\n        { { CBUtil.writeEnumValue(change, dest); CBUtil.writeString(keyspace, dest); CBUtil.writeString(table, dest); }', 'ground_truth': 'protected void serializeEvent(ByteBuf dest, int version) { CBUtil.writeEnumValue(status, dest); CBUtil.writeInet(node, dest); }', 'output': 'public void serializeEvent(ByteBuf dest) { CBUtil.writeEnumValue(change, dest); CBUtil.writeInet(node, dest); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private MarkupContainer createTreeComponent(final String id) { val scalarModel = scalarModel(); val tree = (MarkupContainer) IsisToWicketTreeAdapter.adapt(id, scalarModel); tree.add(getTreeThemeProvider().treeThemeFor(scalarModel)); return tree; }', 'ground_truth': 'private MarkupContainer createTreeComponent(final MarkupContainer parent, final String id) { val scalarModel = scalarModel(); val tree = IsisToWicketTreeAdapter.adapt(id, scalarModel); parent.add(tree); parent.add(getTreeThemeProvider().treeThemeFor(scalarModel)); return (MarkupContainer) tree; }', 'output': 'private Component createTreeComponent(final String id) { val scalarModel = scalarModel(); val tree = (Component) IsisToWicketTreeAdapter.adapt(id, scalarModel); tree.add(getTreeThemeProvider().treeThemeFor(scalarModel)); return tree; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void mutateRepaired(SSTableReader sstable, long repairedAt, UUID pendingRepair) throws IOException { Descriptor descriptor = sstable.descriptor; descriptor.getMetadataSerializer().mutateRepaired(descriptor, repairedAt, pendingRepair); sstable.reloadSSTableMetadata(); }', 'ground_truth': 'private static void mutateRepaired(SSTableReader sstable, long repairedAt, UUID pendingRepair, boolean isTransient) throws IOException { Descriptor descriptor = sstable.descriptor; descriptor.getMetadataSerializer().mutateRepairMetadata(descriptor, repairedAt, pendingRepair, isTransient); sstable.reloadSSTableMetadata(); }', 'output': 'private static void mutateRepaired(SSTableReader sstable, long repairedAt, UUID pendingRepair) throws IOException { SSTableMetadata metadata = sstable.metadata(); metadata.mutateRepaired(repairedAt, pendingRepair); sstable.setMetadata(metadata); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Selection wildcard(TableMetadata table) { List<ColumnMetadata> all = new ArrayList<>(table.columns().size()); Iterators.addAll(all, table.allColumnsInSelectOrder()); return new SimpleSelection(table, all, true); }', 'ground_truth': 'public static Selection wildcard(TableMetadata table, boolean isJson) { List<ColumnMetadata> all = new ArrayList<>(table.columns().size()); Iterators.addAll(all, table.allColumnsInSelectOrder()); return new SimpleSelection(table, all, Collections.emptySet(), true, isJson); }', 'output': 'public static Selection wildcard(TableMetadata table, boolean includeSystemColumns) { List<ColumnMetadata> all = new ArrayList<>(table.columns().size()); Iterators.addAll(all, table.allColumnsInSelectOrder()); return new SimpleSelection(table, all, includeSystemColumns); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void addFunctionToSchemaMutation(UDFunction function, long timestamp, Mutation mutation) { RowUpdateBuilder adder = new RowUpdateBuilder(SystemKeyspace.LegacyFunctions, timestamp, mutation) .clustering(function.name().name, functionSignatureWithTypes(function)); adder.add("body", function.body()) .add("language", function.language()) .add("return_type", function.returnType().toString()) .add("called_on_null_input", function.isCalledOnNullInput()); adder.resetCollection("argument_names") .resetCollection("argument_types"); for (int i = 0; i < function.argNames().size(); i++) { adder.addListEntry("argument_names", function.argNames().get(i).bytes) .addListEntry("argument_types", function.argTypes().get(i).toString()); } adder.build(); }', 'ground_truth': 'private static void addFunctionToSchemaMutation(UDFunction function, Mutation.SimpleBuilder builder) { Row.SimpleBuilder adder = builder.update(SystemKeyspace.LegacyFunctions) .row(function.name().name, functionSignatureWithTypes(function)); adder.add("body", function.body()) .add("language", function.language()) .add("return_type", function.returnType().toString()) .add("called_on_null_input", function.isCalledOnNullInput()); List<ByteBuffer> names = new ArrayList<>(); List<String> types = new ArrayList<>(); for (int i = 0; i < function.argNames().size(); i++) { names.add(function.argNames().get(i).bytes); types.add(function.argTypes().get(i).toString()); } adder.add("argument_names", names) .add("argument_types", types); }', 'output': 'private static void addFunctionToSchemaMutation(UDFunction function, long timestamp, Mutation mutation) { RowUpdateBuilder adder = new RowUpdateBuilder(SystemKeyspace.LegacyFunctions, timestamp, mutation) .clustering(function.name().name, functionSignatureWithTypes(function)); adder.add("body", function.body()) .add("language", function.language()) .add("return_type", function.returnType().toString()) .add("called_on_null_input", function.isCalledOnNullInput()); adder.resetCollection("argument_names") .resetCollection("argument_types"); for (int i = 0; i < function.argNames().size(); i++) { adder.addListEntry("argument_names", function.argNames().get(i).bytes) .addListEntry("argument_types", function.argTypes().get(i).toString()); } adder.build(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static SSTableWriter create(Descriptor descriptor, Long keyCount, Long repairedAt, CFMetaData metadata,  IPartitioner partitioner, MetadataCollector metadataCollector) { Factory writerFactory = descriptor.getFormat().getWriterFactory(); return writerFactory.open(descriptor, keyCount, repairedAt, metadata, partitioner, metadataCollector); }', 'ground_truth': 'public static SSTableWriter create(Descriptor descriptor, Long keyCount, Long repairedAt, CFMetaData metadata, IPartitioner partitioner, MetadataCollector metadataCollector, SerializationHeader header) { Factory writerFactory = descriptor.getFormat().getWriterFactory(); return writerFactory.open(descriptor, keyCount, repairedAt, metadata, partitioner, metadataCollector, header); }', 'output': 'public static SSTableWriter create(Descriptor descriptor, long keyCount, long repairedAt, CFMetaData metadata, IPartitioner partitioner, MetadataCollector metadataCollector) { Factory writerFactory = descriptor.getFormat().getWriterFactory(); return writerFactory.open(descriptor, keyCount, repairedAt, metadata, partitioner, metadataCollector); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void commitPaxos(Commit proposal, ConsistencyLevel consistencyLevel) throws WriteTimeoutException { boolean shouldBlock = consistencyLevel != ConsistencyLevel.ANY; Keyspace keyspace = Keyspace.open(proposal.update.metadata().ksName); Token tk = StorageService.getPartitioner().getToken(proposal.key); List<InetAddress> naturalEndpoints = StorageService.instance.getNaturalEndpoints(keyspace.getName(), tk); Collection<InetAddress> pendingEndpoints = StorageService.instance.getTokenMetadata().pendingEndpointsFor(tk, keyspace.getName()); AbstractWriteResponseHandler responseHandler = null; if (shouldBlock) { AbstractReplicationStrategy rs = keyspace.getReplicationStrategy(); responseHandler = rs.getWriteResponseHandler(naturalEndpoints, pendingEndpoints, consistencyLevel, null, WriteType.SIMPLE); } MessageOut<Commit> message = new MessageOut<Commit>(MessagingService.Verb.PAXOS_COMMIT, proposal, Commit.serializer); for (InetAddress destination : Iterables.concat(naturalEndpoints, pendingEndpoints)) { if (FailureDetector.instance.isAlive(destination)) { if (shouldBlock) MessagingService.instance().sendRR(message, destination, responseHandler); else MessagingService.instance().sendOneWay(message, destination); } } if (shouldBlock) responseHandler.get(); }', 'ground_truth': 'private static void commitPaxos(Commit proposal, ConsistencyLevel consistencyLevel, boolean shouldHint) throws WriteTimeoutException { boolean shouldBlock = consistencyLevel != ConsistencyLevel.ANY; Keyspace keyspace = Keyspace.open(proposal.update.metadata().ksName); Token tk = StorageService.getPartitioner().getToken(proposal.key); List<InetAddress> naturalEndpoints = StorageService.instance.getNaturalEndpoints(keyspace.getName(), tk); Collection<InetAddress> pendingEndpoints = StorageService.instance.getTokenMetadata().pendingEndpointsFor(tk, keyspace.getName()); AbstractWriteResponseHandler responseHandler = null; if (shouldBlock) { AbstractReplicationStrategy rs = keyspace.getReplicationStrategy(); responseHandler = rs.getWriteResponseHandler(naturalEndpoints, pendingEndpoints, consistencyLevel, null, WriteType.SIMPLE); } MessageOut<Commit> message = new MessageOut<Commit>(MessagingService.Verb.PAXOS_COMMIT, proposal, Commit.serializer); for (InetAddress destination : Iterables.concat(naturalEndpoints, pendingEndpoints)) { if (FailureDetector.instance.isAlive(destination)) { if (shouldBlock) MessagingService.instance().sendRR(message, destination, responseHandler, shouldHint); else MessagingService.instance().sendOneWay(message, destination); } else if (shouldHint) { submitHint(proposal.makeMutation(), destination, null); } } if (shouldBlock) responseHandler.get(); }', 'output': 'private static void commitPaxos(CommitProposal proposal, ConsistencyLevel consistencyLevel) throws WriteTimeoutException { boolean shouldBlock = consistencyLevel != ConsistencyLevel.ANY; Keyspace keyspace = Keyspace.open(proposal.update.metadata().ksName); Token tk = StorageService.getPartitioner().getToken(proposal.key); List<InetAddress> naturalEndpoints = StorageService.instance.getNaturalEndpoints(keyspace.getName(), tk); Collection<InetAddress> pendingEndpoints = StorageService.instance.getTokenMetadata().pendingEndpointsFor(tk, keyspace.getName()); AbstractWriteResponseHandler responseHandler = null; if (shouldBlock) { AbstractReplicationStrategy rs = keyspace.getReplicationStrategy(); responseHandler = rs.getWriteResponseHandler(naturalEndpoints, pendingEndpoints, consistencyLevel, null, WriteType.SIMPLE); } MessageOut<CommitProposal> message = new MessageOut<CommitProposal>(MessagingService.Verb.PAXOS_COMMIT, proposal, CommitProposal.serializer); for (InetAddress destination : Iterables.concat(naturalEndpoints, pendingEndpoints)) { if (FailureDetector.instance.isAlive(destination)) { if (shouldBlock) MessagingService.instance().sendRR(message, destination, responseHandler); else MessagingService.instance().sendOneWay(message, destination); } } if (shouldBlock) responseHandler.get(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private GridCacheTx txStart0(GridCacheTxConcurrency concurrency, GridCacheTxIsolation isolation, long timeout, int txSize) { GridTransactionsConfiguration cfg = cctx.gridConfig().getTransactionsConfiguration(); if (!cfg.isTxSerializableEnabled() && isolation == SERIALIZABLE) throw new IllegalArgumentException("SERIALIZABLE isolation level is disabled (to enable change " + "\'txSerializableEnabled\' configuration property)"); GridCacheTxEx<K, V> tx = (GridCacheTxEx<K, V>)cctx.tm().userTx(); if (tx != null) throw new IllegalStateException("Failed to start new transaction " + "(current thread already has a transaction): " + tx); tx = cctx.tm().newTx( false, false, concurrency, isolation, timeout, false, true, txSize, /** group lock keys */null, /** partition lock */false ); assert tx != null; return new GridCacheTxProxyImpl<>(tx, cctx); }', 'ground_truth': 'private GridCacheTx txStart0(GridCacheTxConcurrency concurrency, GridCacheTxIsolation isolation, long timeout, int txSize, boolean sys) { GridTransactionsConfiguration cfg = cctx.gridConfig().getTransactionsConfiguration(); if (!cfg.isTxSerializableEnabled() && isolation == SERIALIZABLE) throw new IllegalArgumentException("SERIALIZABLE isolation level is disabled (to enable change " + "\'txSerializableEnabled\' configuration property)"); GridCacheTxEx<K, V> tx = (GridCacheTxEx<K, V>)cctx.tm().userTx(); if (tx != null) throw new IllegalStateException("Failed to start new transaction " + "(current thread already has a transaction): " + tx); tx = cctx.tm().newTx( false, false, sys, concurrency, isolation, timeout, false, true, txSize, /** group lock keys */null, /** partition lock */false ); assert tx != null; return new GridCacheTxProxyImpl<>(tx, cctx); }', 'output': 'private GridCacheTx txStart(GridCacheTxConcurrency concurrency, GridCacheTxIsolation isolation, long timeout, int txSize) { GridTransactionsConfiguration cfg = cctx.gridConfig().getTransactionsConfiguration(); if (!cfg.isTxSerializableEnabled() && isolation == SERIALIZABLE) throw new IllegalArgumentException("SERIALIZABLE isolation level is disabled (to enable change " + "\'txSerializableEnabled\' configuration property)"); GridCacheTxEx<K, V> tx = (GridCacheTxEx<K, V>)cctx.tm().userTx(); if (tx != null) throw new IllegalStateException("Failed to start new transaction " + "(current thread already has a transaction): " + tx); tx = cctx.tm().newTx( false, false, concurrency, isolation, timeout, false, true, txSize, /** group lock keys */null, /** partition lock */false ); assert tx != null; return new GridCacheTxProxyImpl<>(tx, cctx); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private int lockTail(Page page, long backId, long fwdId, int lvl) throws IgniteCheckedException { assert tail != null; this.pageId = page.id(); this.page = page; this.fwdId = fwdId; this.backId = backId; if (backId == 0) return doLockTail(lvl); Page back = page(backId); try { return writePage(back, lockBackAndTail, this, lvl); } finally { if (canRelease(back, lvl)) back.close(); } }', 'ground_truth': 'private int lockTail(long pageId, Page page, long backId, long fwdId, int lvl) throws IgniteCheckedException { assert tail != null; this.pageId = pageId; this.page = page; this.fwdId = fwdId; this.backId = backId; if (backId == 0) return doLockTail(lvl); Page back = page(backId); try { return writePage(backId, back, lockBackAndTail, this, lvl); } finally { if (canRelease(backId, back, lvl)) back.close(); } }', 'output': 'private int lockTail(Page page, long backId, long fwdId, int lvl) throws IgniteCheckedException { assert tail != null; this.pageId = page.id(); this.page = page; this.fwdId = fwdId; this.backId = backId; if (backId == 0) return doLockTail(lvl); Page back = page(backId); try { return writePage(back, lockBackAndTail, this, lvl); } finally { if (canRelease(back, lvl)) back.close(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public UnfilteredRowIterator deserialize(DataInputPlus in, int version, SerializationHelper.Flag flag, Header header) throws IOException { if (header.isEmpty) return UnfilteredRowIterators.emptyIterator(header.metadata, header.key, header.isReversed); final SerializationHelper helper = new SerializationHelper(header.metadata, version, flag); final SerializationHeader sHeader = header.sHeader; return new AbstractUnfilteredRowIterator(header.metadata, header.key, header.partitionDeletion, sHeader.columns(), header.staticRow, header.isReversed, sHeader.stats()) { private final Row.Builder builder = ArrayBackedRow.sortedBuilder(sHeader.columns().regulars); protected Unfiltered computeNext() { try { Unfiltered unfiltered = UnfilteredSerializer.serializer.deserialize(in, sHeader, helper, builder); return unfiltered == null ? endOfData() : unfiltered; } catch (IOException e) { throw new IOError(e); } } }; }', 'ground_truth': 'public UnfilteredRowIterator deserialize(DataInputPlus in, int version, CFMetaData metadata, SerializationHelper.Flag flag, Header header) throws IOException { if (header.isEmpty) return UnfilteredRowIterators.emptyIterator(metadata, header.key, header.isReversed); final SerializationHelper helper = new SerializationHelper(metadata, version, flag); final SerializationHeader sHeader = header.sHeader; return new AbstractUnfilteredRowIterator(metadata, header.key, header.partitionDeletion, sHeader.columns(), header.staticRow, header.isReversed, sHeader.stats()) { private final Row.Builder builder = ArrayBackedRow.sortedBuilder(sHeader.columns().regulars); protected Unfiltered computeNext() { try { Unfiltered unfiltered = UnfilteredSerializer.serializer.deserialize(in, sHeader, helper, builder); return unfiltered == null ? endOfData() : unfiltered; } catch (IOException e) { throw new IOError(e); } } }; }', 'output': 'public UnfilteredRowIterator deserialize(DataInputPlus in, int version, SerializationHelper.Flag flag, Header header) throws IOException { if (header.isEmpty) return UnfilteredRowIterators.emptyIterator(header.metadata, header.key, header.isReversed); final SerializationHelper helper = new SerializationHelper(header.metadata, version, flag); final SerializationHeader sHeader = header.sHeader; return new AbstractUnfilteredRowIterator(header.metadata, header.key, header.partitionDeletion, sHeader.columns(), header.staticRow, header.isReversed, sHeader.stats()) { private final Row.Builder builder = ArrayBackedRow.sortedBuilder(sHeader.columns().regulars); protected Unfiltered computeNext() { try { Unfiltered unfiltered = UnfilteredSerializer.serializer.deserialize(in, sHeader, helper, builder); return unfiltered == null ? endOfData() : unfiltered; } catch (IOException e) { throw new IOError(e); } } }; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ExchangeActions onStateChangeRequest(ChangeGlobalStateMessage msg, AffinityTopologyVersion topVer) throws IgniteCheckedException { return cachesInfo.onStateChangeRequest(msg, topVer); }', 'ground_truth': 'public ExchangeActions onStateChangeRequest( ChangeGlobalStateMessage msg, AffinityTopologyVersion topVer, DiscoveryDataClusterState curState ) throws IgniteCheckedException { return cachesInfo.onStateChangeRequest(msg, topVer, curState); }', 'output': 'public ExchangeActions onStateChangeRequest(ChangeGlobalStateMessage msg, AffinityTopologyVersion topVer, Collection<ClusterNode> nodes) throws IgniteCheckedException { return cachesInfo.onStateChangeRequest(msg, topVer, nodes); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private ServiceProxyHolder(Object proxy, Class clazz) { assert proxy != null; assert clazz != null; this.proxy = proxy; serviceClass = clazz; }', 'ground_truth': 'private ServiceProxyHolder(Object proxy, Class clazz, PlatformContext ctx) { super(ctx); assert proxy != null; assert clazz != null; this.proxy = proxy; serviceClass = clazz; }', 'output': 'private ServiceProxyHolder(Object proxy, Class<?> clazz) { assert proxy != null; assert clazz != null; this.proxy = proxy; serviceClass = clazz; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Resource populateResource(Resource resource, Request request, Predicate predicate) throws SystemException { Set<String> ids = getRequestPropertyIds(request, predicate); Set<String> unsupportedIds = new HashSet<String>(); String componentName = (String) resource.getPropertyValue(componentNamePropertyId); if (getComponentMetrics().get(componentName) == null) { return resource; } for (String id : ids) { if (request.getTemporalInfo(id) != null) { unsupportedIds.add(id); } if (!isSupportedPropertyId(componentName, id)) { unsupportedIds.add(id); } } ids.removeAll(unsupportedIds); if (ids.isEmpty()) { return resource; } if (statePropertyId != null) { String state = (String) resource.getPropertyValue(statePropertyId); if (state != null && !healthyStates.contains(state)) { return resource; } } String clusterName = (String) resource.getPropertyValue(clusterNamePropertyId); String port = getPort(clusterName, componentName); if (port == null) { LOG.warn("Unable to get JMX metrics.  No port value for " + componentName); return resource; } Set<String> hostNames = getHosts(resource, clusterName, componentName); if (hostNames == null || hostNames.isEmpty()) { LOG.warn("Unable to get JMX metrics.  No host name for " + componentName); return resource; } String protocol = getJMXProtocol(clusterName, componentName); InputStream in = null; try { try { for (String hostName : hostNames) { try { in = streamProvider.readFrom(getSpec(protocol, hostName, port, componentName)); if (null == componentName || !componentName.equals(STORM_REST_API)) { getHadoopMetricValue(in, ids, resource, request); } else { getStormMetricValue(in, ids, resource); } } catch (IOException e) { logException(e); } } } finally { if (in != null) { in.close(); } } } catch (IOException e) { logException(e); } return resource; }', 'ground_truth': 'private Resource populateResource(Resource resource, Request request, Predicate predicate, Ticket ticket) throws SystemException { Set<String> ids = getRequestPropertyIds(request, predicate); Set<String> unsupportedIds = new HashSet<String>(); String componentName = (String) resource.getPropertyValue(componentNamePropertyId); if (getComponentMetrics().get(componentName) == null) { return resource; } for (String id : ids) { if (request.getTemporalInfo(id) != null) { unsupportedIds.add(id); } if (!isSupportedPropertyId(componentName, id)) { unsupportedIds.add(id); } } ids.removeAll(unsupportedIds); if (ids.isEmpty()) { return resource; } if (statePropertyId != null) { String state = (String) resource.getPropertyValue(statePropertyId); if (state != null && !healthyStates.contains(state)) { return resource; } } String clusterName = (String) resource.getPropertyValue(clusterNamePropertyId); String port = getPort(clusterName, componentName); if (port == null) { LOG.warn("Unable to get JMX metrics.  No port value for " + componentName); return resource; } Set<String> hostNames = getHosts(resource, clusterName, componentName); if (hostNames == null || hostNames.isEmpty()) { LOG.warn("Unable to get JMX metrics.  No host name for " + componentName); return resource; } String protocol = getJMXProtocol(clusterName, componentName); InputStream in = null; try { try { for (String hostName : hostNames) { try { in = streamProvider.readFrom(getSpec(protocol, hostName, port, componentName)); if (!ticket.isValid()) { return resource; } if (null == componentName || !componentName.equals(STORM_REST_API)) { getHadoopMetricValue(in, ids, resource, request, ticket); } else { getStormMetricValue(in, ids, resource, ticket); } } catch (IOException e) { logException(e); } } } finally { if (in != null) { in.close(); } } } catch (IOException e) { logException(e); } return resource; }', 'output': 'private Resource populateResource(Resource resource, Request request, Predicate predicate) throws SystemException { Set<String> ids = getRequestPropertyIds(request, predicate); Set<String> unsupportedIds = new HashSet<String>(); String componentName = (String) resource.getPropertyValue(componentNamePropertyId); if (getComponentMetrics().get(componentName) == null) { return resource; } for (String id : ids) { if (request.getTemporalInfo(id) != null) { unsupportedIds.add(id); } if (!isSupportedPropertyId(componentName, id)) { unsupportedIds.add(id); } } ids.removeAll(unsupportedIds); if (ids.isEmpty()) { return resource; } if (statePropertyId != null) { String state = (String) resource.getPropertyValue(statePropertyId); if (state != null && !healthyStates.contains(state)) { return resource; } } String clusterName = (String) resource.getPropertyValue(clusterNamePropertyId); String port = getPort(clusterName, componentName); if (port == null) { LOG.warn("Unable to get JMX metrics.  No port value for " + componentName); return resource; } Set<String> hostNames = getHosts(resource, clusterName, componentName); if (hostNames == null || hostNames.isEmpty()) { LOG.warn("Unable to get JMX metrics.  No host name for " + componentName); return resource; } String protocol = getJMXProtocol(clusterName, componentName); InputStream in = null; try { try { for (String hostName : hostNames) { try { in = streamProvider.readFrom(getSpec(protocol, hostName, port, componentName)); if (null == componentName || !componentName.equals(STORM_REST_API)) { getHadoopMetricValue(in, ids, resource, request); } else { getStormMetricValue(in, ids); } } catch (IOException e) { logException(e); } } } finally { if (in != null) { in.close(); } } } catch (IOException e) { logException(e); } return resource; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GridCacheTxLocalAdapter<K, V> newTx( boolean implicit, boolean implicitSingle, GridCacheTxConcurrency concurrency, GridCacheTxIsolation isolation, long timeout, boolean invalidate, boolean storeEnabled, int txSize, @Nullable GridCacheTxKey grpLockKey, boolean partLock) { UUID subjId = null; int taskNameHash = cctx.kernalContext().job().currentTaskNameHash(); GridNearTxLocal<K, V> tx = new GridNearTxLocal<>( cctx, implicit, implicitSingle, concurrency, isolation, timeout, invalidate, storeEnabled, txSize, grpLockKey, partLock, subjId, taskNameHash); return onCreated(tx); }', 'ground_truth': 'public GridCacheTxLocalAdapter<K, V> newTx( boolean implicit, boolean implicitSingle, boolean sys, GridCacheTxConcurrency concurrency, GridCacheTxIsolation isolation, long timeout, boolean invalidate, boolean storeEnabled, int txSize, @Nullable GridCacheTxKey grpLockKey, boolean partLock) { UUID subjId = null; int taskNameHash = cctx.kernalContext().job().currentTaskNameHash(); GridNearTxLocal<K, V> tx = new GridNearTxLocal<>( cctx, implicit, implicitSingle, sys, concurrency, isolation, timeout, invalidate, storeEnabled, txSize, grpLockKey, partLock, subjId, taskNameHash); return onCreated(tx); }', 'output': 'public GridCacheTxLocalAdapter<K, V> newTx( boolean implicit, boolean implicitSingle, GridCacheTxConcurrency concurrency, GridCacheTxIsolation isolation, long timeout, boolean invalidate, boolean storeEnabled, int txSize, @Nullable GridCacheTxKey grpLockKey, boolean partLock) { UUID subjId = null; int taskNameHash = cctx.kernalContext().job().currentTaskNameHash(); GridNearTxLocal<K, V> tx = new GridNearTxLocal<>( cctx, implicit, implicitSingle, concurrency, isolation, timeout, invalidate, storeEnabled, txSize, grpLockKey, partLock, subjId, taskNameHash); return onCreated(tx); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void marshalCredentials(TcpDiscoveryNode node) throws IgniteSpiException { try { Map<String, Object> attrs = new HashMap<>(node.getAttributes()); attrs.put(IgniteNodeAttributes.ATTR_SECURITY_CREDENTIALS, U.marshal(spi.marshaller(), attrs.get(IgniteNodeAttributes.ATTR_SECURITY_CREDENTIALS))); node.setAttributes(attrs); } catch (IgniteCheckedException e) { throw new IgniteSpiException("Failed to marshal node security credentials: " + node.id(), e); } } private SecurityCredentials unmarshalCredentials(TcpDiscoveryNode node) throws IgniteSpiException { try { byte[] credBytes = (byte[])node.getAttributes().get(IgniteNodeAttributes.ATTR_SECURITY_CREDENTIALS); if (credBytes == null) return null; return U.unmarshal(spi.marshaller(), credBytes, null); } catch (IgniteCheckedException e) { throw new IgniteSpiException("Failed to unmarshal node security credentials: " + node.id(), e); } }', 'ground_truth': 'private void marshalCredentials(TcpDiscoveryNode node, SecurityCredentials cred) throws IgniteSpiException { try { Map<String, Object> attrs = new HashMap<>(node.getAttributes()); attrs.put(IgniteNodeAttributes.ATTR_SECURITY_CREDENTIALS, spi.marshaller().marshal(cred)); node.setAttributes(attrs); } catch (IgniteCheckedException e) { throw new IgniteSpiException("Failed to marshal node security credentials: " + node.id(), e); } }', 'output': 'private void marshalCredentials(TcpDiscoveryNode node) throws IgniteSpiException { try { Map<String, Object> attrs = new HashMap<>(node.getAttributes()); attrs.put(IgniteNodeAttributes.ATTR_SECURITY_CREDENTIALS, U.marshal(spi.marshaller(), attrs.get(IgniteNodeAttributes.ATTR_SECURITY_CREDENTIALS))); node.setAttributes(attrs); } catch (IgniteCheckedException e) { throw new IgniteSpiException("Failed to marshal node security credentials: " + node.id(), e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public AggregationPartitionIterator(int pageSize, ConsistencyLevel consistency, ClientState clientState) { super(pageSize, consistency, clientState); }', 'ground_truth': 'public AggregationPartitionIterator(int pageSize, ConsistencyLevel consistency, ClientState clientState, long queryStartNanoTime) { super(pageSize, consistency, clientState, queryStartNanoTime); }', 'output': 'public AggregationPartitionIterator(int pageSize, ConsistencyLevel consistency, ClientState clientState, int numShards) { super(pageSize, consistency, clientState, numShards); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<Future<?>> doAntiCompaction(UUID parentRepairSession) { assert parentRepairSession != null; ParentRepairSession prs = getParentRepairSession(parentRepairSession); List<Future<?>> futures = new ArrayList<>(); for (Map.Entry<UUID, ColumnFamilyStore> columnFamilyStoreEntry : prs.columnFamilyStores.entrySet()) { Collection<SSTableReader> sstables = new HashSet<>(prs.getAndReferenceSSTables(columnFamilyStoreEntry.getKey())); ColumnFamilyStore cfs = columnFamilyStoreEntry.getValue(); boolean success = false; while (!success) { for (SSTableReader compactingSSTable : cfs.getDataTracker().getCompacting()) { if (sstables.remove(compactingSSTable)) SSTableReader.releaseReferences(Arrays.asList(compactingSSTable)); } success = sstables.isEmpty() || cfs.getDataTracker().markCompacting(sstables); } futures.add(CompactionManager.instance.submitAntiCompaction(cfs, prs.ranges, sstables, prs.repairedAt)); } return futures; }', 'ground_truth': 'public List<Future<?>> doAntiCompaction(UUID parentRepairSession, Collection<Range<Token>> successfulRanges) { assert parentRepairSession != null; ParentRepairSession prs = getParentRepairSession(parentRepairSession); assert prs.ranges.containsAll(successfulRanges) : "Trying to perform anticompaction on unknown ranges"; List<Future<?>> futures = new ArrayList<>(); if (successfulRanges.isEmpty()) return futures; for (Map.Entry<UUID, ColumnFamilyStore> columnFamilyStoreEntry : prs.columnFamilyStores.entrySet()) { Collection<SSTableReader> sstables = new HashSet<>(prs.getAndReferenceSSTables(columnFamilyStoreEntry.getKey())); ColumnFamilyStore cfs = columnFamilyStoreEntry.getValue(); boolean success = false; while (!success) { for (SSTableReader compactingSSTable : cfs.getDataTracker().getCompacting()) { if (sstables.remove(compactingSSTable)) SSTableReader.releaseReferences(Arrays.asList(compactingSSTable)); } success = sstables.isEmpty() || cfs.getDataTracker().markCompacting(sstables); } futures.add(CompactionManager.instance.submitAntiCompaction(cfs, successfulRanges, sstables, prs.repairedAt)); } return futures; }', 'output': 'public List<Future<?>> doAntiCompaction(UUID parentRepairSession) { assert parentRepairSession != null; ParentRepairSession prs = getParentRepairSession(parentRepairSession); List<Future<?>> futures = new ArrayList<>(); for (Map.Entry<UUID, ColumnFamilyStore> columnFamilyStoreEntry : prs.columnFamilyStores.entrySet()) { Collection<SSTableReader> sstables = new HashSet<>(prs.getAndReferenceSSTables(columnFamilyStoreEntry.getKey())); ColumnFamilyStore cfs = columnFamilyStoreEntry.getValue(); boolean success = false; while (!success) { for (SSTableReader compactingSSTable : cfs.getDataTracker().getCompacting()) { if (sstables.remove(compactingSSTable)) SSTableReader.releaseReferences(Arrays.asList(compactingSSTable)); } success = sstables.isEmpty() || cfs.getDataTracker().markCompacting(sstables); } futures.add(CompactionManager.instance.submitAntiCompaction(cfs, prs.ranges, sstables, prs.repairedAt)); } return futures; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RowMutation mutationForKey(CFDefinition cfDef, ByteBuffer key, ColumnNameBuilder builder, boolean isRange, UpdateParameters params) throws InvalidRequestException { QueryProcessor.validateKey(key); ColumnFamily cf = ColumnFamily.create(Schema.instance.getCFMetaData(cfDef.cfm.ksName, columnFamily())); if (toRemove.isEmpty() && builder.componentCount() == 0) { cf.delete(new DeletionInfo(params.timestamp, params.localDeletionTime)); } else { if (isRange) { assert toRemove.isEmpty(); ByteBuffer start = builder.build(); ByteBuffer end = builder.buildAsEndOfRange(); cf.addAtom(params.makeRangeTombstone(start, end)); } else { if (cfDef.isCompact) { ByteBuffer columnName = builder.build(); cf.addColumn(params.makeTombstone(columnName)); } else { for (Operation op : toRemove) op.execute(key, cf, builder.copy(), params); } } } return new RowMutation(cfDef.cfm.ksName, key, cf); }', 'ground_truth': 'public RowMutation mutationForKey(CFDefinition cfDef, ByteBuffer key, ColumnNameBuilder builder, boolean isRange, UpdateParameters params, boolean isBatch) throws InvalidRequestException { QueryProcessor.validateKey(key); ColumnFamily cf = ColumnFamily.create(Schema.instance.getCFMetaData(cfDef.cfm.ksName, columnFamily())); if (toRemove.isEmpty() && builder.componentCount() == 0) { cf.delete(new DeletionInfo(params.timestamp, params.localDeletionTime)); } else { if (isRange) { assert toRemove.isEmpty(); ByteBuffer start = builder.build(); ByteBuffer end = builder.buildAsEndOfRange(); cf.addAtom(params.makeRangeTombstone(start, end)); } else { if (cfDef.isCompact) { ByteBuffer columnName = builder.build(); cf.addColumn(params.makeTombstone(columnName)); } else { for (Operation op : toRemove) op.execute(key, cf, builder.copy(), params); } } } RowMutation rm; if (isBatch) { rm = new RowMutation(cfDef.cfm.ksName, key); rm.add(cf); } else { rm = new RowMutation(cfDef.cfm.ksName, key, cf); } return rm; }', 'output': 'public RowMutation mutationForKey(CFDefinition cfDef, ByteBuffer key, ColumnNameBuilder builder, boolean isRange, UpdateParameters params) throws InvalidRequestException { QueryProcessor.validateKey(key); ColumnFamily cf = ColumnFamily.create(Schema.instance.getCFMetaData(cfDef.cfm.ksName, columnFamily())); if (toRemove.isEmpty() && builder.componentCount() == 0) { cf.delete(new DeletionInfo(params.timestamp, params.localDeletionTime)); } else { if (isRange) { assert toRemove.isEmpty(); ByteBuffer start = builder.build(); ByteBuffer end = builder.buildAsEndOfRange(); cf.addAtom(params.makeRangeTombstone(start, end)); } else { if (cfDef.isCompact) { ByteBuffer columnName = builder.build(); cf.addColumn(params.makeTombstone(columnName)); } else { for (Operation op : toRemove) op.execute(key, cf, builder.copy(), params); } } } return new RowMutation(cfDef.cfm.ksName, key, cf); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected Statement statementListNoChild(AST node) { BlockStatement block = new BlockStatement(); for (; node != null; node = node.getNextSibling()) { block.addStatement(statement(node)); } return block; }', 'ground_truth': 'protected Statement statementListNoChild(AST node, AST alternativeConfigureNode) { BlockStatement block = new BlockStatement(); if (node!=null) { configureAST(block, node); } else { configureAST(block, alternativeConfigureNode); } for (; node != null; node = node.getNextSibling()) { block.addStatement(statement(node)); } return block; }', 'output': 'public Statement statementListNoChild(AST node) { List<Statement> list = new ArrayList<>(); for (; node != null; node = node.getNextSibling()) { list.add(statement(node)); } return list; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String getFlushPath(String version) { long guessedSize = 2L * memsize.value() * 1024*1024; String location = DatabaseDescriptor.getDataFileLocationForTable(table.name, guessedSize); if (location == null) throw new RuntimeException("Insufficient disk space to flush"); return getTempSSTablePath(location, version); }', 'ground_truth': 'public String getFlushPath(long estimatedSize, String version) { String location = table.getDataFileLocation(estimatedSize); if (location == null) throw new RuntimeException("Insufficient disk space to flush " + estimatedSize + " bytes"); return getTempSSTablePath(location, version); }', 'output': 'public String getFlushPath(String tableName, String version) { long guessedSize = 2L * memsize.value() * 1024*1024; String location = DatabaseDescriptor.getDataFileLocationForTable(tableName, guessedSize); if (location == null) throw new RuntimeException("Insufficient disk space to flush"); return getTempSSTablePath(location, version); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GridCacheConfiguration() { /* No-op. */ } public GridCacheConfiguration(GridCacheConfiguration cc) { /* * NOTE: MAKE SURE TO PRESERVE ALPHABETIC ORDER! * ============================================== */ aff = cc.getAffinity(); affMapper = cc.getAffinityMapper(); atomicityMode = cc.getAtomicityMode(); atomicWriteOrderMode = cc.getAtomicWriteOrderMode(); backups = cc.getBackups(); cacheMode = cc.getCacheMode(); cloner = cc.getCloner(); dfltLockTimeout = cc.getDefaultLockTimeout(); dfltQryTimeout = cc.getDefaultQueryTimeout(); distro = cc.getDistributionMode(); eagerTtl = cc.isEagerTtl(); evictFilter = cc.getEvictionFilter(); evictKeyBufSize = cc.getEvictSynchronizedKeyBufferSize(); evictMaxOverflowRatio = cc.getEvictMaxOverflowRatio(); evictNearSync = cc.isEvictNearSynchronized(); evictPlc = cc.getEvictionPolicy(); evictSync = cc.isEvictSynchronized(); evictSyncConcurrencyLvl = cc.getEvictSynchronizedConcurrencyLevel(); evictSyncTimeout = cc.getEvictSynchronizedTimeout(); expiryPolicyFactory = cc.getExpiryPolicyFactory(); indexingSpiName = cc.getIndexingSpiName(); interceptor = cc.getInterceptor(); invalidate = cc.isInvalidate(); keepPortableInStore = cc.isKeepPortableInStore(); offHeapMaxMem = cc.getOffHeapMaxMemory(); maxConcurrentAsyncOps = cc.getMaxConcurrentAsyncOperations(); maxQryIterCnt = cc.getMaximumQueryIteratorCount(); memMode = cc.getMemoryMode(); name = cc.getName(); nearStartSize = cc.getNearStartSize(); nearEvictPlc = cc.getNearEvictionPolicy(); portableEnabled = cc.isPortableEnabled(); preloadMode = cc.getPreloadMode(); preloadBatchSize = cc.getPreloadBatchSize(); preloadDelay = cc.getPreloadPartitionedDelay(); preloadOrder = cc.getPreloadOrder(); preloadPoolSize = cc.getPreloadThreadPoolSize(); preloadTimeout = cc.getPreloadTimeout(); preloadThrottle = cc.getPreloadThrottle(); qryCfg = cc.getQueryConfiguration(); qryIdxEnabled = cc.isQueryIndexEnabled(); seqReserveSize = cc.getAtomicSequenceReserveSize(); startSize = cc.getStartSize(); store = cc.getStore(); storeValBytes = cc.isStoreValueBytes(); swapEnabled = cc.isSwapEnabled(); tmLookupClsName = cc.getTransactionManagerLookupClassName(); ttl = cc.getDefaultTimeToLive(); writeBehindBatchSize = cc.getWriteBehindBatchSize(); writeBehindEnabled = cc.isWriteBehindEnabled(); writeBehindFlushFreq = cc.getWriteBehindFlushFrequency(); writeBehindFlushSize = cc.getWriteBehindFlushSize(); writeBehindFlushThreadCnt = cc.getWriteBehindFlushThreadCount(); writeSync = cc.getWriteSynchronizationMode(); }', 'ground_truth': 'public GridCacheConfiguration(CompleteConfiguration cfg) { super(cfg); if (!(cfg instanceof GridCacheConfiguration)) return; GridCacheConfiguration cc = (GridCacheConfiguration)cfg; /* * NOTE: MAKE SURE TO PRESERVE ALPHABETIC ORDER! * ============================================== */ aff = cc.getAffinity(); affMapper = cc.getAffinityMapper(); atomicityMode = cc.getAtomicityMode(); atomicWriteOrderMode = cc.getAtomicWriteOrderMode(); backups = cc.getBackups(); cacheMode = cc.getCacheMode(); cloner = cc.getCloner(); dfltLockTimeout = cc.getDefaultLockTimeout(); dfltQryTimeout = cc.getDefaultQueryTimeout(); distro = cc.getDistributionMode(); eagerTtl = cc.isEagerTtl(); evictFilter = cc.getEvictionFilter(); evictKeyBufSize = cc.getEvictSynchronizedKeyBufferSize(); evictMaxOverflowRatio = cc.getEvictMaxOverflowRatio(); evictNearSync = cc.isEvictNearSynchronized(); evictPlc = cc.getEvictionPolicy(); evictSync = cc.isEvictSynchronized(); evictSyncConcurrencyLvl = cc.getEvictSynchronizedConcurrencyLevel(); evictSyncTimeout = cc.getEvictSynchronizedTimeout(); expiryPolicyFactory = cc.getExpiryPolicyFactory(); indexingSpiName = cc.getIndexingSpiName(); interceptor = cc.getInterceptor(); invalidate = cc.isInvalidate(); keepPortableInStore = cc.isKeepPortableInStore(); offHeapMaxMem = cc.getOffHeapMaxMemory(); maxConcurrentAsyncOps = cc.getMaxConcurrentAsyncOperations(); maxQryIterCnt = cc.getMaximumQueryIteratorCount(); memMode = cc.getMemoryMode(); name = cc.getName(); nearStartSize = cc.getNearStartSize(); nearEvictPlc = cc.getNearEvictionPolicy(); portableEnabled = cc.isPortableEnabled(); preloadMode = cc.getPreloadMode(); preloadBatchSize = cc.getPreloadBatchSize(); preloadDelay = cc.getPreloadPartitionedDelay(); preloadOrder = cc.getPreloadOrder(); preloadPoolSize = cc.getPreloadThreadPoolSize(); preloadTimeout = cc.getPreloadTimeout(); preloadThrottle = cc.getPreloadThrottle(); qryCfg = cc.getQueryConfiguration(); qryIdxEnabled = cc.isQueryIndexEnabled(); seqReserveSize = cc.getAtomicSequenceReserveSize(); startSize = cc.getStartSize(); store = cc.getStore(); storeValBytes = cc.isStoreValueBytes(); swapEnabled = cc.isSwapEnabled(); tmLookupClsName = cc.getTransactionManagerLookupClassName(); ttl = cc.getDefaultTimeToLive(); writeBehindBatchSize = cc.getWriteBehindBatchSize(); writeBehindEnabled = cc.isWriteBehindEnabled(); writeBehindFlushFreq = cc.getWriteBehindFlushFrequency(); writeBehindFlushSize = cc.getWriteBehindFlushSize(); writeBehindFlushThreadCnt = cc.getWriteBehindFlushThreadCount(); writeSync = cc.getWriteSynchronizationMode(); }', 'output': 'public CacheConfiguration() { /* No-op. */ } public CacheConfiguration(CacheConfiguration cc) { /* * NOTE: MAKE SURE TO PRESERVE ALPHABETIC ORDER! * ============================================== */ aff = cc.getAffinity(); affMapper = cc.getAffinityMapper(); atomicityMode = cc.getAtomicityMode(); atomicWriteOrderMode = cc.getAtomicWriteOrderMode(); backups = cc.getBackups(); cacheMode = cc.getCacheMode(); cloner = cc.getCloner(); dfltLockTimeout = cc.getDefaultLockTimeout(); dfltQryTimeout = cc.getDefaultQueryTimeout(); distro = cc.getDistributionMode(); eagerTtl = cc.isEagerTtl(); evictFilter = cc.getEvictionFilter(); evictKeyBufSize = cc.getEvictSynchronizedKeyBufferSize(); evictMaxOverflowRatio = cc.getEvictMaxOverflowRatio(); evictNearSync = cc.isEvictNearSynchronized(); evictPlc = cc.getEvictionPolicy(); evictSync = cc.isEvictSynchronized(); evictSyncConcurrencyLvl = cc.getEvictSynchronizedConcurrencyLevel(); evictSyncTimeout = cc.getEvictSynchronizedTimeout(); expiryPolicyFactory = cc.getExpiryPolicyFactory(); indexingSpiName = cc.getIndexingSpiName(); interceptor = cc.getInterceptor(); invalidate = cc.isInvalidate(); keepPortableInStore = cc.isKeepPortableInStore(); offHeapMaxMem = cc.getOffHeapMaxMemory(); maxConcurrentAsyncOps = cc.getMaxConcurrentAsyncOperations(); maxQryIterCnt = cc.getMaximumQueryIteratorCount(); memMode = cc.getMemoryMode(); name = cc.getName(); nearStartSize = cc.getNearStartSize(); nearEvictPlc = cc.getNearEvictionPolicy(); portableEnabled = cc.isPortableEnabled(); preloadMode = cc.getPreloadMode(); preloadBatchSize = cc.getPreloadBatchSize(); preloadDelay = cc.getPreloadPartitionedDelay(); preloadOrder = cc.get'}
{'instruction': 'Please refactor the following code:', 'input': 'public MockGridNearTxLocal(GridCacheSharedContext ctx, boolean implicit, boolean implicitSingle, boolean sys, byte plc, TransactionConcurrency concurrency, TransactionIsolation isolation, long timeout, boolean storeEnabled, Boolean mvccOp, int txSize, @Nullable UUID subjId, int taskNameHash, @Nullable String lb) { super(ctx, implicit, implicitSingle, sys, plc, concurrency, isolation, timeout, storeEnabled, mvccOp, txSize, subjId, taskNameHash, lb); }', 'ground_truth': 'public MockGridNearTxLocal(GridCacheSharedContext ctx, boolean implicit, boolean implicitSingle, boolean sys, byte plc, TransactionConcurrency concurrency, TransactionIsolation isolation, long timeout, boolean storeEnabled, Boolean mvccOp, int txSize, @Nullable UUID subjId, int taskNameHash, @Nullable String lb, IgniteTxManager.TxDumpsThrottling txDumpsThrottling) { super(ctx, implicit, implicitSingle, sys, plc, concurrency, isolation, timeout, storeEnabled, mvccOp, txSize, subjId, taskNameHash, lb, txDumpsThrottling); }', 'output': 'public MockGridNearTxLocal(IgniteEx ignite, boolean implicit, boolean implicitSingle, boolean sys, byte plc, TransactionConcurrency concurrency, TransactionIsolation isolation, long timeout, boolean storeEnabled, Boolean mvccOp, int txSize, @Nullable UUID subjId, int taskNameHash, @Nullable String lb) { super(ignite, implicit, implicitSingle, sys, plc, concurrency, isolation, timeout, storeEnabled, mvccOp, txSize, subjId, taskNameHash, lb); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean batch_insert_superColumn_blocking(batch_mutation_super_t batchMutationSuper) throws InvalidRequestException { logger.debug("batch_insert_SuperColumn_blocking"); RowMutation rm = RowMutation.getRowMutation(batchMutationSuper); validateCommand(rm.key(), rm.table(), rm.columnFamilyNames().toArray(new String[0])); return StorageProxy.insertBlocking(rm); } public void batch_insert_superColumn(batch_mutation_super_t batchMutationSuper)\n    { { logger.debug("batch_insert_SuperColumn"); RowMutation rm = RowMutation.getRowMutation(batchMutationSuper); try { validateCommand(rm.key(), rm.table(), rm.columnFamilyNames().toArray(new String[0])); } catch (InvalidRequestException e) { throw new RuntimeException(e); } StorageProxy.insert(rm); }', 'ground_truth': 'public void batch_insert_superColumn(batch_mutation_super_t batchMutationSuper, boolean block) throws InvalidRequestException, UnavailableException { logger.debug("batch_insert_SuperColumn"); RowMutation rm = RowMutation.getRowMutation(batchMutationSuper); Set<String> cfNames = rm.columnFamilyNames(); validateCommand(rm.key(), rm.table(), cfNames.toArray(new String[cfNames.size()])); if (block) { StorageProxy.insertBlocking(rm); } else { StorageProxy.insert(rm); } }', 'output': 'public void batch_insert_superColumn(batch_mutation_super_t batchMutationSuper) { logger.debug("batch_insert_SuperColumn"); RowMutation rm = RowMutation.getRowMutation(batchMutationSuper); try { validateCommand(rm.key(), rm.table(), rm.columnFamilyNames().toArray(new String[0])); } catch (InvalidRequestException e) { throw new RuntimeException(e); } StorageProxy.insert(rm); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public CFMetaData(String tableName, String cfName, String columnType, AbstractType comparator, AbstractType subcolumnComparator, String comment, double rowCacheSize, double keyCacheSize) { this(tableName, cfName, columnType, comparator, subcolumnComparator, comment, rowCacheSize, keyCacheSize, DEFAULT_READ_REPAIR_CHANCE, nextId()); } public CFMetaData(String tableName, String cfName, String columnType, AbstractType comparator, AbstractType subcolumnComparator, String comment, double rowCacheSize, double keyCacheSize, double readRepairChance)\n    { { this(tableName, cfName, columnType, comparator, subcolumnComparator, comment, rowCacheSize, keyCacheSize, readRepairChance, nextId()); }', 'ground_truth': 'public CFMetaData(String tableName, String cfName, String columnType, AbstractType comparator, AbstractType subcolumnComparator, String comment, double rowCacheSize, boolean preloadRowCache, double keyCacheSize) { this(tableName, cfName, columnType, comparator, subcolumnComparator, comment, rowCacheSize, preloadRowCache, keyCacheSize, DEFAULT_READ_REPAIR_CHANCE, nextId()); }', 'output': 'public CFMetaData(String tableName, String cfName, String columnType, AbstractType comparator, AbstractType subcolumnComparator, String comment, double rowCacheSize, double keyCacheSize, Double readRepairChance) { this(tableName, cfName, columnType, comparator, subcolumnComparator, comment, rowCacheSize, keyCacheSize, readRepairChance, nextId()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static String toCQL(String keyspace, String cf, IndexMetadata indexMetadata) { if (indexMetadata.isCustom()) { Map<String, String> options = new HashMap<>(); indexMetadata.options.forEach((k, v) -> { if (!k.equals(IndexTarget.TARGET_OPTION_NAME) && !k.equals(IndexTarget.CUSTOM_INDEX_OPTION_NAME)) options.put(k, v); }); return String.format("CREATE CUSTOM INDEX %s ON %s.%s (%s) USING \'%s\'%s;", quoteIdentifier(indexMetadata.name), quoteIdentifier(keyspace), quoteIdentifier(cf), indexMetadata.options.get(IndexTarget.TARGET_OPTION_NAME), indexMetadata.options.get(IndexTarget.CUSTOM_INDEX_OPTION_NAME), options.isEmpty() ? "" : " WITH OPTIONS " + toCQL(options)); } else { return String.format("CREATE INDEX %s ON %s.%s (%s);", quoteIdentifier(indexMetadata.name), quoteIdentifier(keyspace), quoteIdentifier(cf), indexMetadata.options.get(IndexTarget.TARGET_OPTION_NAME)); } }', 'ground_truth': 'private static String toCQL(TableMetadata baseTable, IndexMetadata indexMetadata) { if (indexMetadata.isCustom()) { Map<String, String> options = new HashMap<>(); indexMetadata.options.forEach((k, v) -> { if (!k.equals(IndexTarget.TARGET_OPTION_NAME) && !k.equals(IndexTarget.CUSTOM_INDEX_OPTION_NAME)) options.put(k, v); }); return String.format("CREATE CUSTOM INDEX %s ON %s (%s) USING \'%s\'%s;", indexMetadata.toCQLString(), baseTable.toString(), indexMetadata.options.get(IndexTarget.TARGET_OPTION_NAME), indexMetadata.options.get(IndexTarget.CUSTOM_INDEX_OPTION_NAME), options.isEmpty() ? "" : " WITH OPTIONS " + toCQL(options)); } else { return String.format("CREATE INDEX %s ON %s (%s);", indexMetadata.toCQLString(), baseTable.toString(), indexMetadata.options.get(IndexTarget.TARGET_OPTION_NAME)); } }', 'output': 'private static String toCQL(String keyspace, String cf, IndexMetadata indexMetadata, boolean isCustom) { if (isCustom) { Map<String, String> options = new HashMap<>(); indexMetadata.options.forEach((k, v) -> { if (!k.equals(IndexTarget.TARGET_OPTION_NAME) && !k.equals(IndexTarget.CUSTOM_INDEX_OPTION_NAME)) options.put(k, v); }); return String.format("CREATE CUSTOM INDEX %s ON %s.%s (%s) USING \'%s\'%s;", quoteIdentifier(indexMetadata.name), quoteIdentifier(keyspace), quoteIdentifier(cf), indexMetadata.options.get(IndexTarget.TARGET_OPTION_NAME), indexMetadata.options.get(IndexTarget.CUSTOM_INDEX_OPTION_NAME), options.isEmpty() ? "" : " WITH OPTIONS " + toCQL(options)); } else { return String.format("CREATE INDEX %s ON %s.%s (%s);", quoteIdentifier(indexMetadata.name), quoteIdentifier(keyspace), quoteIdentifier(cf), indexMetadata.options.get(IndexTarget.TARGET_OPTION_NAME)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public int bytesPastMark() { assert position >= markedPosition; return position - markedPosition; }', 'ground_truth': 'public int bytesPastMark(FileMark mark) { assert mark instanceof MappedFileDataInputMark; assert position >= ((MappedFileDataInputMark) mark).position; return position - ((MappedFileDataInputMark) mark).position; }', 'output': 'public long bytesPastMark() { assert position >= markedPosition; return position - markedPosition; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void testCompressedReadWith(long[] valuesToCheck, boolean testTruncate) throws Exception { assert valuesToCheck != null && valuesToCheck.length > 0; File tmp = new File(File.createTempFile("cassandra", "unittest").getParent(), "ks-cf-ib-1-Data.db"); Descriptor desc = Descriptor.fromFilename(tmp.getAbsolutePath()); MetadataCollector collector = new MetadataCollector(new SimpleDenseCellNameType(BytesType.instance)); CompressionParameters param = new CompressionParameters(SnappyCompressor.instance, 32, Collections.<String, String>emptyMap()); Map<Long, Long> index = new HashMap<>(); try (CompressedSequentialWriter writer = new CompressedSequentialWriter(tmp, desc.filenameFor(Component.COMPRESSION_INFO), param, collector)) { for (long l = 0L; l < 1000; l++) { index.put(l, writer.getFilePointer()); writer.stream.writeLong(l); } writer.finish(); } CompressionMetadata comp = CompressionMetadata.create(tmp.getAbsolutePath()); List<Pair<Long, Long>> sections = new ArrayList<>(); for (long l : valuesToCheck) { long position = index.get(l); sections.add(Pair.create(position, position + 8)); } CompressionMetadata.Chunk[] chunks = comp.getChunksForSections(sections); long totalSize = comp.getTotalSizeForSections(sections); long expectedSize = 0; for (CompressionMetadata.Chunk c : chunks) expectedSize += c.length + 4; assertEquals(expectedSize, totalSize); int size = 0; for (CompressionMetadata.Chunk c : chunks) size += (c.length + 4); byte[] toRead = new byte[size]; try (RandomAccessFile f = new RandomAccessFile(tmp, "r")) { int pos = 0; for (CompressionMetadata.Chunk c : chunks) { f.seek(c.offset); pos += f.read(toRead, pos, c.length + 4); } } if (testTruncate) { byte [] actuallyRead = new byte[50]; System.arraycopy(toRead, 0, actuallyRead, 0, 50); toRead = actuallyRead; } CompressionInfo info = new CompressionInfo(chunks, param); CompressedInputStream input = new CompressedInputStream(new ByteArrayInputStream(toRead), info); try (DataInputStream in = new DataInputStream(input)) { for (int i = 0; i < sections.size(); i++) { input.position(sections.get(i).left); long readValue = in.readLong(); assertEquals("expected " + valuesToCheck[i] + " but was " + readValue, valuesToCheck[i], readValue); } } }', 'ground_truth': 'private void testCompressedReadWith(long[] valuesToCheck, boolean testTruncate, boolean testException) throws Exception { assert valuesToCheck != null && valuesToCheck.length > 0; File tmp = new File(File.createTempFile("cassandra", "unittest").getParent(), "ks-cf-ib-1-Data.db"); Descriptor desc = Descriptor.fromFilename(tmp.getAbsolutePath()); MetadataCollector collector = new MetadataCollector(new SimpleDenseCellNameType(BytesType.instance)); CompressionParameters param = new CompressionParameters(SnappyCompressor.instance, 32, Collections.<String, String>emptyMap()); Map<Long, Long> index = new HashMap<>(); try (CompressedSequentialWriter writer = new CompressedSequentialWriter(tmp, desc.filenameFor(Component.COMPRESSION_INFO), param, collector)) { for (long l = 0L; l < 1000; l++) { index.put(l, writer.getFilePointer()); writer.stream.writeLong(l); } writer.finish(); } CompressionMetadata comp = CompressionMetadata.create(tmp.getAbsolutePath()); List<Pair<Long, Long>> sections = new ArrayList<>(); for (long l : valuesToCheck) { long position = index.get(l); sections.add(Pair.create(position, position + 8)); } CompressionMetadata.Chunk[] chunks = comp.getChunksForSections(sections); long totalSize = comp.getTotalSizeForSections(sections); long expectedSize = 0; for (CompressionMetadata.Chunk c : chunks) expectedSize += c.length + 4; assertEquals(expectedSize, totalSize); int size = 0; for (CompressionMetadata.Chunk c : chunks) size += (c.length + 4); byte[] toRead = new byte[size]; try (RandomAccessFile f = new RandomAccessFile(tmp, "r")) { int pos = 0; for (CompressionMetadata.Chunk c : chunks) { f.seek(c.offset); pos += f.read(toRead, pos, c.length + 4); } } if (testTruncate) { byte [] actuallyRead = new byte[50]; System.arraycopy(toRead, 0, actuallyRead, 0, 50); toRead = actuallyRead; } CompressionInfo info = new CompressionInfo(chunks, param); if (testException) { testException(sections, info); return; } CompressedInputStream input = new CompressedInputStream(new ByteArrayInputStream(toRead), info); try (DataInputStream in = new DataInputStream(input)) { for (int i = 0; i < sections.size(); i++) { input.position(sections.get(i).left); long readValue = in.readLong(); assertEquals("expected " + valuesToCheck[i] + " but was " + readValue, valuesToCheck[i], readValue); } } }', 'output': 'private void testCompressedReadWith(long[] valuesToCheck, boolean testTruncate) throws Exception { assert valuesToCheck != null && valuesToCheck.length > 0; File tmp = new File(File.createTempFile("cassandra", "unittest").getParent(), "ks-cf-ib-1-Data.db"); Descriptor desc = Descriptor.fromFilename(tmp.getAbsolutePath()); MetadataCollector collector = new MetadataCollector(new SimpleDenseCellNameType(BytesType.instance)); CompressionParameters param = new CompressionParameters(SnappyCompressor.instance, 32, Collections.<String, String>emptyMap()); Map<Long, Long> index = new HashMap<>(); try (CompressedSequentialWriter writer = new CompressedSequentialWriter(tmp, desc.filenameFor(Component.COMPRESSION_INFO), param, collector)) { for (long l = 0L; l < 1000; l++) { index.put(l, writer.getFilePointer()); writer.stream.writeLong(l); } writer.finish(); } CompressionMetadata comp = CompressionMetadata.create(tmp.getAbsolutePath()); List<Pair<Long, Long>> sections = new ArrayList<>(); for (long l : valuesToCheck) { long position = index.get(l); sections.add(Pair.create(position, position + 8)); } CompressionMetadata.Chunk[] chunks = comp.getChunksForSections(sections); long totalSize = comp.getTotalSizeForSections(sections); long expectedSize = 0; for (CompressionMetadata.Chunk c : chunks) expectedSize += c.length + 4; assertEquals(expectedSize, totalSize); int size = 0; for (CompressionMetadata.Chunk c : chunks) size += (c.length + 4); byte[] toRead = new byte[size]; try (RandomAccessFile f = new RandomAccessFile(tmp, "r")) { int pos = 0; for (CompressionMetadata.Chunk c : chunks) { f.seek(c.offset); pos += f.read(toRead, pos, c.length + 4); } } if (testTruncate) { byte [] actuallyRead = new byte[5'}
{'instruction': 'Please refactor the following code:', 'input': 'private List<TokenRange> getRangeMap(Configuration conf) throws IOException { Cassandra.Client client = ConfigHelper.getClientFromInputAddressList(conf); List<TokenRange> map; try { map = client.describe_local_ring(ConfigHelper.getInputKeyspace(conf)); } catch (TException e) { throw new RuntimeException(e); } return map; }', 'ground_truth': 'private Map<TokenRange, Set<Host>> getRangeMap(Configuration conf, String keyspace) { Session session = CqlConfigHelper.getInputCluster(ConfigHelper.getInputInitialAddress(conf).split(","), conf).connect(); Map<TokenRange, Set<Host>> map = new HashMap<>(); Metadata metadata = session.getCluster().getMetadata(); for (TokenRange tokenRange : metadata.getTokenRanges()) map.put(tokenRange, metadata.getReplicas(keyspace, tokenRange)); return map; }', 'output': 'private List<TokenRange> getRangeMap(Configuration conf, String keyspace) throws IOException { Cassandra.Client client = ConfigHelper.getClientFromInputAddressList(conf); List<TokenRange> map; try { map = client.describe_local_ring(keyspace); } catch (TException e) { throw new RuntimeException(e); } return map; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public CqlCounterGetter(Timer timer, PartitionGenerator generator, StressSettings settings) { super(Command.COUNTER_READ, timer, generator, settings); }', 'ground_truth': 'public CqlCounterGetter(Timer timer, PartitionGenerator generator, SeedManager seedManager, StressSettings settings) { super(Command.COUNTER_READ, timer, generator, seedManager, settings); }', 'output': 'public CqlCounterGetter(Timer timer, PartitionGenerator generator, StressSettings settings, String tableName) { super(Command.COUNTER_READ, timer, generator, settings, tableName); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void addReplacement(Class<?> klass, List<Replacement> replacements, String newName, Replaces r) { String oldName = r.oldName(); boolean deprecated = r.deprecated(); replacements.add(new Replacement(klass, oldName, newName, deprecated)); }', 'ground_truth': 'private static void addReplacement(Class<?> klass, List<Replacement> replacements, String newName, Class<?> newType, Replaces r) { String oldName = r.oldName(); boolean deprecated = r.deprecated(); Class<?> oldType = r.converter().getInputType(); if (oldType == null) oldType = newType; replacements.add(new Replacement(klass, oldName, oldType, newName, r.converter(), deprecated)); }', 'output': 'private static void addReplacement(Class<?> classe, List<Replacement> replacements, String newName, Replaces r) { String oldName = r.oldName(); boolean deprecated = r.deprecated(); replacements.add(new Replacement(classe, oldName, newName, deprecated)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected abstract UserType makeUpdatedType(UserType toUpdate, XX XXX) throws InvalidRequestException; public static AlterTypeStatement addition(UTName name, ColumnIdentifier fieldName, CQL3Type.Raw type) { return new AddOrAlter(name, true, fieldName, type); } protected UserType makeUpdatedType(UserType toUpdate, XX XXX) throws InvalidRequestException\n        { { return isAdd ? doAdd(toUpdate) : doAlter(toUpdate, ksm); } protected UserType makeUpdatedType(UserType toUpdate, XX XXX) throws InvalidRequestException\n        { { checkTypeNotUsedByAggregate(ksm); List<ByteBuffer> newNames = new ArrayList<>(toUpdate.fieldNames()); List<AbstractType<?>> newTypes = new ArrayList<>(toUpdate.fieldTypes()); for (Map.Entry<ColumnIdentifier, ColumnIdentifier> entry : renames.entrySet()) { ColumnIdentifier from = entry.getKey(); ColumnIdentifier to = entry.getValue(); int idx = getIdxOfField(toUpdate, from); if (idx < 0) throw new InvalidRequestException(String.format("Unknown field %s in type %s", from, name)); newNames.set(idx, to.bytes); } UserType updated = new UserType(toUpdate.keyspace, toUpdate.name, newNames, newTypes); CreateTypeStatement.checkForDuplicateNames(updated); return updated; }', 'ground_truth': 'protected UserType makeUpdatedType(UserType toUpdate, KeyspaceMetadata ksm) throws InvalidRequestException { if (getIdxOfField(toUpdate, fieldName) >= 0) throw new InvalidRequestException(String.format("Cannot add new field %s to type %s: a field of the same name already exists", fieldName, name)); List<ByteBuffer> newNames = new ArrayList<>(toUpdate.size() + 1); newNames.addAll(toUpdate.fieldNames()); newNames.add(fieldName.bytes); AbstractType<?> addType = type.prepare(keyspace()).getType(); if (addType.referencesUserType(toUpdate.getNameAsString())) throw new InvalidRequestException(String.format("Cannot add new field %s of type %s to type %s as this would create a circular reference", fieldName, type, name)); List<AbstractType<?>> newTypes = new ArrayList<>(toUpdate.size() + 1); newTypes.addAll(toUpdate.fieldTypes()); newTypes.add(addType); return new UserType(toUpdate.keyspace, toUpdate.name, newNames, newTypes); }', 'output': 'public static AlterTypeStatement addition(UTName name, ColumnIdentifier fieldName, CQL3Type.Raw type) { return new AddOrAlter(name, true, fieldName, type); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Value fromSerialized(ByteBuffer value, SetType type) throws InvalidRequestException { try { Set<?> s = (Set<?>)type.compose(value); Set<ByteBuffer> elements = new LinkedHashSet<ByteBuffer>(s.size()); for (Object element : s) elements.add(type.elements.decompose(element)); return new Value(elements); } catch (MarshalException e) { throw new InvalidRequestException(e.getMessage()); } }', 'ground_truth': 'public static Value fromSerialized(ByteBuffer value, SetType type, int version) throws InvalidRequestException { try { Set<?> s = (Set<?>)type.getSerializer().deserializeForNativeProtocol(value, version); Set<ByteBuffer> elements = new LinkedHashSet<ByteBuffer>(s.size()); for (Object element : s) elements.add(type.elements.decompose(element)); return new Value(elements); } catch (MarshalException e) { throw new InvalidRequestException(e.getMessage()); } }', 'output': 'public static Value fromSerialized(ByteBuffer value, SetType type) throws InvalidRequestException { try { Set<?> s = (Set<?>)type.compose(value); Set<ByteBuffer> elements = new LinkedHashSet<ByteBuffer>(s.size()); for (Object element : s) elements.add(type.elements.decompose(element)); return new Value(elements); } catch (MarshalException e) { throw new InvalidRequestException(e.getMessage()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void dumpPendingObjects() { U.warn(cctx.kernalContext().cluster().diagnosticLog(), "Failed to wait for partition release future [topVer=" + topologyVersion() + ", node=" + cctx.localNodeId() + "]. Dumping pending objects that might be the cause: "); try { cctx.exchange().dumpDebugInfo(this); } catch (Exception e) { U.error(cctx.kernalContext().cluster().diagnosticLog(), "Failed to dump debug information: " + e, e); } }', 'ground_truth': 'private void dumpPendingObjects(IgniteInternalFuture<?> partReleaseFut) { U.warn(cctx.kernalContext().cluster().diagnosticLog(), "Failed to wait for partition release future [topVer=" + initialVersion() + ", node=" + cctx.localNodeId() + "]"); U.warn(log, "Partition release future: " + partReleaseFut); U.warn(cctx.kernalContext().cluster().diagnosticLog(), "Dumping pending objects that might be the cause: "); try { cctx.exchange().dumpDebugInfo(this); } catch (Exception e) { U.error(cctx.kernalContext().cluster().diagnosticLog(), "Failed to dump debug information: " + e, e); } }', 'output': 'private void dumpPendingObjects() { U.info(cctx.kernalContext().cluster().diagnosticLog(), "Failed to wait for partition release future [topVer=" + topologyVersion() + ", node=" + cctx.localNodeId() + "]. Dumping pending objects that might be the cause: "); try { cctx.exchange().dumpDebugInfo(this); } catch (Exception e) { U.error(cctx.kernalContext().cluster().diagnosticLog(), "Failed to dump debug information: " + e, e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GridNearTxFinishRequest( IgniteUuid futId, GridCacheVersion xidVer, long threadId, boolean commit, boolean invalidate, boolean sys, byte plc, boolean syncCommit, boolean syncRollback, boolean explicitLock, boolean storeEnabled, @NotNull AffinityTopologyVersion topVer, GridCacheVersion baseVer, Collection<GridCacheVersion> committedVers, Collection<GridCacheVersion> rolledbackVers, int txSize, @Nullable UUID subjId, int taskNameHash) { super( xidVer, futId, null, threadId, commit, invalidate, sys, plc, syncCommit, syncRollback, baseVer, committedVers, rolledbackVers, txSize ); this.explicitLock = explicitLock; this.storeEnabled = storeEnabled; this.topVer = topVer; this.subjId = subjId; this.taskNameHash = taskNameHash; }', 'ground_truth': 'public GridNearTxFinishRequest( IgniteUuid futId, GridCacheVersion xidVer, long threadId, boolean commit, boolean invalidate, boolean sys, byte plc, boolean syncCommit, boolean syncRollback, boolean explicitLock, boolean storeEnabled, @NotNull AffinityTopologyVersion topVer, GridCacheVersion baseVer, Collection<GridCacheVersion> committedVers, Collection<GridCacheVersion> rolledbackVers, int txSize, @Nullable UUID subjId, int taskNameHash, boolean addDepInfo) { super( xidVer, futId, null, threadId, commit, invalidate, sys, plc, syncCommit, syncRollback, baseVer, committedVers, rolledbackVers, txSize, addDepInfo ); this.explicitLock = explicitLock; this.storeEnabled = storeEnabled; this.topVer = topVer; this.subjId = subjId; this.taskNameHash = taskNameHash; }', 'output': 'public GridNearTxFinishRequest( IgniteUuid futId, CacheVersion xidVer, long threadId, boolean commit, boolean invalidate, boolean sys, byte plc, boolean syncCommit, boolean syncRollback, boolean explicitLock, boolean storeEnabled, @NotNull AffinityTopologyVersion topVer, CacheVersion baseVer, Collection<CacheVersion> committedVers, Collection<CacheVersion> rolledbackVers, int txSize, @Nullable UUID subjId, int taskNameHash) { super( xidVer, futId, null, threadId, commit, invalidate, sys, plc, syncCommit, syncRollback, baseVer, committedVers, rolledbackVers, txSize ); this.explicitLock = explicitLock; this.storeEnabled = storeEnabled; this.topVer = topVer; this.subjId = subjId; this.taskNameHash = taskNameHash; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Response getRepository(@Context HttpHeaders headers, @Context UriInfo ui, @PathParam("stackName") String stackName, @PathParam("stackVersion") String stackVersion, @PathParam("osType") String osType, @PathParam("repoId") String repoId) { return handleRequest(headers, null, ui, Request.Type.GET, createRepositoryResource(stackName, stackVersion, osType, repoId)); }', 'ground_truth': 'public Response getRepository(String body, @Context HttpHeaders headers, @Context UriInfo ui, @PathParam("stackName") String stackName, @PathParam("stackVersion") String stackVersion, @PathParam("osType") String osType, @PathParam("repoId") String repoId) { return handleRequest(headers, body, ui, Request.Type.GET, createRepositoryResource(stackName, stackVersion, osType, repoId)); }', 'output': 'public Response getRepository(@Context HttpHeaders headers, @Context UriInfo ui, @PathParam("stackName") String stackName, @PathParam("stackVersion") String stackVersion, @PathParam("osType") String osType, @PathParam("repoId") String repoId, @QueryParam("query") String query) { return handleRequest(headers, null, ui, Request.Type.GET, createRepositoryResource(stackName, stackVersion, osType, repoId, query)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private byte[] getData(long index, int column) { final long key = (column * repeatFrequency) + (index % repeatFrequency); byte[] r = cache.get(key); if (r != null) return r; MessageDigest md = FBUtilities.threadLocalMD5Digest(); r = md.digest(Long.toString(key).getBytes(UTF_8)); cache.putIfAbsent(key, r); return r; }', 'ground_truth': 'private byte[] getData(long index, int column, ByteBuffer seed) { final long key = (column * repeatFrequency) + ((seed == null ? index : Math.abs(seed.hashCode())) % repeatFrequency); byte[] r = cache.get(key); if (r != null) return r; MessageDigest md = FBUtilities.threadLocalMD5Digest(); r = md.digest(Long.toString(key).getBytes(UTF_8)); cache.putIfAbsent(key, r); return r; }', 'output': 'private byte[] getData(long index, int column) { final long key = (column * repeatFrequency) + (index % repeatFrequency); byte[] r = cache.get(key); if (r != null) return r; MessageDigest md = FBUtilities.threadLocalMD5Digest(); r = md.digest(Long.toString(key).getBytes(UTF_8)); cache.putIfAbsent(key, r); return r; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public IgniteFuture<Map<K, V>> loadAsync(@Nullable IgniteTxEx tx, @Nullable Collection<? extends K> keys, boolean reload, boolean forcePrimary, @Nullable UUID subjId, String taskName, boolean deserializePortable, @Nullable ExpiryPolicy expiryPlc) { if (F.isEmpty(keys)) return new GridFinishedFuture<>(ctx.kernalContext(), Collections.<K, V>emptyMap()); if (keyCheck) validateCacheKeys(keys); IgniteTxLocalEx<K, V> txx = (tx != null && tx.local()) ? (IgniteTxLocalEx<K, V>)tx : null; final GetExpiryPolicy expiry = accessExpiryPolicy(expiryPlc); GridNearGetFuture<K, V> fut = new GridNearGetFuture<>(ctx, keys, true, reload, forcePrimary, txx, subjId, taskName, deserializePortable, expiry); fut.init(); return ctx.wrapCloneMap(fut); }', 'ground_truth': 'public IgniteFuture<Map<K, V>> loadAsync(@Nullable IgniteTxEx tx, @Nullable Collection<? extends K> keys, boolean reload, boolean forcePrimary, @Nullable UUID subjId, String taskName, boolean deserializePortable, @Nullable ExpiryPolicy expiryPlc, boolean skipVal ) { if (F.isEmpty(keys)) return new GridFinishedFuture<>(ctx.kernalContext(), Collections.<K, V>emptyMap()); if (keyCheck) validateCacheKeys(keys); IgniteTxLocalEx<K, V> txx = (tx != null && tx.local()) ? (IgniteTxLocalEx<K, V>)tx : null; final GetExpiryPolicy expiry = accessExpiryPolicy(expiryPlc); GridNearGetFuture<K, V> fut = new GridNearGetFuture<>(ctx, keys, true, reload, forcePrimary, txx, subjId, taskName, deserializePortable, expiry, skipVal); fut.init(); return ctx.wrapCloneMap(fut); }', 'output': 'public IgniteFuture<Map<K, V>> loadAsync(@Nullable IgniteTxEx tx, @Nullable Collection<? extends K> keys, boolean reload, boolean forcePrimary, @Nullable UUID subjId, String taskName, boolean deserializePortable, @Nullable ExpiryPolicy expiryPlc) { if (F.isEmpty(keys)) return new GridFinishedFuture<>(ctx.kernalContext(), Collections.<K, V>emptyMap()); if (keyCheck) validateCacheKeys(keys); IgniteTxLocalEx<K, V> txx = (tx != null && tx.local()) ? (IgniteTxLocalEx<K, V>)tx : null; final GetExpiryPolicy expiry = accessExpiryPolicy(expiryPlc); GridNearGetFuture<K, V> fut = new GridNearGetFuture<>(ctx, keys, true, reload, forcePrimary, txx, subjId, taskName, deserializePortable, expiry); fut.init(); return ctx.wrapCloneMap(fut); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void doVerb(final Message message) { try { Collection<Column> cols = MigrationManager.makeColumns(message); for (Column col : cols) { final UUID version = UUIDGen.getUUID(col.name()); if (version.timestamp() > DatabaseDescriptor.getDefsVersion().timestamp()) { final Migration m = Migration.deserialize(col.value()); assert m.getVersion().equals(version); StageManager.getStage(Stage.MIGRATION).submit(new WrappedRunnable() { @Override protected void runMayThrow() throws Exception { if (DatabaseDescriptor.getDefsVersion().timestamp() == version.timestamp()) logger.debug("Not appling (equal) " + version.toString()); else if (DatabaseDescriptor.getDefsVersion().timestamp() > version.timestamp()) logger.debug("Not applying (before)" + version.toString()); else { logger.debug("Applying {} from {}", m.getClass().getSimpleName(), message.getFrom()); try { m.apply(); m.passiveAnnounce(); } catch (ConfigurationException ex) { logger.debug("Migration not applied " + ex.getMessage()); } } } }); } } } catch (IOException ex) { throw new IOError(ex); } }', 'ground_truth': 'public void doVerb(final Message message, String id) { try { Collection<Column> cols = MigrationManager.makeColumns(message); for (Column col : cols) { final UUID version = UUIDGen.getUUID(col.name()); if (version.timestamp() > DatabaseDescriptor.getDefsVersion().timestamp()) { final Migration m = Migration.deserialize(col.value()); assert m.getVersion().equals(version); StageManager.getStage(Stage.MIGRATION).submit(new WrappedRunnable() { @Override protected void runMayThrow() throws Exception { if (DatabaseDescriptor.getDefsVersion().timestamp() == version.timestamp()) logger.debug("Not appling (equal) " + version.toString()); else if (DatabaseDescriptor.getDefsVersion().timestamp() > version.timestamp()) logger.debug("Not applying (before)" + version.toString()); else { logger.debug("Applying {} from {}", m.getClass().getSimpleName(), message.getFrom()); try { m.apply(); m.passiveAnnounce(); } catch (ConfigurationException ex) { logger.debug("Migration not applied " + ex.getMessage()); } } } }); } } } catch (IOException ex) { throw new IOError(ex); } }', 'output': 'public void doVerb(final Message message) { try { List<Column> cols = MigrationManager.makeColumns(message); for (Column col : cols) { final UUID version = UUIDGen.getUUID(col.name()); if (version.timestamp() > DatabaseDescriptor.getDefsVersion().timestamp()) { final Migration m = Migration.deserialize(col.value()); assert m.getVersion().equals(version); StageManager.getStage(Stage.MIGRATION).submit(new WrappedRunnable() { @Override protected void runMayThrow() throws Exception { if (DatabaseDescriptor.getDefsVersion().timestamp() == version.timestamp()) logger.debug("Not appling (equal) " + version.toString()); else if (DatabaseDescriptor.getDefsVersion().timestamp() > version.timestamp()) logger.debug("Not applying (before)" + version.toString()); else { logger.debug("Applying {} from {}", m.getClass().getSimpleName(), message.getFrom()); try { m.apply(); m.passiveAnnounce(); } catch (ConfigurationException ex) { logger.debug("Migration not applied " + ex.getMessage()); } } } }); } } } catch (IOException ex) { throw new IOError(ex); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean handleHostsAvailabilityError(Throwable e, int attempt, String msg) { if (attempt >= CQL_EXECUTION_ATTEMPTS_COUNT) { log.error("Host availability problem detected. " + "Number of CQL execution attempts reached maximum " + CQL_EXECUTION_ATTEMPTS_COUNT + ", exception will be thrown to upper execution layer.", e); throw msg == null ? new IgniteException(e) : new IgniteException(msg, e); } if (attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 4  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 2  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 2 + CQL_EXECUTION_ATTEMPTS_COUNT / 4  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT - 1) { log.warning("Host availability problem detected, CQL execution attempt  " + (attempt + 1) + ", " + "refreshing Cassandra session", e); refresh(); log.warning("Cassandra session refreshed"); return true; } log.warning("Host availability problem detected, CQL execution attempt " + (attempt + 1) + ", " + "sleeping extra " + CQL_EXECUTION_ATTEMPT_MAX_TIMEOUT + " milliseconds", e); try { Thread.sleep(CQL_EXECUTION_ATTEMPT_MAX_TIMEOUT); } catch (InterruptedException ignored) { } log.warning("Sleep completed"); return false; }', 'ground_truth': 'private boolean handleHostsAvailabilityError(long sesGeneration, Throwable e, int attempt, String msg) { if (attempt >= CQL_EXECUTION_ATTEMPTS_COUNT) { log.error("Host availability problem detected. " + "Number of CQL execution attempts reached maximum " + CQL_EXECUTION_ATTEMPTS_COUNT + ", exception will be thrown to upper execution layer.", e); throw msg == null ? new IgniteException(e) : new IgniteException(msg, e); } if (attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 4  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 2  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 2 + CQL_EXECUTION_ATTEMPTS_COUNT / 4  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT - 1) { refreshLock.lock(); try { if (sesGeneration < generation) log.warning("Host availability problem detected, but already handled by another thread"); else { log.warning("Host availability problem detected, CQL execution attempt  " + (attempt + 1) + ", " + "refreshing Cassandra session", e); refresh(); log.warning("Cassandra session refreshed"); return true; } } finally { refreshLock.unlock(); } } log.warning("Host availability problem detected, CQL execution attempt " + (attempt + 1) + ", " + "sleeping extra " + CQL_EXECUTION_ATTEMPT_MAX_TIMEOUT + " milliseconds", e); try { Thread.sleep(CQL_EXECUTION_ATTEMPT_MAX_TIMEOUT); } catch (InterruptedException ignored) { } log.warning("Sleep completed"); return false; }', 'output': 'private boolean handleHostsAvailabilityError(Throwable e, int attempt, String msg) { if (attempt >= CQL_EXECUTION_ATTEMPTS_COUNT) { log.error("Host availability problem detected. " + "Number of CQL execution attempts reached maximum " + CQL_EXECUTION_ATTEMPTS_COUNT + ", exception will be thrown to upper execution layer.", e); throw msg == null ? new IgniteException(e) : new IgniteException(msg, e); } if (attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 4  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 2  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 2 + CQL_EXECUTION_ATTEMPTS_COUNT / 4  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT - 1) { log.warning("Host availability problem detected, CQL execution attempt  " + (attempt + 1) + ", " + "refreshing Cassandra session", e); refresh(); log.warning("Cassandra session refreshed"); return true; } log.warning("Host availability problem detected, CQL execution attempt " + (attempt + 1) + ", " + "sleeping extra " + CQL_EXECUTION_ATTEMPT_MAX_TIMEOUT + " milliseconds", e); try { Thread.sleep(CQL_EXECUTION_ATTEMPT_MAX_TIMEOUT); } catch (InterruptedException ignored) { } log.warning("Sleep completed"); return false; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ColumnFamily create(String tableName, String cfName) { ColumnFamilyType cfType = DatabaseDescriptor.getColumnFamilyType(tableName, cfName); AbstractType comparator = DatabaseDescriptor.getComparator(tableName, cfName); AbstractType subcolumnComparator = DatabaseDescriptor.getSubComparator(tableName, cfName); XXX id = CFMetaData.getId(tableName, cfName); return new ColumnFamily(cfName, cfType, comparator, subcolumnComparator, id); }', 'ground_truth': 'public static ColumnFamily create(String tableName, String cfName) { ColumnFamilyType cfType = DatabaseDescriptor.getColumnFamilyType(tableName, cfName); AbstractType comparator = DatabaseDescriptor.getComparator(tableName, cfName); AbstractType subcolumnComparator = DatabaseDescriptor.getSubComparator(tableName, cfName); Integer id = CFMetaData.getId(tableName, cfName); if (id == null) throw new RuntimeException("Cannot create ColumnFamily for non-existant table/cf pair."); return new ColumnFamily(cfName, cfType, comparator, subcolumnComparator, id); }', 'output': 'public static ColumnFamily create(String tableName, String cfName) { ColumnFamilyType cfType = DatabaseDescriptor.getColumnFamilyType(tableName, cfName); AbstractType comparator = DatabaseDescriptor.getComparator(tableName, cfName); AbstractType subcolumnComparator = DatabaseDescriptor.getSubComparator(tableName, cfName); UUID id = CFMetaData.getId(tableName, cfName); return new ColumnFamily(cfName, cfType, comparator, subcolumnComparator, id); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void test() throws Exception { XXX ignite = (IgniteKernal)startGrid(0); CacheConfiguration<Integer, Integer> cfg = new CacheConfiguration<>(DEFAULT_CACHE_NAME); cfg.setCacheMode(CacheMode.PARTITIONED); cfg.setRebalanceMode(CacheRebalanceMode.SYNC); cfg.setBackups(1); ignite(0).getOrCreateCache(cfg); startGrid(1); startGrid(2); startGrid(3); awaitPartitionMapExchange(true, true, null); for (int i = 0; i < 2; i++) { stopGrid(3); awaitPartitionMapExchange(true, true, null); startGrid(3); awaitPartitionMapExchange(true, true, null); } startGrid(4); awaitPartitionMapExchange(true, true, null); assert rs.isEmpty(); record = true; ignite.context().cache().context().exchange().scheduleResendPartitions(); while (rs.size() < 3) { U.sleep(10); } ignite(0).destroyCache(DEFAULT_CACHE_NAME); ignite(0).getOrCreateCache(cfg); awaitPartitionMapExchange(); for (Runnable r : rs.values()) r.run(); U.sleep(10000); stopGrid(3); awaitPartitionMapExchange(); long topVer0 = grid(0).context().cache().context().exchange().readyAffinityVersion().topologyVersion(); long topVer1 = grid(1).context().cache().context().exchange().readyAffinityVersion().topologyVersion(); long topVer2 = grid(2).context().cache().context().exchange().readyAffinityVersion().topologyVersion(); stopGrid(4); awaitPartitionMapExchange(); assert grid(0).context().cache().context().exchange().readyAffinityVersion().topologyVersion() > topVer0; assert grid(1).context().cache().context().exchange().readyAffinityVersion().topologyVersion() > topVer1; assert grid(2).context().cache().context().exchange().readyAffinityVersion().topologyVersion() > topVer2; }', 'ground_truth': 'public void test() throws Exception { IgniteEx ignite = startGrid(0); CacheConfiguration<Integer, Integer> cfg = new CacheConfiguration<>(DEFAULT_CACHE_NAME); cfg.setCacheMode(CacheMode.PARTITIONED); cfg.setRebalanceMode(CacheRebalanceMode.SYNC); cfg.setBackups(1); ignite(0).getOrCreateCache(cfg); startGrid(1); startGrid(2); startGrid(3); awaitPartitionMapExchange(true, true, null); for (int i = 0; i < 2; i++) { stopGrid(3); awaitPartitionMapExchange(true, true, null); startGrid(3); awaitPartitionMapExchange(true, true, null); } startGrid(4); awaitPartitionMapExchange(true, true, null); assert rs.isEmpty(); record = true; ignite.context().cache().context().exchange().scheduleResendPartitions(); while (rs.size() < 3) { U.sleep(10); } ignite(0).destroyCache(DEFAULT_CACHE_NAME); ignite(0).getOrCreateCache(cfg); awaitPartitionMapExchange(); replayMessages(); stopGrid(3); awaitPartitionMapExchange(); long topVer0 = grid(0).context().cache().context().exchange().readyAffinityVersion().topologyVersion(); long topVer1 = grid(1).context().cache().context().exchange().readyAffinityVersion().topologyVersion(); long topVer2 = grid(2).context().cache().context().exchange().readyAffinityVersion().topologyVersion(); stopGrid(4); awaitPartitionMapExchange(); assert grid(0).context().cache().context().exchange().readyAffinityVersion().topologyVersion() > topVer0; assert grid(1).context().cache().context().exchange().readyAffinityVersion().topologyVersion() > topVer1; assert grid(2).context().cache().context().exchange().readyAffinityVersion().topologyVersion() > topVer2; }', 'output': 'public void test() throws Exception { IgniteKernal ignite = (IgniteKernal)startGrid(0); CacheConfiguration<Integer, Integer> cfg = new CacheConfiguration<>(DEFAULT_CACHE_NAME); cfg.setCacheMode(CacheMode.PARTITIONED); cfg.setRebalanceMode(CacheRebalanceMode.SYNC); cfg.setBackups(1); ignite(0).getOrCreateCache(cfg); startGrid(1); startGrid(2); startGrid(3); awaitPartitionMapExchange(true, true, null); for (int i = 0; i < 2; i++) { stopGrid(3); awaitPartitionMapExchange(true, true, null); startGrid(3); awaitPartitionMapExchange(true, true, null); } startGrid(4); awaitPartitionMapExchange(true, true, null); assert rs.isEmpty(); record = true; ignite.context().cache().context().exchange().scheduleResendPartitions(); while (rs.size() < 3) { U.sleep(10); } ignite(0).destroyCache(DEFAULT_CACHE_NAME); ignite(0).getOrCreateCache(cfg); awaitPartitionMapExchange(); for (Runnable r : rs.values()) r.run(); U.sleep(10000); stopGrid(3); awaitPartitionMapExchange(); long topVer0 = grid(0).context().cache().context().exchange().readyAffinityVersion().topologyVersion(); long topVer1 = grid(1).context().cache().context().exchange().readyAffinityVersion().topologyVersion(); long topVer2 = grid(2).context().cache().context().exchange().readyAffinityVersion().topologyVersion(); stopGrid(4); awaitPartitionMapExchange(); assert grid(0).context().cache().context().exchange().readyAffinityVersion().topologyVersion() > topVer0; assert grid(1).context().cache().context().exchange().readyAffinityVersion().topologyVersion() > topVer1; assert grid(2).context().cache().context().exchange().readyAffinityVersion().topologyVersion() > topVer2; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void assertNameEquals(String[] names, Object[] metaMethods) { Set metaSet = new HashSet(); for (int i = 0; i < metaMethods.length; i++) { String[] strings = (String[]) metaMethods[i]; metaSet.add(strings[Inspector.MEMBER_NAME_IDX]); } XXX nameSet = new HashSet(Arrays.asList(names)); assertEquals(nameSet, metaSet); }', 'ground_truth': 'private void assertNameEquals(String[] names, Object[] metaMethods) { Set<String> metaSet = new TreeSet<>(); for (Object metaMethod : metaMethods) { String[] strings = (String[]) metaMethod; metaSet.add(strings[Inspector.MEMBER_NAME_IDX]); } Set<String> nameSet = new TreeSet<>(Arrays.asList(names)); assertEquals(nameSet, metaSet); }', 'output': 'private void assertNameEquals(String[] names, Object[] metaMethods) { Set metaSet = new HashSet(); for (int i = 0; i < metaMethods.length; i++) { String[] strings = (String[]) metaMethods[i]; metaSet.add(strings[Inspector.MEMBER_NAME_IDX]); } Set nameSet = new HashSet(Arrays.asList(names)); assertEquals(nameSet, metaSet); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPutxShouldNotTriggerLoadWithTx() throws Exception { GridCache<Integer, Integer> cache = cache(); try (IgniteTx tx = cache.txStart()) { assertTrue(cache.putx(1, 1)); assertTrue(cache.putx(2, 2, (IgnitePredicate)null)); tx.commit(); } assertEquals(0, loads.get()); }', 'ground_truth': 'public void testPutxShouldNotTriggerLoadWithTx() throws Exception { Cache<Integer, Integer> cache = cache(); try (IgniteTx tx = cache.txStart()) { assertTrue(cache.putx(1, 1)); assertTrue(cache.putx(2, 2, (IgnitePredicate)null)); tx.commit(); } assertEquals(0, loads.get()); }', 'output': 'public void testPutxShouldNotTriggerLoadWithTx() throws Exception { Cache<Integer, Integer> cache = cache(); try (IgniteTx tx = cache.txStart()) { assertTrue(cache.putx(1, 1)); assertTrue(cache.putx(2, 2, (IgnitePredicate)null)); tx.commit(); } assertEquals(0, loads.get()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPut() throws Exception { final GridCacheProjection<String, Integer> prj = cache().projection(kvFilter); prj.put("key", 1); prj.put("k", 2); assert prj.containsKey("key"); assert !prj.containsKey("k"); assertFlagException(new CAX() { @Override public void applyx() throws IgniteCheckedException { prj.flagsOn(LOCAL).put("key", 1); } }); assertFlagException(new CAX() { @Override public void applyx() throws IgniteCheckedException { prj.flagsOn(READ).put("key", 1); } }); }', 'ground_truth': 'public void testPut() throws Exception { final CacheProjection<String, Integer> prj = cache().projection(kvFilter); prj.put("key", 1); prj.put("k", 2); assert prj.containsKey("key"); assert !prj.containsKey("k"); assertFlagException(new CAX() { @Override public void applyx() throws IgniteCheckedException { prj.flagsOn(LOCAL).put("key", 1); } }); assertFlagException(new CAX() { @Override public void applyx() throws IgniteCheckedException { prj.flagsOn(READ).put("key", 1); } }); }', 'output': 'public void testPut() throws Exception { final CacheProjection<String, Integer> prj = cache().projection(kvFilter); prj.put("key", 1); prj.put("k", 2); assert prj.containsKey("key"); assert !prj.containsKey("k"); assertFlagException(new CAX() { @Override public void applyx() throws IgniteCheckedException { prj.flagsOn(LOCAL).put("key", 1); } }); assertFlagException(new CAX() { @Override public void applyx() throws IgniteCheckedException { prj.flagsOn(READ).put("key", 1); } }); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void blockingBeforeLookup(Class<?> type) { if (type == Object.class) { java.util.Set all; Object prev = null; try { prev = CookieSet.entryAllClassesMode(); Object ignoreResult = node.getCookie(Node.Cookie.class); } finally { all = CookieSet.exitAllClassesMode(prev); } XXX it = all.iterator(); while (it.hasNext()) { Class c = (Class) it.next(); updateLookupAsCookiesAreChanged(c); } if (!queriedCookieClasses.contains(Node.Cookie.class)) { updateLookupAsCookiesAreChanged(Node.Cookie.class); } } if (!queriedCookieClasses.contains(type)) { updateLookupAsCookiesAreChanged(type); } }', 'ground_truth': 'private void blockingBeforeLookup(Class<?> type) { if (type == Object.class) { java.util.Set all; Object prev = null; try { prev = CookieSet.entryAllClassesMode(); Object ignoreResult = node.getCookie(Node.Cookie.class); } finally { all = CookieSet.exitAllClassesMode(prev); } Iterator<Class> it = all.iterator(); while (it.hasNext()) { Class c = it.next(); updateLookupAsCookiesAreChanged(c); } if (!queriedCookieClasses.contains(Node.Cookie.class)) { updateLookupAsCookiesAreChanged(Node.Cookie.class); } } if (!queriedCookieClasses.contains(type)) { updateLookupAsCookiesAreChanged(type); } }', 'output': 'private void blockingBeforeLookup(Class<?> type) { if (type == Object.class) { java.util.Set all; Object prev = null; try { prev = CookieSet.entryAllClassesMode(); Object ignoreResult = node.getCookie(Node.Cookie.class); } finally { all = CookieSet.exitAllClassesMode(prev); } Iterator<Class> it = all.iterator(); while (it.hasNext()) { Class c = (Class) it.next(); updateLookupAsCookiesAreChanged(c); } if (!queriedCookieClasses.contains(Node.Cookie.class)) { updateLookupAsCookiesAreChanged(Node.Cookie.class); } } if (!queriedCookieClasses.contains(type)) { updateLookupAsCookiesAreChanged(type); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void handleStatusRequest() throws Exception { XXX sessionID = registerSession(); InstrumentedLocalSessions sessions = new InstrumentedLocalSessions(); sessions.start(); LocalSession session = sessions.prepareForTest(sessionID); Assert.assertEquals(PREPARED, session.getState()); sessions.sentMessages.clear(); sessions.handleStatusRequest(PARTICIPANT2, new StatusRequest(sessionID)); assertNoMessagesSent(sessions, PARTICIPANT1); assertMessagesSent(sessions, PARTICIPANT2, new StatusResponse(sessionID, PREPARED)); assertNoMessagesSent(sessions, PARTICIPANT3); } public void handleStatusRequestNoSession() throws Exception\n    { { InstrumentedLocalSessions sessions = new InstrumentedLocalSessions(); sessions.start(); sessions.sentMessages.clear(); XXX sessionID = UUIDGen.getTimeUUID(); sessions.handleStatusRequest(PARTICIPANT2, new StatusRequest(sessionID)); assertNoMessagesSent(sessions, PARTICIPANT1); assertMessagesSent(sessions, PARTICIPANT2, new StatusResponse(sessionID, FAILED)); assertNoMessagesSent(sessions, PARTICIPANT3); }', 'ground_truth': 'public void handleStatusRequest() throws Exception { TimeUUID sessionID = registerSession(); InstrumentedLocalSessions sessions = new InstrumentedLocalSessions(); sessions.start(); LocalSession session = sessions.prepareForTest(sessionID); Assert.assertEquals(PREPARED, session.getState()); sessions.sentMessages.clear(); sessions.handleStatusRequest(PARTICIPANT2, new StatusRequest(sessionID)); assertNoMessagesSent(sessions, PARTICIPANT1); assertMessagesSent(sessions, PARTICIPANT2, new StatusResponse(sessionID, PREPARED)); assertNoMessagesSent(sessions, PARTICIPANT3); }', 'output': 'public void handleStatusRequest() throws Exception { String sessionID = registerSession(); InstrumentedLocalSessions sessions = new InstrumentedLocalSessions(); sessions.start(); LocalSession session = sessions.prepareForTest(sessionID); Assert.assertEquals(PREPARED, session.getState()); sessions.sentMessages.clear(); sessions.handleStatusRequest(PARTICIPANT2, new StatusRequest(sessionID)); assertNoMessagesSent(sessions, PARTICIPANT1); assertMessagesSent(sessions, PARTICIPANT2, new StatusResponse(sessionID, PREPARED)); assertNoMessagesSent(sessions, PARTICIPANT3); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCompactionInfoToStringFormat() { UUID tableId = UUID.randomUUID(); XXX taskId = UUID.randomUUID(); ColumnFamilyStore cfs = MockSchema.newCFS(builder -> builder.id(TableId.fromUUID(tableId))); CompactionInfo compactionInfo = new CompactionInfo(cfs.metadata(), OperationType.COMPACTION, 0, 1000, taskId, new ArrayList<>()); Assertions.assertThat(compactionInfo.toString()) .isEqualTo("Compaction(%s, 0 / 1000 bytes)@%s(mockks, mockcf1)", taskId, tableId); }', 'ground_truth': 'public void testCompactionInfoToStringFormat() { UUID tableId = UUID.randomUUID(); TimeUUID taskId = nextTimeUUID(); ColumnFamilyStore cfs = MockSchema.newCFS(builder -> builder.id(TableId.fromUUID(tableId))); CompactionInfo compactionInfo = new CompactionInfo(cfs.metadata(), OperationType.COMPACTION, 0, 1000, taskId, new ArrayList<>()); Assertions.assertThat(compactionInfo.toString()) .isEqualTo("Compaction(%s, 0 / 1000 bytes)@%s(mockks, mockcf1)", taskId, tableId); }', 'output': 'public void testCompactionInfoToStringFormat() { UUID tableId = UUID.randomUUID(); UUID taskId = UUID.randomUUID(); ColumnFamilyStore cfs = MockSchema.newCFS(builder -> builder.id(TableId.fromUUID(tableId))); CompactionInfo compactionInfo = new CompactionInfo(cfs.metadata(), OperationType.COMPACTION, 0, 1000, taskId, new ArrayList<>()); Assertions.assertThat(compactionInfo.toString()) .isEqualTo("Compaction(%s, 0 / 1000 bytes)@%s(mockks, mockcf1)", taskId, tableId); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void whenPackageWithHasParent() throws Exception { final XXX applicationFeatureId = ApplicationFeatureId.newNamespace("com.mycompany"); final List<ApplicationFeatureId> parentIds = applicationFeatureId.getParentIds(); assertThat(parentIds, contains(ApplicationFeatureId.newNamespace("com"))); } public void whenPackageWithHasParents() throws Exception { final XXX applicationFeatureId = ApplicationFeatureId.newNamespace("com.mycompany.bish.bosh"); final List<ApplicationFeatureId> parentIds = applicationFeatureId.getParentIds(); assertThat(parentIds, contains( ApplicationFeatureId.newNamespace("com.mycompany.bish"), ApplicationFeatureId.newNamespace("com.mycompany"), ApplicationFeatureId.newNamespace("com") )); }', 'ground_truth': 'public void whenPackageWithHasParent() throws Exception { val applicationFeatureId = ApplicationFeatureId.newNamespace("com.mycompany"); final List<ApplicationFeatureId> parentIds = applicationFeatureId.getParentIds(); assertThat(parentIds, contains(ApplicationFeatureId.newNamespace("com"))); }', 'output': 'public void whenPackageWithHasParent() throws Exception { final ApplicationFeatureId applicationFeatureId = ApplicationFeatureId.newNamespace("com.mycompany"); final List<ApplicationFeatureId> parentIds = applicationFeatureId.getParentIds(); assertThat(parentIds, contains(ApplicationFeatureId.newNamespace("com"))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void getResourcesTest(Authentication authentication, String requestedUsername) throws Exception { final UserPrivilegeResourceProvider resourceProvider = new UserPrivilegeResourceProvider(); final UserDAO userDAO = createNiceMock(UserDAO.class); final GroupDAO groupDAO = createNiceMock(GroupDAO.class); final ClusterDAO clusterDAO = createNiceMock(ClusterDAO.class); final ViewInstanceDAO viewInstanceDAO = createNiceMock(ViewInstanceDAO.class); final UserEntity userEntity = createNiceMock(UserEntity.class); final PrincipalEntity principalEntity = createNiceMock(PrincipalEntity.class); final PrivilegeEntity privilegeEntity = createNiceMock(PrivilegeEntity.class); final PermissionEntity permissionEntity = createNiceMock(PermissionEntity.class); final PrincipalTypeEntity principalTypeEntity = createNiceMock(PrincipalTypeEntity.class); final ResourceEntity resourceEntity = createNiceMock(ResourceEntity.class); final ResourceTypeEntity resourceTypeEntity = createNiceMock(ResourceTypeEntity.class); final PrivilegeDAO privilegeDAO = createMock(PrivilegeDAO.class); final MemberDAO memberDAO = createMock(MemberDAO.class); final XXX users = new TestUsers(); users.setPrivilegeDAO(privilegeDAO); users.setMemberDAO(memberDAO); List<PrincipalEntity> userPrincipals = new LinkedList<>(); userPrincipals.add(principalEntity); expect(privilegeDAO.findAllByPrincipal(userPrincipals)). andReturn(Collections.singletonList(privilegeEntity)) .atLeastOnce(); expect(memberDAO.findAllMembersByUser(userEntity)). andReturn(Collections.<MemberEntity>emptyList()) .atLeastOnce(); expect(userDAO.findLocalUserByName(requestedUsername)).andReturn(userEntity).anyTimes(); expect(userDAO.findAll()).andReturn(Collections.<UserEntity>emptyList()).anyTimes(); expect(userEntity.getPrincipal()).andReturn(principalEntity).anyTimes(); expect(userEntity.getMemberEntities()).andReturn(Collections.<MemberEntity>emptySet()).anyTimes(); expect(privilegeEntity.getPermission()).andReturn(permissionEntity).anyTimes(); expect(privilegeEntity.getPrincipal()).andReturn(principalEntity).anyTimes(); expect(principalEntity.getPrincipalType()).andReturn(principalTypeEntity).anyTimes(); expect(principalTypeEntity.getName()).andReturn(PrincipalTypeEntity.USER_PRINCIPAL_TYPE_NAME).anyTimes(); expect(principalEntity.getPrivileges()).andReturn(new HashSet<PrivilegeEntity>() { { add(privilegeEntity); } }).anyTimes(); expect(userDAO.findUserByPrincipal(anyObject(PrincipalEntity.class))).andReturn(userEntity).anyTimes(); expect(userEntity.getUserName()).andReturn(requestedUsername).anyTimes(); expect(privilegeEntity.getResource()).andReturn(resourceEntity).anyTimes(); expect(resourceEntity.getResourceType()).andReturn(resourceTypeEntity).anyTimes(); expect(resourceTypeEntity.getName()).andReturn(ResourceType.AMBARI.name()); expect(viewInstanceDAO.findAll()).andReturn(new ArrayList<ViewInstanceEntity>()).anyTimes(); replayAll(); UserPrivilegeResourceProvider.init(userDAO, clusterDAO, groupDAO, viewInstanceDAO, users); final Set<String> propertyIds = new HashSet<>(); propertyIds.add(UserPrivilegeResourceProvider.PRIVILEGE_USER_NAME_PROPERTY_ID); final Predicate predicate = new PredicateBuilder() .property(UserPrivilegeResourceProvider.PRIVILEGE_USER_NAME_PROPERTY_ID) .equals(requestedUsername) .toPredicate(); Request request = PropertyHelper.getReadRequest(propertyIds); SecurityContextHolder.getContext().setAuthentication(authentication); Set<Resource> resources = resourceProvider.getResources(request, predicate); Assert.assertEquals(1, resources.size()); for (Resource resource : resources) { String userName = (String) resource.getPropertyValue(UserPrivilegeResourceProvider.PRIVILEGE_USER_NAME_PROPERTY_ID); Assert.assertEquals(requestedUsername, userName); } verifyAll(); }', 'ground_truth': 'private void getResourcesTest(Authentication authentication, String requestedUsername) throws Exception { Injector injector = createInjector(); final UserPrivilegeResourceProvider resourceProvider = new UserPrivilegeResourceProvider(); final UserDAO userDAO = injector.getInstance(UserDAO.class); final GroupDAO groupDAO = injector.getInstance(GroupDAO.class); final ClusterDAO clusterDAO = injector.getInstance(ClusterDAO.class); final ViewInstanceDAO viewInstanceDAO = injector.getInstance(ViewInstanceDAO.class); final PrivilegeDAO privilegeDAO = injector.getInstance(PrivilegeDAO.class); final MemberDAO memberDAO = injector.getInstance(MemberDAO.class); final UserEntity userEntity = createNiceMock(UserEntity.class); final PrincipalEntity principalEntity = createNiceMock(PrincipalEntity.class); final PrivilegeEntity privilegeEntity = createNiceMock(PrivilegeEntity.class); final PermissionEntity permissionEntity = createNiceMock(PermissionEntity.class); final PrincipalTypeEntity principalTypeEntity = createNiceMock(PrincipalTypeEntity.class); final ResourceEntity resourceEntity = createNiceMock(ResourceEntity.class); final ResourceTypeEntity resourceTypeEntity = createNiceMock(ResourceTypeEntity.class); final Users users = injector.getInstance(Users.class); List<PrincipalEntity> userPrincipals = new LinkedList<>(); userPrincipals.add(principalEntity); expect(privilegeDAO.findAllByPrincipal(userPrincipals)). andReturn(Collections.singletonList(privilegeEntity)) .atLeastOnce(); expect(memberDAO.findAllMembersByUser(userEntity)). andReturn(Collections.<MemberEntity>emptyList()) .atLeastOnce(); expect(userDAO.findUserByName(requestedUsername)).andReturn(userEntity).anyTimes(); expect(userDAO.findAll()).andReturn(Collections.<UserEntity>emptyList()).anyTimes(); expect(userEntity.getPrincipal()).andReturn(principalEntity).anyTimes(); expect(userEntity.getMemberEntities()).andReturn(Collections.<MemberEntity>emptySet()).anyTimes(); expect(privilegeEntity.getPermission()).andReturn(permissionEntity).anyTimes(); expect(privilegeEntity.getPrincipal()).andReturn(principalEntity).anyTimes(); expect(principalEntity.getPrincipalType()).andReturn(principalTypeEntity).anyTimes(); expect(principalTypeEntity.getName()).andReturn(PrincipalTypeEntity.USER_PRINCIPAL_TYPE_NAME).anyTimes(); expect(principalEntity.getPrivileges()).andReturn(new HashSet<PrivilegeEntity>() { { add(privilegeEntity); } }).anyTimes(); expect(userDAO.findUserByPrincipal(anyObject(PrincipalEntity.class))).andReturn(userEntity).anyTimes(); expect(userEntity.getUserName()).andReturn(requestedUsername).anyTimes(); expect(privilegeEntity.getResource()).andReturn(resourceEntity).anyTimes(); expect(resourceEntity.getResourceType()).andReturn(resourceTypeEntity).anyTimes(); expect(resourceTypeEntity.getName()).andReturn(ResourceType.AMBARI.name()); expect(viewInstanceDAO.findAll()).andReturn(new ArrayList<ViewInstanceEntity>()).anyTimes(); replayAll(); UserPrivilegeResourceProvider.init(userDAO, clusterDAO, groupDAO, viewInstanceDAO, users); final Set<String> propertyIds = new HashSet<>(); propertyIds.add(UserPrivilegeResourceProvider.PRIVILEGE_USER_NAME_PROPERTY_ID); final Predicate predicate = new PredicateBuilder() .property(UserPrivilegeResourceProvider.PRIVILEGE_USER_NAME_PROPERTY_ID) .equals(requestedUsername) .toPredicate(); Request request = PropertyHelper.getReadRequest(propertyIds); SecurityContextHolder.getContext().setAuthentication(authentication); Set<Resource> resources = resourceProvider.getResources(request, predicate); Assert.assertEquals(1, resources.size()); for (Resource resource : resources) { String userName = (String) resource.getPropertyValue(UserPrivilegeResourceProvider.PRIVILEGE_USER_NAME_PROPERTY_ID); Assert.assertEquals(requestedUsername, userName); } verifyAll(); }', 'output': 'private void getResourcesTest(Authentication authentication, String requestedUsername) throws Exception { final UserPrivilegeResourceProvider resourceProvider = new UserPrivilegeResourceProvider(); final UserDAO userDAO = createNiceMock(UserDAO.class); final GroupDAO groupDAO = createNiceMock(GroupDAO.class); final ClusterDAO clusterDAO = createNiceMock(ClusterDAO.class); final ViewInstanceDAO viewInstanceDAO = createNiceMock(ViewInstanceDAO.class); final UserEntity userEntity = createNiceMock(UserEntity.class); final PrincipalEntity principalEntity = createNiceMock(PrincipalEntity.class); final PrivilegeEntity privilegeEntity = createNiceMock(PrivilegeEntity.class); final PermissionEntity permissionEntity = createNiceMock(PermissionEntity.class); final PrincipalTypeEntity principalTypeEntity = createNiceMock(PrincipalTypeEntity.class); final ResourceEntity resourceEntity = createNiceMock(ResourceEntity.class); final ResourceTypeEntity resourceTypeEntity = createNiceMock(ResourceTypeEntity.class); final PrivilegeDAO privilegeDAO = createMock(PrivilegeDAO.class); final MemberDAO memberDAO = createMock(MemberDAO.class); final TestUsers users = new TestUsers(); users.setPrivilegeDAO(privilegeDAO); users.setMemberDAO(memberDAO); List<PrincipalEntity> userPrincipals = new LinkedList<>(); userPrincipals.add(principalEntity); expect(privilegeDAO.findAllByPrincipal(userPrincipals)). andReturn(Collections.singletonList(privilegeEntity)) .atLeastOnce(); expect(memberDAO.findAllMembersByUser(userEntity)). andReturn(Collections.<MemberEntity>emptyList()) .atLeastOnce(); expect(userDAO.findLocalUserByName(requestedUsername)).andReturn(userEntity).anyTimes(); expect(userDAO.findAll()).andReturn(Collections.<UserEntity>emptyList()).anyTimes(); expect(userEntity.getPrincipal()).andReturn(principalEntity).anyTimes(); expect(userEntity.getMemberEntities()).andReturn(Collections.<MemberEntity>emptySet()).any'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testWriteDisplayData() { XXX write = AvroIO.write(GenericClass.class) .to("foo") .withShardNameTemplate("-SS-of-NN-") .withSuffix("bar") .withNumShards(100) .withCodec(CodecFactory.snappyCodec()); DisplayData displayData = DisplayData.from(write); assertThat(displayData, hasDisplayItem("filePrefix", "foo")); assertThat(displayData, hasDisplayItem("shardNameTemplate", "-SS-of-NN-")); assertThat(displayData, hasDisplayItem("fileSuffix", "bar")); assertThat(displayData, hasDisplayItem("schema", GenericClass.class)); assertThat(displayData, hasDisplayItem("numShards", 100)); assertThat(displayData, hasDisplayItem("codec", CodecFactory.snappyCodec().toString())); }', 'ground_truth': 'public void testWriteDisplayData() { AvroIO.Write<GenericClass> write = AvroIO.write(GenericClass.class) .to("foo") .withShardNameTemplate("-SS-of-NN-") .withSuffix("bar") .withNumShards(100) .withCodec(CodecFactory.snappyCodec()); DisplayData displayData = DisplayData.from(write); assertThat(displayData, hasDisplayItem("filePrefix", "foo")); assertThat(displayData, hasDisplayItem("shardNameTemplate", "-SS-of-NN-")); assertThat(displayData, hasDisplayItem("fileSuffix", "bar")); assertThat(displayData, hasDisplayItem("schema", GenericClass.class)); assertThat(displayData, hasDisplayItem("numShards", 100)); assertThat(displayData, hasDisplayItem("codec", CodecFactory.snappyCodec().toString())); }', 'output': 'public void testWriteDisplayData() { DataWriter<GenericClass> write = AvroIO.write(GenericClass.class) .to("foo") .withShardNameTemplate("-SS-of-NN-") .withSuffix("bar") .withNumShards(100) .withCodec(CodecFactory.snappyCodec()); DisplayData displayData = DisplayData.from(write); assertThat(displayData, hasDisplayItem("filePrefix", "foo")); assertThat(displayData, hasDisplayItem("shardNameTemplate", "-SS-of-NN-")); assertThat(displayData, hasDisplayItem("fileSuffix", "bar")); assertThat(displayData, hasDisplayItem("schema", GenericClass.class)); assertThat(displayData, hasDisplayItem("numShards", 100)); assertThat(displayData, hasDisplayItem("codec", CodecFactory.snappyCodec().toString())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private XXX request(RepositoryVersionEntity targetRepo) { XXX request = new PrereqCheckRequest(CLUSTER_NAME); request.setSourceStackId(SOURCE_STACK); request.setTargetRepositoryVersion(targetRepo); return request; }', 'ground_truth': 'private UpgradeCheckRequest request(RepositoryVersionEntity targetRepo) { m_checkHelper.m_repositoryVersionDAO = repositoryVersionDAO; expect(repositoryVersionDAO.findByPK(1L)).andReturn(targetRepo).anyTimes(); RepositoryVersion repositoryVersion = createNiceMock(RepositoryVersion.class); expect(repositoryVersion.getId()).andReturn(1L).anyTimes(); expect(repositoryVersion.getRepositoryType()).andReturn(RepositoryType.STANDARD).anyTimes(); expect(repositoryVersion.getStackId()).andReturn(SOURCE_STACK.getStackId()).anyTimes(); ClusterInformation clusterInformation = new ClusterInformation(CLUSTER_NAME, false, null, null); UpgradeCheckRequest request = new UpgradeCheckRequest(clusterInformation, UpgradeType.ROLLING, repositoryVersion, null); return request; }', 'output': 'private PrereqCheckRequest request(RepositoryVersionEntity targetRepo) { PrereqCheckRequest request = new PrereqCheckRequest(CLUSTER_NAME); request.setSourceStackId(SOURCE_STACK); request.setTargetRepositoryVersion(targetRepo); return request; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void executeBatches(List<BatchRequest> batches) throws IOException { ListeningExecutorService executor = MoreExecutors.listeningDecorator( MoreExecutors.getExitingExecutorService( new ThreadPoolExecutor( MAX_CONCURRENT_BATCHES, MAX_CONCURRENT_BATCHES, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<>()))); XXX futures = new LinkedList<>(); for (final BatchRequest batch : batches) { futures.add( executor.submit( () -> { batch.execute(); return null; })); } try { Futures.allAsList(futures).get(); } catch (InterruptedException e) { Thread.currentThread().interrupt(); throw new IOException("Interrupted while executing batch GCS request", e); } catch (ExecutionException e) { if (e.getCause() instanceof FileNotFoundException) { throw (FileNotFoundException) e.getCause(); } throw new IOException("Error executing batch GCS request", e); } finally { executor.shutdown(); } }', 'ground_truth': 'private static void executeBatches(List<BatchRequest> batches) throws IOException { ExecutorService executor = MoreExecutors.listeningDecorator( MoreExecutors.getExitingExecutorService( new ThreadPoolExecutor( MAX_CONCURRENT_BATCHES, MAX_CONCURRENT_BATCHES, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<>()))); List<CompletionStage<Void>> futures = new LinkedList<>(); for (final BatchRequest batch : batches) { futures.add(MoreFutures.runAsync( () -> { batch.execute(); }, executor)); } try { MoreFutures.get(MoreFutures.allAsList(futures)); } catch (InterruptedException e) { Thread.currentThread().interrupt(); throw new IOException("Interrupted while executing batch GCS request", e); } catch (ExecutionException e) { if (e.getCause() instanceof FileNotFoundException) { throw (FileNotFoundException) e.getCause(); } throw new IOException("Error executing batch GCS request", e); } finally { executor.shutdown(); } }', 'output': 'private static void executeBatches(List<BatchRequest> batches) throws IOException { ListeningExecutorService executor = MoreExecutors.listeningDecorator( MoreExecutors.getExitingExecutorService( new ThreadPoolExecutor( MAX_CONCURRENT_BATCHES, MAX_CONCURRENT_BATCHES, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<>()))); List<Future<?>> futures = new ArrayList<>(); for (final BatchRequest batch : batches) { futures.add( executor.submit( () -> { batch.execute(); return null; })); } try { Futures.allAsList(futures).get(); } catch (InterruptedException e) { Thread.currentThread().interrupt(); throw new IOException("Interrupted while executing batch GCS request", e); } catch (ExecutionException e) { if (e.getCause() instanceof FileNotFoundException) { throw (FileNotFoundException) e.getCause(); } throw new IOException("Error executing batch GCS request", e); } finally { executor.shutdown(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void updateLocal(int p, UUID nodeId, GridDhtPartitionState state, long updateSeq) { assert lock.isWriteLockedByCurrentThread(); assert nodeId.equals(cctx.nodeId()); ClusterNode oldest = CU.oldestAliveCacheServerNode(cctx.shared(), topVer); assert oldest != null; if (oldest.id().equals(cctx.nodeId())) { long seq = node2part.updateSequence(); if (seq != updateSeq) { if (seq > updateSeq) { if (this.updateSeq.get() < seq) { boolean b = this.updateSeq.compareAndSet(this.updateSeq.get(), seq + 1); assert b : "Invalid update sequence [updateSeq=" + updateSeq + ", seq=" + seq + ", curUpdateSeq=" + this.updateSeq.get() + ", node2part=" + node2part.toFullString() + \']\'; updateSeq = seq + 1; } else updateSeq = seq; } node2part.updateSequence(updateSeq); } } XXX map = node2part.get(nodeId); if (map == null) node2part.put(nodeId, map = new GridDhtPartitionMap(nodeId, updateSeq, Collections.<Integer, GridDhtPartitionState>emptyMap(), false)); map.updateSequence(updateSeq); map.put(p, state); Set<UUID> ids = part2node.get(p); if (ids == null) part2node.put(p, ids = U.newHashSet(3)); ids.add(nodeId); }', 'ground_truth': 'private void updateLocal(int p, UUID nodeId, GridDhtPartitionState state, long updateSeq) { assert lock.isWriteLockedByCurrentThread(); assert nodeId.equals(cctx.nodeId()); ClusterNode oldest = CU.oldestAliveCacheServerNode(cctx.shared(), topVer); assert oldest != null; if (oldest.id().equals(cctx.nodeId())) { long seq = node2part.updateSequence(); if (seq != updateSeq) { if (seq > updateSeq) { if (this.updateSeq.get() < seq) { boolean b = this.updateSeq.compareAndSet(this.updateSeq.get(), seq + 1); assert b : "Invalid update sequence [updateSeq=" + updateSeq + ", seq=" + seq + ", curUpdateSeq=" + this.updateSeq.get() + ", node2part=" + node2part.toFullString() + \']\'; updateSeq = seq + 1; } else updateSeq = seq; } node2part.updateSequence(updateSeq); } } GridDhtPartitionMap2 map = node2part.get(nodeId); if (map == null) node2part.put(nodeId, map = new GridDhtPartitionMap2(nodeId, updateSeq, topVer, Collections.<Integer, GridDhtPartitionState>emptyMap(), false)); map.updateSequence(updateSeq); map.put(p, state); Set<UUID> ids = part2node.get(p); if (ids == null) part2node.put(p, ids = U.newHashSet(3)); ids.add(nodeId); }', 'output': 'private void updateLocal(int p, UUID nodeId, GridDhtPartitionState state, long updateSeq) { assert lock.isWriteLockedByCurrentThread(); assert nodeId.equals(cctx.nodeId()); ClusterNode oldest = CU.oldestAliveCacheServerNode(cctx.shared(), topVer); assert oldest != null; if (oldest.id().equals(cctx.nodeId())) { long seq = node2part.updateSequence(); if (seq != updateSeq) { if (seq > updateSeq) { if (this.updateSeq.get() < seq) { boolean b = this.updateSeq.compareAndSet(this.updateSeq.get(), seq + 1); assert b : "Invalid update sequence [updateSeq=" + updateSeq + ", seq=" + seq + ", curUpdateSeq=" + this.updateSeq.get() + ", node2part=" + node2part.toFullString() + \']\'; updateSeq = seq + 1; } else updateSeq = seq; } node2part.updateSequence(updateSeq); } GridDhtPartitionMap map = node2part.get(nodeId); if (map == null) node2part.put(nodeId, map = new GridDhtPartitionMap(nodeId, updateSeq, Collections.<Integer, GridDhtPartitionState>emptyMap(), false)); map.updateSequence(updateSeq); map.put(p, state); Set<UUID> ids = part2node.get(p); if (ids == null) part2node.put(p, ids = U.newHashSet(3)); ids.add(nodeId); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testStopCancel() throws Exception { XXX cfg = new GridConfiguration(); cfg.setRestEnabled(false); Ignite ignite = G.start(cfg); ignite.compute().execute(TestTask.class, null); G.stop(true); }', 'ground_truth': 'public void testStopCancel() throws Exception { IgniteConfiguration cfg = new IgniteConfiguration(); cfg.setRestEnabled(false); Ignite ignite = G.start(cfg); ignite.compute().execute(TestTask.class, null); G.stop(true); }', 'output': 'public void testStopCancel() throws Exception { IgniteConfiguration cfg = new IgniteConfiguration(); cfg.setRestEnabled(false); Ignite ignite = G.start(cfg); ignite.compute().execute(TestTask.class, null); G.stop(true); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCreate() { MessageFactory[] factories = {new TestMessageFactoryPovider(), new TestMessageFactory()}; XXX msgFactory = new IgniteMessageFactoryImpl(factories); Message msg; msg = msgFactory.create(TEST_MSG_1_TYPE); assertTrue(msg instanceof TestMessage1); msg = msgFactory.create(TEST_MSG_2_TYPE); assertTrue(msg instanceof TestMessage2); msg = msgFactory.create(TEST_MSG_2_TYPE); assertTrue(msg instanceof TestMessage2); } public void testCreate_UnknownMessageType() { MessageFactory[] factories = {new TestMessageFactoryPovider(), new TestMessageFactory()}; XXX msgFactory = new IgniteMessageFactoryImpl(factories); msgFactory.create(UNKNOWN_MSG_TYPE); }', 'ground_truth': 'public void testCreate() { MessageFactory[] factories = {new TestMessageFactoryPovider(), new TestMessageFactory()}; IgniteMessageFactoryImpl msgFactory = new IgniteMessageFactoryImpl(factories); Message msg; msg = msgFactory.create(TEST_MSG_1_TYPE); assertTrue(msg instanceof TestMessage1); msg = msgFactory.create(TEST_MSG_2_TYPE); assertTrue(msg instanceof TestMessage2); msg = msgFactory.create(TEST_MSG_42_TYPE); assertTrue(msg instanceof TestMessage42); short[] directTypes = msgFactory.registeredDirectTypes(); assertArrayEquals(directTypes, new short[] {TEST_MSG_1_TYPE, TEST_MSG_2_TYPE, TEST_MSG_42_TYPE}); }', 'output': 'public void testCreate() { MessageFactory[] factories = {new TestMessageFactoryPovider(), new TestMessageFactory()}; IgniteMessageFactory msgFactory = new IgniteMessageFactoryImpl(factories); Message msg; msg = msgFactory.create(TEST_MSG_1_TYPE); assertTrue(msg instanceof TestMessage1); msg = msgFactory.create(TEST_MSG_2_TYPE); assertTrue(msg instanceof TestMessage2); msg = msgFactory.create(TEST_MSG_2_TYPE); assertTrue(msg instanceof TestMessage2); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void printHeaders(JTextPane pane, JSONObject headers, StyledDocument doc, Style boldStyle, Style defaultStyle) throws BadLocationException { assert headers != null; XXX keys = new TreeSet(new Comparator<Object>() { @Override public int compare(Object o1, Object o2) { return ((String)o1).compareToIgnoreCase((String)o2); } }); keys.addAll(headers.keySet()); for (Object oo : keys) { String key = (String)oo; doc.insertString(doc.getLength(), key+": ", boldStyle); String value = (String)headers.get(key); doc.insertString(doc.getLength(), value+"\\n", defaultStyle); } }', 'ground_truth': 'private void printHeaders(JTextPane pane, JSONObject headers, StyledDocument doc, Style boldStyle, Style defaultStyle) throws BadLocationException { assert headers != null; Set<String> keys = new TreeSet<>(new Comparator<String>() { @Override public int compare(String o1, String o2) { return o1.compareToIgnoreCase(o2); } }); keys.addAll(headers.keySet()); for (String key : keys) { doc.insertString(doc.getLength(), key+": ", boldStyle); String value = (String)headers.get(key); doc.insertString(doc.getLength(), value+"\\n", defaultStyle); } }', 'output': 'private void printHeaders(JTextPane pane, JSONObject headers, StyledDocument doc, Style boldStyle, Style defaultStyle) throws BadLocationException { assert headers != null; Set<String> keys = new TreeSet(new Comparator<Object>() { @Override public int compare(Object o1, Object o2) { return ((String)o1).compareToIgnoreCase((String)o2); } }); keys.addAll(headers.keySet()); for (String key : keys) { doc.insertString(doc.getLength(), key+": ", boldStyle); String value = (String)headers.get(key); doc.insertString(doc.getLength(), value+"\\n", defaultStyle); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testNullPointerExceptionUponDataStreamerClosing() throws Exception { try { startGrids(5); final CyclicBarrier barrier = new CyclicBarrier(2); multithreadedAsync(new Callable<Object>() { @Override public Object call() throws Exception { U.awaitQuiet(barrier); G.stopAll(true); return null; } }, 1); Ignite g4 = grid(4); IgniteDataStreamer<Object, Object> dataLdr = g4.dataStreamer(null); dataLdr.perNodeBufferSize(32); for (int i = 0; i < 100000; i += 2) { dataLdr.addData(i, i); dataLdr.removeData(i + 1); } U.awaitQuiet(barrier); info("Closing data streamer."); try { dataLdr.close(true); } catch (XXX ignore) { } } finally { G.stopAll(true); } }', 'ground_truth': 'public void testNullPointerExceptionUponDataStreamerClosing() throws Exception { startGrids(5); final CyclicBarrier barrier = new CyclicBarrier(2); multithreadedAsync(new Callable<Object>() { @Override public Object call() throws Exception { U.awaitQuiet(barrier); G.stopAll(true); return null; } }, 1); Ignite g4 = grid(4); IgniteDataStreamer<Object, Object> dataLdr = g4.dataStreamer(null); dataLdr.perNodeBufferSize(32); for (int i = 0; i < 100000; i += 2) { dataLdr.addData(i, i); dataLdr.removeData(i + 1); } U.awaitQuiet(barrier); info("Closing data streamer."); try { dataLdr.close(true); } catch (CacheException | IllegalStateException ignore) { } }', 'output': 'public void testNullPointerExceptionUponDataStreamerClosing() throws Exception { try { startGrids(5); final CyclicBarrier barrier = new CyclicBarrier(2); multithreadedAsync(new Callable<Object>() { @Override public Object call() throws Exception { U.awaitQuiet(barrier); G.stopAll(true); return null; } }, 1); Ignite g4 = grid(4); IgniteDataStreamer<Object, Object> dataLdr = g4.dataStreamer(null); dataLdr.perNodeBufferSize(32); for (int i = 0; i < 100000; i += 2) { dataLdr.addData(i, i); dataLdr.removeData(i + 1); } U.awaitQuiet(barrier); info("Closing data streamer."); try { dataLdr.close(true); } catch (NullPointerException ignore) { } } finally { G.stopAll(true); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void useApplicableMaterializations() { final XXX config = context.unwrap(OptiqConnectionConfig.class); if (config == null || !config.materializationsEnabled()) { return; } DirectedGraph<List<String>, DefaultEdge> usesGraph = DefaultDirectedGraph.create(); for (RelOptMaterialization materialization : materializations) { if (materialization.table != null) { for (RelOptTable usedTable : findTables(materialization.queryRel)) { usesGraph.addVertex( materialization.table.getQualifiedName()); usesGraph.addVertex( usedTable.getQualifiedName()); usesGraph.addEdge( materialization.table.getQualifiedName(), usedTable.getQualifiedName()); } } } final Graphs.FrozenGraph<List<String>, DefaultEdge> frozenGraph = Graphs.makeImmutable(usesGraph); final Set<RelOptTable> queryTables = findTables(originalRoot); for (RelOptMaterialization materialization : materializations) { if (materialization.table != null) { if (usesTable(materialization.table, queryTables, frozenGraph)) { useMaterialization(materialization); } } } final List<Pair<RelOptLattice, RelNode>> latticeUses = Lists.newArrayList(); final Set<List<String>> queryTableNames = Sets.newHashSet(Iterables.transform(queryTables, GET_QUALIFIED_NAME)); final Supplier<RelNode> leafJoinRoot = Suppliers.memoize( new Supplier<RelNode>() { public RelNode get() { return RelOptMaterialization.toLeafJoinForm(originalRoot); } }); for (RelOptLattice lattice : latticeByName.values()) { if (queryTableNames.contains(lattice.rootTable().getQualifiedName())) { RelNode rel2 = lattice.rewrite(leafJoinRoot.get()); if (rel2 != null) { if (OptiqPrepareImpl.DEBUG) { System.out.println("use lattice:\\n" + RelOptUtil.toString(rel2)); } latticeUses.add(Pair.of(lattice, rel2)); } } } if (!latticeUses.isEmpty()) { useLattice(latticeUses.get(0).left, latticeUses.get(0).right); } }', 'ground_truth': 'private void useApplicableMaterializations() { final CalciteConnectionConfig config = context.unwrap(CalciteConnectionConfig.class); if (config == null || !config.materializationsEnabled()) { return; } DirectedGraph<List<String>, DefaultEdge> usesGraph = DefaultDirectedGraph.create(); for (RelOptMaterialization materialization : materializations) { if (materialization.table != null) { for (RelOptTable usedTable : findTables(materialization.queryRel)) { usesGraph.addVertex( materialization.table.getQualifiedName()); usesGraph.addVertex( usedTable.getQualifiedName()); usesGraph.addEdge( materialization.table.getQualifiedName(), usedTable.getQualifiedName()); } } } final Graphs.FrozenGraph<List<String>, DefaultEdge> frozenGraph = Graphs.makeImmutable(usesGraph); final Set<RelOptTable> queryTables = findTables(originalRoot); for (RelOptMaterialization materialization : materializations) { if (materialization.table != null) { if (usesTable(materialization.table, queryTables, frozenGraph)) { useMaterialization(materialization); } } } final List<Pair<RelOptLattice, RelNode>> latticeUses = Lists.newArrayList(); final Set<List<String>> queryTableNames = Sets.newHashSet(Iterables.transform(queryTables, GET_QUALIFIED_NAME)); final Supplier<RelNode> leafJoinRoot = Suppliers.memoize( new Supplier<RelNode>() { public RelNode get() { return RelOptMaterialization.toLeafJoinForm(originalRoot); } }); for (RelOptLattice lattice : latticeByName.values()) { if (queryTableNames.contains(lattice.rootTable().getQualifiedName())) { RelNode rel2 = lattice.rewrite(leafJoinRoot.get()); if (rel2 != null) { if (CalcitePrepareImpl.DEBUG) { System.out.println("use lattice:\\n" + RelOptUtil.toString(rel2)); } latticeUses.add(Pair.of(lattice, rel2)); } } } if (!latticeUses.isEmpty()) { useLattice(latticeUses.get(0).left, latticeUses.get(0).right); } }', 'output': 'private void useApplicableMaterializations() { final OptiqConnectionConfig config = context.unwrap(OptiqConnectionConfig.class); if (config == null || !config.materializationsEnabled()) { return; } DirectedGraph<List<String>, DefaultEdge> usesGraph = DefaultDirectedGraph.create(); for (RelOptMaterialization materialization : materializations) { if (materialization.table != null) { for (RelOptTable usedTable : findTables(materialization.queryRel)) { usesGraph.addVertex( materialization.table.getQualifiedName()); usesGraph.addVertex( usedTable.getQualifiedName()); usesGraph.addEdge( materialization.table.getQualifiedName(), usedTable.getQualifiedName()); } } } final Graphs.FrozenGraph<List<String>, DefaultEdge> frozenGraph = Graphs.makeImmutable(usesGraph); final Set<RelOptTable> queryTables = findTables(originalRoot); for (RelOptMaterialization materialization : materializations) { if (materialization.table != null) { if (usesTable(materialization.table, queryTables, frozenGraph)) { useMaterialization(materialization); } } } final List<Pair<RelOptLattice, RelNode>> latticeUses = Lists.newArrayList(); final Set<List<String>> queryTableNames = Sets.newHashSet(Iterables.transform(queryTables, GET_QUALIFIED_NAME)); final Supplier<RelNode> leafJoinRoot = Suppliers.memoize( new Supplier<RelNode>() { public RelNode get() { return RelOptMaterialization.toLeafJoinForm(originalRoot); } } ); for (RelOptLattice lattice : latticeByName.values()) { if (queryTableNames.contains(lattice.rootTable().getQualifiedName())) { RelNode rel2 = lattice.rewrite(leafJoinRoot.get()); if (rel2 != null) { if (OptiqPrepareImpl.DEBUG) { System.out.println("use lattice:\\n" + RelOptUtil.toString(rel2)); } latticeUses.add(Pair.of(lattice, rel2)); } } } if (!latticeUses.isEmpty()) { useLattice(latticeUses.get(0).left, latticeUses'}
{'instruction': 'Please refactor the following code:', 'input': 'public void visit(JsonRoot root) { final Pair<String, Schema> pair = Pair.<String, Schema>of(null, connection.getRootSchema()); push(schemaStack, pair); for (JsonSchema schema : root.schemas) { schema.accept(this); } pop(schemaStack, pair); if (root.defaultSchema != null) { try { connection.setSchema(root.defaultSchema); } catch (SQLException e) { throw new RuntimeException(e); } } }', 'ground_truth': 'public void visit(JsonRoot root) { final Pair<String, SchemaPlus> pair = Pair.of(null, connection.getRootSchema()); push(schemaStack, pair); for (JsonSchema schema : root.schemas) { schema.accept(this); } pop(schemaStack, pair); if (root.defaultSchema != null) { try { connection.setSchema(root.defaultSchema); } catch (SQLException e) { throw new RuntimeException(e); } } }', 'output': 'public void visit(JsonRoot root) { final Pair<String, Schema> pair = Pair.<String, Schema>of(null, connection.getRootSchema()); push(schemaStack, pair); for (JsonSchema schema : root.schemas) { schema.accept(this); } pop(schemaStack, pair); if (root.defaultSchema != null) { try { connection.setSchema(root.defaultSchema); } catch (SQLException e) { throw new RuntimeException(e); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAffinityWithPut() throws Exception { waitTopologyUpdate(); Ignite mg = grid(0); GridCache<Integer, String> mc = mg.cache(null); int keyCnt = 10; printAffinity(mg, keyCnt); info("Registering event listener..."); compute(mg.cluster().forRemotes()).run(new ListenerJob(keyCnt, mg.name())); for (int i = 0; i < keyCnt; i++) { if (failFlag.get()) fail("testAffinityWithPut failed."); info("Before putting key [key=" + i + ", grid=" + mg.name() + \']\'); mc.putx(i, Integer.toString(i)); if (failFlag.get()) fail("testAffinityWithPut failed."); } Thread.sleep(1000); if (failFlag.get()) fail("testAffinityWithPut failed."); }', 'ground_truth': 'public void testAffinityWithPut() throws Exception { waitTopologyUpdate(); Ignite mg = grid(0); IgniteCache<Integer, String> mc = mg.jcache(null); int keyCnt = 10; printAffinity(mg, keyCnt); info("Registering event listener..."); compute(mg.cluster().forRemotes()).run(new ListenerJob(keyCnt, mg.name())); for (int i = 0; i < keyCnt; i++) { if (failFlag.get()) fail("testAffinityWithPut failed."); info("Before putting key [key=" + i + ", grid=" + mg.name() + \']\'); mc.put(i, Integer.toString(i)); if (failFlag.get()) fail("testAffinityWithPut failed."); } Thread.sleep(1000); if (failFlag.get()) fail("testAffinityWithPut failed."); }', 'output': 'public void testAffinityWithPut() throws Exception { waitTopologyUpdate(); Ignite mg = grid(0); Cache<Integer, String> mc = mg.cache(null); int keyCnt = 10; printAffinity(mg, keyCnt); info("Registering event listener..."); compute(mg.cluster().forRemotes()).run(new ListenerJob(keyCnt, mg.name())); for (int i = 0; i < keyCnt; i++) { if (failFlag.get()) fail("testAffinityWithPut failed."); info("Before putting key [key=" + i + ", grid=" + mg.name() + \']\'); mc.putx(i, Integer.toString(i)); if (failFlag.get()) fail("testAffinityWithPut failed."); } Thread.sleep(1000); if (failFlag.get()) fail("testAffinityWithPut failed."); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void onElement(WindowedValue<InputT> value) throws Exception { Instant minimumAllowedTimestamp = timerManager.currentWatermarkTime().minus(allowedLateness); if (minimumAllowedTimestamp.isAfter(value.getTimestamp())) { droppedDueToLateness.addValue((long) value.getWindows().size()); return; } @SuppressWarnings("unchecked") Collection<W> windows = (Collection<W>) value.getWindows(); warmUpCache(windows); for (W window : windows) { BitSet finishedSet = lookupFinishedSet(window); if (isRootFinished(finishedSet)) { droppedDueToClosedWindow.addValue(1L); continue; } if (activeWindows.add(window)) { scheduleCleanup(window); } outputBuffer.addValue(bufferContext(window), value.getValue()); watermarkHolder.addHold(bufferContext(window), value.getTimestamp(), timerManager.currentWatermarkTime().isAfter(value.getTimestamp())); BitSet originalFinishedSet = (BitSet) finishedSet.clone(); XXX e = new OnElementEvent<W>(value.getValue(), value.getTimestamp(), window); TriggerResult result = rootTrigger.invokeElement(context(finishedSet), e); boolean stillExists = true; if (result.isFire()) { stillExists = mergeIfAppropriate(window); } if (stillExists) { handleResult(rootTrigger, window, originalFinishedSet, finishedSet, result); } } }', 'ground_truth': 'public void onElement(WindowedValue<InputT> value) throws Exception { Instant minimumAllowedTimestamp = timerManager.currentWatermarkTime().minus(allowedLateness); if (minimumAllowedTimestamp.isAfter(value.getTimestamp())) { droppedDueToLateness.addValue((long) value.getWindows().size()); return; } @SuppressWarnings("unchecked") Collection<W> windows = (Collection<W>) value.getWindows(); warmUpCache(windows); for (W window : windows) { BitSet finishedSet = lookupFinishedSet(window); if (isRootFinished(finishedSet)) { droppedDueToClosedWindow.addValue(1L); continue; } if (activeWindows.add(window)) { scheduleCleanup(window); } outputBuffer.addValue(bufferContext(window), value.getValue()); watermarkHolder.addHold(bufferContext(window), value.getTimestamp(), timerManager.currentWatermarkTime().isAfter(value.getTimestamp())); BitSet originalFinishedSet = (BitSet) finishedSet.clone(); OnElementContextImpl e = new OnElementContextImpl( context(finishedSet), value.getValue(), value.getTimestamp(), window); TriggerResult result = rootTrigger.invokeElement(e); boolean stillExists = true; if (result.isFire()) { stillExists = mergeIfAppropriate(window); } if (stillExists) { handleResult(rootTrigger, window, originalFinishedSet, finishedSet, result); } } }', 'output': 'public void onElement(WindowedValue<InputT> value) throws Exception { Instant minimumAllowedTimestamp = timerManager.currentWatermarkTime().minus(allowedLateness); if (minimumAllowedTimestamp.isAfter(value.getTimestamp())) { droppedDueToLateness.addValue((long) value.getWindows().size()); return; } @SuppressWarnings("unchecked") Collection<W> windows = (Collection<W>) value.getWindows(); warmUpCache(windows); for (W window : windows) { BitSet finishedSet = lookupFinishedSet(window); if (isRootFinished(finishedSet)) { droppedDueToClosedWindow.addValue(1L); continue; } if (activeWindows.add(window)) { scheduleCleanup(window); } outputBuffer.addValue(bufferContext(window), value.getValue()); watermarkHolder.addHold(bufferContext(window), value.getTimestamp(), timerManager.currentWatermarkTime().isAfter(value.getTimestamp())); BitSet originalFinishedSet = (BitSet) finishedSet.clone(); OnElementEvent<W> e = new OnElementEvent<W>(value.getValue(), value.getTimestamp(), window); TriggerResult result = rootTrigger.invokeElement(context(finishedSet), e); boolean stillExists = true; if (result.isFire()) { stillExists = mergeIfAppropriate(window); } if (stillExists) { handleResult(rootTrigger, window, originalFinishedSet, finishedSet, result); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void _testJtaTwoCaches() throws Exception { UserTransaction jtaTx = jotm.getUserTransaction(); GridCache<String, Integer> cache1 = cache(); GridCache<String, Integer> cache2 = grid(0).cache("cache-2"); assertNull(cache1.tx()); assertNull(cache2.tx()); jtaTx.begin(); try { cache1.put("key", 1); cache2.put("key", 1); assertEquals(1, (int)cache1.get("key")); assertEquals(1, (int)cache2.get("key")); assertEquals(cache1.tx().state(), ACTIVE); assertEquals(cache2.tx().state(), ACTIVE); jtaTx.commit(); assertNull(cache1.tx()); assertNull(cache2.tx()); assertEquals(1, (int)cache1.get("key")); assertEquals(1, (int)cache2.get("key")); } finally { if (jtaTx.getStatus() == Status.STATUS_ACTIVE) jtaTx.rollback(); } assertEquals(1, (int)cache1.get("key")); assertEquals(1, (int)cache2.get("key")); }', 'ground_truth': 'public void _testJtaTwoCaches() throws Exception { UserTransaction jtaTx = jotm.getUserTransaction(); Cache<String, Integer> cache1 = cache(); Cache<String, Integer> cache2 = grid(0).cache("cache-2"); assertNull(cache1.tx()); assertNull(cache2.tx()); jtaTx.begin(); try { cache1.put("key", 1); cache2.put("key", 1); assertEquals(1, (int)cache1.get("key")); assertEquals(1, (int)cache2.get("key")); assertEquals(cache1.tx().state(), ACTIVE); assertEquals(cache2.tx().state(), ACTIVE); jtaTx.commit(); assertNull(cache1.tx()); assertNull(cache2.tx()); assertEquals(1, (int)cache1.get("key")); assertEquals(1, (int)cache2.get("key")); } finally { if (jtaTx.getStatus() == Status.STATUS_ACTIVE) jtaTx.rollback(); } assertEquals(1, (int)cache1.get("key")); assertEquals(1, (int)cache2.get("key")); }', 'output': 'public void testJtaTwoCaches() throws Exception { UserTransaction jtaTx = jotm.getUserTransaction(); Cache<String, Integer> cache1 = cache(); Cache<String, Integer> cache2 = grid(0).cache("cache-2"); assertNull(cache1.tx()); assertNull(cache2.tx()); jtaTx.begin(); try { cache1.put("key", 1); cache2.put("key", 1); assertEquals(1, (int)cache1.get("key")); assertEquals(1, (int)cache2.get("key")); assertEquals(cache1.tx().state(), ACTIVE); assertEquals(cache2.tx().state(), ACTIVE); jtaTx.commit(); assertNull(cache1.tx()); assertNull(cache2.tx()); assertEquals(1, (int)cache1.get("key")); assertEquals(1, (int)cache2.get("key")); } finally { if (jtaTx.getStatus() == Status.STATUS_ACTIVE) jtaTx.rollback(); } assertEquals(1, (int)cache1.get("key")); assertEquals(1, (int)cache2.get("key")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void fireEntryAdded(String publicId) { XXX it = catalogListeners.iterator(); while (it.hasNext()) { CatalogListener listener = (CatalogListener)it.next(); listener.notifyNew(publicId); } }', 'ground_truth': 'protected void fireEntryAdded(String publicId) { Iterator<CatalogListener> it = catalogListeners.iterator(); while (it.hasNext()) { CatalogListener listener = it.next(); listener.notifyNew(publicId); } }', 'output': 'protected void fireEntryAdded(String publicId) { Iterator<CatalogListener> it = catalogListeners.iterator(); while (it.hasNext()) { CatalogListener listener = (CatalogListener)it.next(); listener.notifyNew(publicId); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSerializedSize() throws IOException { final RowIndexEntry simple = new RowIndexEntry(123); DataOutputBuffer buffer = new DataOutputBuffer(); RowIndexEntry.Serializer serializer = new RowIndexEntry.Serializer(new SimpleDenseCellNameType(UTF8Type.instance)); serializer.serialize(simple, buffer); Assert.assertEquals(buffer.getLength(), serializer.serializedSize(simple)); buffer = new DataOutputBuffer(); Schema.instance.setKeyspaceDefinition(KSMetaData.newKeyspace("Keyspace1", SimpleStrategy.class, Collections.<String,String>emptyMap(), false, Collections.singleton(standardCFMD("Keyspace1", "Standard1")))); ColumnFamily cf = ArrayBackedSortedColumns.factory.create("Keyspace1", "Standard1"); ColumnIndex columnIndex = new ColumnIndex.Builder(cf, ByteBufferUtil.bytes("a"), new DataOutputBuffer()) {{ int idx = 0, size = 0; Cell column; do { column = new BufferCell(CellNames.simpleDense(ByteBufferUtil.bytes("c" + idx++)), ByteBufferUtil.bytes("v"), FBUtilities.timestampMicros()); size += column.serializedSize(new SimpleDenseCellNameType(UTF8Type.instance), TypeSizes.NATIVE); add(column); } while (size < DatabaseDescriptor.getColumnIndexSize() * 3); }}.build(); XXX withIndex = RowIndexEntry.create(0xdeadbeef, DeletionTime.LIVE, columnIndex); serializer.serialize(withIndex, buffer); Assert.assertEquals(buffer.getLength(), serializer.serializedSize(withIndex)); }', 'ground_truth': 'public void testSerializedSize() throws IOException { final RowIndexEntry<IndexHelper.IndexInfo> simple = new RowIndexEntry<>(123); DataOutputBuffer buffer = new DataOutputBuffer(); RowIndexEntry.Serializer serializer = new RowIndexEntry.Serializer(new IndexHelper.IndexInfo.Serializer(new SimpleDenseCellNameType(UTF8Type.instance))); serializer.serialize(simple, buffer); Assert.assertEquals(buffer.getLength(), serializer.serializedSize(simple)); buffer = new DataOutputBuffer(); Schema.instance.setKeyspaceDefinition(KSMetaData.newKeyspace("Keyspace1", SimpleStrategy.class, Collections.<String,String>emptyMap(), false, Collections.singleton(standardCFMD("Keyspace1", "Standard1")))); ColumnFamily cf = ArrayBackedSortedColumns.factory.create("Keyspace1", "Standard1"); ColumnIndex columnIndex = new ColumnIndex.Builder(cf, ByteBufferUtil.bytes("a"), new DataOutputBuffer()) {{ int idx = 0, size = 0; Cell column; do { column = new BufferCell(CellNames.simpleDense(ByteBufferUtil.bytes("c" + idx++)), ByteBufferUtil.bytes("v"), FBUtilities.timestampMicros()); size += column.serializedSize(new SimpleDenseCellNameType(UTF8Type.instance), TypeSizes.NATIVE); add(column); } while (size < DatabaseDescriptor.getColumnIndexSize() * 3); }}.build(); RowIndexEntry<IndexHelper.IndexInfo> withIndex = RowIndexEntry.create(0xdeadbeef, DeletionTime.LIVE, columnIndex); serializer.serialize(withIndex, buffer); Assert.assertEquals(buffer.getLength(), serializer.serializedSize(withIndex)); }', 'output': 'public void testSerializedSize() throws IOException { final RowIndexEntry simple = new RowIndexEntry(123); DataOutputBuffer buffer = new DataOutputBuffer(); RowIndexEntry.Serializer serializer = new RowIndexEntry.Serializer(new SimpleDenseCellNameType(UTF8Type.instance)); serializer.serialize(simple, buffer); Assert.assertEquals(buffer.getLength(), serializer.serializedSize(simple)); buffer = new DataOutputBuffer(); Schema.instance.setKeyspaceDefinition(KSMetaData.newKeyspace("Keyspace1", SimpleStrategy.class, Collections.<String,String>emptyMap(), false, Collections.singleton(standardCFMD("Keyspace1", "Standard1")))); ColumnFamily cf = ArrayBackedSortedColumns.factory.create("Keyspace1", "Standard1"); ColumnIndex columnIndex = new ColumnIndex.Builder(cf, ByteBufferUtil.bytes("a"), new DataOutputBuffer()) {{ int idx = 0, size = 0; Cell column; do { column = new BufferCell(CellNames.simpleDense(ByteBufferUtil.bytes("c" + idx++)), ByteBufferUtil.bytes("v"), FBUtilities.timestampMicros()); size += column.serializedSize(new SimpleDenseCellNameType(UTF8Type.instance), TypeSizes.NATIVE); add(column); } while (size < DatabaseDescriptor.getColumnIndexSize() * 3); }}.build(); RowIndex withIndex = RowIndexEntry.create(0xdeadbeef, DeletionTime.LIVE, columnIndex); serializer.serialize(withIndex, buffer); Assert.assertEquals(buffer.getLength(), serializer.serializedSize(withIndex)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGeneric() throws Exception { Schema schema = new Schema.Parser().parse(Resources.getResource("person.avsc").openStream()); GenericRecord savedRecord = new GenericData.Record(schema); savedRecord.put("name", "John Doe"); savedRecord.put("age", 42); savedRecord.put("siblingnames", Lists.newArrayList("Jimmy", "Jane")); populateGenericFile(Lists.newArrayList(savedRecord), schema); XXX p = pipelineRule.createPipeline(); PCollection<GenericRecord> input = p.apply( AvroIO.readGenericRecords(schema).from(inputFile.getAbsolutePath())); input.apply(AvroIO.writeGenericRecords(schema).to(outputDir.getAbsolutePath())); p.run().waitUntilFinish(); List<GenericRecord> records = readGenericFile(); assertEquals(Lists.newArrayList(savedRecord), records); }', 'ground_truth': 'public void testGeneric() throws Exception { Schema schema = new Schema.Parser().parse(Resources.getResource("person.avsc").openStream()); GenericRecord savedRecord = new GenericData.Record(schema); savedRecord.put("name", "John Doe"); savedRecord.put("age", 42); savedRecord.put("siblingnames", Lists.newArrayList("Jimmy", "Jane")); populateGenericFile(Lists.newArrayList(savedRecord), schema); PCollection<GenericRecord> input = pipeline.apply( AvroIO.readGenericRecords(schema).from(inputFile.getAbsolutePath())); input.apply(AvroIO.writeGenericRecords(schema).to(outputDir.getAbsolutePath())); pipeline.run(); List<GenericRecord> records = readGenericFile(); assertEquals(Lists.newArrayList(savedRecord), records); }', 'output': 'public void testGeneric() throws Exception { Schema schema = new Schema.Parser().parse(Resources.getResource("person.avsc").openStream()); GenericRecord savedRecord = new GenericData.Record(schema); savedRecord.put("name", "John Doe"); savedRecord.put("age", 42); savedRecord.put("siblingnames", Lists.newArrayList("Jimmy", "Jane")); populateGenericFile(Lists.newArrayList(savedRecord), schema); Pipeline p = pipelineRule.createPipeline(); PCollection<GenericRecord> input = p.apply( AvroIO.readGenericRecords(schema).from(inputFile.getAbsolutePath())); input.apply(AvroIO.writeGenericRecords(schema).to(outputDir.getAbsolutePath())); p.run().waitUntilFinish(); List<GenericRecord> records = readGenericFile(); assertEquals(Lists.newArrayList(savedRecord), records); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMetaData2() throws Exception { PortableBuilder builder = builder("org.test.MetaTest2"); builder.setField("objectField", "a", Object.class); PortableObject po = builder.build(); XXX meta = po.metaData(); assertEquals("MetaTest2", meta.typeName()); assertEquals("Object", meta.fieldTypeName("objectField")); } public void testMetaData() throws Exception { PortableBuilder builder = builder("org.test.MetaTest"); builder.hashCode(100); builder.setField("intField", 1); builder.setField("byteArrayField", new byte[] {1, 2, 3}); PortableObject po = builder.build(); XXX meta = po.metaData(); assertEquals("MetaTest", meta.typeName()); Collection<String> fields = meta.fields(); assertEquals(2, fields.size()); assertTrue(fields.contains("intField")); assertTrue(fields.contains("byteArrayField")); assertEquals("int", meta.fieldTypeName("intField")); assertEquals("byte[]", meta.fieldTypeName("byteArrayField")); builder = builder("org.test.MetaTest"); builder.hashCode(100); builder.setField("intField", 2); builder.setField("uuidField", UUID.randomUUID()); po = builder.build(); meta = po.metaData(); assertEquals("MetaTest", meta.typeName()); fields = meta.fields(); assertEquals(3, fields.size()); assertTrue(fields.contains("intField")); assertTrue(fields.contains("byteArrayField")); assertTrue(fields.contains("uuidField")); assertEquals("int", meta.fieldTypeName("intField")); assertEquals("byte[]", meta.fieldTypeName("byteArrayField")); assertEquals("UUID", meta.fieldTypeName("uuidField")); }', 'ground_truth': 'public void testMetaData() throws Exception { BinaryObjectBuilder builder = builder("org.test.MetaTest"); builder.hashCode(100); builder.setField("intField", 1); builder.setField("byteArrayField", new byte[] {1, 2, 3}); BinaryObject po = builder.build(); BinaryType meta = po.type(); assertEquals("MetaTest", meta.typeName()); Collection<String> fields = meta.fields(); assertEquals(2, fields.size()); assertTrue(fields.contains("intField")); assertTrue(fields.contains("byteArrayField")); assertEquals("int", meta.fieldTypeName("intField")); assertEquals("byte[]", meta.fieldTypeName("byteArrayField")); builder = builder("org.test.MetaTest"); builder.hashCode(100); builder.setField("intField", 2); builder.setField("uuidField", UUID.randomUUID()); po = builder.build(); meta = po.type(); assertEquals("MetaTest", meta.typeName()); fields = meta.fields(); assertEquals(3, fields.size()); assertTrue(fields.contains("intField")); assertTrue(fields.contains("byteArrayField")); assertTrue(fields.contains("uuidField")); assertEquals("int", meta.fieldTypeName("intField")); assertEquals("byte[]", meta.fieldTypeName("byteArrayField")); assertEquals("UUID", meta.fieldTypeName("uuidField")); }', 'output': 'public void testMetaData() throws Exception { PortableBuilder builder = builder("org.test.MetaTest"); builder.hashCode(100); builder.setField("intField", 1); builder.setField("byteArrayField", new byte[] {1, 2, 3}); PortableObject po = builder.build(); MetaData meta = po.metaData(); assertEquals("MetaTest", meta.typeName()); Collection<String> fields = meta.fields(); assertEquals(2, fields.size()); assertTrue(fields.contains("intField")); assertTrue(fields.contains("byteArrayField")); assertEquals("int", meta.fieldTypeName("intField")); assertEquals("byte[]", meta.fieldTypeName("byteArrayField")); builder = builder("org.test.MetaTest"); builder.hashCode(100); builder.setField("intField", 2); builder.setField("uuidField", UUID.randomUUID()); po = builder.build(); meta = po.metaData(); assertEquals("MetaTest", meta.typeName()); fields = meta.fields(); assertEquals(3, fields.size()); assertTrue(fields.contains("intField")); assertTrue(fields.contains("byteArrayField")); assertTrue(fields.contains("uuidField")); assertEquals("int", meta.fieldTypeName("intField")); assertEquals("byte[]", meta.fieldTypeName("byteArrayField")); assertEquals("UUID", meta.fieldTypeName("uuidField")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testEqualsLong() { XXX p1 = new PropertyIdImpl("prop1", "cat1", false); Resource resource = new ResourceImpl(Resource.Type.Cluster); resource.setProperty(p1, 1L); PredicateBuilder pb = new PredicateBuilder(); Predicate predicate1 = pb.property(p1).equals(1L).toPredicate(); Assert.assertTrue(predicate1.evaluate(resource)); PredicateBuilder pb2 = new PredicateBuilder(); Predicate predicate2 = pb2.property(p1).equals(99L).toPredicate(); Assert.assertFalse(predicate2.evaluate(resource)); }', 'ground_truth': 'public void testEqualsLong() { String p1 = PropertyHelper.getPropertyId("cat1", "prop1"); Resource resource = new ResourceImpl(Resource.Type.Cluster); resource.setProperty(p1, 1L); PredicateBuilder pb = new PredicateBuilder(); Predicate predicate1 = pb.property(p1).equals(1L).toPredicate(); Assert.assertTrue(predicate1.evaluate(resource)); PredicateBuilder pb2 = new PredicateBuilder(); Predicate predicate2 = pb2.property(p1).equals(99L).toPredicate(); Assert.assertFalse(predicate2.evaluate(resource)); }', 'output': 'public void testEqualsLong() { PropertyIdImpl p1 = new PropertyIdImpl("prop1", "cat1", false); Resource resource = new ResourceImpl(Resource.Type.Cluster); resource.setProperty(p1, 1L); PredicateBuilder pb = new PredicateBuilder(); Predicate predicate1 = pb.property(p1).equals(1L).toPredicate(); Assert.assertTrue(predicate1.evaluate(resource)); PredicateBuilder pb2 = new PredicateBuilder(); Predicate predicate2 = pb2.property(p1).equals(99L).toPredicate(); Assert.assertFalse(predicate2.evaluate(resource)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void visitUsingMapKeysToNodes() throws GridException { final Ignite g = GridGain.grid(); Collection<Integer> keys = new ArrayList<>(KEY_CNT); for (int i = 0; i < KEY_CNT; i++) keys.add(i); Map<GridNode, Collection<Integer>> mappings = g.cluster().mapKeysToNodes(CACHE_NAME, keys); for (Map.Entry<GridNode, Collection<Integer>> mapping : mappings.entrySet()) { GridNode node = mapping.getKey(); final Collection<Integer> mappedKeys = mapping.getValue(); if (node != null) { g.compute(g.cluster().forNode(node)).run(new GridRunnable() { @Override public void run() { GridCache<Integer, String> cache = g.cache(CACHE_NAME); for (Integer key : mappedKeys) System.out.println("Co-located using mapKeysToNodes [key= " + key + ", value=" + cache.peek(key) + \']\'); } }); } } }', 'ground_truth': 'private static void visitUsingMapKeysToNodes() throws GridException { final Ignite g = GridGain.grid(); Collection<Integer> keys = new ArrayList<>(KEY_CNT); for (int i = 0; i < KEY_CNT; i++) keys.add(i); Map<ClusterNode, Collection<Integer>> mappings = g.cluster().mapKeysToNodes(CACHE_NAME, keys); for (Map.Entry<ClusterNode, Collection<Integer>> mapping : mappings.entrySet()) { ClusterNode node = mapping.getKey(); final Collection<Integer> mappedKeys = mapping.getValue(); if (node != null) { g.compute(g.cluster().forNode(node)).run(new GridRunnable() { @Override public void run() { GridCache<Integer, String> cache = g.cache(CACHE_NAME); for (Integer key : mappedKeys) System.out.println("Co-located using mapKeysToNodes [key= " + key + ", value=" + cache.peek(key) + \']\'); } }); } } }', 'output': 'private static void visitUsingMapKeysToNodes() throws GridException { final Ignite g = GridGain.grid(); Collection<Integer> keys = new ArrayList<>(KEY_CNT); for (int i = 0; i < KEY_CNT; i++) keys.add(i); Map<ClusterNode, Collection<Integer>> mappings = g.cluster().mapKeysToNodes(CACHE_NAME, keys); for (Map.Entry<ClusterNode, Collection<Integer>> mapping : mappings.entrySet()) { ClusterNode node = mapping.getKey(); final Collection<Integer> mappedKeys = mapping.getValue(); if (node != null) { g.compute(g.cluster().forNode(node)).run(new GridRunnable() { @Override public void run() { GridCache<Integer, String> cache = g.cache(CACHE_NAME); for (Integer key : mappedKeys) System.out.println("Co-located using mapKeysToNodes [key= " + key + ", value=" + cache.peek(key) + \']\'); } }); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testNodeSerialize() throws Exception { for (GridDiscoverySpi spi : spis) { XXX node = spi.getLocalNode(); assert node != null; writeObject(node); info("Serialize node success [nodeId=" + node.id() + ", spiIdx=" + spis.indexOf(spi) + \']\'); } }', 'ground_truth': 'public void testNodeSerialize() throws Exception { for (GridDiscoverySpi spi : spis) { ClusterNode node = spi.getLocalNode(); assert node != null; writeObject(node); info("Serialize node success [nodeId=" + node.id() + ", spiIdx=" + spis.indexOf(spi) + \']\'); } }', 'output': 'public void testNodeSerialize() throws Exception { for (GridDiscoverySpi spi : spis) { IgniteInternalNode node = spi.getLocalNode(); assert node != null; writeObject(node); info("Serialize node success [nodeId=" + node.id() + ", spiIdx=" + spis.indexOf(spi) + \']\'); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testOnElementCombiningAccumulating() throws Exception { TriggerTester<Integer, Integer, IntervalWindow> tester = TriggerTester.combining( FixedWindows.of(Duration.millis(10)), mockTrigger, AccumulationMode.ACCUMULATING_FIRED_PANES, new Sum.SumIntegerFn().<String>asKeyedFn(), VarIntCoder.of(), Duration.millis(100)); injectElement(tester, 1, TriggerResult.CONTINUE); injectElement(tester, 2, TriggerResult.FIRE); injectElement(tester, 3, TriggerResult.FIRE_AND_FINISH); injectElement(tester, 4, null); assertThat(tester.extractOutput(), Matchers.contains( isSingleWindowedValue(Matchers.equalTo(3), 1, 0, 10), isSingleWindowedValue(Matchers.equalTo(6), 3, 0, 10))); assertTrue(tester.isMarkedFinished(firstWindow)); tester.assertHasOnlyGlobalAndFinishedSetsFor(firstWindow); }', 'ground_truth': 'public void testOnElementCombiningAccumulating() throws Exception { ReduceFnTester<Integer, Integer, IntervalWindow> tester = ReduceFnTester.combining(FixedWindows.of(Duration.millis(10)), mockTrigger, AccumulationMode.ACCUMULATING_FIRED_PANES, new Sum.SumIntegerFn().<String>asKeyedFn(), VarIntCoder.of(), Duration.millis(100)); injectElement(tester, 1, TriggerResult.CONTINUE); injectElement(tester, 2, TriggerResult.FIRE); injectElement(tester, 3, TriggerResult.FIRE_AND_FINISH); injectElement(tester, 4, null); assertThat( tester.extractOutput(), Matchers.contains( isSingleWindowedValue(Matchers.equalTo(3), 1, 0, 10), isSingleWindowedValue(Matchers.equalTo(6), 3, 0, 10))); assertTrue(tester.isMarkedFinished(firstWindow)); tester.assertHasOnlyGlobalAndFinishedSetsFor(firstWindow); }', 'output': 'public void testOnElementCombiningAccumulating() throws Exception { TriggerTester<Integer, Integer, IntervalWindow> tester = TriggerTester.combining( FixedWindows.of(Duration.millis(10)), mockTrigger, AccumulationMode.ACCUMULATING_FIRED_PANES, new Sum.SumIntegerFn().<String>asKeyedFn(), VarIntCoder.of(), Duration.millis(100)); injectElement(tester, 1, TriggerResult.CONTINUE); injectElement(tester, 2, TriggerResult.FIRE); injectElement(tester, 3, TriggerResult.FIRE_AND_FINISH); injectElement(tester, 4, null); assertThat(tester.extractOutput(), Matchers.contains( isSingleWindowedValue(Matchers.equalTo(3), 1, 0, 10), isSingleWindowedValue(Matchers.equalTo(6), 3, 0, 10))); assertTrue(tester.isMarkedFinished(firstWindow)); tester.assertHasOnlyGlobalAndFinishedSetsFor(firstWindow); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testInvokeProcessElementTimeBoundedWithStartupDelay() throws Exception { SplittableProcessElementInvoker<Void, String, OffsetRange, OffsetRangeTracker>.Result res = runTest(10000, Duration.standardSeconds(3), Integer.MAX_VALUE, Duration.millis(100)); assertFalse(res.getContinuation().shouldResume()); OffsetRange residualRange = res.getResidualRestriction(); assertThat(residualRange.getFrom(), greaterThan(10L)); assertThat(residualRange.getFrom(), lessThan(100L)); assertEquals(10000, residualRange.getTo()); }', 'ground_truth': 'public void testInvokeProcessElementTimeBoundedWithStartupDelay() throws Exception { SplittableProcessElementInvoker<Void, String, OffsetRange, Long>.Result res = runTest(10000, Duration.standardSeconds(3), Integer.MAX_VALUE, Duration.millis(100)); assertFalse(res.getContinuation().shouldResume()); OffsetRange residualRange = res.getResidualRestriction(); assertThat(residualRange.getFrom(), greaterThan(10L)); assertThat(residualRange.getFrom(), lessThan(100L)); assertEquals(10000, residualRange.getTo()); }', 'output': 'public void testInvokeProcessElementTimeBoundedWithStartupDelay() throws Exception { SplittableProcessElementInvoker.Result res = runTest(10000, Duration.standardSeconds(3), Integer.MAX_VALUE, Duration.millis(100)); assertFalse(res.getContinuation().shouldResume()); OffsetRange residualRange = res.getResidualRestriction(); assertThat(residualRange.getFrom(), greaterThan(10L)); assertThat(residualRange.getFrom(), lessThan(100L)); assertEquals(10000, residualRange.getTo()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public BloomFilter deserialize(DataInput dis) throws IOException { int hashes = dis.readInt(); XXX bitLength = dis.readInt(); long[] bits = new long[bitLength]; for (int i = 0; i < bitLength; i++) bits[i] = dis.readLong(); OpenBitSet bs = new OpenBitSet(bits, bitLength); return new BloomFilter(hashes, bs); }', 'ground_truth': 'public BloomFilter deserialize(DataInput dis) throws IOException { int hashes = dis.readInt(); long bitLength = dis.readInt(); OpenBitSet bs = new OpenBitSet(bitLength << 6); int pageSize = bs.getPageSize(); int pageCount = bs.getPageCount(); for (int p = 0; p < pageCount; p++) { long[] bits = bs.getPage(p); for (int i = 0; i < pageSize && bitLength-- > 0; i++) bits[i] = dis.readLong(); } return new BloomFilter(hashes, bs); }', 'output': 'public BloomFilter deserialize(DataInput dis) throws IOException { int hashes = dis.readInt(); int bitLength = dis.readInt(); long[] bits = new long[bitLength]; for (int i = 0; i < bitLength; i++) bits[i] = dis.readLong(); OpenBitSet bs = new OpenBitSet(bits, bitLength); return new BloomFilter(hashes, bs); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void test() { int size = 4; BoundedStatsDeque bsd = new BoundedStatsDeque(size); assertEquals(0, bsd.size()); assertEquals(0, bsd.sum(), 0.001d); assertEquals(0, bsd.mean(), 0.001d); bsd.add(1d); bsd.add(2d); bsd.add(3d); bsd.add(4d); bsd.add(5d); XXX iter = bsd.iterator(); assertTrue(iter.hasNext()); assertEquals(2d, iter.next(), 0); assertTrue(iter.hasNext()); assertEquals(3d, iter.next(), 0); assertTrue(iter.hasNext()); assertEquals(4d, iter.next(), 0); assertTrue(iter.hasNext()); assertEquals(5d, iter.next(), 0); assertFalse(iter.hasNext()); assertEquals(size, bsd.size()); assertEquals(14, bsd.sum(), 0.001d); assertEquals(3.5, bsd.mean(), 0.001d); bsd.clear(); assertFalse(bsd.iterator().hasNext()); }', 'ground_truth': 'public void test() { int size = 4; BoundedStatsDeque bsd = new BoundedStatsDeque(size); assertEquals(0, bsd.size()); assertEquals(0, bsd.sum(), 0.001d); assertEquals(0, bsd.mean(), 0.001d); bsd.add(1L); bsd.add(2L); bsd.add(3L); bsd.add(4L); bsd.add(5L); Iterator<Long> iter = bsd.iterator(); assertTrue(iter.hasNext()); assertEquals(2L, iter.next(), 0); assertTrue(iter.hasNext()); assertEquals(3L, iter.next(), 0); assertTrue(iter.hasNext()); assertEquals(4L, iter.next(), 0); assertTrue(iter.hasNext()); assertEquals(5L, iter.next(), 0); assertFalse(iter.hasNext()); assertEquals(size, bsd.size()); assertEquals(14, bsd.sum(), 0.001d); assertEquals(3.5, bsd.mean(), 0.001d); bsd.clear(); assertFalse(bsd.iterator().hasNext()); }', 'output': 'public void test() { int size = 4; BoundedStatsDeque bsd = new BoundedStatsDeque(size); assertEquals(0, bsd.size()); assertEquals(0, bsd.sum(), 0.001d); assertEquals(0, bsd.mean(), 0.001d); bsd.add(1d); bsd.add(2d); bsd.add(3d); bsd.add(4d); bsd.add(5d); Iterator<Double> iter = bsd.iterator(); assertTrue(iter.hasNext()); assertEquals(2d, iter.next(), 0); assertTrue(iter.hasNext()); assertEquals(3d, iter.next(), 0); assertTrue(iter.hasNext()); assertEquals(4d, iter.next(), 0); assertTrue(iter.hasNext()); assertEquals(5d, iter.next(), 0); assertFalse(iter.hasNext()); assertEquals(size, bsd.size()); assertEquals(14, bsd.sum(), 0.001d); assertEquals(3.5, bsd.mean(), 0.001d); bsd.clear(); assertFalse(bsd.iterator().hasNext()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public long maxTimestamp() { long maxTimestamp = deletionInfo().maxTimestamp(); for (XXX column : columns) maxTimestamp = Math.max(maxTimestamp, column.maxTimestamp()); return maxTimestamp; }', 'ground_truth': 'public long maxTimestamp() { long maxTimestamp = deletionInfo().maxTimestamp(); for (Column column : columns) maxTimestamp = Math.max(maxTimestamp, column.maxTimestamp()); return maxTimestamp; }', 'output': 'public long maxTimestamp() { long maxTimestamp = deletionInfo().maxTimestamp(); for (ColumnMetadata column : columns) maxTimestamp = Math.max(maxTimestamp, column.maxTimestamp()); return maxTimestamp; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void localPoolRun() { X.println("Local thread pool run..."); ExecutorService exe = new IgniteThreadPoolExecutor(400, 400, 0, new ArrayBlockingQueue<Runnable>(400) { @Override public boolean offer(Runnable runnable) { try { put(runnable); } catch (InterruptedException e) { e.printStackTrace(); } return true; } }); long start = System.currentTimeMillis(); final GridCache<GridTestKey, Long> cache = G.ignite().cache("partitioned"); for (long i = 0; i < GridTestConstants.ENTRY_COUNT; i++) { final long key = i; exe.submit(new Runnable() { @Override public void run() { Long val = cache.peek(new GridTestKey(key)); if (val == null || val != key) throw new RuntimeException("Invalid value found [key=" + key + ", val=" + val + \']\'); } }); if (i % 10000 == 0) X.println("Executed jobs: " + i); } long end = System.currentTimeMillis(); X.println("Executed " + GridTestConstants.ENTRY_COUNT + " computations in " + (end - start) + "ms."); }', 'ground_truth': 'private static void localPoolRun() { X.println("Local thread pool run..."); ExecutorService exe = new IgniteThreadPoolExecutor(400, 400, 0, new ArrayBlockingQueue<Runnable>(400) { @Override public boolean offer(Runnable runnable) { try { put(runnable); } catch (InterruptedException e) { e.printStackTrace(); } return true; } }); long start = System.currentTimeMillis(); final IgniteCache<GridTestKey, Long> cache = G.ignite().jcache("partitioned"); for (long i = 0; i < GridTestConstants.ENTRY_COUNT; i++) { final long key = i; exe.submit(new Runnable() { @Override public void run() { Long val = cache.peek(new GridTestKey(key)); if (val == null || val != key) throw new RuntimeException("Invalid value found [key=" + key + ", val=" + val + \']\'); } }); if (i % 10000 == 0) X.println("Executed jobs: " + i); } long end = System.currentTimeMillis(); X.println("Executed " + GridTestConstants.ENTRY_COUNT + " computations in " + (end - start) + "ms."); }', 'output': 'private static void localPoolRun() { X.println("Local thread pool run..."); IgniteThreadPoolExecutor exe = new IgniteThreadPoolExecutor(400, 400, 0, new ArrayBlockingQueue<Runnable>(400) { @Override public boolean offer(Runnable runnable) { try { put(runnable); } catch (InterruptedException e) { e.printStackTrace(); } return true; } }); long start = System.currentTimeMillis(); final GridCache<GridTestKey, Long> cache = G.ignite().cache("partitioned"); for (long i = 0; i < GridTestConstants.ENTRY_COUNT; i++) { final long key = i; exe.submit(new Runnable() { @Override public void run() { Long val = cache.peek(new GridTestKey(key)); if (val == null || val != key) throw new RuntimeException("Invalid value found [key=" + key + ", val=" + val + \']\'); } }); if (i % 10000 == 0) X.println("Executed jobs: " + i); } long end = System.currentTimeMillis(); X.println("Executed " + GridTestConstants.ENTRY_COUNT + " computations in " + (end - start) + "ms."); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testInternalTaskMetrics() throws Exception { Ignite ignite = grid(); ignite.compute().withName("visor-test-task").execute(new TestInternalTask(), "testArg"); final CountDownLatch latch = new CountDownLatch(2); ignite.events().localListen(new GridPredicate<GridEvent>() { @Override public boolean apply(GridEvent evt) { assert evt.type() == EVT_NODE_METRICS_UPDATED; latch.countDown(); return true; } }, EVT_NODE_METRICS_UPDATED); latch.await(); XXX metrics = ignite.cluster().localNode().metrics(); info("Node metrics: " + metrics); assert metrics.getAverageActiveJobs() == 0; assert metrics.getAverageCancelledJobs() == 0; assert metrics.getAverageJobExecuteTime() == 0; assert metrics.getAverageJobWaitTime() == 0; assert metrics.getAverageRejectedJobs() == 0; assert metrics.getAverageWaitingJobs() == 0; assert metrics.getCurrentActiveJobs() == 0; assert metrics.getCurrentCancelledJobs() == 0; assert metrics.getCurrentJobExecuteTime() == 0; assert metrics.getCurrentJobWaitTime() == 0; assert metrics.getCurrentWaitingJobs() == 0; assert metrics.getMaximumActiveJobs() == 0; assert metrics.getMaximumCancelledJobs() == 0; assert metrics.getMaximumJobExecuteTime() == 0; assert metrics.getMaximumJobWaitTime() == 0; assert metrics.getMaximumRejectedJobs() == 0; assert metrics.getMaximumWaitingJobs() == 0; assert metrics.getTotalCancelledJobs() == 0; assert metrics.getTotalExecutedJobs() == 0; assert metrics.getTotalRejectedJobs() == 0; assert metrics.getTotalExecutedTasks() == 0; assertTrue("MaximumJobExecuteTime=" + metrics.getMaximumJobExecuteTime() + " is less than AverageJobExecuteTime=" + metrics.getAverageJobExecuteTime(), metrics.getMaximumJobExecuteTime() >= metrics.getAverageJobExecuteTime()); }', 'ground_truth': 'public void testInternalTaskMetrics() throws Exception { Ignite ignite = grid(); ignite.compute().withName("visor-test-task").execute(new TestInternalTask(), "testArg"); final CountDownLatch latch = new CountDownLatch(2); ignite.events().localListen(new GridPredicate<GridEvent>() { @Override public boolean apply(GridEvent evt) { assert evt.type() == EVT_NODE_METRICS_UPDATED; latch.countDown(); return true; } }, EVT_NODE_METRICS_UPDATED); latch.await(); ClusterNodeMetrics metrics = ignite.cluster().localNode().metrics(); info("Node metrics: " + metrics); assert metrics.getAverageActiveJobs() == 0; assert metrics.getAverageCancelledJobs() == 0; assert metrics.getAverageJobExecuteTime() == 0; assert metrics.getAverageJobWaitTime() == 0; assert metrics.getAverageRejectedJobs() == 0; assert metrics.getAverageWaitingJobs() == 0; assert metrics.getCurrentActiveJobs() == 0; assert metrics.getCurrentCancelledJobs() == 0; assert metrics.getCurrentJobExecuteTime() == 0; assert metrics.getCurrentJobWaitTime() == 0; assert metrics.getCurrentWaitingJobs() == 0; assert metrics.getMaximumActiveJobs() == 0; assert metrics.getMaximumCancelledJobs() == 0; assert metrics.getMaximumJobExecuteTime() == 0; assert metrics.getMaximumJobWaitTime() == 0; assert metrics.getMaximumRejectedJobs() == 0; assert metrics.getMaximumWaitingJobs() == 0; assert metrics.getTotalCancelledJobs() == 0; assert metrics.getTotalExecutedJobs() == 0; assert metrics.getTotalRejectedJobs() == 0; assert metrics.getTotalExecutedTasks() == 0; assertTrue("MaximumJobExecuteTime=" + metrics.getMaximumJobExecuteTime() + " is less than AverageJobExecuteTime=" + metrics.getAverageJobExecuteTime(), metrics.getMaximumJobExecuteTime() >= metrics.getAverageJobExecuteTime()); }', 'output': 'public void testInternalTaskMetrics() throws Exception { Ignite ignite = grid(); ignite.compute().withName("visor-test-task").execute(new TestInternalTask(), "testArg"); final CountDownLatch latch = new CountDownLatch(2); ignite.events().localListen(new GridPredicate<GridEvent>() { @Override public boolean apply(GridEvent evt) { assert evt.type() == EVT_NODE_METRICS_UPDATED; latch.countDown(); return true; } }, EVT_NODE_METRICS_UPDATED); latch.await(); ClusterMetrics metrics = ignite.cluster().localNode().metrics(); info("Node metrics: " + metrics); assert metrics.getAverageActiveJobs() == 0; assert metrics.getAverageCancelledJobs() == 0; assert metrics.getAverageJobExecuteTime() == 0; assert metrics.getAverageJobWaitTime() == 0; assert metrics.getAverageRejectedJobs() == 0; assert metrics.getAverageWaitingJobs() == 0; assert metrics.getCurrentActiveJobs() == 0; assert metrics.getCurrentCancelledJobs() == 0; assert metrics.getCurrentJobExecuteTime() == 0; assert metrics.getCurrentJobWaitTime() == 0; assert metrics.getCurrentWaitingJobs() == 0; assert metrics.getMaximumActiveJobs() == 0; assert metrics.getMaximumCancelledJobs() == 0; assert metrics.getMaximumJobExecuteTime() == 0; assert metrics.getMaximumJobWaitTime() == 0; assert metrics.getMaximumRejectedJobs() == 0; assert metrics.getMaximumWaitingJobs() == 0; assert metrics.getTotalCancelledJobs() == 0; assert metrics.getTotalExecutedJobs() == 0; assert metrics.getTotalRejectedJobs() == 0; assert metrics.getTotalExecutedTasks() == 0; assertTrue("MaximumJobExecuteTime=" + metrics.getMaximumJobExecuteTime() + " is less than AverageJobExecuteTime=" + metrics.getAverageJobExecuteTime(), metrics.getMaximum'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean prepareDataset(Dataset<Cell> output) { final AtomicReference<ByteBuffer[]> bufferedKeys = new AtomicReference<>(); CellComparator comparator = new CellComparator(); Dataset<Pair<Integer, String>> hfiles = ReduceByKey.of(output) .keyBy(c -> toRegionIdUninitialized(bufferedKeys, endKeys, ByteBuffer.wrap(c.getRowArray(), c.getRowOffset(), c.getRowLength()))) .reduceBy((Stream<Cell> s, Collector<String> ctx) -> { Iterator<Cell> iterator = s.iterator(); Cell first = iterator.next(); int id = toRegionIdUninitialized(bufferedKeys, endKeys, ByteBuffer.wrap( first.getRowArray(), first.getRowOffset(), first.getRowLength())); HadoopWriter<ImmutableBytesWritable, Cell> writer; writer = (HadoopWriter<ImmutableBytesWritable, Cell>) rawSink.openWriter(id); FileOutputCommitter committer = (FileOutputCommitter) writer.getOutputCommitter(); ImmutableBytesWritable w = new ImmutableBytesWritable(); try { Cell c = first; for (;;) { w.set(c.getRowArray(), c.getRowOffset(), c.getRowLength()); writer.write(Pair.of(w, c)); if (!iterator.hasNext()) { break; } c = iterator.next(); } writer.close(); writer.commit(); ctx.collect(committer.getCommittedTaskPath( writer.getTaskAttemptContext()).toString()); } catch (Exception ex) { try { writer.rollback(); } catch (IOException ex1) { LOG.error("Failed to rollback writer {}", id, ex1); } throw new RuntimeException(ex); } }) .withSortedValues(comparator::compare) .applyIf(windowing != null, b -> b.windowBy(windowing)) .output(); String outputDir = rawSink.getConfiguration().get(FileOutputFormat.OUTDIR); ReduceWindow.of(hfiles) .valueBy(Pair::getSecond) .reduceBy((Stream<String> s, Collector<Void> ctx) -> { ctx.getWindow(); Path o = new Path(new Path(outputDir), folderNaming.apply(ctx.getWindow())); s.forEach(p -> moveTo(p, o)); loadIncrementalHFiles(o); }) .output() .persist(new VoidSink<>()); return true; }', 'ground_truth': 'public boolean prepareDataset(Dataset<Cell> output) { final AtomicReference<ByteBuffer[]> bufferedKeys = new AtomicReference<>(); CellComparator comparator = new CellComparator(); Dataset<String> hfiles = ReduceByKey.of(output) .keyBy(c -> toRegionIdUninitialized(bufferedKeys, endKeys, ByteBuffer.wrap(c.getRowArray(), c.getRowOffset(), c.getRowLength()))) .reduceBy((Stream<Cell> s, Collector<String> ctx) -> { Iterator<Cell> iterator = s.iterator(); Cell first = iterator.next(); int id = toRegionIdUninitialized(bufferedKeys, endKeys, ByteBuffer.wrap( first.getRowArray(), first.getRowOffset(), first.getRowLength())); HadoopWriter<ImmutableBytesWritable, Cell> writer; writer = (HadoopWriter<ImmutableBytesWritable, Cell>) rawSink.openWriter(id); FileOutputCommitter committer = (FileOutputCommitter) writer.getOutputCommitter(); ImmutableBytesWritable w = new ImmutableBytesWritable(); try { Cell c = first; for (;;) { w.set(c.getRowArray(), c.getRowOffset(), c.getRowLength()); writer.write(Pair.of(w, c)); if (!iterator.hasNext()) { break; } c = iterator.next(); } writer.close(); writer.commit(); ctx.collect(committer.getCommittedTaskPath( writer.getTaskAttemptContext()).toString()); } catch (Exception ex) { try { writer.rollback(); } catch (IOException ex1) { LOG.error("Failed to rollback writer {}", id, ex1); } throw new RuntimeException(ex); } }) .withSortedValues(comparator::compare) .applyIf(windowing != null, b -> b.windowBy(windowing)) .outputValues(); String outputDir = rawSink.getConfiguration().get(FileOutputFormat.OUTDIR); ReduceWindow.of(hfiles) .reduceBy((Stream<String> s, Collector<Void> ctx) -> { ctx.getWindow(); String subdir = folderNaming.apply(ctx.getWindow()); Path o = Strings.isNullOrEmpty(subdir) ? new Path(outputDir) : new Path(new Path(outputDir), folderNaming.apply(ctx.getWindow())); s.forEach(p -> moveTo(p, o)); loadIncrementalHFiles(o); try { o.getFileSystem(rawSink.getConfiguration()).delete(o, true); } catch (IOException ex) { LOG.warn("Exception while removing the bulk-loaded directory {}", o, ex); } }) .output() .persist(new VoidSink<>()); return true; }', 'output': 'public boolean prepareDataset(Dataset<Cell> output) { final AtomicReference<ByteBuffer[]> bufferedKeys = new AtomicReference<>(); CellComparator comparator = new CellComparator(); Dataset<Pair<Integer, String>> hfiles = ReduceByKey.of(output) .keyBy(c -> toRegionIdUninitialized(bufferedKeys, endKeys, ByteBuffer.wrap(c.getRowArray(), c.getRowOffset(), c.getRowLength()))) .reduceBy((Stream<Cell> s, Collector<String> ctx) -> { Iterator<Cell> iterator = s.iterator(); Cell first = iterator.next(); int id = toRegionIdUninitialized(bufferedKeys, endKeys, ByteBuffer.wrap( first.getRowArray(), first.getRowOffset(), first.getRowLength())); HadoopWriter<ImmutableBytesWritable, Cell> writer; writer = (HadoopWriter<ImmutableBytesWritable, Cell>) rawSink.openWriter(id); FileOutputCommitter committer = (FileOutputCommitter) writer.getOutputCommitter(); ImmutableBytesWritable w = new ImmutableBytesWritable(); try { Cell c = first; for (;;) { w.set(c.getRowArray(), c.getRowOffset(), c.getRowLength()); writer.write(Pair.of(w, c)); if (!iterator.hasNext()) { break; } c = iterator.next(); } writer.close(); writer.commit(); ctx.collect(committer.getCommittedTaskPath( writer.getTaskAttemptContext()).toString()); } catch (Exception ex) { try { writer.rollback(); } catch (IOException ex1) { LOG.error("Failed to rollback writer {}", id, ex1); } throw new RuntimeException(ex); } }) .withSortedValues(comparator::compare) .applyIf(windowing != null, b -> b.windowBy(windowing)) .output(); String outputDir = rawSink.getConfiguration().get(FileOutputFormat.OUTDIR); ReduceWindow.of(hfiles) .valueBy(Pair::getSecond) .reduceBy((Stream<String> s, Collector<Void> ctx) -> { ctx.getWindow(); Path o = new Path(new Path('}
{'instruction': 'Please refactor the following code:', 'input': 'private void resetClientIpFinder(int idx) throws Exception { XXX disco = (GridTcpClientDiscoverySpi)G.grid("client-" + idx).configuration().getDiscoverySpi(); GridTcpDiscoveryVmIpFinder ipFinder = (GridTcpDiscoveryVmIpFinder)disco.getIpFinder(); String addr = IP_FINDER.getRegisteredAddresses().iterator().next().toString(); if (addr.startsWith("/")) addr = addr.substring(1); ipFinder.setAddresses(Arrays.asList(addr)); }', 'ground_truth': 'private void resetClientIpFinder(int idx) throws Exception { TcpClientDiscoverySpi disco = (TcpClientDiscoverySpi)G.grid("client-" + idx).configuration().getDiscoverySpi(); GridTcpDiscoveryVmIpFinder ipFinder = (GridTcpDiscoveryVmIpFinder)disco.getIpFinder(); String addr = IP_FINDER.getRegisteredAddresses().iterator().next().toString(); if (addr.startsWith("/")) addr = addr.substring(1); ipFinder.setAddresses(Arrays.asList(addr)); }', 'output': 'private void resetClientIpFinder(int idx) throws Exception { IgniteTcpDiscoverySpi disco = (IgniteTcpDiscoverySpi)G.grid("client-" + idx).configuration().getDiscoverySpi(); IgniteTcpDiscoveryVmIpFinder ipFinder = (IgniteTcpDiscoveryVmIpFinder)disco.getIpFinder(); String addr = IP_FINDER.getRegisteredAddresses().iterator().next().toString(); if (addr.startsWith("/")) addr = addr.substring(1); ipFinder.setAddresses(Arrays.asList(addr)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void shrink() { int max = this.max; int startSize = queue.sizex(); for (int i = 0; i < startSize && queue.sizex() > max; i++) { GridCacheEntry<K, V> entry = queue.poll(); if (entry == null) break; if (!entry.evict()) { entry.removeMeta(meta); touch(entry); } } }', 'ground_truth': 'private void shrink() { int max = this.max; int startSize = queue.sizex(); for (int i = 0; i < startSize && queue.sizex() > max; i++) { CacheEntry<K, V> entry = queue.poll(); if (entry == null) break; if (!entry.evict()) { entry.removeMeta(meta); touch(entry); } } }', 'output': 'private void shrink() { int max = this.max; int startSize = queue.sizex(); for (int i = 0; i < startSize && queue.sizex() > max; i++) { CacheEntry<K, V> entry = queue.poll(); if (entry == null) break; if (!entry.evict()) { entry.removeMeta(meta); touch(entry); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected String dump(boolean canonical) { XXX sb = new StringBuffer(); if (canonical) sb.append(\'<\'); sb.append(getNodeTypeSymbol()); if (canonical) sb.append("/>"); return sb.toString(); }', 'ground_truth': 'protected String dump(boolean canonical) { StringBuilder sb = new StringBuilder(); if (canonical) sb.append(\'<\'); sb.append(getNodeTypeSymbol()); if (canonical) sb.append("/>"); return sb.toString(); }', 'output': 'protected String dump(boolean canonical) { StringBuilder sb = new StringBuilder(); if (canonical) sb.append(\'<\'); sb.append(getNodeTypeSymbol()); if (canonical) sb.append("/>"); return sb.toString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void main(String[] args) throws IgniteCheckedException { try (Ignite g = Ignition.start("examples/config/example-cache.xml")) { System.out.println(); System.out.println(">>> Cache atomic sequence example started."); final String seqName = UUID.randomUUID().toString(); XXX seq = g.cache(CACHE_NAME).dataStructures().atomicSequence(seqName, 0, true); long firstVal = seq.get(); System.out.println("Sequence initial value: " + firstVal); g.compute().run(new SequenceClosure(CACHE_NAME, seqName)); System.out.println("Sequence after incrementing [expected=" + (firstVal + RETRIES) + ", actual=" + seq.get() + \']\'); } System.out.println(); System.out.println("Finished atomic sequence example..."); System.out.println("Check all nodes for output (this node is also part of the grid)."); }', 'ground_truth': 'public static void main(String[] args) throws IgniteCheckedException { try (Ignite g = Ignition.start("examples/config/example-cache.xml")) { System.out.println(); System.out.println(">>> Cache atomic sequence example started."); final String seqName = UUID.randomUUID().toString(); IgniteAtomicSequence seq = g.cache(CACHE_NAME).dataStructures().atomicSequence(seqName, 0, true); long firstVal = seq.get(); System.out.println("Sequence initial value: " + firstVal); g.compute().run(new SequenceClosure(CACHE_NAME, seqName)); System.out.println("Sequence after incrementing [expected=" + (firstVal + RETRIES) + ", actual=" + seq.get() + \']\'); } System.out.println(); System.out.println("Finished atomic sequence example..."); System.out.println("Check all nodes for output (this node is also part of the grid)."); }', 'output': 'public static void main(String[] args) throws IgniteCheckedException { try (Ignite g = Ignition.start("examples/config/example-cache.xml")) { System.out.println(); System.out.println(">>> Cache atomic sequence example started."); final String seqName = UUID.randomUUID().toString(); AtomicSequence seq = g.cache(CACHE_NAME).dataStructures().atomicSequence(seqName, 0, true); long firstVal = seq.get(); System.out.println("Sequence initial value: " + firstVal); g.compute().run(new SequenceClosure(CACHE_NAME, seqName)); System.out.println("Sequence after incrementing [expected=" + (firstVal + RETRIES) + ", actual=" + seq.get() + \']\'); } System.out.println(); System.out.println("Finished atomic sequence example..."); System.out.println("Check all nodes for output (this node is also part of the grid)."); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void customEventType() throws Exception { SomeDomainObject sdo = new SomeDomainObject(); SomeReferencedObject other = new SomeReferencedObject(); XXX identifier = FeatureIdentifier.propertyOrCollectionIdentifier( TypeIdentifier.fqcn(SomeDomainObject.class), "references"); final CollectionDomainEvent<SomeDomainObject, SomeReferencedObject> ev = Utils.domainEventHelper().newCollectionDomainEvent( SomeDomainObjectCollectionRemovedFromDomainEvent.class, AbstractDomainEvent.Phase.EXECUTED, identifier, sdo, CollectionDomainEvent.Of.REMOVE_FROM, other); assertThat(ev.getSource(), is(sdo)); assertThat(ev.getIdentifier(), is(identifier)); assertThat(ev.getOf(), is(CollectionDomainEvent.Of.REMOVE_FROM)); assertThat(ev.getValue(), is(other)); }', 'ground_truth': 'public void customEventType() throws Exception { SomeDomainObject sdo = new SomeDomainObject(); SomeReferencedObject other = new SomeReferencedObject(); Identifier identifier = Identifier.propertyOrCollectionIdentifier( TypeIdentifier.fqcn(SomeDomainObject.class), "references"); final CollectionDomainEvent<SomeDomainObject, SomeReferencedObject> ev = Utils.domainEventHelper().newCollectionDomainEvent( SomeDomainObjectCollectionRemovedFromDomainEvent.class, AbstractDomainEvent.Phase.EXECUTED, identifier, sdo, CollectionDomainEvent.Of.REMOVE_FROM, other); assertThat(ev.getSource(), is(sdo)); assertThat(ev.getIdentifier(), is(identifier)); assertThat(ev.getOf(), is(CollectionDomainEvent.Of.REMOVE_FROM)); assertThat(ev.getValue(), is(other)); }', 'output': 'public void customEventType() throws Exception { SomeDomainObject sdo = new SomeDomainObject(); SomeReferencedObject other = new SomeReferencedObject(); Identifier identifier = FeatureIdentifier.propertyOrCollectionIdentifier( TypeIdentifier.fqcn(SomeDomainObject.class), "references"); final CollectionDomainEvent<SomeDomainObject, SomeReferencedObject> ev = Utils.domainEventHelper().newCollectionDomainEvent( SomeDomainObjectCollectionRemovedFromDomainEvent.class, AbstractDomainEvent.Phase.EXECUTED, identifier, sdo, CollectionDomainEvent.Of.REMOVE_FROM, other); assertThat(ev.getSource(), is(sdo)); assertThat(ev.getIdentifier(), is(identifier)); assertThat(ev.getOf(), is(CollectionDomainEvent.Of.REMOVE_FROM)); assertThat(ev.getValue(), is(other)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void main(String[] args) { CassandraDaemon daemon = new CassandraDaemon(); String pidFile = System.getProperty("cassandra-pidfile"); try { daemon.setup(); if (pidFile != null) { new File(pidFile).deleteOnExit(); } if (System.getProperty("cassandra-foreground") == null) { System.out.close(); System.err.close(); } daemon.start(); } catch (XXX e) { String msg = "XXX encountered during startup."; logger.error(msg, e); System.out.println(msg); e.printStackTrace(); System.exit(3); } }', 'ground_truth': 'public static void main(String[] args) { CassandraDaemon daemon = new CassandraDaemon(); String pidFile = System.getProperty("cassandra-pidfile"); try { daemon.setup(); if (pidFile != null) { new File(pidFile).deleteOnExit(); } if (System.getProperty("cassandra-foreground") == null) { System.out.close(); System.err.close(); } daemon.start(); } catch (Throwable e) { String msg = "Exception encountered during startup."; logger.error(msg, e); System.out.println(msg); e.printStackTrace(); System.exit(3); } }', 'output': 'public static void main(String... args) { CassandraDaemon daemon = new CassandraDaemon(); String pidFile = System.getProperty("cassandra-pidfile"); try { daemon.setup(); if (pidFile != null) { new File(pidFile).deleteOnExit(); } if (System.getProperty("cassandra-foreground") == null) { System.out.close(); System.err.close(); } daemon.start(); } catch (Exception e) { String msg = "Exception encountered during startup."; logger.error(msg, e); System.out.println(msg); e.printStackTrace(); System.exit(3); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Map createGenericsSpec(ClassNode sn) { XXX ret = new HashMap(); ClassNode stop = sn.getSuperClass(); for (ClassNode current = classNode; current!=null && current.redirect()!=stop; current=current.getUnresolvedSuperClass(false)) { GenericsType[] sgts = current.getGenericsTypes(); if (sgts==null) { ret.clear(); continue; } ClassNode[] spec = new ClassNode[sgts.length]; for (int i = 0; i < spec.length; i++) { spec[i]=correctToGenericsSpec(ret, sgts[i].getType()); } GenericsType[] newGts = current.redirect().getGenericsTypes(); ret.clear(); for (int i = 0; i < spec.length; i++) { ret.put(newGts[i].getName(), spec[i]); } } return ret; }', 'ground_truth': 'private Map createGenericsSpec(ClassNode sn) { Map ret = new HashMap(); ClassNode stop = sn.getSuperClass(); for (ClassNode current = classNode; current!=null && current.redirect()!=stop; current=current.getUnresolvedSuperClass(false)) { GenericsType[] sgts = current.getGenericsTypes(); if (sgts==null) { ret.clear(); continue; } ClassNode[] spec = new ClassNode[sgts.length]; for (int i = 0; i < spec.length; i++) { spec[i]=correctToGenericsSpec(ret, sgts[i].getType()); } GenericsType[] newGts = current.redirect().getGenericsTypes(); ret.clear(); for (int i = 0; i < spec.length; i++) { ret.put(newGts[i].getName(), spec[i]); } } return ret; }', 'output': 'private Map<String, ClassNode> createGenericsSpec(ClassNode sn) { XXX ret = new HashMap<>(); ClassNode stop = sn.getSuperClass(); for (ClassNode current = classNode; current != null && current.redirect() != stop; current = current.getUnresolvedSuperClass(false)) { GenericsType[] sgts = current.getGenericsTypes(); if (sgts == null) { ret.clear(); continue; } ClassNode[] spec = new ClassNode[sgts.length]; for (int i = 0; i < spec.length; i++) { spec[i] = correctToGenericsSpec(ret, sgts[i].getType()); } GenericsType[] newGts = current.redirect().getGenericsTypes(); ret.clear(); for (int i = 0; i < spec.length; i++) { ret.put(newGts[i].getName(), spec[i]); } } return ret; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Iterator<Row> createRowIterator() { XXX update = new PartitionUpdate(cfm, dk, cfm.regularAndStaticColumns(), 1); for (int i = 0; i < 5; i++) addRow(update, i, i); return update.iterator(); }', 'ground_truth': 'private Iterator<Row> createRowIterator() { PartitionUpdate.Builder update = new PartitionUpdate.Builder(cfm, dk, cfm.regularAndStaticColumns(), 1); for (int i = 0; i < 5; i++) addRow(update, i, i); return update.build().iterator(); }', 'output': 'private Iterator<Row> createRowIterator() { RowUpdate update = new RowUpdate(cfm, dk, cfm.regularAndStaticColumns(), 1); for (int i = 0; i < 5; i++) addRow(update, i, i); return update.iterator(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testLongQueries() throws Exception { XXX spi = getIndexing(); long longQryExecTime = 100; GridStringLogger log = new GridStringLogger(false, this.log); IgniteLogger oldLog = GridTestUtils.getFieldValue(spi, "log"); spi.configuration().setLongQueryExecutionTimeout(longQryExecTime); spi.configuration().setLongQueryExplain(true); try { GridTestUtils.setFieldValue(spi, "log", log); String sql = "select sum(x) FROM SYSTEM_RANGE(?, ?)"; long now = U.currentTimeMillis(); long time = now; long range = 1000000L; while (now - time <= longQryExecTime * 3 / 2) { time = now; range *= 3; GridQueryFieldsResult res = spi.queryFields(null, sql, Arrays.<Object>asList(1, range), null); assert res.iterator().hasNext(); now = U.currentTimeMillis(); } String res = log.toString(); F.println(res); assert res.contains("/* PUBLIC.RANGE_INDEX */"); } finally { GridTestUtils.setFieldValue(spi, "log", oldLog); spi.configuration().setLongQueryExecutionTimeout(3000); } }', 'ground_truth': 'public void testLongQueries() throws Exception { IgniteH2Indexing spi = getIndexing(); long longQryExecTime = 100; GridStringLogger log = new GridStringLogger(false, this.log); IgniteLogger oldLog = GridTestUtils.getFieldValue(spi, "log"); spi.configuration().setLongQueryExecutionTimeout(longQryExecTime); spi.configuration().setLongQueryExplain(true); try { GridTestUtils.setFieldValue(spi, "log", log); String sql = "select sum(x) FROM SYSTEM_RANGE(?, ?)"; long now = U.currentTimeMillis(); long time = now; long range = 1000000L; while (now - time <= longQryExecTime * 3 / 2) { time = now; range *= 3; GridQueryFieldsResult res = spi.queryFields(null, sql, Arrays.<Object>asList(1, range), null); assert res.iterator().hasNext(); now = U.currentTimeMillis(); } String res = log.toString(); F.println(res); assert res.contains("/* PUBLIC.RANGE_INDEX */"); } finally { GridTestUtils.setFieldValue(spi, "log", oldLog); spi.configuration().setLongQueryExecutionTimeout(3000); } }', 'output': 'public void testLongQueries() throws Exception { IndexingSpi spi = getIndexing(); long longQryExecTime = 100; GridStringLogger log = new GridStringLogger(false, this.log); IgniteLogger oldLog = GridTestUtils.getFieldValue(spi, "log"); spi.configuration().setLongQueryExecutionTimeout(longQryExecTime); spi.configuration().setLongQueryExplain(true); try { GridTestUtils.setFieldValue(spi, "log", log); String sql = "select sum(x) FROM SYSTEM_RANGE(?, ?)"; long now = U.currentTimeMillis(); long time = now; long range = 1000000L; while (now - time <= longQryExecTime * 3 / 2) { time = now; range *= 3; GridQueryFieldsResult res = spi.queryFields(null, sql, Arrays.<Object>asList(1, range), null); assert res.iterator().hasNext(); now = U.currentTimeMillis(); } String res = log.toString(); F.println(res); assert res.contains("/* PUBLIC.RANGE_INDEX */"); } finally { GridTestUtils.setFieldValue(spi, "log", oldLog); spi.configuration().setLongQueryExecutionTimeout(3000); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testExpiredTombstoneConversion() { int now = 100; Cell cell = expiring(cfm, "val", "a", now, 10, now + 10); XXX purged = cell.purge(new SimplePurger(now), now + 11); Assert.assertEquals(deleted(cfm, "val", now, now), purged); }', 'ground_truth': 'public void testExpiredTombstoneConversion() { int now = 100; Cell<?> cell = expiring(cfm, "val", "a", now, 10, now + 10); Cell<?> purged = cell.purge(new SimplePurger(now), now + 11); Assert.assertEquals(deleted(cfm, "val", now, now), purged); }', 'output': 'public void testExpiredTombstoneConversion() { int now = 100; Cell cell = expiring(cfm, "val", "a", now, 10, now + 10); TimeUUID purged = cell.purge(new SimplePurger(now), now + 11); Assert.assertEquals(deleted(cfm, "val", now, now), purged); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testClassComment() throws Exception { XXX srcList = new ArrayList(); srcList.add("groovy/xml/DOMBuilder.java"); xmlTool.add(srcList); MockOutputTool output = new MockOutputTool(); xmlTool.renderToOutput(output, MOCK_DIR); String domBuilderDoc = output.getText(MOCK_DIR + "/groovy/xml/DOMBuilder.html"); assertTrue(domBuilderDoc.indexOf("A helper class for creating a W3C DOM tree") > 0); }', 'ground_truth': 'public void testClassComment() throws Exception { List<String> srcList = new ArrayList<String>(); srcList.add("groovy/xml/DOMBuilder.java"); xmlTool.add(srcList); MockOutputTool output = new MockOutputTool(); xmlTool.renderToOutput(output, MOCK_DIR); String domBuilderDoc = output.getText(MOCK_DIR + "/groovy/xml/DOMBuilder.html"); assertTrue(domBuilderDoc.indexOf("A helper class for creating a W3C DOM tree") > 0); }', 'output': 'public void testClassComment() throws Exception { List<String> srcList = new ArrayList<>(); srcList.add("groovy/xml/DOMBuilder.java"); xmlTool.add(srcList); MockOutputTool output = new MockOutputTool(); xmlTool.renderToOutput(output, MOCK_DIR); String domBuilderDoc = output.getText(MOCK_DIR + "/groovy/xml/DOMBuilder.html"); assertTrue(domBuilderDoc.indexOf("A helper class for creating a W3C DOM tree") > 0); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Operation getQuery(String name, Timer timer, PartitionGenerator generator, SeedManager seeds, StressSettings settings, boolean isWarmup) { name = name.toLowerCase(); if (!queries.containsKey(name)) throw new IllegalArgumentException("No query defined with name " + name); if (queryStatements == null) { synchronized (this) { if (queryStatements == null) { JavaDriverClient jclient = settings.getJavaDriverClient(keyspaceName); Map<String, PreparedStatement> stmts = new HashMap<>(); Map<String, SchemaQuery.ArgSelect> args = new HashMap<>(); for (Map.Entry<String, StressYaml.QueryDef> e : queries.entrySet()) { stmts.put(e.getKey().toLowerCase(), jclient.prepare(e.getValue().cql)); args.put(e.getKey().toLowerCase(), e.getValue().fields == null ? SchemaQuery.ArgSelect.MULTIROW : SchemaQuery.ArgSelect.valueOf(e.getValue().fields.toUpperCase())); } queryStatements = stmts; argSelects = args; } } } return new SchemaQuery(timer, settings, generator, seeds, queryStatements.get(name), settings.command.consistencyLevel, argSelects.get(name)); }', 'ground_truth': 'public Operation getQuery(String name, Timer timer, PartitionGenerator generator, SeedManager seeds, StressSettings settings, boolean isWarmup) { name = name.toLowerCase(); if (!queries.containsKey(name)) throw new IllegalArgumentException("No query defined with name " + name); if (queryStatements == null) { synchronized (this) { if (queryStatements == null) { JavaDriverClient jclient = settings.getJavaDriverClient(keyspaceName); Map<String, PreparedStatement> stmts = new HashMap<>(); Map<String, SchemaStatement.ArgSelect> args = new HashMap<>(); for (Map.Entry<String, StressYaml.QueryDef> e : queries.entrySet()) { stmts.put(e.getKey().toLowerCase(), jclient.prepare(e.getValue().cql)); args.put(e.getKey().toLowerCase(), e.getValue().fields == null ? SchemaStatement.ArgSelect.MULTIROW : SchemaStatement.ArgSelect.valueOf(e.getValue().fields.toUpperCase())); } queryStatements = stmts; argSelects = args; } } } if (dynamicConditionExists(queryStatements.get(name))) return new CASQuery(timer, settings, generator, seeds, queryStatements.get(name), settings.command.consistencyLevel, argSelects.get(name), tableName); return new SchemaQuery(timer, settings, generator, seeds, queryStatements.get(name), settings.command.consistencyLevel, argSelects.get(name)); }', 'output': 'public Operation getQuery(String name, Timer timer, PartitionGenerator generator, SeedManager seeds, StressSettings settings, boolean isWarmup) { name = name.toLowerCase(); if (!queries.containsKey(name)) throw new IllegalArgumentException("No query defined with name " + name); if (queryStatements == null) { synchronized (this) { if (queryStatements == null) { JavaDriverClient jclient = settings.getJavaDriverClient(keyspaceName); Map<String, PreparedStatement> stmts = new HashMap<>(); Map<String, SchemaQuery.ArgSelect> args = new HashMap<>(); for (Map.Entry<String, StressYaml.QueryDef> e : queries.entrySet()) { stmts.put(e.getKey().toLowerCase(), jclient.prepare(e.getValue().cql)); args.put(e.getKey().toLowerCase(), e.getValue().fields == null ? SchemaQuery.ArgSelect.MULTIROW : SchemaQuery.ArgSelect.valueOf(e.getValue().fields.toUpperCase())); } queryStatements = stmts; argSelects = args; } } } return new SchemaQuery(timer, settings, generator, seeds, queryStatements.get(name), settings.command.consistencyLevel, argSelects.get(name)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void finalizeCommitNonExistantSession() { InstrumentedLocalSessions sessions = new InstrumentedLocalSessions(); sessions.start(); XXX fakeID = UUIDGen.getTimeUUID(); sessions.handleFinalizeCommitMessage(PARTICIPANT1, new FinalizeCommit(fakeID)); Assert.assertNull(sessions.getSession(fakeID)); Assert.assertTrue(sessions.sentMessages.isEmpty()); }', 'ground_truth': 'public void finalizeCommitNonExistantSession() { InstrumentedLocalSessions sessions = new InstrumentedLocalSessions(); sessions.start(); TimeUUID fakeID = nextTimeUUID(); sessions.handleFinalizeCommitMessage(PARTICIPANT1, new FinalizeCommit(fakeID)); Assert.assertNull(sessions.getSession(fakeID)); Assert.assertTrue(sessions.sentMessages.isEmpty()); }', 'output': 'public void finalizeCommitNonExistantSession() { InstrumentedLocalSessions sessions = new InstrumentedLocalSessions(); sessions.start(); UUID fakeID = UUIDGen.getTimeUUID(); sessions.handleFinalizeCommitMessage(PARTICIPANT1, new FinalizeCommit(fakeID)); Assert.assertNull(sessions.getSession(fakeID)); Assert.assertTrue(sessions.sentMessages.isEmpty()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void testCandidates(int gridCnt) throws Exception { try { Ignite ignite = startGridsMultiThreaded(gridCnt); GridCache<Integer, Integer> cache = ignite.cache(null); IgniteTx tx = cache.txStart(); cache.put(1, 1); tx.commit(); for (int i = 0; i < gridCnt; i++) { assert ((GridKernal)grid(i)).internalCache().context().mvcc().localCandidates().isEmpty(); assert ((GridKernal)grid(i)).internalCache().context().mvcc().remoteCandidates().isEmpty(); } } finally { stopAllGrids(); } }', 'ground_truth': 'private void testCandidates(int gridCnt) throws Exception { try { Ignite ignite = startGridsMultiThreaded(gridCnt); Cache<Integer, Integer> cache = ignite.cache(null); IgniteTx tx = cache.txStart(); cache.put(1, 1); tx.commit(); for (int i = 0; i < gridCnt; i++) { assert ((GridKernal)grid(i)).internalCache().context().mvcc().localCandidates().isEmpty(); assert ((GridKernal)grid(i)).internalCache().context().mvcc().remoteCandidates().isEmpty(); } } finally { stopAllGrids(); } }', 'output': 'private void testCandidates(int gridCnt) throws Exception { try { Ignite ignite = startGridsMultiThreaded(gridCnt); IgniteCache<Integer, Integer> cache = ignite.jcache(null); IgniteTx tx = cache.txStart(); cache.put(1, 1); tx.commit(); for (int i = 0; i < gridCnt; i++) { assert (!((GridKernal)grid(i)).internalCache().context().mvcc().localCandidates().isEmpty()); assert (!((GridKernal)grid(i)).internalCache().context().mvcc().remoteCandidates().isEmpty()); } } finally { stopAllGrids(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static ComboBoxModel createCookieClassModel() { XXX cookieClassModel = new DefaultComboBoxModel(); for (String fqcn : DataModel.PREDEFINED_COOKIE_CLASSES) { String name = DataModel.parseClassName(fqcn); cookieClassModel.addElement(name); } return cookieClassModel; }', 'ground_truth': 'private static ComboBoxModel createCookieClassModel() { DefaultComboBoxModel<String> cookieClassModel = new DefaultComboBoxModel<>(); for (String fqcn : DataModel.PREDEFINED_COOKIE_CLASSES) { String name = DataModel.parseClassName(fqcn); cookieClassModel.addElement(name); } return cookieClassModel; }', 'output': 'private static ComboBoxModel createCookieClassModel() { DefaultComboBoxModel<String> cookieClassModel = new DefaultComboBoxModel<>(); for (String fqcn : DataModel.PREDEFINED_COOKIE_CLASSES) { String name = DataModel.parseClassName(fqcn); cookieClassModel.addElement(name); } return cookieClassModel; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void testRowMutationWrite() throws IOException { RowMutation emptyRm = new RowMutation(Statics.KS, Statics.Key); RowMutation standardRowRm = new RowMutation(Statics.KS, Statics.StandardRow); RowMutation superRowRm = new RowMutation(Statics.KS, Statics.SuperRow); RowMutation standardRm = new RowMutation(Statics.KS, Statics.Key); standardRm.add(Statics.StandardCf); RowMutation superRm = new RowMutation(Statics.KS, Statics.Key); superRm.add(Statics.SuperCf); Map<Integer, ColumnFamily> mods = new HashMap<Integer, ColumnFamily>(); mods.put(Statics.StandardCf.metadata().cfId, Statics.StandardCf); mods.put(Statics.SuperCf.metadata().cfId, Statics.SuperCf); RowMutation mixedRm = new RowMutation(Statics.KS, Statics.Key, mods); DataOutputStream out = getOutput("db.RowMutation.bin"); RowMutation.serializer.serialize(emptyRm, out, getVersion()); RowMutation.serializer.serialize(standardRowRm, out, getVersion()); RowMutation.serializer.serialize(superRowRm, out, getVersion()); RowMutation.serializer.serialize(standardRm, out, getVersion()); RowMutation.serializer.serialize(superRm, out, getVersion()); RowMutation.serializer.serialize(mixedRm, out, getVersion()); emptyRm.createMessage().serialize(out, getVersion()); standardRowRm.createMessage().serialize(out, getVersion()); superRowRm.createMessage().serialize(out, getVersion()); standardRm.createMessage().serialize(out, getVersion()); superRm.createMessage().serialize(out, getVersion()); mixedRm.createMessage().serialize(out, getVersion()); out.close(); testSerializedSize(emptyRm, RowMutation.serializer); testSerializedSize(standardRowRm, RowMutation.serializer); testSerializedSize(superRowRm, RowMutation.serializer); testSerializedSize(standardRm, RowMutation.serializer); testSerializedSize(superRm, RowMutation.serializer); testSerializedSize(mixedRm, RowMutation.serializer); }', 'ground_truth': 'private void testRowMutationWrite() throws IOException { RowMutation emptyRm = new RowMutation(Statics.KS, Statics.Key); RowMutation standardRowRm = new RowMutation(Statics.KS, Statics.StandardRow); RowMutation superRowRm = new RowMutation(Statics.KS, Statics.SuperRow); RowMutation standardRm = new RowMutation(Statics.KS, Statics.Key); standardRm.add(Statics.StandardCf); RowMutation superRm = new RowMutation(Statics.KS, Statics.Key); superRm.add(Statics.SuperCf); Map<UUID, ColumnFamily> mods = new HashMap<UUID, ColumnFamily>(); mods.put(Statics.StandardCf.metadata().cfId, Statics.StandardCf); mods.put(Statics.SuperCf.metadata().cfId, Statics.SuperCf); RowMutation mixedRm = new RowMutation(Statics.KS, Statics.Key, mods); DataOutputStream out = getOutput("db.RowMutation.bin"); RowMutation.serializer.serialize(emptyRm, out, getVersion()); RowMutation.serializer.serialize(standardRowRm, out, getVersion()); RowMutation.serializer.serialize(superRowRm, out, getVersion()); RowMutation.serializer.serialize(standardRm, out, getVersion()); RowMutation.serializer.serialize(superRm, out, getVersion()); RowMutation.serializer.serialize(mixedRm, out, getVersion()); emptyRm.createMessage().serialize(out, getVersion()); standardRowRm.createMessage().serialize(out, getVersion()); superRowRm.createMessage().serialize(out, getVersion()); standardRm.createMessage().serialize(out, getVersion()); superRm.createMessage().serialize(out, getVersion()); mixedRm.createMessage().serialize(out, getVersion()); out.close(); testSerializedSize(emptyRm, RowMutation.serializer); testSerializedSize(standardRowRm, RowMutation.serializer); testSerializedSize(superRowRm, RowMutation.serializer); testSerializedSize(standardRm, RowMutation.serializer); testSerializedSize(superRm, RowMutation.serializer); testSerializedSize(mixedRm, RowMutation.serializer); }', 'output': 'private void testRowMutationWrite() throws IOException { RowMutation emptyRm = new RowMutation(Statics.KS, Statics.Key); RowMutation standardRowRm = new RowMutation(Statics.KS, Statics.StandardRow); RowMutation superRowRm = new RowMutation(Statics.KS, Statics.SuperRow); RowMutation standardRm = new RowMutation(Statics.KS, Statics.Key); standardRm.add(Statics.StandardCf); RowMutation superRm = new RowMutation(Statics.KS, Statics.Key); superRm.add(Statics.SuperCf); Map<Integer, ColumnFamily> mods = new HashMap<Integer, ColumnFamily>(); mods.put(Statics.StandardCf.metadata().cfId, Statics.StandardCf); mods.put(Statics.SuperCf.metadata().cfId, Statics.SuperCf); RowMutation mixedRm = new RowMutation(Statics.KS, Statics.Key, mods); DataOutputStream out = getOutput("db.RowMutation.bin"); RowMutation.serializer.serialize(emptyRm, out, getVersion()); RowMutation.serializer.serialize(standardRowRm, out, getVersion()); RowMutation.serializer.serialize(superRowRm, out, getVersion()); RowMutation.serializer.serialize(standardRm, out, getVersion()); RowMutation.serializer.serialize(superRm, out, getVersion()); RowMutation.serializer.serialize(mixedRm, out, getVersion()); emptyRm.createMessage().serialize(out, getVersion()); standardRowRm.createMessage().serialize(out, getVersion()); superRowRm.createMessage().serialize(out, getVersion()); standardRm.createMessage().serialize(out, getVersion()); superRm.createMessage().serialize(out, getVersion()); mixedRm.createMessage().serialize(out, getVersion()); out.close(); testSerializedSize(emptyRm, RowMutation.serializer); testSerializedSize('}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGetAsync() throws Exception { cache().put("key1", 1); cache().put("key2", 2); IgniteFuture<Integer> fut1 = cache().getAsync("key1"); IgniteFuture<Integer> fut2 = cache().getAsync("key2"); XXX fut3 = cache().getAsync("wrongKey"); assert fut1.get() == 1; assert fut2.get() == 2; assert fut3.get() == null; } public void testGetAsyncFiltered() throws Exception { cache().put("key1", 1); cache().put("key2", 100); IgniteFuture<Integer> fut1 = cache().projection(gte100).getAsync("key1"); IgniteFuture<Integer> fut2 = cache().projection(gte100).getAsync("key2"); assert fut1.get() == null; assert fut2.get() == 100; }', 'ground_truth': 'public void testGetAsync() throws Exception { cache().put("key1", 1); cache().put("key2", 2); IgniteInternalFuture<Integer> fut1 = cache().getAsync("key1"); IgniteInternalFuture<Integer> fut2 = cache().getAsync("key2"); IgniteInternalFuture<Integer> fut3 = cache().getAsync("wrongKey"); assert fut1.get() == 1; assert fut2.get() == 2; assert fut3.get() == null; }', 'output': 'public void testGetAsync() throws Exception { cache().put("key1", 1); cache().put("key2", 2); IgniteFuture<Integer> fut1 = cache().getAsync("key1"); IgniteFuture<Integer> fut2 = cache().getAsync("key2"); IgniteFuture<Integer> fut3 = cache().getAsync("wrongKey"); assert fut1.get() == 1; assert fut2.get() == 2; assert fut3.get() == null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void givenClassInRootPackage() throws Exception { final ApplicationFeatureId applicationFeatureId = ApplicationFeatureId.newType("Bar"); final XXX parentPackageId = applicationFeatureId.getParentPackageId(); assertThat(parentPackageId.getType(), is(ApplicationFeatureType.NAMESPACE)); assertThat(parentPackageId.getNamespace(), is("")); assertThat(parentPackageId.getTypeSimpleName(), is(nullValue())); assertThat(parentPackageId.getMemberName(), is(nullValue())); }', 'ground_truth': 'public void givenClassInRootPackage() throws Exception { val applicationFeatureId = ApplicationFeatureId.newType("Bar"); val parentPackageId = applicationFeatureId.getParentPackageId(); assertThat(parentPackageId.getSort(), is(ApplicationFeatureSort.NAMESPACE)); assertThat(parentPackageId.getNamespace(), is("")); assertThat(parentPackageId.getTypeSimpleName(), is(nullValue())); assertThat(parentPackageId.getMemberName(), is(nullValue())); }', 'output': 'public void givenClassInRootPackage() throws Exception { final ApplicationFeatureId applicationFeatureId = ApplicationFeatureId.newType("Bar"); final PackageId parentPackageId = applicationFeatureId.getParentPackageId(); assertThat(parentPackageId.getType(), is(PackageType.NAMESPACE)); assertThat(parentPackageId.getNamespace(), is("")); assertThat(parentPackageId.getTypeSimpleName(), is(nullValue())); assertThat(parentPackageId.getMemberName(), is(nullValue())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static synchronized HighlightingSpiPackageAccessor get() { try { XXX clazz = Class.forName(HighlightsLayer.class.getName()); } catch (ClassNotFoundException e) { } assert ACCESSOR != null : "There is no package accessor available!"; return ACCESSOR; } public abstract HighlightsLayerAccessor getHighlightsLayerAccessor(HighlightsLayer layer);\n    \n    public abstract int getZOrderRack(ZOrder zOrder);\n} public abstract int getZOrderRack(ZOrder zOrder); } } public abstract int getZOrderRack(ZOrder zOrder);\n} } }', 'ground_truth': 'public static synchronized HighlightingSpiPackageAccessor get() { try { Class<?> clazz = Class.forName(HighlightsLayer.class.getName()); } catch (ClassNotFoundException e) { } assert ACCESSOR != null : "There is no package accessor available!"; return ACCESSOR; }', 'output': 'public static synchronized HighlightingSpiPackageAccessor get() { try { Class<?> clazz = Class.forName(HighlightsLayer.class.getName()); } catch (ClassNotFoundException e) { } assert ACCESSOR != null : "There is no package accessor available!"; return ACCESSOR; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMoveRenameFileSourceParentRootDestinationMissingPartially() throws Exception { XXX file = new GridGgfsPath("/" + FILE_NEW.name()); create(ggfsSecondary, paths(DIR_NEW, SUBDIR_NEW), paths(file)); create(ggfs, paths(DIR_NEW), null); ggfs.rename(file, FILE_NEW); checkExist(ggfs, SUBDIR_NEW); checkExist(ggfs, ggfsSecondary, FILE_NEW); checkNotExist(ggfs, ggfsSecondary, file); } public void testMoveRenameFileSourceParentRootDestinationMissing() throws Exception { XXX file = new GridGgfsPath("/" + FILE_NEW.name()); create(ggfsSecondary, paths(DIR_NEW, SUBDIR_NEW), paths(file)); create(ggfs, null, null); ggfs.rename(file, FILE_NEW); checkExist(ggfs, DIR_NEW, SUBDIR_NEW); checkExist(ggfs, ggfsSecondary, FILE_NEW); checkNotExist(ggfs, ggfsSecondary, file); }', 'ground_truth': 'public void testMoveRenameFileSourceParentRootDestinationMissing() throws Exception { IgniteFsPath file = new IgniteFsPath("/" + FILE_NEW.name()); create(ggfsSecondary, paths(DIR_NEW, SUBDIR_NEW), paths(file)); create(ggfs, null, null); ggfs.rename(file, FILE_NEW); checkExist(ggfs, DIR_NEW, SUBDIR_NEW); checkExist(ggfs, ggfsSecondary, FILE_NEW); checkNotExist(ggfs, ggfsSecondary, file); }', 'output': 'public void testMoveRenameFileSourceParentRootDestinationMissing() throws Exception { IgniteFsPath file = new IgniteFsPath("/" + FILE_NEW.name()); create(ggfsSecondary, paths(DIR_NEW, SUBDIR_NEW), paths(file)); create(ggfs, null, null); ggfs.rename(file, FILE_NEW); checkExist(ggfs, DIR_NEW, SUBDIR_NEW); checkExist(ggfs, ggfsSecondary, FILE_NEW); checkNotExist(ggfs, ggfsSecondary, file); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private CacheStoreHolder initDir(File cacheWorkDir, int grpId, int partitions, AllocatedPageTracker allocatedTracker) throws IgniteCheckedException { try { boolean dirExisted = checkAndInitCacheWorkDir(cacheWorkDir); File idxFile = new File(cacheWorkDir, INDEX_FILE_NAME); if (dirExisted && !idxFile.exists()) grpsWithoutIdx.add(grpId); FilePageStoreFactory pageStoreFactory = new FileVersionCheckingFactory( pageStoreFileIoFactory, pageStoreV1FileIoFactory, igniteCfg.getDataStorageConfiguration()); FilePageStore idxStore = pageStoreFactory.createPageStore( PageMemory.FLAG_IDX, idxFile, allocatedTracker); FilePageStore[] partStores = new FilePageStore[partitions]; for (int partId = 0; partId < partStores.length; partId++) { FilePageStore partStore = pageStoreFactory.createPageStore( PageMemory.FLAG_DATA, getPartitionFile(cacheWorkDir, partId), allocatedTracker); partStores[partId] = partStore; } return new CacheStoreHolder(idxStore, partStores); } catch (XXX e) { cctx.kernalContext().failure().process(new FailureContext(FailureType.CRITICAL_ERROR, e)); throw e; } }', 'ground_truth': 'private CacheStoreHolder initDir(File cacheWorkDir, int grpId, int partitions, AllocatedPageTracker allocatedTracker) throws IgniteCheckedException { try { boolean dirExisted = checkAndInitCacheWorkDir(cacheWorkDir); File idxFile = new File(cacheWorkDir, INDEX_FILE_NAME); if (dirExisted && !idxFile.exists()) grpsWithoutIdx.add(grpId); FilePageStoreFactory pageStoreFactory = new FileVersionCheckingFactory( pageStoreFileIoFactory, pageStoreV1FileIoFactory, igniteCfg.getDataStorageConfiguration()); FilePageStore idxStore = pageStoreFactory.createPageStore( PageMemory.FLAG_IDX, idxFile, allocatedTracker); FilePageStore[] partStores = new FilePageStore[partitions]; for (int partId = 0; partId < partStores.length; partId++) { FilePageStore partStore = pageStoreFactory.createPageStore( PageMemory.FLAG_DATA, getPartitionFile(cacheWorkDir, partId), allocatedTracker); partStores[partId] = partStore; } return new CacheStoreHolder(idxStore, partStores); } catch (StorageException e) { cctx.kernalContext().failure().process(new FailureContext(FailureType.CRITICAL_ERROR, e)); throw e; } }', 'output': 'private CacheStoreHolder initDir(File cacheWorkDir, int grpId, int partitions, AllocatedPageTracker allocatedTracker) throws IgniteCheckedException { try { boolean dirExisted = checkAndInitCacheWorkDir(cacheWorkDir); File idxFile = new File(cacheWorkDir, INDEX_FILE_NAME); if (dirExisted && !idxFile.exists()) grpsWithoutIdx.add(grpId); FilePageStoreFactory pageStoreFactory = new FileVersionCheckingFactory( pageStoreFileIoFactory, pageStoreV1FileIoFactory, igniteCfg.getDataStorageConfiguration()); FilePageStore idxStore = pageStoreFactory.createPageStore( PageMemory.FLAG_IDX, idxFile, allocatedTracker); FilePageStore[] partStores = new FilePageStore[partitions]; for (int partId = 0; partId < partStores.length; partId++) { FilePageStore partStore = pageStoreFactory.createPageStore( PageMemory.FLAG_DATA, getPartitionFile(cacheWorkDir, partId), allocatedTracker); partStores[partId] = partStore; } return new CacheStoreHolder(idxStore, partStores); } catch (IgniteCheckedException e) { cctx.kernalContext().failure().process(new FailureContext(FailureType.CRITICAL_ERROR, e)); throw e; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGridifyHierarchyPrivate() throws Exception { XXX target = new GridTarget(); target.methodB(); }', 'ground_truth': 'public void testGridifyHierarchyPrivate() throws Exception { Target target = new Target(); target.methodB(); }', 'output': 'public void testGridifyHierarchyPrivate() throws Exception { Target target = new Target(); target.methodB(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Collection<InetAddress> getWriteEndpoints(Token token, String table, Collection<InetAddress> naturalEndpoints) { Map<Range, Collection<InetAddress>> ranges = getPendingRanges(table); if (ranges.isEmpty()) return naturalEndpoints; Set<InetAddress> endpoints = new HashSet<InetAddress>(naturalEndpoints); for (Map.Entry<Range, Collection<InetAddress>> entry : ranges.entrySet()) { if (entry.getKey().contains(token)) { endpoints.addAll(entry.getValue()); } } return endpoints; }', 'ground_truth': 'public Collection<InetAddress> getWriteEndpoints(Token token, String table, Collection<InetAddress> naturalEndpoints) { Map<Range<Token>, Collection<InetAddress>> ranges = getPendingRanges(table); if (ranges.isEmpty()) return naturalEndpoints; Set<InetAddress> endpoints = new HashSet<InetAddress>(naturalEndpoints); for (Map.Entry<Range<Token>, Collection<InetAddress>> entry : ranges.entrySet()) { if (entry.getKey().contains(token)) { endpoints.addAll(entry.getValue()); } } return endpoints; }', 'output': 'public Collection<InetAddress> getWriteEndpoints(Token token, String table, Collection<InetAddress> naturalEndpoints) { Map<Range, Collection<InetAddress>> ranges = getPendingRanges(table); if (ranges.isEmpty()) return naturalEndpoints; Set<InetAddress> endpoints = new HashSet<InetAddress>(naturalEndpoints); for (Map.Entry<Range, Collection<InetAddress>> entry : ranges.entrySet()) { if (entry.getKey().contains(token)) { endpoints.addAll(entry.getValue()); } } return endpoints; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testScriptWithDerivedBaseClass() throws Exception { Binding context = new Binding(); XXX config = new CompilerConfig(); config.setScriptBaseClass(DerivedScript.class.getName()); GroovyShell shell = new GroovyShell(context, config); Object result = shell.evaluate("x = \'abc\'; doSomething(cheese)"); assertEquals("I like Cheddar", result); assertEquals("abc", context.getVariable("x")); }', 'ground_truth': 'public void testScriptWithDerivedBaseClass() throws Exception { Binding context = new Binding(); CompilerConfiguration config = new CompilerConfiguration(); config.setScriptBaseClass(DerivedScript.class.getName()); GroovyShell shell = new GroovyShell(context, config); Object result = shell.evaluate("x = \'abc\'; doSomething(cheese)"); assertEquals("I like Cheddar", result); assertEquals("abc", context.getVariable("x")); }', 'output': 'public void testScriptWithDerivedBaseClass() throws Exception { Binding context = new Binding(); CompilerConfig config = new CompilerConfig(); config.setScriptBaseClass(DerivedScript.class.getName()); GroovyShell shell = new GroovyShell(context, config); Object result = shell.evaluate("x = \'abc\'; doSomething(cheese)"); assertEquals("I like Cheddar", result); assertEquals("abc", context.getVariable("x")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void run(CQLQueryExecutor executor) throws IOException { if (session.getColumnFamilyType() == ColumnFamilyType.Super) throw new RuntimeException("Super columns are not implemented for CQL"); if (cqlQuery == null) { StringBuilder query = new StringBuilder("SELECT FIRST ").append(session.getColumnsPerKey()) .append(" \'\'..\'\' FROM Standard1"); if (session.cqlVersion.startsWith("2")) query.append(" USING CONSISTENCY ").append(session.getConsistencyLevel().toString()); cqlQuery = query.append(" WHERE KEY > ?").toString(); } String key = String.format("%0" +  session.getTotalKeysLength() + "d", index); List<String> queryParams = Collections.singletonList(getUnQuotedCqlBlob(key, session.cqlVersion.startsWith("3"))); XXX context = session.latency.time(); boolean success = false; String exceptionMessage = null; for (int t = 0; t < session.getRetryTimes(); t++) { if (success) break; try { success = executor.execute(cqlQuery, queryParams); } catch (Exception e) { System.err.println(e); exceptionMessage = getExceptionMessage(e); success = false; } } if (!success) { error(String.format("Operation [%d] retried %d times - error executing range slice with offset %s %s%n", index, session.getRetryTimes(), key, (exceptionMessage == null) ? "" : "(" + exceptionMessage + ")")); } session.operations.getAndIncrement(); session.keys.getAndAdd(lastRowCount); context.stop(); }', 'ground_truth': 'protected void run(CQLQueryExecutor executor) throws IOException { if (session.getColumnFamilyType() == ColumnFamilyType.Super) throw new RuntimeException("Super columns are not implemented for CQL"); if (cqlQuery == null) { StringBuilder query = new StringBuilder("SELECT FIRST ").append(session.getColumnsPerKey()) .append(" \'\'..\'\' FROM Standard1"); if (session.cqlVersion.startsWith("2")) query.append(" USING CONSISTENCY ").append(session.getConsistencyLevel().toString()); cqlQuery = query.append(" WHERE KEY > ?").toString(); } String key = String.format("%0" +  session.getTotalKeysLength() + "d", index); List<String> queryParams = Collections.singletonList(getUnQuotedCqlBlob(key, session.cqlVersion.startsWith("3"))); TimerContext context = session.latency.time(); boolean success = false; String exceptionMessage = null; for (int t = 0; t < session.getRetryTimes(); t++) { if (success) break; try { success = executor.execute(cqlQuery, queryParams); } catch (Exception e) { System.err.println(e); exceptionMessage = getExceptionMessage(e); success = false; } } if (!success) { error(String.format("Operation [%d] retried %d times - error executing range slice with offset %s %s%n", index, session.getRetryTimes(), key, (exceptionMessage == null) ? "" : "(" + exceptionMessage + ")")); } session.operations.getAndIncrement(); session.keys.getAndAdd(lastRowCount); context.stop(); }', 'output': 'protected void run(CQLQueryExecutor executor) throws IOException { if (session.getColumnFamilyType() == ColumnFamilyType.Super) throw new RuntimeException("Super columns are not implemented for CQL"); if (cqlQuery == null) { StringBuilder query = new StringBuilder("SELECT FIRST ").append(session.getColumnsPerKey()) .append(" \'\'..\'\' FROM Standard1"); if (session.cqlVersion.startsWith("2")) query.append(" USING CONSISTENCY ").append(session.getConsistencyLevel().toString()); cqlQuery = query.append(" WHERE KEY > ?").toString(); } String key = String.format("%0" +  session.getTotalKeysLength() + "d", index); List<String> queryParams = Collections.singletonList(getUnQuotedCqlBlob(key, session.cqlVersion.startsWith("3"))); TimeContext context = session.latency.time(); boolean success = false; String exceptionMessage = null; for (int t = 0; t < session.getRetryTimes(); t++) { if (success) break; try { success = executor.execute(cqlQuery, queryParams); } catch (Exception e) { System.err.println(e); exceptionMessage = getExceptionMessage(e); success = false; } } if (!success) { error(String.format("Operation [%d] retried %d times - error executing range slice with offset %s %s%n", index, session.getRetryTimes(), key, (exceptionMessage == null) ? "" : "(" + exceptionMessage + ")")); } session.operations.getAndIncrement(); session.keys.getAndAdd(lastRowCount); context.stop(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void userDefinedTaskTest() { PendingRepairManager prm = csm.getPendingRepairManagers().get(0); XXX repairId = registerSession(cfs, true, true); SSTableReader sstable = makeSSTable(true); mutateRepaired(sstable, repairId, false); prm.addSSTable(sstable); try (CompactionTasks tasks = csm.getUserDefinedTasks(Collections.singleton(sstable), 100)) { Assert.assertEquals(1, tasks.size()); } }', 'ground_truth': 'public void userDefinedTaskTest() { PendingRepairManager prm = csm.getPendingRepairManagers().get(0); TimeUUID repairId = registerSession(cfs, true, true); SSTableReader sstable = makeSSTable(true); mutateRepaired(sstable, repairId, false); prm.addSSTable(sstable); try (CompactionTasks tasks = csm.getUserDefinedTasks(Collections.singleton(sstable), 100)) { Assert.assertEquals(1, tasks.size()); } }', 'output': 'public void userDefinedTaskTest() { PendingRepairManager prm = csm.getPendingRepairManagers().get(0); UUID repairId = registerSession(cfs, true, true); SSTableReader sstable = makeSSTable(true); mutateRepaired(sstable, repairId, false); prm.addSSTable(sstable); try (CompactionTasks tasks = csm.getUserDefinedTasks(Collections.singleton(sstable), 100)) { Assert.assertEquals(1, tasks.size()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public DAG<Operator<?, ?, ?>> getBasicOps() { Flow flow = input.getFlow(); ReduceByKey<IN, IN, Void, Void, W, Dataset<Pair<IN, Void>>> reduce; reduce = new ReduceByKey<>( flow, input, e -> e, e -> null, windowing, (CombinableReduceFunction<Void>) e -> null); reduce.setPartitioning(getPartitioning()); Dataset<Pair<IN, Void>> reduced = reduce.output(); Map<Pair<IN, Void>, IN, Dataset<IN>> format = new Map<>( flow, reduced, Pair::getFirst); DAG<Operator<?, ?, ?>> dag = DAG.of(reduce); dag.add(format, reduce); return dag; }', 'ground_truth': 'public DAG<Operator<?, ?>> getBasicOps() { Flow flow = input.getFlow(); ReduceByKey<IN, IN, Void, Void, W> reduce; reduce = new ReduceByKey<>( flow, input, e -> e, e -> null, windowing, (CombinableReduceFunction<Void>) e -> null); reduce.setPartitioning(getPartitioning()); Dataset<Pair<IN, Void>> reduced = reduce.output(); Map<Pair<IN, Void>, IN> format = new Map<>(flow, reduced, Pair::getFirst); DAG<Operator<?, ?>> dag = DAG.of(reduce); dag.add(format, reduce); return dag; }', 'output': 'public DAG<Operator<?, ?, ?>> getBasicOps() { Flow flow = input.getFlow(); ReduceByKey<IN, IN, Void, Void, W, Dataset<Tuple2<IN, Void>>> reduce; reduce = new ReduceByKey<>( flow, input, e -> e, e -> null, windowing, (CombinableReduceFunction<Void>) e -> null); reduce.setPartitioning(getPartitioning()); Dataset<Tuple2<IN, Void>> reduced = reduce.output(); Map<Tuple2<IN, Void>, IN, Dataset<IN>> format = new Map<>( flow, reduced, Tuple2::getFirst); DAG<Operator<?, ?, ?>> dag = DAG.of(reduce); dag.add(format, reduce); return dag; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static <T> void translateReadHelper( Read.Bound<T> transform, DataflowPipelineTranslator.TranslationContext context) { try { Source<T> anySource = transform.getSource(); if (!(anySource instanceof BoundedSource)) { throw new IllegalArgumentException("Unexpected read from a user-defined unbounded source"); } XXX source = (BoundedSource<T>) anySource; context.addStep(transform, "ParallelRead"); context.addInput(PropertyNames.FORMAT, PropertyNames.CUSTOM_SOURCE_FORMAT); context.addInput( PropertyNames.SOURCE_STEP_INPUT, cloudSourceToDictionary(serializeToCloudSource(source, context.getPipelineOptions()))); context.addValueOnlyOutput(PropertyNames.OUTPUT, context.getOutput(transform)); } catch (Exception e) { throw new RuntimeException(e); } }', 'ground_truth': 'public static <T> void translateReadHelper( Read.Bound<T> transform, DataflowPipelineTranslator.TranslationContext context) { try { Source<T> source = transform.getSource(); context.addStep(transform, "ParallelRead"); context.addInput(PropertyNames.FORMAT, PropertyNames.CUSTOM_SOURCE_FORMAT); context.addInput( PropertyNames.SOURCE_STEP_INPUT, cloudSourceToDictionary(serializeToCloudSource(source, context.getPipelineOptions()))); context.addValueOnlyOutput(PropertyNames.OUTPUT, context.getOutput(transform)); } catch (Exception e) { throw new RuntimeException(e); } }', 'output': 'public static <T> void translateReadHelper( Read.Bound<T> transform, DataflowPipelineTranslator.TranslationContext context) { try { Source<T> anySource = transform.getSource(); if (!(anySource instanceof BoundedSource)) { throw new IllegalArgumentException("Unexpected read from a user-defined unbounded source"); } BoundedSource<T> source = (BoundedSource<T>) anySource; context.addStep(transform, "ParallelRead"); context.addInput(PropertyNames.FORMAT, PropertyNames.CUSTOM_SOURCE_FORMAT); context.addInput( PropertyNames.SOURCE_STEP_INPUT, cloudSourceToDictionary(serializeToCloudSource(source, context.getPipelineOptions()))); context.addValueOnlyOutput(PropertyNames.OUTPUT, context.getOutput(transform)); } catch (Exception e) { throw new RuntimeException(e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testResultFailed() throws Exception { Ignite ignite = G.grid(getTestGridName()); ignite.compute().localDeployTask(GridTaskFailedTestTask.class, GridTaskFailedTestTask.class.getClassLoader()); XXX fut = executeAsync(ignite.compute(), GridTaskFailedTestTask.class.getName(), FailType.RESULT); try { fut.get(); assert false; } catch (GridException e) { info("Got expected grid exception: " + e); } IgniteUuid sesId = fut.getTaskSession().getId(); List<GridEvent> evts = ignite.events().remoteQuery(new TaskFailedEventFilter(sesId), 0); assert evts.size() == 1; info("Task failed event: " + evts.get(0)); }', 'ground_truth': 'public void testResultFailed() throws Exception { Ignite ignite = G.grid(getTestGridName()); ignite.compute().localDeployTask(GridTaskFailedTestTask.class, GridTaskFailedTestTask.class.getClassLoader()); ComputeTaskFuture<?> fut = executeAsync(ignite.compute(), GridTaskFailedTestTask.class.getName(), FailType.RESULT); try { fut.get(); assert false; } catch (GridException e) { info("Got expected grid exception: " + e); } IgniteUuid sesId = fut.getTaskSession().getId(); List<GridEvent> evts = ignite.events().remoteQuery(new TaskFailedEventFilter(sesId), 0); assert evts.size() == 1; info("Task failed event: " + evts.get(0)); }', 'output': 'public void testResultFailed() throws Exception { Ignite ignite = G.grid(getTestGridName()); ignite.compute().localDeployTask(GridTaskFailedTestTask.class, GridTaskFailedTestTask.class.getClassLoader()); IgniteFuture<?> fut = executeAsync(ignite.compute(), GridTaskFailedTestTask.class.getName(), FailType.RESULT); try { fut.get(); assert false; } catch (GridException e) { info("Got expected grid exception: " + e); } IgniteUuid sesId = fut.getTaskSession().getId(); List<GridEvent> evts = ignite.events().remoteQuery(new TaskFailedEventFilter(sesId), 0); assert evts.size() == 1; info("Task failed event: " + evts.get(0)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testTupleWithNatives() { for (XXX version = Server.MIN_SUPPORTED_VERSION; version <= Server.CURRENT_VERSION; version++) { for (int n = 0; n < 100; n++) { Value value = generateTupleValue(version, randomTupleType(0), true); compareCqlLiteral(version, value); } } }', 'ground_truth': 'public void testTupleWithNatives() { for (ProtocolVersion version : ProtocolVersion.SUPPORTED) { for (int n = 0; n < 100; n++) { Value value = generateTupleValue(version, randomTupleType(0), true); compareCqlLiteral(version, value); } } }', 'output': 'public void testTupleWithNatives() { for (final Version version = Server.MIN_SUPPORTED_VERSION; version <= Server.CURRENT_VERSION; version++) { for (int n = 0; n < 100; n++) { Value value = generateTupleValue(version, randomTupleType(0), true); compareCqlLiteral(version, value); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void _testNodeJoinWithP2P() throws Exception { final Collection<UUID> nodeIds = new HashSet<>(); final AtomicInteger cnt = new AtomicInteger(); final CountDownLatch latch = new CountDownLatch(GRID_CNT + 1); ClassLoader ldr = getExternalClassLoader(); GridPredicate<ClusterNode> prjPred = (GridPredicate<ClusterNode>)ldr.loadClass(PRJ_PRED_CLS_NAME).newInstance(); XXX filter = (GridPredicate<GridEvent>)ldr.loadClass(FILTER_CLS_NAME).newInstance(); UUID consumeId = events(grid(0).forPredicate(prjPred)).remoteListen(new P2<UUID, GridEvent>() { @Override public boolean apply(UUID nodeId, GridEvent evt) { info("Event from " + nodeId + " [" + evt.shortDisplay() + \']\'); assertEquals(EVT_JOB_STARTED, evt.type()); nodeIds.add(nodeId); cnt.incrementAndGet(); latch.countDown(); return true; } }, filter, EVT_JOB_STARTED); try { assertNotNull(consumeId); startGrid("anotherGrid"); grid(0).compute().broadcast(F.noop()); assert latch.await(2, SECONDS); assertEquals(GRID_CNT + 1, nodeIds.size()); assertEquals(GRID_CNT + 1, cnt.get()); } finally { stopGrid("anotherGrid1"); stopGrid("anotherGrid2"); grid(0).events().stopRemoteListen(consumeId); } }', 'ground_truth': 'public void _testNodeJoinWithP2P() throws Exception { final Collection<UUID> nodeIds = new HashSet<>(); final AtomicInteger cnt = new AtomicInteger(); final CountDownLatch latch = new CountDownLatch(GRID_CNT + 1); ClassLoader ldr = getExternalClassLoader(); IgnitePredicate<ClusterNode> prjPred = (IgnitePredicate<ClusterNode>)ldr.loadClass(PRJ_PRED_CLS_NAME).newInstance(); IgnitePredicate<GridEvent> filter = (IgnitePredicate<GridEvent>)ldr.loadClass(FILTER_CLS_NAME).newInstance(); UUID consumeId = events(grid(0).forPredicate(prjPred)).remoteListen(new P2<UUID, GridEvent>() { @Override public boolean apply(UUID nodeId, GridEvent evt) { info("Event from " + nodeId + " [" + evt.shortDisplay() + \']\'); assertEquals(EVT_JOB_STARTED, evt.type()); nodeIds.add(nodeId); cnt.incrementAndGet(); latch.countDown(); return true; } }, filter, EVT_JOB_STARTED); try { assertNotNull(consumeId); startGrid("anotherGrid"); grid(0).compute().broadcast(F.noop()); assert latch.await(2, SECONDS); assertEquals(GRID_CNT + 1, nodeIds.size()); assertEquals(GRID_CNT + 1, cnt.get()); } finally { stopGrid("anotherGrid1"); stopGrid("anotherGrid2"); grid(0).events().stopRemoteListen(consumeId); } }', 'output': 'public void _testNodeJoinWithP2P() throws Exception { final Collection<UUID> nodeIds = new HashSet<>(); final AtomicInteger cnt = new AtomicInteger(); final CountDownLatch latch = new CountDownLatch(GRID_CNT + 1); ClassLoader ldr = getExternalClassLoader(); GridPredicate<ClusterNode> prjPred = (GridPredicate<ClusterNode>)ldr.loadClass(PRJ_PRED_CLS_NAME).newInstance(); GridPredicate<GridEvent> filter = (GridPredicate<GridEvent>)ldr.loadClass(FILTER_CLS_NAME).newInstance(); UUID consumeId = events(grid(0).forPredicate(prjPred)).remoteListen(new P2<UUID, GridEvent>() { @Override public boolean apply(UUID nodeId, GridEvent evt) { info("Event from " + nodeId + " [" + evt.shortDisplay() + \']\'); assertEquals(EVT_JOB_STARTED, evt.type()); nodeIds.add(nodeId); cnt.incrementAndGet(); latch.countDown(); return true; } }, filter, EVT_JOB_STARTED); try { assertNotNull(consumeId); startGrid("anotherGrid"); grid(0).compute().broadcast(F.noop()); assert latch.await(2, SECONDS); assertEquals(GRID_CNT + 1, nodeIds.size()); assertEquals(GRID_CNT + 1, cnt.get()); } finally { stopGrid("anotherGrid1"); stopGrid("anotherGrid2"); grid(0).events().stopRemoteListen(consumeId); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testLoadCache() throws Exception { GridCache<Integer, Integer> cache = grid(0).cache(null); GridCacheContinuousQuery<Integer, Integer> qry = cache.queries().createContinuousQuery(); final Map<Integer, Integer> map = new ConcurrentHashMap8<>(); final CountDownLatch latch = new CountDownLatch(10); qry.localCallback(new P2<UUID, Collection<org.apache.ignite.cache.query.GridCacheContinuousQueryEntry<Integer, Integer>>>() { @Override public boolean apply(UUID nodeId, Collection<org.apache.ignite.cache.query.GridCacheContinuousQueryEntry<Integer, Integer>> entries) { for (Map.Entry<Integer, Integer> e : entries) { map.put(e.getKey(), e.getValue()); latch.countDown(); } return true; } }); try { qry.execute(); for (int i = 0; i < gridCount(); i++) grid(i).cache(null).loadCache(null, 0); assert latch.await(LATCH_TIMEOUT, MILLISECONDS) : "Count: " + latch.getCount(); assertEquals(10, map.size()); for (int i = 0; i < 10; i++) assertEquals(i, (int)map.get(i)); } finally { qry.close(); } }', 'ground_truth': 'public void testLoadCache() throws Exception { Cache<Integer, Integer> cache = grid(0).cache(null); CacheContinuousQuery<Integer, Integer> qry = cache.queries().createContinuousQuery(); final Map<Integer, Integer> map = new ConcurrentHashMap8<>(); final CountDownLatch latch = new CountDownLatch(10); qry.localCallback(new P2<UUID, Collection<CacheContinuousQueryEntry<Integer, Integer>>>() { @Override public boolean apply(UUID nodeId, Collection<CacheContinuousQueryEntry<Integer, Integer>> entries) { for (Map.Entry<Integer, Integer> e : entries) { map.put(e.getKey(), e.getValue()); latch.countDown(); } return true; } }); try { qry.execute(); for (int i = 0; i < gridCount(); i++) grid(i).cache(null).loadCache(null, 0); assert latch.await(LATCH_TIMEOUT, MILLISECONDS) : "Count: " + latch.getCount(); assertEquals(10, map.size()); for (int i = 0; i < 10; i++) assertEquals(i, (int)map.get(i)); } finally { qry.close(); } }', 'output': 'public void testLoadCache() throws Exception { Cache<Integer, Integer> cache = grid(0).cache(null); CacheContinuousQuery<Integer, Integer> qry = cache.queries().createContinuousQuery(); final Map<Integer, Integer> map = new ConcurrentHashMap8<>(); final CountDownLatch latch = new CountDownLatch(10); qry.localCallback(new P2<UUID, Collection<org.apache.ignite.cache.query.CacheContinuousQueryEntry<Integer, Integer>>>() { @Override public boolean apply(UUID nodeId, Collection<org.apache.ignite.cache.query.CacheContinuousQueryEntry<Integer, Integer>> entries) { for (Map.Entry<Integer, Integer> e : entries) { map.put(e.getKey(), e.getValue()); latch.countDown(); } return true; } }); try { qry.execute(); for (int i = 0; i < gridCount(); i++) grid(i).cache(null).loadCache(null, 0); assert latch.await(LATCH_TIMEOUT, MILLISECONDS) : "Count: " + latch.getCount(); assertEquals(10, map.size()); for (int i = 0; i < 10; i++) assertEquals(i, (int)map.get(i)); } finally { qry.close(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Map<MetadataType, MetadataComponent> constructMetadata() { CommitLogPosition club = new CommitLogPosition(11L, 12); CommitLogPosition cllb = new CommitLogPosition(9L, 12); XXX cfm = SchemaLoader.standardCFMD("ks1", "cf1"); MetadataCollector collector = new MetadataCollector(cfm.comparator) .commitLogIntervals(new IntervalSet<>(cllb, club)); String partitioner = RandomPartitioner.class.getCanonicalName(); double bfFpChance = 0.1; Map<MetadataType, MetadataComponent> originalMetadata = collector.finalizeMetadata(partitioner, bfFpChance, 0, SerializationHeader.make(cfm, Collections.emptyList())); return originalMetadata; }', 'ground_truth': 'public Map<MetadataType, MetadataComponent> constructMetadata() { CommitLogPosition club = new CommitLogPosition(11L, 12); CommitLogPosition cllb = new CommitLogPosition(9L, 12); TableMetadata cfm = SchemaLoader.standardCFMD("ks1", "cf1").build(); MetadataCollector collector = new MetadataCollector(cfm.comparator) .commitLogIntervals(new IntervalSet<>(cllb, club)); String partitioner = RandomPartitioner.class.getCanonicalName(); double bfFpChance = 0.1; Map<MetadataType, MetadataComponent> originalMetadata = collector.finalizeMetadata(partitioner, bfFpChance, 0, SerializationHeader.make(cfm, Collections.emptyList())); return originalMetadata; }', 'output': 'public Map<MetadataType, MetadataComponent> constructMetadata() { CommitLogPosition club = new CommitLogPosition(11L, 12); CommitLogPosition cllb = new CommitLogPosition(9L, 12); CFMetaData cfm = SchemaLoader.standardCFMD("ks1", "cf1"); MetadataCollector collector = new MetadataCollector(cfm.comparator) .commitLogIntervals(new IntervalSet<>(cllb, club)); String partitioner = RandomPartitioner.class.getCanonicalName(); double bfFpChance = 0.1; Map<MetadataType, MetadataComponent> originalMetadata = collector.finalizeMetadata(partitioner, bfFpChance, 0, SerializationHeader.make(cfm, Collections.emptyList())); return originalMetadata; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<UpgradeCheck> getFilteredUpgradeChecks(UpgradePack upgradePack) throws AmbariException { List<UpgradeCheck> builtInRequiredChecks = new ArrayList<>(); for (UpgradeCheck builtInCheck : m_builtInChecks) { if (isBuiltInCheckRequired(builtInCheck, upgradePack.getType())) { builtInRequiredChecks.add(builtInCheck); } } XXX pluginChecks = m_pluginChecks.get(upgradePack); if (null == pluginChecks) { pluginChecks = new TreeSet<>(new PreUpgradeCheckComparator()); m_pluginChecks.put(upgradePack, pluginChecks); List<String> pluginCheckClassNames = upgradePack.getPrerequisiteChecks(); if (null != pluginCheckClassNames && !pluginCheckClassNames.isEmpty()) { loadPluginUpgradeChecksFromStack(upgradePack, pluginChecks); } } final Set<UpgradeCheck> combinedUpgradeChecks = new TreeSet<>(new PreUpgradeCheckComparator()); combinedUpgradeChecks.addAll(builtInRequiredChecks); combinedUpgradeChecks.addAll(pluginChecks); return new LinkedList<>(combinedUpgradeChecks); }', 'ground_truth': 'public List<UpgradeCheck> getFilteredUpgradeChecks(UpgradePack upgradePack) throws AmbariException { List<UpgradeCheck> builtInRequiredChecks = new ArrayList<>(); for (UpgradeCheck builtInCheck : m_builtInChecks) { if (isBuiltInCheckRequired(builtInCheck, upgradePack.getType())) { builtInRequiredChecks.add(builtInCheck); } } PluginUpgradeChecks pluginChecks = m_pluginChecks.get(upgradePack); if (null == pluginChecks) { pluginChecks = new PluginUpgradeChecks(new TreeSet<>(new PreUpgradeCheckComparator()), new TreeSet<>()); m_pluginChecks.put(upgradePack, pluginChecks); List<String> pluginCheckClassNames = upgradePack.getPrerequisiteChecks(); if (null != pluginCheckClassNames && !pluginCheckClassNames.isEmpty()) { loadPluginUpgradeChecksFromStack(upgradePack, pluginChecks); } } final Set<UpgradeCheck> combinedUpgradeChecks = new TreeSet<>(new PreUpgradeCheckComparator()); combinedUpgradeChecks.addAll(builtInRequiredChecks); combinedUpgradeChecks.addAll(pluginChecks.m_loadedChecks); return new LinkedList<>(combinedUpgradeChecks); }', 'output': 'public List<UpgradeCheck> getFilteredUpgradeChecks(UpgradePack upgradePack) throws AmbariException { List<UpgradeCheck> builtInRequiredChecks = new ArrayList<>(); for (UpgradeCheck builtInCheck : m_builtInChecks) { if (isBuiltInCheckRequired(builtInCheck, upgradePack.getType())) { builtInRequiredChecks.add(builtInCheck); } } Collection<UpgradeCheck> pluginChecks = m_pluginChecks.get(upgradePack); if (null == pluginChecks) { pluginChecks = new TreeSet<>(new PreUpgradeCheckComparator()); m_pluginChecks.put(upgradePack, pluginChecks); List<String> pluginCheckClassNames = upgradePack.getPrerequisiteChecks(); if (null != pluginCheckClassNames && !pluginCheckClassNames.isEmpty()) { loadPluginUpgradeChecksFromStack(upgradePack, pluginChecks); } } final Set<UpgradeCheck> combinedUpgradeChecks = new TreeSet<>(new PreUpgradeCheckComparator()); combinedUpgradeChecks.addAll(builtInRequiredChecks); combinedUpgradeChecks.addAll(pluginChecks); return new LinkedList<>(combinedUpgradeChecks); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testTaskAsync() throws Exception { U.sleep(3000); assertFalse(ggfs.isAsync()); XXX ggfsAsync = ggfs.enableAsync(); assertTrue(ggfsAsync.isAsync()); for (int i = 0; i < REPEAT_CNT; i++) { String arg = DICTIONARY[new Random(System.currentTimeMillis()).nextInt(DICTIONARY.length)]; generateFile(TOTAL_WORDS); Long genLen = ggfs.info(FILE).length(); assertNull(ggfsAsync.execute( new Task(), new GridGgfsStringDelimiterRecordResolver(" "), Collections.singleton(FILE), arg)); IgniteFuture<IgniteBiTuple<Long, Integer>> fut = ggfsAsync.future(); assertNotNull(fut); IgniteBiTuple<Long, Integer> taskRes = fut.get(); assert F.eq(genLen, taskRes.getKey()); assert F.eq(TOTAL_WORDS, taskRes.getValue()); } ggfsAsync.format(); IgniteFuture<?> fut = ggfsAsync.future(); assertNotNull(fut); fut.get(); }', 'ground_truth': 'public void testTaskAsync() throws Exception { U.sleep(3000); assertFalse(ggfs.isAsync()); IgniteFs ggfsAsync = ggfs.enableAsync(); assertTrue(ggfsAsync.isAsync()); for (int i = 0; i < REPEAT_CNT; i++) { String arg = DICTIONARY[new Random(System.currentTimeMillis()).nextInt(DICTIONARY.length)]; generateFile(TOTAL_WORDS); Long genLen = ggfs.info(FILE).length(); assertNull(ggfsAsync.execute( new Task(), new GridGgfsStringDelimiterRecordResolver(" "), Collections.singleton(FILE), arg)); IgniteFuture<IgniteBiTuple<Long, Integer>> fut = ggfsAsync.future(); assertNotNull(fut); IgniteBiTuple<Long, Integer> taskRes = fut.get(); assert F.eq(genLen, taskRes.getKey()); assert F.eq(TOTAL_WORDS, taskRes.getValue()); } ggfsAsync.format(); IgniteFuture<?> fut = ggfsAsync.future(); assertNotNull(fut); fut.get(); }', 'output': 'public void testTaskAsync() throws Exception { U.sleep(3000); assertFalse(ggfs.isAsync()); GridGgfsAsync ggfsAsync = ggfs.enableAsync(); assertTrue(ggfsAsync.isAsync()); for (int i = 0; i < REPEAT_CNT; i++) { String arg = DICTIONARY[new Random(System.currentTimeMillis()).nextInt(DICTIONARY.length)]; generateFile(TOTAL_WORDS); Long genLen = ggfs.info(FILE).length(); assertNull(ggfsAsync.execute( new Task(), new GridGgfsStringDelimiterRecordResolver(" "), Collections.singleton(FILE), arg)); IgniteFuture<IgniteBiTuple<Long, Integer>> fut = ggfsAsync.future(); assertNotNull(fut); IgniteBiTuple<Long, Integer> taskRes = fut.get(); assert F.eq(genLen, taskRes.getKey()); assert F.eq(TOTAL_WORDS, taskRes.getValue()); } ggfsAsync.format(); IgniteFuture<?> fut = ggfsAsync.future(); assertNotNull(fut); fut.get(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testReloadAll() throws Exception { for (IgniteCache<Integer, String> cache : caches) { Iterable<Integer> keys = primaryKeys(cache, 100); info("Values [cache=" + caches.indexOf(cache) + ", size=" + F.size(keys.iterator()) +  ", keys=" + keys + "]"); for (Integer key : keys) map.put(key, "val" + key); } Collection<IgniteCache<Integer, String>> emptyCaches = new ArrayList<>(caches); for (IgniteCache<Integer, String> cache : caches) { info("Reloading cache: " + caches.indexOf(cache)); if (!nearEnabled()) { for (IgniteCache<Integer, String> eCache : emptyCaches) assertEquals("Non-null values found in cache [cache=" + caches.indexOf(eCache) + ", size=" + eCache.size() + ", size=" + eCache.size() + "]", 0, eCache.size()); } loadAll(cache, map.keySet(), true); for (Integer key : map.keySet()) { CacheAffinity<Integer> affinity = affinity(cache); if (affinity.isPrimary(localNode(cache), key) || affinity.isBackup(localNode(cache), key) || nearEnabled()) assertEquals(map.get(key), cache.localPeek(key, CachePeekMode.ONHEAP)); else assertNull(cache.localPeek(key, CachePeekMode.ONHEAP)); } emptyCaches.remove(cache); } }', 'ground_truth': 'public void testReloadAll() throws Exception { for (GridCache<Integer, String> cache : caches) { Iterable<Integer> keys = primaryKeysForCache(cache, 100); info("Values [cache=" + caches.indexOf(cache) + ", size=" + F.size(keys.iterator()) +  ", keys=" + keys + "]"); for (Integer key : keys) map.put(key, "val" + key); } Collection<GridCache<Integer, String>> emptyCaches = new ArrayList<>(caches); for (GridCache<Integer, String> cache : caches) { info("Reloading cache: " + caches.indexOf(cache)); if (!nearEnabled()) { for (GridCache<Integer, String> eCache : emptyCaches) assertEquals("Non-null values found in cache [cache=" + caches.indexOf(eCache) + ", size=" + eCache.size() + ", size=" + eCache.size() + ", entrySetSize=" + eCache.entrySet().size() + "]", 0, eCache.size()); } cache.reloadAll(map.keySet()); for (Integer key : map.keySet()) { CacheEntry entry = cache.entry(key); if (entry.primary() || entry.backup() || nearEnabled()) assertEquals(map.get(key), cache.peek(key)); else assertNull(cache.peek(key)); } emptyCaches.remove(cache); } }', 'output': 'public void testReloadAll() throws Exception { for (IgniteCache<Integer, String> cache : caches) { Iterable<Integer> keys = primaryKeys(cache, 100); info("Values [cache=" + caches.indexOf(cache) + ", size=" + F.size(keys.iterator()) +  ", keys=" + keys + "]"); for (Integer key : keys) map.put(key, "val" + key); } Collection<IgniteCache<Integer, String>> emptyCaches = new ArrayList<>(caches); for (IgniteCache<Integer, String> cache : caches) { info("Reloading cache: " + caches.indexOf(cache)); if (!nearEnabled()) { for (IgniteCache<Integer, String> eCache : emptyCaches) assertEquals("Non-null values found in cache [cache=" + caches.indexOf(eCache) + ", size=" + eCache.size() + ", size=" + eCache.size() + "]", 0, eCache.size()); } loadAll(cache, map.keySet(), true); for (Integer key : map.keySet()) { CacheAffinity<Integer> affinity = affinity(cache); if (affinity.isPrimary(localNode(cache), key) || affinity.isBackup(localNode(cache), key) || nearEnabled()) assertEquals(map.get(key), cache.localPeek(key, CachePeekMode.ONHEAP)); else assertNull(cache.localPeek(key, CachePeekMode.ONHEAP)); } emptyCaches.remove(cache); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testStop() throws Exception { create(ggfs, paths(DIR, SUBDIR), null); XXX os = ggfs.create(FILE, true); os.write(chunk); ggfs.stop(); afterTestsStopped(); beforeTestsStarted(); }', 'ground_truth': 'public void testStop() throws Exception { create(ggfs, paths(DIR, SUBDIR), null); IgfsOutputStream os = ggfs.create(FILE, true); os.write(chunk); ggfs.stop(); afterTestsStopped(); beforeTestsStarted(); }', 'output': 'public void testStop() throws Exception { create(ggfs, paths(DIR, SUBDIR), null); IgfsFile os = ggfs.create(FILE, true); os.write(chunk); ggfs.stop(); afterTestsStopped(); beforeTestsStarted(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean filterByTaskName(IgniteEvent e, String taskName) { if (e.getClass().equals(GridTaskEvent.class)) { GridTaskEvent te = (GridTaskEvent)e; return containsInTaskName(te.taskName(), te.taskClassName(), taskName); } if (e.getClass().equals(GridJobEvent.class)) { GridJobEvent je = (GridJobEvent)e; return containsInTaskName(je.taskName(), je.taskName(), taskName); } if (e.getClass().equals(GridDeploymentEvent.class)) { XXX de = (GridDeploymentEvent)e; return de.alias().toLowerCase().contains(taskName); } return true; }', 'ground_truth': 'private boolean filterByTaskName(IgniteEvent e, String taskName) { if (e.getClass().equals(GridTaskEvent.class)) { GridTaskEvent te = (GridTaskEvent)e; return containsInTaskName(te.taskName(), te.taskClassName(), taskName); } if (e.getClass().equals(GridJobEvent.class)) { GridJobEvent je = (GridJobEvent)e; return containsInTaskName(je.taskName(), je.taskName(), taskName); } if (e.getClass().equals(IgniteDeploymentEvent.class)) { IgniteDeploymentEvent de = (IgniteDeploymentEvent)e; return de.alias().toLowerCase().contains(taskName); } return true; }', 'output': 'private boolean filterByTaskName(IgniteEvent e, String taskName) { if (e.getClass().equals(GridTaskEvent.class)) { GridTaskEvent te = (GridTaskEvent)e; return containsInTaskName(te.taskName(), te.taskClassName(), taskName); } if (e.getClass().equals(GridJobEvent.class)) { GridJobEvent je = (GridJobEvent)e; return containsInTaskName(je.taskName(), je.taskName(), taskName); } if (e.getClass().equals(GridDeploymentEvent.class)) { GridDeploymentEvent de = (GridDeploymentEvent)e; return de.alias().toLowerCase().contains(taskName); } return true; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void doVerb(Message message) { byte[] body = message.getMessageBody(); ByteArrayInputStream bufIn = new ByteArrayInputStream(body); try { XXX streamStatus = CompletedFileStatus.serializer().deserialize(new DataInputStream(bufIn)); switch (streamStatus.getAction()) { case DELETE: StreamOutManager.get(message.getFrom()).finishAndStartNext(streamStatus.getFile()); break; case STREAM: if (logger.isDebugEnabled()) logger.debug("Need to re-stream file " + streamStatus.getFile()); StreamOutManager.get(message.getFrom()).startNext(); break; default: break; } } catch (IOException ex) { throw new IOError(ex); } }', 'ground_truth': 'public void doVerb(Message message) { byte[] body = message.getMessageBody(); ByteArrayInputStream bufIn = new ByteArrayInputStream(body); try { FileStatus streamStatus = FileStatus.serializer().deserialize(new DataInputStream(bufIn)); switch (streamStatus.getAction()) { case DELETE: StreamOutManager.get(message.getFrom()).finishAndStartNext(streamStatus.getFile()); break; case STREAM: if (logger.isDebugEnabled()) logger.debug("Need to re-stream file " + streamStatus.getFile()); StreamOutManager.get(message.getFrom()).startNext(); break; default: break; } } catch (IOException ex) { throw new IOError(ex); } }', 'output': 'public void doVerb(Message message) { byte[] body = message.getMessageBody(); ByteArrayInputStream bufIn = new ByteArrayInputStream(body); try { CompletedFileStatus status = CompletedFileStatus.serializer().deserialize(new DataInputStream(bufIn)); switch (status.getAction()) { case DELETE: StreamOutManager.get(message.getFrom()).finishAndStartNext(status.getFile()); break; case STREAM: if (logger.isDebugEnabled()) logger.debug("Need to re-stream file " + status.getFile()); StreamOutManager.get(message.getFrom()).startNext(); break; default: break; } } catch (IOException ex) { throw new IOError(ex); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkUniqueHashIndexMultithreaded(int threadCnt, final int iters) throws Exception { GridStreamerIndexProvider<String, String, Integer> idxProvider = indexProvider(false, "idx", new IndexUpdater(), EVENT_TRACKING_ON_DEDUP, true); for (int i = 0; i < iters && !Thread.currentThread().isInterrupted(); i++) { final XXX win = new GridStreamerBoundedSizeWindow<>(); win.setMaximumSize(threadCnt * 2); win.setIndexes(idxProvider); win.start(); final String evt = "evt" + i; final AtomicInteger nIdxErrors = new AtomicInteger(); runMultiThreaded(new CAX() { @Override public void applyx() throws GridException { try { win.enqueue(evt); } catch (GridException e) { if (e.getMessage().contains("Index unique key violation")) nIdxErrors.incrementAndGet(); else throw e; } } }, threadCnt, "put"); assertEquals(threadCnt - 1, nIdxErrors.get()); GridStreamerIndex<String, String, Integer> idx = win.index("idx"); assertEquals(1, idx.entries(0).size()); assertEquals((Integer)1, idx.entry(evt).value()); } }', 'ground_truth': 'private void checkUniqueHashIndexMultithreaded(int threadCnt, final int iters) throws Exception { GridStreamerIndexProvider<String, String, Integer> idxProvider = indexProvider(false, "idx", new IndexUpdater(), EVENT_TRACKING_ON_DEDUP, true); for (int i = 0; i < iters && !Thread.currentThread().isInterrupted(); i++) { final StreamerBoundedSizeWindow<String> win = new StreamerBoundedSizeWindow<>(); win.setMaximumSize(threadCnt * 2); win.setIndexes(idxProvider); win.start(); final String evt = "evt" + i; final AtomicInteger nIdxErrors = new AtomicInteger(); runMultiThreaded(new CAX() { @Override public void applyx() throws GridException { try { win.enqueue(evt); } catch (GridException e) { if (e.getMessage().contains("Index unique key violation")) nIdxErrors.incrementAndGet(); else throw e; } } }, threadCnt, "put"); assertEquals(threadCnt - 1, nIdxErrors.get()); GridStreamerIndex<String, String, Integer> idx = win.index("idx"); assertEquals(1, idx.entries(0).size()); assertEquals((Integer)1, idx.entry(evt).value()); } }', 'output': 'private void checkUniqueHashIndexMultithreaded(int threadCnt, final int iters) throws Exception { GridStreamerIndexProvider<String, String, Integer> idxProvider = indexProvider(false, "idx", new IndexUpdater(), EVENT_TRACKING_ON_DEDUP, true); for (int i = 0; i < iters && !Thread.currentThread().isInterrupted(); i++) { final GridStreamerBoundedSizeWindow<String> win = new GridStreamerBoundedSizeWindow<>(); win.setMaximumSize(threadCnt * 2); win.setIndexes(idxProvider); win.start(); final String evt = "evt" + i; final AtomicInteger nIdxErrors = new AtomicInteger(); runMultiThreaded(new CAX() { @Override public void applyx() throws GridException { try { win.enqueue(evt); } catch (GridException e) { if (e.getMessage().contains("Index unique key violation")) nIdxErrors.incrementAndGet(); else throw e; } } }, threadCnt, "put"); assertEquals(threadCnt - 1, nIdxErrors.get()); GridStreamerIndex<String, String, Integer> idx = win.index("idx"); assertEquals(1, idx.entries(0).size()); assertEquals((Integer)1, idx.entry(evt).value()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static boolean isAssignableFrom(Class klazz, Class aClass) { if (klazz == aClass) return true; XXX val = assignableMap.getOrPut(klazz, aClass); if (val.value == null) { val.value = Boolean.valueOf(klazz.isAssignableFrom(aClass)); } return ((Boolean)val.value).booleanValue(); }', 'ground_truth': 'public static boolean isAssignableFrom(Class klazz, Class aClass) { if (klazz == aClass) return true; SoftDoubleKeyHashMap.Entry val = assignableMap.getOrPut(klazz, aClass); if (val.value == null) { val.value = Boolean.valueOf(klazz.isAssignableFrom(aClass)); } return ((Boolean)val.value).booleanValue(); }', 'output': 'public static boolean isAssignableFrom(Class<?> klazz, Class<?> aClass) { if (klazz == aClass) return true; Map.Entry<Class<?>, Boolean> val = assignableMap.getOrPut(klazz, aClass); if (val.value == null) { val.value = Boolean.valueOf(klazz.isAssignableFrom(aClass)); } return ((Boolean)val.value).booleanValue(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Map<String, Integer> getRecentlyDroppedMessages() { Map<String, Integer> map = new HashMap<String, Integer>(); for (Map.Entry<StorageService.Verb, AtomicInteger> entry : droppedMessages.entrySet()) { StorageService.Verb verb = entry.getKey(); Integer dropped = entry.getValue().get(); Integer recentlyDropped = dropped - lastDropped.get(verb); map.put(verb.toString(), recentlyDropped); lastDropped.put(verb, dropped); } return map; }', 'ground_truth': 'public Map<String, Integer> getRecentlyDroppedMessages() { Map<String, Integer> map = new HashMap<String, Integer>(); for (Map.Entry<Verb, AtomicInteger> entry : droppedMessages.entrySet()) { Verb verb = entry.getKey(); Integer dropped = entry.getValue().get(); Integer recentlyDropped = dropped - lastDropped.get(verb); map.put(verb.toString(), recentlyDropped); lastDropped.put(verb, dropped); } return map; }', 'output': 'public Map<String, Long> getRecentlyDroppedMessages() { Map<String, Long> map = new HashMap<String, Long>(); for (Map.Entry<StorageService.Verb, AtomicLong> entry : droppedMessages.entrySet()) { StorageService.Verb verb = entry.getKey(); Long dropped = entry.getValue().get(); Long recentlyDropped = dropped - lastDropped.get(verb); map.put(verb.toString(), recentlyDropped); lastDropped.put(verb, dropped); } return map; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPredictWithErasedLabelsAndChangedThreshold() { Vector weights = new DenseVector(new double[]{1.0, 1.0}); XXX mdl = new SVMLinearBinaryClassificationModel(weights, 1.0).withThreshold(5); Vector observation = new DenseVector(new double[]{1.0, 1.0}); TestUtils.assertEquals(0.0, mdl.apply(observation), PRECISION); observation = new DenseVector(new double[]{3.0, 4.0}); TestUtils.assertEquals(1.0, mdl.apply(observation), PRECISION); TestUtils.assertEquals(5, mdl.threshold(), PRECISION); }', 'ground_truth': 'public void testPredictWithErasedLabelsAndChangedThreshold() { Vector weights = new DenseVector(new double[]{1.0, 1.0}); SVMLinearClassificationModel mdl = new SVMLinearClassificationModel(weights, 1.0).withThreshold(5); Vector observation = new DenseVector(new double[]{1.0, 1.0}); TestUtils.assertEquals(0.0, mdl.apply(observation), PRECISION); observation = new DenseVector(new double[]{3.0, 4.0}); TestUtils.assertEquals(1.0, mdl.apply(observation), PRECISION); TestUtils.assertEquals(5, mdl.threshold(), PRECISION); }', 'output': 'public void testPredictWithErasedLabelsAndChangedThreshold() { Vector weights = new DenseVector(new double[]{1.0, 1.0}); SVMLinearBinaryClassificationModel mdl = new SVMLinearBinaryClassificationModel(weights, 1.0).withThreshold(5); Vector observation = new DenseVector(new double[]{1.0, 1.0}); TestUtils.assertEquals(0.0, mdl.apply(observation), PRECISION); observation = new DenseVector(new double[]{3.0, 4.0}); TestUtils.assertEquals(1.0, mdl.apply(observation), PRECISION); TestUtils.assertEquals(5, mdl.threshold(), PRECISION); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public <T> void writeToTagList(CodedTupleTag<T> tag, T value, Instant timestamp) { Map<CodedTupleTag<?>, Map<Instant, List<TimestampedValue>>> perKeyTagLists = tagLists.get(getKey()); if (perKeyTagLists == null) { perKeyTagLists = new HashMap<>(); tagLists.put(getKey(), perKeyTagLists); } Map<Instant, List<TimestampedValue>> tagList = perKeyTagLists.get(tag); if (tagList == null) { tagList = new TreeMap<>(); perKeyTagLists.put(tag, tagList); } List<TimestampedValue> timestampList = tagList.get(timestamp); if (timestampList == null) { timestampList = new ArrayList<>(); tagList.put(timestamp, timestampList); } timestampList.add(TimestampedValue.of(value, timestamp)); }', 'ground_truth': 'public <T> void writeToTagList(CodedTupleTag<T> tag, T value, Instant timestamp) { Map<CodedTupleTag<?>, List<?>> perKeyTagLists = tagLists.get(getKey()); if (perKeyTagLists == null) { perKeyTagLists = new HashMap<>(); tagLists.put(getKey(), perKeyTagLists); } @SuppressWarnings("unchecked") List<T> tagList = (List<T>) perKeyTagLists.get(tag); if (tagList == null) { tagList = new ArrayList<>(); perKeyTagLists.put(tag, tagList); } tagList.add(value); }', 'output': 'public <T> void writeToTagList(CodedTupleTag<T> tag, T value, Instant timestamp) { Map<CodedTupleTag<?>, Map<Instant, List<TimestampedValue>>> perKeyTagLists = tagLists.get(getKey()); if (perKeyTagLists == null) { perKeyTagLists = new HashMap<>(); tagLists.put(getKey(), perKeyTagLists); } Map<Instant, List<TimestampedValue>> tagList = perKeyTagLists.get(tag); if (tagList == null) { tagList = new TreeMap<>(); perKeyTagLists.put(tag, tagList); } List<TimestampedValue> timestampList = tagList.get(timestamp); if (timestampList == null) { timestampList = new ArrayList<>(); tagList.put(timestamp, timestampList); } timestampList.add(TimestampedValue.of(value, timestamp)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testInstanceOf() { GridPredicate<Object> p1 = F.instanceOf(Object.class); assert p1 != null; assert p1.apply(this); assert !p1.apply(null); GridPredicate<Object> p2 = F.instanceOf(TestCase.class); assert p2 != null; assert p2.apply(this); assert !p2.apply(new Object()); assert !p2.apply(null); } public void testIn() { XXX p = F.in(Arrays.asList("1", "2", "3")); assert p.apply("1"); assert !p.apply("4"); assert !p.apply(null); p = F.in(Arrays.asList("1", "2", "3", null)); assert p.apply("1"); assert !p.apply("4"); assert p.apply(null); }', 'ground_truth': 'public void testIn() { IgnitePredicate<String> p = F.in(Arrays.asList("1", "2", "3")); assert p.apply("1"); assert !p.apply("4"); assert !p.apply(null); p = F.in(Arrays.asList("1", "2", "3", null)); assert p.apply("1"); assert !p.apply("4"); assert p.apply(null); }', 'output': 'public void testInstanceOf() { IgnitePredicate<Object> p1 = F.instanceOf(Object.class); assert p1 != null; assert p1.apply(this); assert !p1.apply(null); IgnitePredicate<Object> p2 = F.instanceOf(TestCase.class); assert p2 != null; assert p2.apply(this); assert !p2.apply(new Object()); assert !p2.apply(null); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMultipleVersionsP2PDeploy() throws Exception { try { Ignite g1 = startGrid(1); Ignite g2 = startGrid(2); final CountDownLatch latch = new CountDownLatch(2); g2.events().localListen( new IgnitePredicate<GridEvent>() { @Override public boolean apply(GridEvent evt) { info("Received event: " + evt); latch.countDown(); return true; } }, EVT_TASK_UNDEPLOYED ); ClassLoader ldr1 = new GridTestClassLoader( Collections.singletonMap("testResource", "1"), getClass().getClassLoader(), EXCLUDE_CLASSES); ClassLoader ldr2 = new GridTestClassLoader( Collections.singletonMap("testResource", "2"), getClass().getClassLoader(), EXCLUDE_CLASSES); Class<? extends ComputeTask<?, ?>> taskCls1 = (Class<? extends ComputeTask<?, ?>>)ldr1. loadClass(GridDeploymentTestTask.class.getName()); Class<? extends ComputeTask<?, ?>> taskCls2 = (Class<? extends ComputeTask<?, ?>>)ldr2. loadClass(GridDeploymentTestTask.class.getName()); g1.compute().localDeployTask(taskCls1, ldr1); GridComputeTaskFuture fut1 = executeAsync(g1.compute(), "GridDeploymentTestTask", null); assert checkDeployed(g1, "GridDeploymentTestTask"); Thread.sleep(2000); g1.compute().localDeployTask(taskCls2, ldr2); XXX fut2 = executeAsync(g1.compute(), "GridDeploymentTestTask", null); boolean deployed = checkDeployed(g1, "GridDeploymentTestTask"); Object res1 = fut1.get(); Object res2 = fut2.get(); g1.compute().undeployTask("GridDeploymentTestTask"); assert deployed; assert res1.equals(1); assert res2.equals(2); stopGrid(1); assert latch.await(3000, MILLISECONDS); assert !checkDeployed(g2, "GridDeploymentTestTask"); } finally { stopGrid(2); stopGrid(1); } }', 'ground_truth': 'public void testMultipleVersionsP2PDeploy() throws Exception { try { Ignite g1 = startGrid(1); Ignite g2 = startGrid(2); final CountDownLatch latch = new CountDownLatch(2); g2.events().localListen( new IgnitePredicate<GridEvent>() { @Override public boolean apply(GridEvent evt) { info("Received event: " + evt); latch.countDown(); return true; } }, EVT_TASK_UNDEPLOYED ); ClassLoader ldr1 = new GridTestClassLoader( Collections.singletonMap("testResource", "1"), getClass().getClassLoader(), EXCLUDE_CLASSES); ClassLoader ldr2 = new GridTestClassLoader( Collections.singletonMap("testResource", "2"), getClass().getClassLoader(), EXCLUDE_CLASSES); Class<? extends ComputeTask<?, ?>> taskCls1 = (Class<? extends ComputeTask<?, ?>>)ldr1. loadClass(GridDeploymentTestTask.class.getName()); Class<? extends ComputeTask<?, ?>> taskCls2 = (Class<? extends ComputeTask<?, ?>>)ldr2. loadClass(GridDeploymentTestTask.class.getName()); g1.compute().localDeployTask(taskCls1, ldr1); ComputeTaskFuture fut1 = executeAsync(g1.compute(), "GridDeploymentTestTask", null); assert checkDeployed(g1, "GridDeploymentTestTask"); Thread.sleep(2000); g1.compute().localDeployTask(taskCls2, ldr2); ComputeTaskFuture fut2 = executeAsync(g1.compute(), "GridDeploymentTestTask", null); boolean deployed = checkDeployed(g1, "GridDeploymentTestTask"); Object res1 = fut1.get(); Object res2 = fut2.get(); g1.compute().undeployTask("GridDeploymentTestTask"); assert deployed; assert res1.equals(1); assert res2.equals(2); stopGrid(1); assert latch.await(3000, MILLISECONDS); assert !checkDeployed(g2, "GridDeploymentTestTask"); } finally { stopGrid(2); stopGrid(1); } }', 'output': 'public void testMultipleVersionsP2PDeploy() throws Exception { try { Ignite g1 = startGrid(1); Ignite g2 = startGrid(2); final CountDownLatch latch = new CountDownLatch(2); g2.events().localListen( new IgnitePredicate<GridEvent>() { @Override public boolean apply(GridEvent evt) { info("Received event: " + evt); latch.countDown(); return true; } }, EVT_TASK_UNDEPLOYED ); ClassLoader ldr1 = new GridTestClassLoader( Collections.singletonMap("testResource", "1"), getClass().getClassLoader(), EXCLUDE_CLASSES); ClassLoader ldr2 = new GridTestClassLoader( Collections.singletonMap("testResource", "2"), getClass().getClassLoader(), EXCLUDE_CLASSES); Class<? extends ComputeTask<?, ?>> taskCls1 = (Class<? extends ComputeTask<?, ?>>)ldr1.loadClass(GridDeploymentTestTask.class.getName()); Class<? extends ComputeTask<?, ?>> taskCls2 = (Class<? extends ComputeTask<?, ?>>)ldr2.loadClass(GridDeploymentTestTask.class.getName()); g1.compute().localDeployTask(taskCls1, ldr1); GridComputeTaskFuture fut1 = executeAsync(g1.compute(), "GridDeploymentTestTask", null); assert checkDeployed(g1, "GridDeploymentTestTask"); Thread.sleep(2000); g1.compute().localDeployTask(taskCls2, ldr2); GridComputeTaskFuture fut2 = executeAsync(g1.compute(), "GridDeploymentTestTask", null); boolean deployed = checkDeployed(g1, "GridDeploymentTestTask"); Object res1 = fut1.get(); Object res2 = fut2.get(); g1.compute().undeployTask("GridDeploymentTestTask"); assert deployed; assert res1.equals(1); assert res2.equals(2); stopGrid(1); assert latch.await(3000, MILLISECONDS); assert !checkDeployed(g2, "GridDeploymentTestTask");'}
{'instruction': 'Please refactor the following code:', 'input': 'private StackAdvisorRequest createStackAdvisorRequest(ClusterTopology clusterTopology, StackAdvisorRequestType requestType) { Stack stack = clusterTopology.getBlueprint().getStack(); SortedMap<String, SortedSet<String>> hgComponentsMap = gatherHostGroupComponents(clusterTopology); SortedMap<String, SortedSet<String>> hgHostsMap = gatherHostGroupBindings(clusterTopology); SortedMap<String, SortedSet<String>> componentHostsMap = gatherComponentsHostsMap(hgComponentsMap, hgHostsMap); return StackAdvisorRequest.StackAdvisorRequestBuilder .forStack(stack.getName(), stack.getVersion()) .forServices(new ArrayList<>(clusterTopology.getBlueprint().getServices())) .forHosts(gatherHosts(clusterTopology)) .forHostsGroupBindings(gatherHostGroupBindings(clusterTopology)) .forHostComponents(gatherHostGroupComponents(clusterTopology)) .withComponentHostsMap(componentHostsMap) .withConfigurations(calculateConfigs(clusterTopology)) .withUserContext(userContext) .ofType(requestType) .build(); }', 'ground_truth': 'private StackAdvisorRequest createStackAdvisorRequest(ClusterTopology clusterTopology, StackAdvisorRequestType requestType) { Stack stack = clusterTopology.getBlueprint().getStack(); Map<String, Set<String>> hgComponentsMap = gatherHostGroupComponents(clusterTopology); Map<String, Set<String>> hgHostsMap = gatherHostGroupBindings(clusterTopology); Map<String, Set<String>> componentHostsMap = gatherComponentsHostsMap(hgComponentsMap, hgHostsMap); return StackAdvisorRequest.StackAdvisorRequestBuilder .forStack(stack.getName(), stack.getVersion()) .forServices(new ArrayList<>(clusterTopology.getBlueprint().getServices())) .forHosts(gatherHosts(clusterTopology)) .forHostsGroupBindings(gatherHostGroupBindings(clusterTopology)) .forHostComponents(gatherHostGroupComponents(clusterTopology)) .withComponentHostsMap(componentHostsMap) .withConfigurations(calculateConfigs(clusterTopology)) .withUserContext(userContext) .ofType(requestType) .build(); }', 'output': 'private StackAdvisorRequest createStackAdvisorRequest(ClusterTopology clusterTopology, StackAdvisorRequestType requestType) { Stack stack = clusterTopology.getBlueprint().getStack(); SortedMap<String, SortedSet<String>> hgComponentsMap = gatherHostGroupComponents(clusterTopology); SortedMap<String, SortedSet<String>> hgHostsMap = gatherHostGroupBindings(clusterTopology); SortedMap<String, SortedSet<String>> componentHostsMap = gatherComponentsHostsMap(hgComponentsMap, hgHostsMap); return StackAdvisorRequest.StackAdvisorRequestBuilder .forStack(stack.getName(), stack.getVersion()) .forServices(new ArrayList<>(clusterTopology.getBlueprint().getServices())) .forHosts(gatherHosts(clusterTopology)) .forHostsGroupBindings(gatherHostGroupBindings(clusterTopology)) .forHostComponents(gatherHostGroupComponents(clusterTopology)) .withComponentHostsMap(componentHostsMap) .withConfigurations(calculateConfigs(clusterTopology)) .withUserContext(userContext) .ofType(requestType) .build(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private long sizeOfComplexColumn(ComplexColumnData data, ColumnMetadata column, boolean hasComplexDeletion, LivenessInfo rowLiveness, SerializationHeader header) { long size = 0; if (hasComplexDeletion) size += header.deletionTimeSerializedSize(data.complexDeletion()); size += TypeSizes.sizeofUnsignedVInt(data.cellsCount()); for (XXX cell : data) size += Cell.serializer.serializedSize(cell, column, rowLiveness, header); return size; }', 'ground_truth': 'private long sizeOfComplexColumn(ComplexColumnData data, ColumnMetadata column, boolean hasComplexDeletion, LivenessInfo rowLiveness, SerializationHeader header) { long size = 0; if (hasComplexDeletion) size += header.deletionTimeSerializedSize(data.complexDeletion()); size += TypeSizes.sizeofUnsignedVInt(data.cellsCount()); for (Cell<?> cell : data) size += Cell.serializer.serializedSize(cell, column, rowLiveness, header); return size; }', 'output': 'private long sizeOfComplexColumn(ComplexColumnData data, ColumnMetadata column, boolean hasComplexDeletion, LivenessInfo rowLiveness, SerializationHeader header) { long size = 0; if (hasComplexDeletion) size += header.deletionTimeSerializedSize(data.complexDeletion()); size += TypeSizes.sizeofUnsignedVInt(data.cellsCount()); for (Cell cell : data) size += Cell.serializer.serializedSize(cell, column, rowLiveness, header); return size; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void blockUntilFinished(@SuperCall Runnable fn) { fn.run(); int id = Integer.parseInt(InstanceIDDefiner.getInstanceId().replace("node", "")); XXX cluster = Objects.requireNonNull(SharedState.cluster); ForkJoinPool.commonPool().execute(() -> { ClusterUtils.stopAbrupt(cluster, cluster.get(id)); SharedState.shutdownComplete.countDown(); }); JVMStabilityInspector.killCurrentJVM(new RuntimeException("Attempting to stop the instance"), false); }', 'ground_truth': 'public static void blockUntilFinished(@SuperCall Runnable fn) { fn.run(); int id = Integer.parseInt(InstanceIDDefiner.getInstanceId().replace("node", "")); ICluster cluster = Objects.requireNonNull(SharedState.cluster); ForkJoinPool.commonPool().execute(() -> { ClusterUtils.stopAbrupt(cluster, cluster.get(id)); SharedState.shutdownComplete.countDown(); }); JVMStabilityInspector.killCurrentJVM(new RuntimeException("Attempting to stop the instance"), false); }', 'output': 'public static void blockUntilFinished(@SuperCall Runnable fn) { fn.run(); int id = Integer.parseInt(InstanceIDDefiner.getInstanceId().replace("node", "")); IgniteCluster cluster = Objects.requireNonNull(SharedState.cluster); ForkJoinPool.commonPool().execute(() -> { ClusterUtils.stopAbrupt(cluster, cluster.get(id)); SharedState.shutdownComplete.countDown(); }); JVMStabilityInspector.killCurrentJVM(new RuntimeException("Attempting to stop the instance"), false); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static <InputT, OutputT> void translateExecutableStage( PTransformNode transformNode, RunnerApi.Pipeline pipeline, SparkTranslationContext context) { RunnerApi.ExecutableStagePayload stagePayload; try { stagePayload = RunnerApi.ExecutableStagePayload.parseFrom( transformNode.getTransform().getSpec().getPayload()); } catch (IOException e) { throw new RuntimeException(e); } String inputPCollectionId = stagePayload.getInput(); Dataset inputDataset = context.popDataset(inputPCollectionId); JavaRDD<WindowedValue<InputT>> inputRdd = ((BoundedDataset<InputT>) inputDataset).getRDD(); Map<String, String> outputs = transformNode.getTransform().getOutputsMap(); BiMap<String, Integer> outputMap = createOutputMap(outputs.values()); XXX function = new SparkExecutableStageFunction<>(stagePayload, context.jobInfo, outputMap); JavaRDD<RawUnionValue> staged = inputRdd.mapPartitions(function); for (String outputId : outputs.values()) { JavaRDD<WindowedValue<OutputT>> outputRdd = staged.flatMap(new SparkExecutableStageExtractionFunction<>(outputMap.get(outputId))); context.pushDataset(outputId, new BoundedDataset<>(outputRdd)); } }', 'ground_truth': 'private static <InputT, OutputT, SideInputT> void translateExecutableStage( PTransformNode transformNode, RunnerApi.Pipeline pipeline, SparkTranslationContext context) { RunnerApi.ExecutableStagePayload stagePayload; try { stagePayload = RunnerApi.ExecutableStagePayload.parseFrom( transformNode.getTransform().getSpec().getPayload()); } catch (IOException e) { throw new RuntimeException(e); } String inputPCollectionId = stagePayload.getInput(); Dataset inputDataset = context.popDataset(inputPCollectionId); JavaRDD<WindowedValue<InputT>> inputRdd = ((BoundedDataset<InputT>) inputDataset).getRDD(); Map<String, String> outputs = transformNode.getTransform().getOutputsMap(); BiMap<String, Integer> outputMap = createOutputMap(outputs.values()); ImmutableMap.Builder<String, Tuple2<Broadcast<List<byte[]>>, WindowedValueCoder<SideInputT>>> broadcastVariablesBuilder = ImmutableMap.builder(); for (SideInputId sideInputId : stagePayload.getSideInputsList()) { RunnerApi.Components components = stagePayload.getComponents(); String collectionId = components .getTransformsOrThrow(sideInputId.getTransformId()) .getInputsOrThrow(sideInputId.getLocalName()); Tuple2<Broadcast<List<byte[]>>, WindowedValueCoder<SideInputT>> tuple2 = broadcastSideInput(collectionId, components, context); broadcastVariablesBuilder.put(collectionId, tuple2); } SparkExecutableStageFunction<InputT, SideInputT> function = new SparkExecutableStageFunction<>( stagePayload, context.jobInfo, outputMap, broadcastVariablesBuilder.build()); JavaRDD<RawUnionValue> staged = inputRdd.mapPartitions(function); for (String outputId : outputs.values()) { JavaRDD<WindowedValue<OutputT>> outputRdd = staged.flatMap(new SparkExecutableStageExtractionFunction<>(outputMap.get(outputId))); context.pushDataset(outputId, new BoundedDataset<>(outputRdd)); } }', 'output': 'private static <InputT, OutputT> void translateExecutableStage( PTransformNode transformNode, RunnerApi.Pipeline pipeline, SparkTranslationContext context) { RunnerApi.ExecutableStagePayload stagePayload; try { stagePayload = RunnerApi.ExecutableStagePayload.parseFrom( transformNode.getTransform().getSpec().getPayload()); } catch (IOException e) { throw new RuntimeException(e); } String inputPCollectionId = stagePayload.getInput(); Dataset inputDataset = context.popDataset(inputPCollectionId); JavaRDD<WindowedValue<InputT>> inputRdd = ((BoundedDataset<InputT>) inputDataset).getRDD(); Map<String, String> outputs = transformNode.getTransform().getOutputsMap(); BiMap<String, Integer> outputMap = createOutputMap(outputs.values()); SparkExecutableStageFunction<InputT, OutputT> function = new SparkExecutableStageFunction<>(stagePayload, context.jobInfo, outputMap); JavaRDD<RawUnionValue> staged = inputRdd.mapPartitions(function); for (String outputId : outputs.values()) { JavaRDD<WindowedValue<OutputT>> outputRdd = staged.flatMap(new SparkExecutableStageExtractionFunction<>(outputMap.get(outputId))); context.pushDataset(outputId, new BoundedDataset<>(outputRdd)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void resolveProjectDependencies() { Set<ClassPathSupport.Item> oldArtifacts = new HashSet<ClassPathSupport.Item>(); EditableProperties projectProperties = updateHelper.getProperties( AntProjectHelper.PROJECT_PROPERTIES_PATH ); oldArtifacts.addAll( cs.itemsList(projectProperties.get( ProjectProperties.JAVAC_CLASSPATH ), ClassPathSupportCallbackImpl.ELEMENT_INCLUDED_LIBRARIES ) ); oldArtifacts.addAll( cs.itemsList( projectProperties.get( ProjectProperties.JAVAC_PROCESSORPATH ) ) ); oldArtifacts.addAll( cs.itemsList(projectProperties.get( ProjectProperties.JAVAC_TEST_CLASSPATH ), null ) ); oldArtifacts.addAll( cs.itemsList(projectProperties.get( ProjectProperties.RUN_CLASSPATH ), null ) ); oldArtifacts.addAll( cs.itemsList(projectProperties.get( ProjectProperties.RUN_TEST_CLASSPATH ), null ) ); oldArtifacts.addAll( cs.itemsList(projectProperties.get( ProjectProperties.ENDORSED_CLASSPATH ), null ) ); Set<ClassPathSupport.Item> newArtifacts = new HashSet<ClassPathSupport.Item>(); newArtifacts.addAll( ClassPathUiSupport.getList( JAVAC_CLASSPATH_MODEL.getDefaultListModel() ) ); newArtifacts.addAll( ClassPathUiSupport.getList( JAVAC_PROCESSORPATH_MODEL ) ); newArtifacts.addAll( ClassPathUiSupport.getList( JAVAC_TEST_CLASSPATH_MODEL ) ); newArtifacts.addAll( ClassPathUiSupport.getList( RUN_CLASSPATH_MODEL ) ); newArtifacts.addAll( ClassPathUiSupport.getList( RUN_TEST_CLASSPATH_MODEL ) ); newArtifacts.addAll( ClassPathUiSupport.getList( ENDORSED_CLASSPATH_MODEL ) ); Set<ClassPathSupport.Item> removed = new HashSet<ClassPathSupport.Item>( oldArtifacts ); removed.removeAll( newArtifacts ); Set<ClassPathSupport.Item> added = new HashSet<ClassPathSupport.Item>(newArtifacts); added.removeAll(oldArtifacts); for( XXX it = removed.iterator(); it.hasNext(); ) { ClassPathSupport.Item item = (ClassPathSupport.Item)it.next(); if ( item.getType() == ClassPathSupport.Item.TYPE_ARTIFACT || item.getType() == ClassPathSupport.Item.TYPE_JAR ) { refHelper.destroyReference(item.getReference()); if (item.getType() == ClassPathSupport.Item.TYPE_JAR) { item.removeSourceAndJavadoc(updateHelper); } } } EditableProperties ep = updateHelper.getProperties( AntProjectHelper.PROJECT_PROPERTIES_PATH ); boolean changed = false; for( XXX it = removed.iterator(); it.hasNext(); ) { ClassPathSupport.Item item = (ClassPathSupport.Item)it.next(); if (item.getType() == ClassPathSupport.Item.TYPE_LIBRARY) { String prop = item.getReference(); prop = prop.substring(2, prop.length()-1); ep.remove(prop); changed = true; } } if (changed) { updateHelper.putProperties(AntProjectHelper.PROJECT_PROPERTIES_PATH, ep); } }', 'ground_truth': 'private void resolveProjectDependencies() { Set<ClassPathSupport.Item> oldArtifacts = new HashSet<ClassPathSupport.Item>(); EditableProperties projectProperties = updateHelper.getProperties( AntProjectHelper.PROJECT_PROPERTIES_PATH ); oldArtifacts.addAll( cs.itemsList(projectProperties.get( ProjectProperties.JAVAC_CLASSPATH ), ClassPathSupportCallbackImpl.ELEMENT_INCLUDED_LIBRARIES ) ); oldArtifacts.addAll( cs.itemsList( projectProperties.get( ProjectProperties.JAVAC_PROCESSORPATH ) ) ); oldArtifacts.addAll( cs.itemsList(projectProperties.get( ProjectProperties.JAVAC_TEST_CLASSPATH ), null ) ); oldArtifacts.addAll( cs.itemsList(projectProperties.get( ProjectProperties.RUN_CLASSPATH ), null ) ); oldArtifacts.addAll( cs.itemsList(projectProperties.get( ProjectProperties.RUN_TEST_CLASSPATH ), null ) ); oldArtifacts.addAll( cs.itemsList(projectProperties.get( ProjectProperties.ENDORSED_CLASSPATH ), null ) ); Set<ClassPathSupport.Item> newArtifacts = new HashSet<ClassPathSupport.Item>(); newArtifacts.addAll( ClassPathUiSupport.getList( JAVAC_CLASSPATH_MODEL.getDefaultListModel() ) ); newArtifacts.addAll( ClassPathUiSupport.getList( JAVAC_PROCESSORPATH_MODEL ) ); newArtifacts.addAll( ClassPathUiSupport.getList( JAVAC_TEST_CLASSPATH_MODEL ) ); newArtifacts.addAll( ClassPathUiSupport.getList( RUN_CLASSPATH_MODEL ) ); newArtifacts.addAll( ClassPathUiSupport.getList( RUN_TEST_CLASSPATH_MODEL ) ); newArtifacts.addAll( ClassPathUiSupport.getList( ENDORSED_CLASSPATH_MODEL ) ); Set<ClassPathSupport.Item> removed = new HashSet<ClassPathSupport.Item>( oldArtifacts ); removed.removeAll( newArtifacts ); Set<ClassPathSupport.Item> added = new HashSet<ClassPathSupport.Item>(newArtifacts); added.removeAll(oldArtifacts); for (Iterator<ClassPathSupport.Item> it = removed.iterator(); it.hasNext(); ) { ClassPathSupport.Item item = (ClassPathSupport.Item)it.next(); if ( item.getType() == ClassPathSupport.Item.TYPE_ARTIFACT || item.getType() == ClassPathSupport.Item.TYPE_JAR ) { refHelper.destroyReference(item.getReference()); if (item.getType() == ClassPathSupport.Item.TYPE_JAR) { item.removeSourceAndJavadoc(updateHelper); } } } EditableProperties ep = updateHelper.getProperties( AntProjectHelper.PROJECT_PROPERTIES_PATH ); boolean changed = false; for (Iterator<ClassPathSupport.Item> it = removed.iterator(); it.hasNext(); ) { ClassPathSupport.Item item = (ClassPathSupport.Item)it.next(); if (item.getType() == ClassPathSupport.Item.TYPE_LIBRARY) { String prop = item.getReference(); prop = prop.substring(2, prop.length()-1); ep.remove(prop); changed = true; } } if (changed) { updateHelper.putProperties(AntProjectHelper.PROJECT_PROPERTIES_PATH, ep); } }', 'output': 'private void resolveProjectDependencies() { Set<ClassPathSupport.Item> oldArtifacts = new HashSet<ClassPathSupport.Item>(); EditableProperties projectProperties = updateHelper.getProperties( AntProjectHelper.PROJECT_PROPERTIES_PATH ); oldArtifacts.addAll( cs.itemsList(projectProperties.get( ProjectProperties.JAVAC_CLASSPATH ), ClassPathSupportCallbackImpl.ELEMENT_INCLUDED_LIBRARIES ) ); oldArtifacts.addAll( cs.itemsList( projectProperties.get( ProjectProperties.JAVAC_PROCESSORPATH ) ) ); oldArtifacts.addAll( cs.itemsList(projectProperties.get( ProjectProperties.JAVAC_TEST_CLASSPATH ), null ) ); oldArtifacts.addAll( cs.itemsList(projectProperties.get( ProjectProperties.RUN_CLASSPATH ), null ) ); oldArtifacts.addAll( cs.itemsList(projectProperties.get( ProjectProperties.RUN_TEST_CLASSPATH ), null ) ); oldArtifacts.addAll( cs.itemsList(projectProperties.get( ProjectProperties.ENDORSED_CLASSPATH ), null ) ); Set<ClassPathSupport.Item> newArtifacts = new HashSet<ClassPathSupport.Item>(); newArtifacts.addAll( ClassPathUiSupport.getList( JAVAC_CLASSPATH_MODEL.getDefaultListModel() ) ); newArtifacts.addAll( ClassPathUiSupport.getList( JAVAC_PROCESSORPATH_MODEL ) ); newArtifacts.addAll( ClassPathUiSupport.getList( JAVAC_TEST_CLASSPATH_MODEL ) ); newArtifacts.addAll( ClassPathUiSupport.getList( RUN_CLASSPATH_MODEL ) ); newArtifacts.addAll( ClassPathUiSupport.getList( RUN_TEST_CLASSPATH_MODEL ) ); newArtifacts.addAll( ClassPathUiSupport.getList( ENDORSED_CLASSPATH_MODEL ) ); Set<ClassPathSupport.Item> removed = new HashSet<ClassPathSupport.Item>( oldArtifacts ); removed.removeAll( newArtifacts ); Set'}
{'instruction': 'Please refactor the following code:', 'input': 'protected String dump(boolean canonical) { XXX buf = new StringBuffer(); if (canonical) buf.append(\'<\'); buf.append(Assignment.getDirectiveName(scope)); if (canonical) { buf.append(\' \'); int ln = getRegulatedChildCount(); for (int i = 0; i < ln; i++) { if (i != 0) { buf.append(", "); } Assignment assignment = (Assignment) getRegulatedChild(i); buf.append(assignment.getCanonicalForm()); } } else { buf.append("-container"); } if (namespaceExp != null) { buf.append(" in "); buf.append(namespaceExp.getCanonicalForm()); } if (canonical) buf.append(">"); return buf.toString(); }', 'ground_truth': 'protected String dump(boolean canonical) { StringBuilder buf = new StringBuilder(); if (canonical) buf.append(\'<\'); buf.append(Assignment.getDirectiveName(scope)); if (canonical) { buf.append(\' \'); int ln = getRegulatedChildCount(); for (int i = 0; i < ln; i++) { if (i != 0) { buf.append(", "); } Assignment assignment = (Assignment) getRegulatedChild(i); buf.append(assignment.getCanonicalForm()); } } else { buf.append("-container"); } if (namespaceExp != null) { buf.append(" in "); buf.append(namespaceExp.getCanonicalForm()); } if (canonical) buf.append(">"); return buf.toString(); }', 'output': 'protected String dump(boolean canonical) { StringBuilder buf = new StringBuilder(); if (canonical) buf.append(\'<\'); buf.append(Assignment.getDirectiveName(scope)); if (canonical) { buf.append(\' \'); int ln = getRegulatedChildCount(); for (int i = 0; i < ln; i++) { if (i != 0) { buf.append(", "); } Assignment assignment = (Assignment) getRegulatedChild(i); buf.append(assignment.getCanonicalForm()); } } else { buf.append("-container"); } if (namespaceExp != null) { buf.append(" in "); buf.append(namespaceExp.getCanonicalForm()); } if (canonical) buf.append(">"); return buf.toString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSwap() throws Exception { for (GridCache<Integer, Object> cache : caches) assert cache.configuration().isSwapEnabled(); byte[] val1 = wrap(1); GridCache<Integer, Object> primaryCache = null; int i = 0; for (GridCache<Integer, Object> cache : caches) { Ignite ignite = ignites[i++]; if (ignite.affinity(null).isPrimary(ignite.cluster().localNode(), SWAP_TEST_KEY)) { primaryCache = cache; break; } } assert primaryCache != null; primaryCache.put(SWAP_TEST_KEY, val1); assert Arrays.equals(val1, (byte[])primaryCache.get(SWAP_TEST_KEY)); assert primaryCache.evict(SWAP_TEST_KEY); assert primaryCache.peek(SWAP_TEST_KEY) == null; assert Arrays.equals(val1, (byte[])primaryCache.promote(SWAP_TEST_KEY)); }', 'ground_truth': 'public void testSwap() throws Exception { for (IgniteCache<Integer, Object> cache : caches) assert cache.configuration().isSwapEnabled(); byte[] val1 = wrap(1); IgniteCache<Integer, Object> primaryCache = null; int i = 0; for (IgniteCache<Integer, Object> cache : caches) { Ignite ignite = ignites[i++]; if (ignite.affinity(null).isPrimary(ignite.cluster().localNode(), SWAP_TEST_KEY)) { primaryCache = cache; break; } } assert primaryCache != null; primaryCache.put(SWAP_TEST_KEY, val1); assert Arrays.equals(val1, (byte[])primaryCache.get(SWAP_TEST_KEY)); primaryCache.evict(SWAP_TEST_KEY); assert primaryCache.peek(SWAP_TEST_KEY) == null; primaryCache.promote(SWAP_TEST_KEY); assert Arrays.equals(val1, (byte[])primaryCache.localPeek(SWAP_TEST_KEY, CachePeekMode.ONHEAP)); }', 'output': 'public void testSwap() throws Exception { for (Cache<Integer, Object> cache : caches) assert cache.configuration().isSwapEnabled(); byte[] val1 = wrap(1); Cache<Integer, Object> primaryCache = null; int i = 0; for (Cache<Integer, Object> cache : caches) { Ignite ignite = ignites[i++]; if (ignite.affinity(null).isPrimary(ignite.cluster().localNode(), SWAP_TEST_KEY)) { primaryCache = cache; break; } } assert primaryCache != null; primaryCache.put(SWAP_TEST_KEY, val1); assert Arrays.equals(val1, (byte[])primaryCache.get(SWAP_TEST_KEY)); assert primaryCache.evict(SWAP_TEST_KEY); assert primaryCache.peek(SWAP_TEST_KEY) == null; assert Arrays.equals(val1, (byte[])primaryCache.promote(SWAP_TEST_KEY)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected RelDataType createTargetRowType( SqlValidatorTable table, SqlNodeList targetColumnList, boolean append) { RelDataType baseRowType = table.getRowType(); if (targetColumnList == null) { return baseRowType; } RelDataTypeField [] targetFields = baseRowType.getFields(); int targetColumnCount = targetColumnList.size(); if (append) { targetColumnCount += baseRowType.getFieldCount(); } RelDataType [] types = new RelDataType[targetColumnCount]; String [] fieldNames = new String[targetColumnCount]; int iTarget = 0; if (append) { iTarget += baseRowType.getFieldCount(); for (int i = 0; i < iTarget; ++i) { types[i] = targetFields[i].getType(); fieldNames[i] = SqlUtil.deriveAliasFromOrdinal(i); } } Set<String> assignedColumnNames = new HashSet<String>(); for (SqlNode node : targetColumnList) { SqlIdentifier id = (SqlIdentifier) node; int iColumn = baseRowType.getFieldOrdinal(id.getSimple()); if (!assignedColumnNames.add(id.getSimple())) { throw newValidationError( id, EigenbaseResource.instance().DuplicateTargetColumn.ex( id.getSimple())); } if (iColumn == -1) { throw newValidationError( id, EigenbaseResource.instance().UnknownTargetColumn.ex( id.getSimple())); } fieldNames[iTarget] = targetFields[iColumn].getName(); types[iTarget] = targetFields[iColumn].getType(); ++iTarget; } return typeFactory.createStructType(types, fieldNames); }', 'ground_truth': 'protected RelDataType createTargetRowType( SqlValidatorTable table, SqlNodeList targetColumnList, boolean append) { RelDataType baseRowType = table.getRowType(); if (targetColumnList == null) { return baseRowType; } List<RelDataTypeField> targetFields = baseRowType.getFieldList(); int targetColumnCount = targetColumnList.size(); if (append) { targetColumnCount += baseRowType.getFieldCount(); } RelDataType [] types = new RelDataType[targetColumnCount]; String [] fieldNames = new String[targetColumnCount]; int iTarget = 0; if (append) { iTarget += baseRowType.getFieldCount(); for (int i = 0; i < iTarget; ++i) { types[i] = targetFields.get(i).getType(); fieldNames[i] = SqlUtil.deriveAliasFromOrdinal(i); } } Set<String> assignedColumnNames = new HashSet<String>(); for (SqlNode node : targetColumnList) { SqlIdentifier id = (SqlIdentifier) node; int iColumn = baseRowType.getFieldOrdinal(id.getSimple()); if (!assignedColumnNames.add(id.getSimple())) { throw newValidationError( id, EigenbaseResource.instance().DuplicateTargetColumn.ex( id.getSimple())); } if (iColumn == -1) { throw newValidationError( id, EigenbaseResource.instance().UnknownTargetColumn.ex( id.getSimple())); } final RelDataTypeField targetField = targetFields.get(iColumn); fieldNames[iTarget] = targetField.getName(); types[iTarget] = targetField.getType(); ++iTarget; } return typeFactory.createStructType(types, fieldNames); }', 'output': 'protected RelDataType createTargetRowType( SqlValidatorTable table, SqlNodeList targetColumnList, boolean append) { RelDataType baseRowType = table.getRowType(); if (targetColumnList == null) { return baseRowType; } RelDataTypeField [] targetFields = baseRowType.getFields(); int targetColumnCount = targetColumnList.size(); if (append) { targetColumnCount += baseRowType.getFieldCount(); } RelDataType [] types = new RelDataType[targetColumnCount]; String [] fieldNames = new String[targetColumnCount]; int iTarget = 0; if (append) { iTarget += baseRowType.getFieldCount(); for (int i = 0; i < iTarget; ++i) { types[i] = targetFields[i].getType(); fieldNames[i] = SqlUtil.deriveAliasFromOrdinal(i); } } Set<String> assignedColumnNames = new HashSet<String>(); for (SqlNode node : targetColumnList) { SqlIdentifier id = (SqlIdentifier) node; int iColumn = baseRowType.getFieldOrdinal(id.getSimple()); if (!assignedColumnNames.add(id.getSimple())) { throw newValidationError( id, EigenbaseResource.instance().DuplicateTargetColumn.ex( id.getSimple())); } if (iColumn == -1) { throw newValidationError( id, EigenbaseResource.instance().UnknownTargetColumn.ex( id.getSimple())); } fieldNames[iTarget] = targetFields[iColumn].getName(); types[iTarget] = targetFields[iColumn].getType(); ++iTarget; } return typeFactory.createStructType(types, fieldNames); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public final void apply() throws IOException, ConfigurationException { if (!schema.getVersion().equals(lastVersion)) throw new ConfigurationException("Previous version mismatch. cannot apply."); if (newVersion.timestamp() <= lastVersion.timestamp()) throw new ConfigurationException("New version timestamp is not newer than the current version timestamp."); assert rm != null; if (!StorageService.instance.isClientMode()) { rm.apply(); long now = System.currentTimeMillis(); ByteBuffer buf = serialize(); RowMutation migration = new RowMutation(Table.SYSTEM_TABLE, MIGRATIONS_KEY); ColumnFamily cf = ColumnFamily.create(Table.SYSTEM_TABLE, MIGRATIONS_CF); column = new Column(ByteBuffer.wrap(UUIDGen.decompose(newVersion)), buf, now); cf.addColumn(column); migration.add(cf); migration.apply(); logger.info("Applying migration {} {}", newVersion.toString(), toString()); migration = new RowMutation(Table.SYSTEM_TABLE, LAST_MIGRATION_KEY); migration.add(new QueryPath(SCHEMA_CF, null, LAST_MIGRATION_KEY), ByteBuffer.wrap(UUIDGen.decompose(newVersion)), now); migration.apply(); ColumnFamilyStore[] schemaStores = new ColumnFamilyStore[] { Table.open(Table.SYSTEM_TABLE).getColumnFamilyStore(Migration.MIGRATIONS_CF), Table.open(Table.SYSTEM_TABLE).getColumnFamilyStore(Migration.SCHEMA_CF) }; List<Future> flushes = new ArrayList<Future>(); for (ColumnFamilyStore cfs : schemaStores) flushes.add(cfs.forceFlush()); for (XXX f : flushes) { if (f == null) continue; try { f.get(); } catch (ExecutionException e) { throw new IOException(e); } catch (InterruptedException e) { throw new IOException(e); } } } applyModels(); }', 'ground_truth': 'public final void apply() throws IOException, ConfigurationException { if (!schema.getVersion().equals(lastVersion)) throw new ConfigurationException("Previous version mismatch. cannot apply."); if (newVersion.timestamp() <= lastVersion.timestamp()) throw new ConfigurationException("New version timestamp is not newer than the current version timestamp."); assert rm != null; if (!StorageService.instance.isClientMode()) { rm.apply(); long now = System.currentTimeMillis(); ByteBuffer buf = serialize(); RowMutation migration = new RowMutation(Table.SYSTEM_TABLE, MIGRATIONS_KEY); ColumnFamily cf = ColumnFamily.create(Table.SYSTEM_TABLE, MIGRATIONS_CF); column = new Column(ByteBuffer.wrap(UUIDGen.decompose(newVersion)), buf, now); cf.addColumn(column); migration.add(cf); migration.apply(); logger.info("Applying migration {} {}", newVersion.toString(), toString()); migration = new RowMutation(Table.SYSTEM_TABLE, LAST_MIGRATION_KEY); migration.add(new QueryPath(SCHEMA_CF, null, LAST_MIGRATION_KEY), ByteBuffer.wrap(UUIDGen.decompose(newVersion)), now); migration.apply(); ColumnFamilyStore[] schemaStores = new ColumnFamilyStore[] { Table.open(Table.SYSTEM_TABLE).getColumnFamilyStore(Migration.MIGRATIONS_CF), Table.open(Table.SYSTEM_TABLE).getColumnFamilyStore(Migration.SCHEMA_CF) }; List<Future<?>> flushes = new ArrayList<Future<?>>(); for (ColumnFamilyStore cfs : schemaStores) flushes.add(cfs.forceFlush()); for (Future<?> f : flushes) { if (f == null) continue; try { f.get(); } catch (ExecutionException e) { throw new IOException(e); } catch (InterruptedException e) { throw new IOException(e); } } } applyModels(); }', 'output': 'public final void apply() throws IOException, ConfigurationException { if (!schema.getVersion().equals(lastVersion)) throw new ConfigurationException("Previous version mismatch. cannot apply."); if (newVersion.timestamp() <= lastVersion.timestamp()) throw new ConfigurationException("New version timestamp is not newer than the current version timestamp."); assert rm != null; if (!StorageService.instance.isClientMode()) { rm.apply(); long now = System.currentTimeMillis(); ByteBuffer buf = serialize(); RowMutation migration = new RowMutation(Table.SYSTEM_TABLE, MIGRATIONS_KEY); ColumnFamily cf = ColumnFamily.create(Table.SYSTEM_TABLE, MIGRATIONS_CF); column = new Column(ByteBuffer.wrap(UUIDGen.decompose(newVersion)), buf, now); cf.addColumn(column); migration.add(cf); migration.apply(); logger.info("Applying migration {} {}", newVersion.toString(), toString()); migration = new RowMutation(Table.SYSTEM_TABLE, LAST_MIGRATION_KEY); migration.add(new QueryPath(SCHEMA_CF, null, LAST_MIGRATION_KEY), ByteBuffer.wrap(UUIDGen.decompose(newVersion)), now); migration.apply(); ColumnFamilyStore[] schemaStores = new ColumnFamilyStore[] { Table.open(Table.SYSTEM_TABLE).getColumnFamilyStore(Migration.MIGRATIONS_CF), Table.open(Table.SYSTEM_TABLE).getColumnFamilyStore(Migration.SCHEMA_CF) }; List<Future> flushes = new ArrayList<Future>(); for (ColumnFamilyStore cfs : schemaStores) flushes.add(cfs.forceFlush()); for (XXX f : flushes) { if (f == null) continue; try { f.get(); } catch (ExecutionException e) { throw new IOException(e); } catch (InterruptedException e) { throw new IOException(e); } } } applyModels(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void createInsertSubmenu(JMenu menu) { if (!(menu.getMenuComponentCount() > 0)) { Set<Class> classes = new HashSet<Class>(); SortedSet<PaletteItem> items = new TreeSet<PaletteItem>(new Comparator<PaletteItem>() { @Override public int compare(PaletteItem item1, PaletteItem item2) { String name1 = item1.getNode().getDisplayName(); String name2 = item2.getNode().getDisplayName(); return name1.compareTo(name2); } }); for (PaletteItem item : PaletteUtils.getAllItems()) { XXX clazz = item.getComponentClass(); if ((clazz != null) && !classes.contains(clazz) && (JMenuItem.class.isAssignableFrom(clazz) || JSeparator.class.isAssignableFrom(clazz))) { classes.add(clazz); items.add(item); } } for (PaletteItem item : items) { JMenuItem menuitem = new JMenuItem(item.getNode().getDisplayName()); menuitem.addActionListener(new AddListener(item)); menu.add(menuitem); } } }', 'ground_truth': 'private void createInsertSubmenu(JMenu menu) { if (!(menu.getMenuComponentCount() > 0)) { Set<Class> classes = new HashSet<Class>(); SortedSet<PaletteItem> items = new TreeSet<PaletteItem>(new Comparator<PaletteItem>() { @Override public int compare(PaletteItem item1, PaletteItem item2) { String name1 = item1.getNode().getDisplayName(); String name2 = item2.getNode().getDisplayName(); return name1.compareTo(name2); } }); for (PaletteItem item : PaletteUtils.getAllItems()) { Class<?> clazz = item.getComponentClass(); if ((clazz != null) && !classes.contains(clazz) && (JMenuItem.class.isAssignableFrom(clazz) || JSeparator.class.isAssignableFrom(clazz))) { classes.add(clazz); items.add(item); } } for (PaletteItem item : items) { JMenuItem menuitem = new JMenuItem(item.getNode().getDisplayName()); menuitem.addActionListener(new AddListener(item)); menu.add(menuitem); } } }', 'output': 'private void createInsertSubmenu(JMenu menu) { if (!(menu.getMenuComponentCount() > 0)) { Set<Class> classes = new HashSet<Class>(); SortedSet<PaletteItem> items = new TreeSet<PaletteItem>(new Comparator<PaletteItem>() { @Override public int compare(PaletteItem item1, PaletteItem item2) { String name1 = item1.getNode().getDisplayName(); String name2 = item2.getNode().getDisplayName(); return name1.compareTo(name2); } }); for (PaletteItem item : PaletteUtils.getAllItems()) { Class clazz = item.getComponentClass(); if ((clazz != null) && !classes.contains(clazz) && (JMenuItem.class.isAssignableFrom(clazz) || JSeparator.class.isAssignableFrom(clazz))) { classes.add(clazz); items.add(item); } } for (PaletteItem item : items) { JMenuItem menuitem = new JMenuItem(item.getNode().getDisplayName()); menuitem.addActionListener(new AddListener(item)); menu.add(menuitem); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void resend(GridNioSession ses) { assert ses instanceof GridSelectorNioSessionImpl; GridNioRecoveryDescriptor recoveryDesc = ses.recoveryDescriptor(); if (recoveryDesc != null && !recoveryDesc.messagesFutures().isEmpty()) { Deque<GridNioFuture<?>> futs = recoveryDesc.messagesFutures(); if (log.isDebugEnabled()) log.debug("Resend messages [rmtNode=" + recoveryDesc.node().id() + ", msgCnt=" + futs.size() + \']\'); GridSelectorNioSessionImpl ses0 = (GridSelectorNioSessionImpl)ses; XXX fut0 = futs.iterator().next(); for (XXX fut : futs) { fut.messageThread(true); ((NioOperationFuture)fut).resetSession(ses0); } ses0.resend(futs); clientWorkers.get(ses0.selectorIndex()).offer(((NioOperationFuture)fut0)); } }', 'ground_truth': 'public void resend(GridNioSession ses) { assert ses instanceof GridSelectorNioSessionImpl; GridNioRecoveryDescriptor recoveryDesc = ses.outRecoveryDescriptor(); if (recoveryDesc != null && !recoveryDesc.messagesRequests().isEmpty()) { Deque<SessionWriteRequest> futs = recoveryDesc.messagesRequests(); if (log.isDebugEnabled()) log.debug("Resend messages [rmtNode=" + recoveryDesc.node().id() + ", msgCnt=" + futs.size() + \']\'); GridSelectorNioSessionImpl ses0 = (GridSelectorNioSessionImpl)ses; SessionWriteRequest fut0 = futs.iterator().next(); for (SessionWriteRequest fut : futs) { fut.messageThread(true); fut.resetSession(ses0); } ses0.resend(futs); ses0.offerStateChange((GridNioServer.SessionChangeRequest)fut0); } }', 'output': 'public void resend(GridNioSession ses) { assert ses instanceof GridSelectorNioSessionImpl; GridNioRecoveryDescriptor recoveryDesc = ses.recoveryDescriptor(); if (recoveryDesc != null && !recoveryDesc.messagesFutures().isEmpty()) { Deque<GridNioFuture<?>> futs = recoveryDesc.messagesFutures(); if (log.isDebugEnabled()) log.debug("Resend messages [rmtNode=" + recoveryDesc.node().id() + ", msgCnt=" + futs.size() + \']\'); GridSelectorNioSessionImpl ses0 = (GridSelectorNioSessionImpl)ses; IgniteInternalFuture<?> fut0 = futs.iterator().next(); for (IgniteInternalFuture<?> fut : futs) { fut.messageThread(true); ((NioOperationFuture)fut).resetSession(ses0); } ses0.resend(futs); clientWorkers.get(ses0.selectorIndex()).offer(((NioOperationFuture)fut0)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void addOperatorSection() { XXX input = Dataset.of(pipeline.apply(Create.of(1, 2, 4, 3))); Dataset<String> mappedElements = MapElements.named("Int2Str").of(input).using(String::valueOf).output(); PAssert.that(mappedElements.getPCollection()).containsInAnyOrder("1", "2", "4", "3"); pipeline.run(); }', 'ground_truth': 'public void addOperatorSection() { PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 4, 3)); PCollection<String> mappedElements = MapElements.named("Int2Str").of(input).using(String::valueOf).output(); PAssert.that(mappedElements).containsInAnyOrder("1", "2", "4", "3"); pipeline.run(); }', 'output': 'public void addOperatorSection() { PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 4, 3)); PCollection<String> mappedElements = MapElements.named("Int2Str").of(input).using(String::valueOf).output(); PAssert.that(mappedElements.getPCollection()).containsInAnyOrder("1", "2", "4", "3"); pipeline.run(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testWaitForEvent() throws Exception { Ignite ignite = grid(); final int usrType = Integer.MAX_VALUE - 1; XXX fut = waitForLocalEvent(ignite.events(), new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent e) { return e.type() == usrType; } }, usrType); try { fut.get(500); fail("GridFutureTimeoutException must have been thrown."); } catch (IgniteFutureTimeoutException e) { info("Caught expected exception: " + e); } ignite.events().recordLocal(new IgniteEventAdapter(null, "Test message.", usrType) { }); assert fut.get() != null; assertEquals(usrType, fut.get().type()); } public void testWaitForEventContinuationTimeout() throws Exception { Ignite ignite = grid(); try { int usrType = Integer.MAX_VALUE - 1; waitForLocalEvent(ignite.events(), F.<IgniteEvent>alwaysTrue(), usrType).get(1000); fail("GridFutureTimeoutException must have been thrown."); } catch (IgniteFutureTimeoutException e) { info("Caught expected exception: " + e); } }', 'ground_truth': 'public void testWaitForEvent() throws Exception { Ignite ignite = grid(); final int usrType = Integer.MAX_VALUE - 1; IgniteInternalFuture<IgniteEvent> fut = waitForLocalEvent(ignite.events(), new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent e) { return e.type() == usrType; } }, usrType); try { fut.get(500); fail("GridFutureTimeoutException must have been thrown."); } catch (IgniteFutureTimeoutException e) { info("Caught expected exception: " + e); } ignite.events().recordLocal(new IgniteEventAdapter(null, "Test message.", usrType) { }); assert fut.get() != null; assertEquals(usrType, fut.get().type()); }', 'output': 'public void testWaitForEvent() throws Exception { Ignite ignite = grid(); final int usrType = Integer.MAX_VALUE - 1; IgniteFuture<?> fut = waitForLocalEvent(ignite.events(), new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent e) { return e.type() == usrType; } }, usrType); try { fut.get(500); fail("GridFutureTimeoutException must have been thrown."); } catch (IgniteFutureTimeoutException e) { info("Caught expected exception: " + e); } ignite.events().recordLocal(new IgniteEventAdapter(null, "Test message.", usrType) { }); assert fut.get() != null; assertEquals(usrType, fut.get().type()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testUserDefinedWithNatives() { for (XXX version = Server.MIN_SUPPORTED_VERSION; version <= Server.CURRENT_VERSION; version++) { for (int n = 0; n < 100; n++) { Value value = generateUserDefinedValue(version, randomUserType(0), true); compareCqlLiteral(version, value); } } }', 'ground_truth': 'public void testUserDefinedWithNatives() { for (ProtocolVersion version : ProtocolVersion.SUPPORTED) { for (int n = 0; n < 100; n++) { Value value = generateUserDefinedValue(version, randomUserType(0), true); compareCqlLiteral(version, value); } } }', 'output': 'public void testUserDefinedWithNatives() { for (Version version = Server.MIN_SUPPORTED_VERSION; version <= Server.CURRENT_VERSION; version++) { for (int n = 0; n < 100; n++) { Value value = generateUserDefinedValue(version, randomUserType(0), true); compareCqlLiteral(version, value); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Set<String> getStopWordsForLanguage(Locale locale) { if (locale == null) return null; String rootLang = locale.getLanguage().substring(0, 2); try { return (!SUPPORTED_LANGUAGES.contains(rootLang)) ? null : STOP_WORDS_CACHE.get(rootLang); } catch (XXX e) { logger.error("Failed to populate Stop Words Cache for language [{}]", locale.getLanguage(), e); return null; } }', 'ground_truth': 'public static Set<String> getStopWordsForLanguage(Locale locale) { if (locale == null) return null; String rootLang = locale.getLanguage().substring(0, 2); try { return (!SUPPORTED_LANGUAGES.contains(rootLang)) ? null : STOP_WORDS_CACHE.get(rootLang); } catch (CompletionException e) { logger.error("Failed to populate Stop Words Cache for language [{}]", locale.getLanguage(), e); return null; } }', 'output': 'public static Collection<String> getStopWordsForLanguage(Locale locale) { if (locale == null) return null; String rootLang = locale.getLanguage().substring(0, 2); try { return (!SUPPORTED_LANGUAGES.contains(rootLang)) ? null : STOP_WORDS_CACHE.get(rootLang); } catch (IOException e) { logger.error("Failed to populate Stop Words Cache for language [{}]", locale.getLanguage(), e); return null; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void onMatch(RelOptRuleCall call) { final XXX multiJoinRel = call.rel(0); final LoptMultiJoin multiJoin = new LoptMultiJoin(multiJoinRel); findRemovableOuterJoins(multiJoin); final RexBuilder rexBuilder = multiJoinRel.getCluster().getRexBuilder(); final LoptSemiJoinOptimizer semiJoinOpt = new LoptSemiJoinOptimizer(multiJoin, rexBuilder); semiJoinOpt.makePossibleSemiJoins(multiJoin); int iterations = 0; do { if (!semiJoinOpt.chooseBestSemiJoin(multiJoin)) { break; } if (iterations++ > 10) { break; } } while (true); multiJoin.setFactorWeights(); findRemovableSelfJoins(multiJoin); findBestOrderings(multiJoin, semiJoinOpt, call); }', 'ground_truth': 'public void onMatch(RelOptRuleCall call) { final MultiJoin multiJoinRel = call.rel(0); final LoptMultiJoin multiJoin = new LoptMultiJoin(multiJoinRel); findRemovableOuterJoins(multiJoin); final RexBuilder rexBuilder = multiJoinRel.getCluster().getRexBuilder(); final LoptSemiJoinOptimizer semiJoinOpt = new LoptSemiJoinOptimizer(multiJoin, rexBuilder); semiJoinOpt.makePossibleSemiJoins(multiJoin); int iterations = 0; do { if (!semiJoinOpt.chooseBestSemiJoin(multiJoin)) { break; } if (iterations++ > 10) { break; } } while (true); multiJoin.setFactorWeights(); findRemovableSelfJoins(multiJoin); findBestOrderings(multiJoin, semiJoinOpt, call); }', 'output': 'public void onMatch(RelOptRuleCall call) { final RelNode multiJoinRel = call.rel(0); final LoptMultiJoin multiJoin = new LoptMultiJoin(multiJoinRel); findRemovableOuterJoins(multiJoin); final RexBuilder rexBuilder = multiJoinRel.getCluster().getRexBuilder(); final LoptSemiJoinOptimizer semiJoinOpt = new LoptSemiJoinOptimizer(multiJoin, rexBuilder); semiJoinOpt.makePossibleSemiJoins(multiJoin); int iterations = 0; do { if (!semiJoinOpt.chooseBestSemiJoin(multiJoin)) { break; } if (iterations++ > 10) { break; } } while (true); multiJoin.setFactorWeights(); findRemovableSelfJoins(multiJoin); findBestOrderings(multiJoin, semiJoinOpt, call); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testUnfoldBasic() { DAG<Operator<?, ?, ?>> unfolded = FlowUnfolder.unfold(flow, Executor.getBasicOps()); assertEquals(7, unfolded.size()); assertEquals(1, unfolded.getRoots().size()); Node<Operator<?, ?, ?>> root = unfolded.getRoots().stream().findFirst().get(); assertEquals(InputOperator.class, root.get().getClass()); assertEquals(1, root.getChildren().size()); Node<Operator<?, ?, ?>> map = root.getChildren().get(0); assertEquals(FlatMap.class, map.get().getClass()); assertEquals(2, map.getChildren().size()); java.util.Map<Class<? extends Operator>, Node<Operator<?, ?, ?>>> childrenMap; childrenMap = toClassMap((List) map.getChildren()); assertTrue(childrenMap.containsKey(FlatMap.class)); assertTrue(childrenMap.containsKey(ReduceStateByKey.class)); Node<Operator<?, ?, ?>> firstFlatMap = childrenMap.get(FlatMap.class); Node<Operator<?, ?, ?>> firstReduceStateByKey = childrenMap.get( ReduceStateByKey.class); Node<Operator<?, ?, ?>> union =  getOnlyAndValidate( firstFlatMap.getChildren(), Union.class); Node<Operator<?, ?, ?>> secondReduceStateByKey = getOnlyAndValidate( union.getChildren(), ReduceStateByKey.class); assertNotNull(secondReduceStateByKey.get().output().getOutputSink()); assertEquals(StdoutSink.class, secondReduceStateByKey.get().output().getOutputSink().getClass()); Node<Operator<?, ?, ?>> secondFlatMap = getOnlyAndValidate( firstReduceStateByKey.getChildren(), FlatMap.class); assertTrue(union == getOnlyAndValidate(secondFlatMap.getChildren(), Union.class)); }', 'ground_truth': 'public void testUnfoldBasic() { DAG<Operator<?, ?>> unfolded = FlowUnfolder.unfold(flow, Executor.getBasicOps()); assertEquals(7, unfolded.size()); assertEquals(1, unfolded.getRoots().size()); Node<Operator<?, ?>> root = unfolded.getRoots().stream().findFirst().get(); assertEquals(InputOperator.class, root.get().getClass()); assertEquals(1, root.getChildren().size()); Node<Operator<?, ?>> map = root.getChildren().get(0); assertEquals(FlatMap.class, map.get().getClass()); assertEquals(2, map.getChildren().size()); java.util.Map<Class<? extends Operator>, Node<Operator<?, ?>>> childrenMap; childrenMap = toClassMap((List) map.getChildren()); assertTrue(childrenMap.containsKey(FlatMap.class)); assertTrue(childrenMap.containsKey(ReduceStateByKey.class)); Node<Operator<?, ?>> firstFlatMap = childrenMap.get(FlatMap.class); Node<Operator<?, ?>> firstReduceStateByKey = childrenMap.get( ReduceStateByKey.class); Node<Operator<?, ?>> union =  getOnlyAndValidate( firstFlatMap.getChildren(), Union.class); Node<Operator<?, ?>> secondReduceStateByKey = getOnlyAndValidate( union.getChildren(), ReduceStateByKey.class); assertNotNull(secondReduceStateByKey.get().output().getOutputSink()); assertEquals(StdoutSink.class, secondReduceStateByKey.get().output().getOutputSink().getClass()); Node<Operator<?, ?>> secondFlatMap = getOnlyAndValidate( firstReduceStateByKey.getChildren(), FlatMap.class); assertTrue(union == getOnlyAndValidate(secondFlatMap.getChildren(), Union.class)); }', 'output': 'public void testUnfoldBasic() { DAG<Operator<?, ?, ?>> unfolded = FlowUnfolder.unfold(flow, Executor.getBasicOps()); assertEquals(7, unfolded.size()); assertEquals(1, unfolded.getRoots().size()); Node<Operator<?, ?, ?>> root = unfolded.getRoots().stream().findFirst().get(); assertEquals(InputOperator.class, root.get().getClass()); assertEquals(1, root.getChildren().size()); Node<Operator<?, ?, ?>> map = root.getChildren().get(0); assertEquals(FlatMap.class, map.get().getClass()); assertEquals(2, map.getChildren().size()); java.util.Map<Class<? extends Operator>, Node<Operator<?, ?, ?>>> childrenMap; childrenMap = toClassMap((List) map.getChildren()); assertTrue(childrenMap.containsKey(FlatMap.class)); assertTrue(childrenMap.containsKey(ReduceStateByKey.class)); Node<Operator<?, ?, ?>> firstFlatMap = childrenMap.get(FlatMap.class); Node<Operator<?, ?, ?>> firstReduceStateByKey = childrenMap.get( ReduceStateByKey.class); Node<Operator<?, ?, ?>> union =  getOnlyAndValidate( firstFlatMap.getChildren(), Union.class); Node<Operator<?, ?, ?>> secondReduceStateByKey = getOnlyAndValidate( union.getChildren(), ReduceStateByKey.class); assertNotNull(secondReduceStateByKey.get().output().getOutputSink()); assertEquals(StdoutSink.class, secondReduceStateByKey.get().output().getOutputSink().getClass()); Node<Operator<?, ?, ?>> secondFlatMap = getOnlyAndValidate( firstReduceStateByKey.getChildren(), FlatMap.class); assertTrue(union == getOnlyAndValidate(secondFlatMap.getChildren(), Union.class)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCleanupWithIndexes() throws IOException, ExecutionException, InterruptedException { Table table = Table.open(TABLE1); ColumnFamilyStore cfs = table.getColumnFamilyStore(CF1); assertEquals(cfs.indexManager.getIndexedColumns().iterator().next(), COLUMN); List<Row> rows; fillCF(cfs, LOOPS); rows = cfs.getRangeSlice(null, Util.range("", ""), 1000, new IdentityQueryFilter()); assertEquals(LOOPS, rows.size()); SecondaryIndex index = cfs.indexManager.getIndexForColumn(COLUMN); long start = System.currentTimeMillis(); while (!index.isIndexBuilt(COLUMN) && System.currentTimeMillis() < start + 10000) Thread.sleep(10); IndexExpression expr = new IndexExpression(COLUMN, IndexOperator.EQ, VALUE); IndexClause clause = new IndexClause(Arrays.asList(expr), ByteBufferUtil.EMPTY_BYTE_BUFFER, Integer.MAX_VALUE); IFilter filter = new IdentityQueryFilter(); IPartitioner p = StorageService.getPartitioner(); XXX range = new Range(p.getMinimumToken(), p.getMinimumToken()); rows = table.getColumnFamilyStore(CF1).search(clause, range, filter); assertEquals(LOOPS, rows.size()); TokenMetadata tmd = StorageService.instance.getTokenMetadata(); byte[] tk1 = new byte[1], tk2 = new byte[1]; tk1[0] = 2; tk2[0] = 1; tmd.updateNormalToken(new BytesToken(tk1), InetAddress.getByName("127.0.0.1")); tmd.updateNormalToken(new BytesToken(tk2), InetAddress.getByName("127.0.0.2")); CompactionManager.instance.performCleanup(cfs, new NodeId.OneShotRenewer()); rows = cfs.getRangeSlice(null, Util.range("", ""), 1000, new IdentityQueryFilter()); assertEquals(0, rows.size()); assert cfs.getSSTables().isEmpty(); rows = cfs.search(clause, range, filter); assertEquals(0, rows.size()); }', 'ground_truth': 'public void testCleanupWithIndexes() throws IOException, ExecutionException, InterruptedException { Table table = Table.open(TABLE1); ColumnFamilyStore cfs = table.getColumnFamilyStore(CF1); assertEquals(cfs.indexManager.getIndexedColumns().iterator().next(), COLUMN); List<Row> rows; fillCF(cfs, LOOPS); rows = cfs.getRangeSlice(null, Util.range("", ""), 1000, new IdentityQueryFilter()); assertEquals(LOOPS, rows.size()); SecondaryIndex index = cfs.indexManager.getIndexForColumn(COLUMN); long start = System.currentTimeMillis(); while (!index.isIndexBuilt(COLUMN) && System.currentTimeMillis() < start + 10000) Thread.sleep(10); IndexExpression expr = new IndexExpression(COLUMN, IndexOperator.EQ, VALUE); IndexClause clause = new IndexClause(Arrays.asList(expr), ByteBufferUtil.EMPTY_BYTE_BUFFER, Integer.MAX_VALUE); IFilter filter = new IdentityQueryFilter(); IPartitioner p = StorageService.getPartitioner(); Range<RowPosition> range = Util.range("", ""); rows = table.getColumnFamilyStore(CF1).search(clause, range, filter); assertEquals(LOOPS, rows.size()); TokenMetadata tmd = StorageService.instance.getTokenMetadata(); byte[] tk1 = new byte[1], tk2 = new byte[1]; tk1[0] = 2; tk2[0] = 1; tmd.updateNormalToken(new BytesToken(tk1), InetAddress.getByName("127.0.0.1")); tmd.updateNormalToken(new BytesToken(tk2), InetAddress.getByName("127.0.0.2")); CompactionManager.instance.performCleanup(cfs, new NodeId.OneShotRenewer()); rows = cfs.getRangeSlice(null, range, 1000, new IdentityQueryFilter()); assertEquals(0, rows.size()); assert cfs.getSSTables().isEmpty(); rows = cfs.search(clause, range, filter); assertEquals(0, rows.size()); }', 'output': 'public void testCleanupWithIndexes() throws IOException, ExecutionException, InterruptedException { Table table = Table.open(TABLE1); ColumnFamilyStore cfs = table.getColumnFamilyStore(CF1); assertEquals(cfs.indexManager.getIndexedColumns().iterator().next(), COLUMN); List<Row> rows; fillCF(cfs, LOOPS); rows = cfs.getRangeSlice(null, Util.range("", ""), 1000, new IdentityQueryFilter()); assertEquals(LOOPS, rows.size()); SecondaryIndex index = cfs.indexManager.getIndexForColumn(COLUMN); long start = System.currentTimeMillis(); while (!index.isIndexBuilt(COLUMN) && System.currentTimeMillis() < start + 10000) Thread.sleep(10); IndexExpression expr = new IndexExpression(COLUMN, IndexOperator.EQ, VALUE); IndexClause clause = new IndexClause(Arrays.asList(expr), ByteBufferUtil.EMPTY_BYTE_BUFFER, Integer.MAX_VALUE); IFilter filter = new IdentityQueryFilter(); IPartitioner p = StorageService.getPartitioner(); Range range = new Range(p.getMinimumToken(), p.getMinimumToken()); rows = table.getColumnFamilyStore(CF1).search(clause, range, filter); assertEquals(LOOPS, rows.size()); TokenMetadata tmd = StorageService.instance.getTokenMetadata(); byte[] tk1 = new byte[1], tk2 = new byte[1]; tk1[0] = 2; tk2[0] = 1; tmd.updateNormalToken(new BytesToken(tk1), InetAddress.getByName("127.0.0.1")); tmd.updateNormalToken(new BytesToken(tk2), InetAddress.getByName("127.0.0.2")); CompactionManager.instance.performCleanup(cfs, new NodeId.OneShotRenewer()); rows = cfs.getRangeSlice(null, Util.range("", ""), 1000, new IdentityQueryFilter()); assertEquals(0, rows.size()); assert cfs.getSSTables().'}
{'instruction': 'Please refactor the following code:', 'input': 'private TargetWindowT loadIfNeeded(SdkFunctionSpec windowMappingFn, BoundedWindow mainWindow) { try { String processRequestInstructionId = idGenerator.getId(); InstructionRequest processRequest = InstructionRequest.newBuilder() .setInstructionId(processRequestInstructionId) .setProcessBundle( ProcessBundleRequest.newBuilder() .setProcessBundleDescriptorReference(registerIfRequired())) .build(); ConcurrentLinkedQueue<WindowedValue<KV<byte[], TargetWindowT>>> outputValue = new ConcurrentLinkedQueue<>(); InboundDataClient waitForInboundTermination = beamFnDataService.receive( LogicalEndpoint.of( processRequestInstructionId, BeamFnApi.Target.newBuilder() .setName("out") .setPrimitiveTransformReference("write") .build()), inboundCoder, outputValue::add); CompletionStage<InstructionResponse> processResponse = instructionRequestHandler.handle(processRequest); try (CloseableFnDataReceiver<WindowedValue<KV<byte[], BoundedWindow>>> outboundConsumer = beamFnDataService.send( LogicalEndpoint.of( processRequestInstructionId, BeamFnApi.Target.newBuilder() .setName("in") .setPrimitiveTransformReference("read") .build()), outboundCoder)) { outboundConsumer.accept(WindowedValue.valueInGlobalWindow(KV.of(EMPTY_ARRAY, mainWindow))); } throwIfFailure(processResponse); waitForInboundTermination.awaitCompletion(); WindowedValue<KV<byte[], TargetWindowT>> sideInputWindow = outputValue.poll(); checkState( sideInputWindow != null, "Expected side input window to have been emitted by SDK harness."); checkState( sideInputWindow.getValue() != null, "Side input window emitted by SDK harness was a WindowedValue with no value in it."); checkState( sideInputWindow.getValue().getValue() != null, "Side input window emitted by SDK harness was a WindowedValue<KV<...>> with a null V."); checkState( outputValue.isEmpty(), "Expected only a single side input window to have been emitted by " + "the SDK harness but also received %s", outputValue); return sideInputWindow.getValue().getValue(); } catch (XXX e) { LOG.error("Unable to map main input window {} to side input window.", mainWindow, e); throw new IllegalStateException(e); } }', 'ground_truth': 'private TargetWindowT loadIfNeeded(SdkFunctionSpec windowMappingFn, BoundedWindow mainWindow) { try { String processRequestInstructionId = idGenerator.getId(); InstructionRequest processRequest = InstructionRequest.newBuilder() .setInstructionId(processRequestInstructionId) .setProcessBundle( ProcessBundleRequest.newBuilder() .setProcessBundleDescriptorReference(registerIfRequired())) .build(); ConcurrentLinkedQueue<WindowedValue<KV<byte[], TargetWindowT>>> outputValue = new ConcurrentLinkedQueue<>(); InboundDataClient waitForInboundTermination = beamFnDataService.receive( LogicalEndpoint.of( processRequestInstructionId, BeamFnApi.Target.newBuilder() .setName("out") .setPrimitiveTransformReference("write") .build()), inboundCoder, outputValue::add); CompletionStage<InstructionResponse> processResponse = instructionRequestHandler.handle(processRequest); try (CloseableFnDataReceiver<WindowedValue<KV<byte[], BoundedWindow>>> outboundConsumer = beamFnDataService.send( LogicalEndpoint.of( processRequestInstructionId, BeamFnApi.Target.newBuilder() .setName("in") .setPrimitiveTransformReference("read") .build()), outboundCoder)) { outboundConsumer.accept(WindowedValue.valueInGlobalWindow(KV.of(EMPTY_ARRAY, mainWindow))); } throwIfFailure(processResponse); waitForInboundTermination.awaitCompletion(); WindowedValue<KV<byte[], TargetWindowT>> sideInputWindow = outputValue.poll(); checkState( sideInputWindow != null, "Expected side input window to have been emitted by SDK harness."); checkState( sideInputWindow.getValue() != null, "Side input window emitted by SDK harness was a WindowedValue with no value in it."); checkState( sideInputWindow.getValue().getValue() != null, "Side input window emitted by SDK harness was a WindowedValue<KV<...>> with a null V."); checkState( outputValue.isEmpty(), "Expected only a single side input window to have been emitted by " + "the SDK harness but also received %s", outputValue); return sideInputWindow.getValue().getValue(); } catch (Throwable e) { LOG.error("Unable to map main input window {} to side input window.", mainWindow, e); throw new IllegalStateException(e); } }', 'output': 'private TargetWindowT loadIfNeeded(SdkFunctionSpec windowMappingFn, BoundedWindow mainWindow) { try { String processRequestInstructionId = idGenerator.getId(); InstructionRequest processRequest = InstructionRequest.newBuilder() .setInstructionId(processRequestInstructionId) .setProcessBundle( ProcessBundleRequest.newBuilder() .setProcessBundleDescriptorReference(registerIfRequired())) .build(); ConcurrentLinkedQueue<WindowedValue<KV<byte[], TargetWindowT>>> outputValue = new ConcurrentLinkedQueue<>(); InboundDataClient waitForInboundTermination = beamFnDataService.receive( LogicalEndpoint.of( processRequestInstructionId, BeamFnApi.Target.newBuilder() .setName("out") .setPrimitiveTransformReference("write") .build()), inboundCoder, outputValue::add); CompletionStage<InstructionResponse> processResponse = instructionRequestHandler.handle(processRequest); try (CloseableFnDataReceiver<WindowedValue<KV<byte[], BoundedWindow>>> outboundConsumer = beamFnDataService.send( LogicalEndpoint.of( processRequestInstructionId, BeamFnApi.Target.newBuilder() .setName("in") .setPrimitiveTransformReference("read") .build()), outboundCoder)) { outboundConsumer.accept(WindowedValue.valueInGlobalWindow(KV.of(EMPTY_ARRAY, mainWindow))); } throwIfFailure(processResponse); waitForInboundTermination.awaitCompletion(); WindowedValue<KV<byte[], TargetWindowT>> sideInputWindow = outputValue.poll(); checkState( sideInputWindow != null, "Expected side input window to have been emitted by SDK harness."); checkState( sideInputWindow.getValue() != null, "Side input window emitted by SDK harness was a WindowedValue with no value in it."); checkState( sideInputWindow.getValue().getValue() != null, "Side input window emitted by SDK harness was a WindowedValue<KV<...>> with a null V."); checkState( outputValue.isEmpty(), "Expected only a single side input window to have been emitted by " + "the SDK harness but also received %s", outputValue); return sideInputWindow.getValue().getValue(); } catch ('}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMultipleNodesQuery() throws Exception { Person p1 = new Person("Jon", 1500); Person p2 = new Person("Jane", 2000); Person p3 = new Person("Mike", 1800); Person p4 = new Person("Bob", 1900); GridCache<UUID, Person> cache0 = grid(0).cache(null); cache0.put(p1.id(), p1); cache0.put(p2.id(), p2); cache0.put(p3.id(), p3); cache0.put(p4.id(), p4); assertEquals(4, cache0.size()); assert grid(0).nodes().size() == gridCount(); GridCacheQuery<Map.Entry<UUID, Person>> qry = cache0.queries().createSqlQuery(Person.class, "salary < 2000"); qry.includeBackups(true); qry.enableDedup(false); qry.keepAll(true); Collection<Map.Entry<UUID, Person>> entries = qry.execute().get(); assert entries != null; info("Queried entries: " + entries); info("Queried persons: " + F.viewReadOnly(entries, F.<Person>mapEntry2Value())); assertEquals(3 * gridCount(), entries.size()); checkResult(entries, p1, p3, p4); qry = cache0.projection(F.<UUID, Person>cachePrimary()).queries().createSqlQuery(Person.class, "salary < 2000"); qry.keepAll(true); entries = qry.execute().get(); assert entries != null; info("Queried persons: " + F.viewReadOnly(entries, F.<Person>mapEntry2Value())); assertEquals(3, entries.size()); checkResult(entries, p1, p3, p4); }', 'ground_truth': 'public void testMultipleNodesQuery() throws Exception { Person p1 = new Person("Jon", 1500); Person p2 = new Person("Jane", 2000); Person p3 = new Person("Mike", 1800); Person p4 = new Person("Bob", 1900); Cache<UUID, Person> cache0 = grid(0).cache(null); cache0.put(p1.id(), p1); cache0.put(p2.id(), p2); cache0.put(p3.id(), p3); cache0.put(p4.id(), p4); assertEquals(4, cache0.size()); assert grid(0).nodes().size() == gridCount(); CacheQuery<Map.Entry<UUID, Person>> qry = cache0.queries().createSqlQuery(Person.class, "salary < 2000"); qry.includeBackups(true); qry.enableDedup(false); qry.keepAll(true); Collection<Map.Entry<UUID, Person>> entries = qry.execute().get(); assert entries != null; info("Queried entries: " + entries); info("Queried persons: " + F.viewReadOnly(entries, F.<Person>mapEntry2Value())); assertEquals(3 * gridCount(), entries.size()); checkResult(entries, p1, p3, p4); qry = cache0.projection(F.<UUID, Person>cachePrimary()).queries().createSqlQuery(Person.class, "salary < 2000"); qry.keepAll(true); entries = qry.execute().get(); assert entries != null; info("Queried persons: " + F.viewReadOnly(entries, F.<Person>mapEntry2Value())); assertEquals(3, entries.size()); checkResult(entries, p1, p3, p4); }', 'output': 'public void testMultipleNodesQuery() throws Exception { Person p1 = new Person("Jon", 1500); Person p2 = new Person("Jane", 2000); Person p3 = new Person("Mike", 1800); Person p4 = new Person("Bob", 1900); GridCache<UUID, Person> cache0 = grid(0).cache(null); cache0.put(p1.id(), p1); cache0.put(p2.id(), p2); cache0.put(p3.id(), p3); cache0.put(p4.id(), p4); assertEquals(4, cache0.size()); assert grid(0).nodes().size() == gridCount(); GridCacheQuery<Map.Entry<UUID, Person>> qry = cache0.queries().createSqlQuery(Person.class, "salary < 2000"); qry.includeBackups(true); qry.enableDedup(false); qry.keepAll(true); Collection<Map.Entry<UUID, Person>> entries = qry.execute().get(); assert entries != null; info("Queried entries: " + entries); info("Queried persons: " + F.viewReadOnly(entries, F.<Person>mapEntry2Value())); assertEquals(3 * gridCount(), entries.size()); checkResult(entries, p1, p3, p4); qry = cache0.projection(F.<UUID, Person>cachePrimary()).queries().createSqlQuery(Person.class, "salary < 2000"); qry.keepAll(true); entries = qry.execute().get(); assert entries != null; info("Queried persons: " + F.viewReadOnly(entries, F.<Person>mapEntry2Value())); assertEquals(3, entries.size()); checkResult(entries, p1, p3, p4); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': "protected String dump(boolean canonical) { if (canonical) { XXX buf = new StringBuffer(); buf.append('<').append(getNodeTypeSymbol()).append('>'); if (getNestedBlock() != null) { buf.append(getNestedBlock().getCanonicalForm()); } return buf.toString(); } else { return getNodeTypeSymbol(); } }", 'ground_truth': "protected String dump(boolean canonical) { if (canonical) { StringBuilder buf = new StringBuilder(); buf.append('<').append(getNodeTypeSymbol()).append('>'); if (getNestedBlock() != null) { buf.append(getNestedBlock().getCanonicalForm()); } return buf.toString(); } else { return getNodeTypeSymbol(); } }", 'output': "protected String dump(boolean canonical) { if (canonical) { StringBuilder buf = new StringBuilder(); buf.append('<').append(getNodeTypeSymbol()).append('>'); if (getNestedBlock() != null) { buf.append(getNestedBlock().getCanonicalForm()); } return buf.toString(); } else { return getNodeTypeSymbol(); } }</s>"}
{'instruction': 'Please refactor the following code:', 'input': 'public void testLockUnlock() throws Exception { if (atomicityMode() == ATOMIC) return; GridCache<Integer, TestValue> c = grid(0).cache(null); checkLockUnlock(primaryKey(c)); checkLockUnlock(backupKey(c)); checkLockUnlock(nearKey(c)); }', 'ground_truth': 'public void testLockUnlock() throws Exception { if (atomicityMode() == ATOMIC) return; IgniteCache<Integer, TestValue> c = grid(0).jcache(null); checkLockUnlock(primaryKey(c)); checkLockUnlock(backupKey(c)); checkLockUnlock(nearKey(c)); }', 'output': 'public void testLockUnlock() throws Exception { if (atomicityMode() == ATOMIC) return; Cache<Integer, TestValue> c = grid(0).cache(null); checkLockUnlock(primaryKey(c)); checkLockUnlock(backupKey(c)); checkLockUnlock(nearKey(c)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static double heapSize(Iterable<ClusterNode> nodes, int precision) { double heap = 0.0; for (ClusterNode n : nodesPerJvm(nodes)) { XXX m = n.metrics(); heap += Math.max(m.getHeapMemoryInitialized(), m.getHeapMemoryMaximum()); } return roundedHeapSize(heap, precision); }', 'ground_truth': 'public static double heapSize(Iterable<ClusterNode> nodes, int precision) { double heap = 0.0; for (ClusterNode n : nodesPerJvm(nodes)) { ClusterNodeMetrics m = n.metrics(); heap += Math.max(m.getHeapMemoryInitialized(), m.getHeapMemoryMaximum()); } return roundedHeapSize(heap, precision); }', 'output': 'public static double heapSize(Iterable<ClusterNode> nodes, int precision) { double heap = 0.0; for (ClusterNode n : nodesPerJvm(nodes)) { Metrics m = n.metrics(); heap += Math.max(m.getHeapMemoryInitialized(), m.getHeapMemoryMaximum()); } return roundedHeapSize(heap, precision); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void run(CQLQueryExecutor executor) throws IOException { if (session.getColumnFamilyType() == ColumnFamilyType.Super) throw new RuntimeException("Super columns are not implemented for CQL"); if (cqlQuery == null) { StringBuilder query = new StringBuilder("SELECT FIRST ").append(session.getColumnsPerKey()) .append(" \'\'..\'\' FROM Standard1"); if (session.cqlVersion.startsWith("2")) query.append(" USING CONSISTENCY ").append(session.getConsistencyLevel().toString()); cqlQuery = query.append(" WHERE KEY > ?").toString(); } String key = String.format("%0" +  session.getTotalKeysLength() + "d", index); List<String> queryParams = Collections.singletonList(getUnQuotedCqlBlob(key, session.cqlVersion.startsWith("3"))); XXX context = session.latency.time(); boolean success = false; String exceptionMessage = null; for (int t = 0; t < session.getRetryTimes(); t++) { if (success) break; try { success = executor.execute(cqlQuery, queryParams); } catch (Exception e) { System.err.println(e); exceptionMessage = getExceptionMessage(e); success = false; } } if (!success) { error(String.format("Operation [%d] retried %d times - error executing range slice with offset %s %s%n", index, session.getRetryTimes(), key, (exceptionMessage == null) ? "" : "(" + exceptionMessage + ")")); } session.operations.getAndIncrement(); session.keys.getAndAdd(lastRowCount); context.stop(); }', 'ground_truth': 'protected void run(CQLQueryExecutor executor) throws IOException { if (session.getColumnFamilyType() == ColumnFamilyType.Super) throw new RuntimeException("Super columns are not implemented for CQL"); if (cqlQuery == null) { StringBuilder query = new StringBuilder("SELECT FIRST ").append(session.getColumnsPerKey()) .append(" \'\'..\'\' FROM Standard1"); if (session.cqlVersion.startsWith("2")) query.append(" USING CONSISTENCY ").append(session.getConsistencyLevel().toString()); cqlQuery = query.append(" WHERE KEY > ?").toString(); } String key = String.format("%0" +  session.getTotalKeysLength() + "d", index); List<String> queryParams = Collections.singletonList(getUnQuotedCqlBlob(key, session.cqlVersion.startsWith("3"))); Context context = session.latency.time(); boolean success = false; String exceptionMessage = null; for (int t = 0; t < session.getRetryTimes(); t++) { if (success) break; try { success = executor.execute(cqlQuery, queryParams); } catch (Exception e) { System.err.println(e); exceptionMessage = getExceptionMessage(e); success = false; } } if (!success) { error(String.format("Operation [%d] retried %d times - error executing range slice with offset %s %s%n", index, session.getRetryTimes(), key, (exceptionMessage == null) ? "" : "(" + exceptionMessage + ")")); } session.operations.getAndIncrement(); session.keys.getAndAdd(lastRowCount); context.stop(); }', 'output': 'protected void run(CQLQueryExecutor executor) throws IOException { if (session.getColumnFamilyType() == ColumnFamilyType.Super) throw new RuntimeException("Super columns are not implemented for CQL"); if (cqlQuery == null) { StringBuilder query = new StringBuilder("SELECT FIRST ").append(session.getColumnsPerKey()) .append(" \'\'..\'\' FROM Standard1"); if (session.cqlVersion.startsWith("2")) query.append(" USING CONSISTENCY ").append(session.getConsistencyLevel().toString()); cqlQuery = query.append(" WHERE KEY > ?").toString(); } String key = String.format("%0" +  session.getTotalKeysLength() + "d", index); List<String> queryParams = Collections.singletonList(getUnQuotedCqlBlob(key, session.cqlVersion.startsWith("3"))); TimeContext context = session.latency.time(); boolean success = false; String exceptionMessage = null; for (int t = 0; t < session.getRetryTimes(); t++) { if (success) break; try { success = executor.execute(cqlQuery, queryParams); } catch (Exception e) { System.err.println(e); exceptionMessage = getExceptionMessage(e); success = false; } } if (!success) { error(String.format("Operation [%d] retried %d times - error executing range slice with offset %s %s%n", index, session.getRetryTimes(), key, (exceptionMessage == null) ? "" : "(" + exceptionMessage + ")")); } session.operations.getAndIncrement(); session.keys.getAndAdd(lastRowCount); context.stop(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testReadXMLWithMultiByteElementName() throws IOException { File file = tempFolder.newFile("trainXMLTiny"); Files.write(file.toPath(), xmlWithMultiByteElementName.getBytes(StandardCharsets.UTF_8)); XXX source = XMLSource.<Train>from(file.toPath().toString()) .withRootElement("") .withRecordElement("") .withRecordClass(Train.class) .withMinBundleSize(1024); List<Train> expectedResults = ImmutableList.of( new Train("Thomas", Train.TRAIN_NUMBER_UNDEFINED, null, null), new Train("Henry", Train.TRAIN_NUMBER_UNDEFINED, null, null), new Train("James", Train.TRAIN_NUMBER_UNDEFINED, null, null)); assertThat( trainsToStrings(expectedResults), containsInAnyOrder( trainsToStrings(readEverythingFromReader(source.createReader(null, null))).toArray())); }', 'ground_truth': 'public void testReadXMLWithMultiByteElementName() throws IOException { File file = tempFolder.newFile("trainXMLTiny"); Files.write(file.toPath(), xmlWithMultiByteElementName.getBytes(StandardCharsets.UTF_8)); XmlSource<Train> source = XmlSource.<Train>from(file.toPath().toString()) .withRootElement("") .withRecordElement("") .withRecordClass(Train.class) .withMinBundleSize(1024); List<Train> expectedResults = ImmutableList.of( new Train("Thomas", Train.TRAIN_NUMBER_UNDEFINED, null, null), new Train("Henry", Train.TRAIN_NUMBER_UNDEFINED, null, null), new Train("James", Train.TRAIN_NUMBER_UNDEFINED, null, null)); assertThat( trainsToStrings(expectedResults), containsInAnyOrder( trainsToStrings(readEverythingFromReader(source.createReader(null, null))).toArray())); }', 'output': 'public void testReadXMLWithMultiByteElementName() throws IOException { File file = tempFolder.newFile("trainXMLTiny"); Files.write(file.toPath(), xmlWithMultiByteElementName.getBytes(StandardCharsets.UTF_8)); Source<Train> source = XMLSource.<Train>from(file.toPath().toString()) .withRootElement("") .withRecordElement("") .withRecordClass(Train.class) .withMinBundleSize(1024); List<Train> expectedResults = ImmutableList.of( new Train("Thomas", Train.TRAIN_NUMBER_UNDEFINED, null, null), new Train("Henry", Train.TRAIN_NUMBER_UNDEFINED, null, null), new Train("James", Train.TRAIN_NUMBER_UNDEFINED, null, null)); assertThat( trainsToStrings(expectedResults), containsInAnyOrder( trainsToStrings(readEverythingFromReader(source.createReader(null, null))).toArray())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void removePath (int[] indices) { java.util.XXX data = getData(); for (int i=indices.length-1; i>=0; i--) { data.remove(indices[i]); } updatePlatform (); fireIntervalRemoved(this,indices[0],indices[indices.length-1]); }', 'ground_truth': 'private void removePath (int[] indices) { List<URL> data = getData(); for (int i=indices.length-1; i>=0; i--) { data.remove(indices[i]); } updatePlatform (); fireIntervalRemoved(this,indices[0],indices[indices.length-1]); }', 'output': 'private void removePath (int[] indices) { List<Integer> indicesList = new ArrayList<>(); for (int i=indices.length-1; i>=0; i--) { indicesList.add(indices[i]); } removePath(indicesList); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testUpdate() { int sampleSize = 1000; Map<double[], Double> sample = new HashMap<>(); for (int i = 0; i < sampleSize; i++) { double x1 = i; double x2 = x1 / 10.0; double x3 = x2 / 10.0; double x4 = x3 / 10.0; sample.put(new double[] {x1, x2, x3, x4}, (double)(i % 2)); } ArrayList<FeatureMeta> meta = new ArrayList<>(); for (int i = 0; i < 4; i++) meta.add(new FeatureMeta("", i, false)); DatasetTrainer<ModelsComposition, Double> trainer = new RandomForestClassifierTrainer(meta) .withAmountOfTrees(100) .withFeaturesCountSelectionStrgy(x -> 2) .withEnvironmentBuilder(TestUtils.testEnvBuilder()); ModelsComposition originalMdl = trainer.fit(sample, parts, (k, v) -> VectorUtils.of(k), (k, v) -> v); ModelsComposition updatedOnSameDS = trainer.update(originalMdl, sample, parts, (k, v) -> VectorUtils.of(k), (k, v) -> v); ModelsComposition updatedOnEmptyDS = trainer.update(originalMdl, new HashMap<double[], Double>(), parts, (k, v) -> VectorUtils.of(k), (k, v) -> v); Vector v = VectorUtils.of(5, 0.5, 0.05, 0.005); assertEquals(originalMdl.predict(v), updatedOnSameDS.predict(v), 0.01); assertEquals(originalMdl.predict(v), updatedOnEmptyDS.predict(v), 0.01); }', 'ground_truth': 'public void testUpdate() { int sampleSize = 1000; Map<Integer, LabeledVector<Double>> sample = new HashMap<>(); for (int i = 0; i < sampleSize; i++) { double x1 = i; double x2 = x1 / 10.0; double x3 = x2 / 10.0; double x4 = x3 / 10.0; sample.put(i, VectorUtils.of(x1, x2, x3, x4).labeled((double) i % 2)); } ArrayList<FeatureMeta> meta = new ArrayList<>(); for (int i = 0; i < 4; i++) meta.add(new FeatureMeta("", i, false)); DatasetTrainer<ModelsComposition, Double> trainer = new RandomForestClassifierTrainer(meta) .withAmountOfTrees(100) .withFeaturesCountSelectionStrgy(x -> 2) .withEnvironmentBuilder(TestUtils.testEnvBuilder()); ModelsComposition originalMdl = trainer.fit(sample, parts, new LabeledDummyVectorizer<>()); ModelsComposition updatedOnSameDS = trainer.update(originalMdl, sample, parts, new LabeledDummyVectorizer<>()); ModelsComposition updatedOnEmptyDS = trainer.update(originalMdl, new HashMap<double[], Double>(), parts, FeatureLabelExtractorWrapper.wrap((k, v) -> VectorUtils.of(k), (k, v) -> v)); Vector v = VectorUtils.of(5, 0.5, 0.05, 0.005); assertEquals(originalMdl.predict(v), updatedOnSameDS.predict(v), 0.01); assertEquals(originalMdl.predict(v), updatedOnEmptyDS.predict(v), 0.01); }', 'output': 'public void testUpdate() { int sampleSize = 1000; Map<double[], Double> sample = new HashMap<>(); for (int i = 0; i < sampleSize; i++) { double x1 = i; double x2 = x1 / 10.0; double x3 = x2 / 10.0; double x4 = x3 / 10.0; sample.put(new double[] {x1, x2, x3, x4}, (double)(i % 2)); } ArrayList<FeatureMeta> meta = new ArrayList<>(); for (int i = 0; i < 4; i++) meta.add(new FeatureMeta("", i, false)); DatasetTrainer<ModelsComposition, Double> trainer = new RandomForestClassifierTrainer(meta) .withAmountOfTrees(100) .withFeaturesCountSelectionStrgy(x -> 2) .withEnvironmentBuilder(TestUtils.testEnvBuilder()); ModelsComposition originalMdl = trainer.fit(sample, parts, (k, v) -> VectorUtils.of(k), (k, v) -> v); ModelsComposition updatedOnSameDS = trainer.update(originalMdl, sample, parts, (k, v) -> VectorUtils.of(k), (k, v) -> v); ModelsComposition updatedOnEmptyDS = trainer.update(originalMdl, new HashMap<double[], Double>(), parts, (k, v) -> VectorUtils.of(k), (k, v) -> v); Vector v = VectorUtils.of(5, 0.5, 0.05, 0.005); assertEquals(originalMdl.predict(v), updatedOnSameDS.predict(v), 0.01); assertEquals(originalMdl.predict(v), updatedOnEmptyDS.predict(v), 0.01); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAtomicStamped() throws Exception { final String stampedName = UUID.randomUUID().toString(); final String val = UUID.randomUUID().toString(); final String stamp = UUID.randomUUID().toString(); final String newVal = UUID.randomUUID().toString(); final String newStamp = UUID.randomUUID().toString(); CacheAtomicStamped<String, String> stamped = grid(0).cache(null).dataStructures() .atomicStamped(stampedName, val, stamp, true); final Ignite ignite = grid(0); ignite.compute().call(new IgniteCallable<String>() { @Override public String call() throws IgniteCheckedException { CacheAtomicStamped<String, String> stamped = ignite.cache(null).dataStructures() .atomicStamped(stampedName, val, stamp, true); assertEquals(val, stamped.value()); assertEquals(stamp, stamped.stamp()); return stamped.value(); } }); stamped.compareAndSet("WRONG EXPECTED VALUE", newVal, "WRONG EXPECTED STAMP", newStamp); ignite.compute().call(new IgniteCallable<String>() { @Override public String call() throws IgniteCheckedException { CacheAtomicStamped<String, String> stamped = ignite.cache(null).dataStructures() .atomicStamped(stampedName, val, stamp, true); assertEquals(val, stamped.value()); assertEquals(stamp, stamped.stamp()); return stamped.value(); } }); stamped.compareAndSet(val, newVal, stamp, newStamp); ignite.compute().call(new IgniteCallable<String>() { @Override public String call() throws IgniteCheckedException { CacheAtomicStamped<String, String> stamped = ignite.cache(null).dataStructures() .atomicStamped(stampedName, val, stamp, true); assertEquals(newVal, stamped.value()); assertEquals(newStamp, stamped.stamp()); return stamped.value(); } }); }', 'ground_truth': 'public void testAtomicStamped() throws Exception { final String stampedName = UUID.randomUUID().toString(); final String val = UUID.randomUUID().toString(); final String stamp = UUID.randomUUID().toString(); final String newVal = UUID.randomUUID().toString(); final String newStamp = UUID.randomUUID().toString(); IgniteAtomicStamped<String, String> stamped = grid(0).cache(null).dataStructures() .atomicStamped(stampedName, val, stamp, true); final Ignite ignite = grid(0); ignite.compute().call(new IgniteCallable<String>() { @Override public String call() throws IgniteCheckedException { IgniteAtomicStamped<String, String> stamped = ignite.cache(null).dataStructures() .atomicStamped(stampedName, val, stamp, true); assertEquals(val, stamped.value()); assertEquals(stamp, stamped.stamp()); return stamped.value(); } }); stamped.compareAndSet("WRONG EXPECTED VALUE", newVal, "WRONG EXPECTED STAMP", newStamp); ignite.compute().call(new IgniteCallable<String>() { @Override public String call() throws IgniteCheckedException { IgniteAtomicStamped<String, String> stamped = ignite.cache(null).dataStructures() .atomicStamped(stampedName, val, stamp, true); assertEquals(val, stamped.value()); assertEquals(stamp, stamped.stamp()); return stamped.value(); } }); stamped.compareAndSet(val, newVal, stamp, newStamp); ignite.compute().call(new IgniteCallable<String>() { @Override public String call() throws IgniteCheckedException { IgniteAtomicStamped<String, String> stamped = ignite.cache(null).dataStructures() .atomicStamped(stampedName, val, stamp, true); assertEquals(newVal, stamped.value()); assertEquals(newStamp, stamped.stamp()); return stamped.value(); } }); }', 'output': 'public void testAtomicStamped() throws Exception { final String stampedName = UUID.randomUUID().toString(); final String val = UUID.randomUUID().toString(); final String stamp = UUID.randomUUID().toString(); final String newVal = UUID.randomUUID().toString(); final String newStamp = UUID.randomUUID().toString(); CacheAtomicStamped<String, String> stamped = grid(0).cache(null).dataStructures() .atomicStamped(stampedName, val, stamp, true); final Ignite ignite = grid(0); ignite.compute().call(new IgniteCallable<String>() { @Override public String call() throws IgniteCheckedException { CacheAtomicStamped<String, String> stamped = ignite.cache(null).dataStructures() .atomicStamped(stampedName, val, stamp, true); assertEquals(val, stamped.value()); assertEquals(stamp, stamped.stamp()); return stamped.value(); } }); stamped.compareAndSet("WRONG EXPECTED VALUE", newVal, "WRONG EXPECTED STAMP", newStamp); ignite.compute().call(new IgniteCallable<String>() { @Override public String call() throws IgniteCheckedException { CacheAtomicStamped<String, String> stamped = ignite.cache(null).dataStructures() .atomicStamped(stampedName, val, stamp, true); assertEquals(newVal, stamped.value()); assertEquals(newStamp, stamped.stamp()); return stamped.value(); } }); stamped.compareAndSet(val, newVal, stamp, newStamp); ignite.compute().call(new IgniteCallable<String>() { @Override public String call() throws IgniteCheckedException { CacheAtomicStamped<String, String> stamped = ignite.cache(null).dataStructures() .atomicStamped(stampedName, val, stamp, true); assertEquals(newVal, stamped.value()); assertEquals(newStamp, stamped.stamp()); return stamped.value(); } }); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void constructNamespaceDecl() { StringBuilder buf = new StringBuilder(); if (explicitDefaultNSPrefix) { buf.append(" xmlns=\\""); buf.append(defaultNS); buf.append("\\""); } for (XXX it = namespacesToPrefixLookup.keySet().iterator(); it.hasNext(); ) { String nsURI = (String) it.next(); if (nsURI == null || nsURI.length() == 0) { continue; } String prefix = (String) namespacesToPrefixLookup.get(nsURI); if (prefix == null) { for (int i = 0; i < 26; i++) { char[] cc = new char[1]; cc[0] = (char) (\'a\' + i); prefix = new String(cc); if (env.getNamespaceForPrefix(prefix) == null) { break; } prefix = null; } if (prefix == null) { throw new RuntimeException("This will almost never happen!"); } namespacesToPrefixLookup.put(nsURI, prefix); } buf.append(" xmlns"); if (prefix.length() > 0) { buf.append(":"); buf.append(prefix); } buf.append("=\\""); buf.append(nsURI); buf.append("\\""); } this.namespaceDecl = buf.toString(); }', 'ground_truth': 'private void constructNamespaceDecl() { StringBuilder buf = new StringBuilder(); if (explicitDefaultNSPrefix) { buf.append(" xmlns=\\""); buf.append(defaultNS); buf.append("\\""); } for (Iterator<String> it = namespacesToPrefixLookup.keySet().iterator(); it.hasNext(); ) { String nsURI = it.next(); if (nsURI == null || nsURI.length() == 0) { continue; } String prefix = namespacesToPrefixLookup.get(nsURI); if (prefix == null) { throw new BugException("No xmlns prefix was associated to URI: " + nsURI); } buf.append(" xmlns"); if (prefix.length() > 0) { buf.append(":"); buf.append(prefix); } buf.append("=\\""); buf.append(nsURI); buf.append("\\""); } this.namespaceDecl = buf.toString(); }', 'output': 'private void constructNamespaceDecl() { StringBuilder buf = new StringBuilder(); if (explicitDefaultNSPrefix) { buf.append(" xmlns=\\""); buf.append(defaultNS); buf.append("\\""); } for (Map.Entry<String, String> it = namespacesToPrefixLookup.entrySet().iterator(); it.hasNext(); ) { String nsURI = it.getKey(); if (nsURI == null || nsURI.length() == 0) { continue; } String prefix = it.getValue(); if (prefix == null) { for (int i = 0; i < 26; i++) { char[] cc = new char[1]; cc[0] = (char) (\'a\' + i); prefix = new String(cc); if (env.getNamespaceForPrefix(prefix) == null) { break; } prefix = null; } if (prefix == null) { throw new RuntimeException("This will almost never happen!"); } namespacesToPrefixLookup.put(nsURI, prefix); } buf.append(" xmlns"); if (prefix.length() > 0) { buf.append(":"); buf.append(prefix); } buf.append("=\\""); buf.append(nsURI); buf.append("\\""); } this.namespaceDecl = buf.toString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void validateColumn(String keyspace, String columnFamily, ByteBuffer name, ByteBuffer value) throws InvalidRequestException { validateColumnName(keyspace, columnFamily, name); XXX validator = DatabaseDescriptor.getValueValidator(keyspace, columnFamily, name); try { if (validator != null) validator.validate(value); } catch (MarshalException me) { throw new InvalidRequestException(String.format("Invalid column value for column (name=%s); %s", ByteBufferUtil.bytesToHex(name), me.getMessage())); } }', 'ground_truth': 'private static void validateColumn(String keyspace, String columnFamily, ByteBuffer name, ByteBuffer value) throws InvalidRequestException { validateColumnName(keyspace, columnFamily, name); AbstractType<?> validator = DatabaseDescriptor.getValueValidator(keyspace, columnFamily, name); try { if (validator != null) validator.validate(value); } catch (MarshalException me) { throw new InvalidRequestException(String.format("Invalid column value for column (name=%s); %s", ByteBufferUtil.bytesToHex(name), me.getMessage())); } }', 'output': 'private static void validateColumn(String keyspace, String columnFamily, ByteBuffer name, ByteBuffer value) throws InvalidRequestException { validateColumnName(keyspace, columnFamily, name); ValueValidator validator = DatabaseDescriptor.getValueValidator(keyspace, columnFamily, name); try { if (validator != null) validator.validate(value); } catch (MarshalException me) { throw new InvalidRequestException(String.format("Invalid column value for column (name=%s); %s", ByteBufferUtil.bytesToHex(name), me.getMessage())); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMetricsSize() { byte[] data = new byte[GridDiscoveryMetricsHelper.METRICS_SIZE]; int off = GridDiscoveryMetricsHelper.serialize(data, 0, createMetrics()); assert off == GridDiscoveryMetricsHelper.METRICS_SIZE; XXX res = GridDiscoveryMetricsHelper.deserialize(data, 0); assert res != null; }', 'ground_truth': 'public void testMetricsSize() { byte[] data = new byte[GridDiscoveryMetricsHelper.METRICS_SIZE]; int off = GridDiscoveryMetricsHelper.serialize(data, 0, createMetrics()); assert off == GridDiscoveryMetricsHelper.METRICS_SIZE; ClusterNodeMetrics res = GridDiscoveryMetricsHelper.deserialize(data, 0); assert res != null; }', 'output': 'public void testMetricsSize() { byte[] data = new byte[GridDiscoveryMetricsHelper.METRICS_SIZE]; int off = GridDiscoveryMetricsHelper.serialize(data, 0, createMetrics()); assert off == GridDiscoveryMetricsHelper.METRICS_SIZE; GridDiscoveryMetrics res = GridDiscoveryMetricsHelper.deserialize(data, 0); assert res != null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static RowMark create(TableMetadata metadata, Row row, ProtocolVersion protocolVersion) { ByteBuffer mark; if (protocolVersion.isSmallerOrEqualTo(ProtocolVersion.V3)) { Iterator<Cell> cells = row.cellsInLegacyOrder(metadata, true).iterator(); if (!cells.hasNext()) { mark = encodeCellName(metadata, row.clustering(), EMPTY_BYTE_BUFFER, null); } else { XXX cell = cells.next(); mark = encodeCellName(metadata, row.clustering(), cell.column().name.bytes, cell.column().isComplex() ? cell.path().get(0) : null); } } else { mark = Clustering.serializer.serialize(row.clustering(), MessagingService.VERSION_30, makeClusteringTypes(metadata)); } return new RowMark(mark, protocolVersion); }', 'ground_truth': 'public static RowMark create(TableMetadata metadata, Row row, ProtocolVersion protocolVersion) { ByteBuffer mark; if (protocolVersion.isSmallerOrEqualTo(ProtocolVersion.V3)) { Iterator<Cell<?>> cells = row.cellsInLegacyOrder(metadata, true).iterator(); if (!cells.hasNext()) { mark = encodeCellName(metadata, row.clustering(), EMPTY_BYTE_BUFFER, null); } else { Cell<?> cell = cells.next(); mark = encodeCellName(metadata, row.clustering(), cell.column().name.bytes, cell.column().isComplex() ? cell.path().get(0) : null); } } else { mark = Clustering.serializer.serialize(row.clustering(), MessagingService.VERSION_30, makeClusteringTypes(metadata)); } return new RowMark(mark, protocolVersion); }', 'output': 'public static RowMark create(TableMetadata metadata, Row row, ProtocolVersion protocolVersion) { ByteBuffer mark; if (protocolVersion.isSmallerOrEqualTo(ProtocolVersion.V3)) { Iterator<Cell> cells = row.cellsInLegacyOrder(metadata, true).iterator(); if (!cells.hasNext()) { mark = encodeCellName(metadata, row.clustering(), EMPTY_BYTE_BUFFER, null); } else { Cell cell = cells.next(); mark = encodeCellName(metadata, row.clustering(), cell.column().name.bytes, cell.column().isComplex() ? cell.path().get(0) : null); } } else { mark = Clustering.serializer.serialize(row.clustering(), MessagingService.VERSION_30, makeClusteringTypes(metadata)); } return new RowMark(mark, protocolVersion); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String getAsText() { try { XXX fileobjs = (List) this.getValue(); StringBuffer result = new StringBuffer (); boolean first = true; for (Iterator<FileObject> it = fileobjs.iterator(); it.hasNext();) { FileObject fo = it.next(); File f = FileUtil.toFile(fo); if (f != null) { if (!first) { result.append (File.pathSeparator); } else { first = false; } result.append(f.getAbsolutePath()); } } return result.toString (); } catch (Exception e) { e.printStackTrace(); return ""; } }', 'ground_truth': 'public String getAsText() { try { List<FileObject> fileobjs = (List<FileObject>)this.getValue(); StringBuffer result = new StringBuffer (); boolean first = true; for (Iterator<FileObject> it = fileobjs.iterator(); it.hasNext();) { FileObject fo = it.next(); File f = FileUtil.toFile(fo); if (f != null) { if (!first) { result.append (File.pathSeparator); } else { first = false; } result.append(f.getAbsolutePath()); } } return result.toString (); } catch (Exception e) { e.printStackTrace(); return ""; } }', 'output': 'public String getAsText() { try { List<? extends FileObject> fileobjs = (List<? extends FileObject>) this.getValue(); StringBuffer result = new StringBuffer (); boolean first = true; for (Iterator<? extends FileObject> it = fileobjs.iterator(); it.hasNext();) { FileObject fo = it.next(); File f = FileUtil.toFile(fo); if (f != null) { if (!first) { result.append (File.pathSeparator); } else { first = false; } result.append(f.getAbsolutePath()); } } return result.toString (); } catch (Exception e) { e.printStackTrace(); return ""; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected AbstractCompactedRow getReduced() { assert !rows.isEmpty(); try { AbstractCompactedRow compactedRow = controller.getCompactedRow(new ArrayList<SSTableIdentityIterator>(rows)); if (compactedRow.isEmpty()) { controller.invalidateCachedRow(compactedRow.key); return null; } else { controller.invalidateCachedRow(compactedRow.key); } return compactedRow; } finally { rows.clear(); if ((row++ % 1000) == 0) { long n = 0; for (XXX scanner : scanners) n += scanner.getFilePointer(); bytesRead = n; controller.mayThrottle(bytesRead); } } }', 'ground_truth': 'protected AbstractCompactedRow getReduced() { assert !rows.isEmpty(); try { AbstractCompactedRow compactedRow = controller.getCompactedRow(new ArrayList<SSTableIdentityIterator>(rows)); if (compactedRow.isEmpty()) { controller.invalidateCachedRow(compactedRow.key); return null; } else { controller.invalidateCachedRow(compactedRow.key); } return compactedRow; } finally { rows.clear(); if ((row++ % 1000) == 0) { long n = 0; for (ICompactionScanner scanner : scanners) n += scanner.getCurrentPosition(); bytesRead = n; controller.mayThrottle(bytesRead); } } }', 'output': 'protected AbstractCompactedRow getReduced() { assert !rows.isEmpty(); try { AbstractCompactedRow compactedRow = controller.getCompactedRow(new ArrayList<SSTableIdentityIterator>(rows)); if (compactedRow.isEmpty()) { controller.invalidateCachedRow(compactedRow.key); return null; } else { controller.invalidateCachedRow(compactedRow.key); } return compactedRow; } finally { rows.clear(); if ((row++ % 1000) == 0) { long n = 0; for (AbstractScanner scanner : scanners) n += scanner.getFilePointer(); bytesRead = n; controller.mayThrottle(bytesRead); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Class getInsecureClass() { XXX ctx = getClassContext(); boolean firstACClass = false; LOOP:   for (int i = 0; i < ctx.length; i++) { if (ctx[i] == accessControllerClass) { if (firstACClass) { return null; } else { firstACClass = true; continue LOOP; } } else if (ctx[i].getClassLoader() != null) { if (isSecureClass(ctx[i])) { if (classLoaderClass.isAssignableFrom(ctx[i])) { return null; } else { continue LOOP; } } return ctx[i]; } else if (classLoaderClass.isAssignableFrom(ctx[i])) { return null; } } return null; }', 'ground_truth': 'private Class getInsecureClass() { Class<?>[] ctx = getClassContext(); boolean firstACClass = false; LOOP:   for (int i = 0; i < ctx.length; i++) { if (ctx[i] == accessControllerClass) { if (firstACClass) { return null; } else { firstACClass = true; continue LOOP; } } else if (ctx[i].getClassLoader() != null) { if (isSecureClass(ctx[i])) { if (classLoaderClass.isAssignableFrom(ctx[i])) { return null; } else { continue LOOP; } } return ctx[i]; } else if (classLoaderClass.isAssignableFrom(ctx[i])) { return null; } } return null; }', 'output': 'private Class getInsecureClass() { SecurityContext ctx = getSecurityContext(); boolean firstACClass = false; LOOP:   for (int i = 0; i < ctx.length; i++) { if (ctx[i] == accessControllerClass) { if (firstACClass) { return null; } else { firstACClass = true; continue LOOP; } } else if (ctx[i].getClassLoader() != null) { if (isSecureClass(ctx[i])) { if (classLoaderClass.isAssignableFrom(ctx[i])) { return null; } else { continue LOOP; } } return ctx[i]; } else if (classLoaderClass.isAssignableFrom(ctx[i])) { return null; } } return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testUtf8Handling() throws Exception { File tmpFile = tmpFolder.newFile(); try (FileOutputStream output = new FileOutputStream(tmpFile)) { output.write( new byte[] {(byte) 0xE2, (byte) 0x82, (byte) 0xAC, \'\\n\', (byte) 0xC2, (byte) 0xA2, \'\\n\'}); } { TextReader<String> textReader = new TextReader<>(tmpFile.getPath(), true, 0L, 3L, StringUtf8Coder.of(), TextIO.CompressionType.UNCOMPRESSED); ExecutorTestUtils.TestReaderObserver observer = new ExecutorTestUtils.TestReaderObserver(textReader); try (XXX iterator = textReader.iterator()) { assertArrayEquals("".getBytes("UTF-8"), iterator.next().getBytes("UTF-8")); assertFalse(iterator.hasNext()); assertEquals(Arrays.asList(4), observer.getActualSizes()); } } { TextReader<String> textReader = new TextReader<>(tmpFile.getPath(), true, 2L, null, StringUtf8Coder.of(), TextIO.CompressionType.UNCOMPRESSED); ExecutorTestUtils.TestReaderObserver observer = new ExecutorTestUtils.TestReaderObserver(textReader); try (XXX iterator = textReader.iterator()) { assertArrayEquals("".getBytes("UTF-8"), iterator.next().getBytes("UTF-8")); assertFalse(iterator.hasNext()); assertEquals(Arrays.asList(3, 3), observer.getActualSizes()); } } }', 'ground_truth': 'public void testUtf8Handling() throws Exception { File tmpFile = tmpFolder.newFile(); try (FileOutputStream output = new FileOutputStream(tmpFile)) { output.write( new byte[] {(byte) 0xE2, (byte) 0x82, (byte) 0xAC, \'\\n\', (byte) 0xC2, (byte) 0xA2, \'\\n\'}); } { TextReader<String> textReader = new TextReader<>(tmpFile.getPath(), true, 0L, 3L, StringUtf8Coder.of(), TextIO.CompressionType.UNCOMPRESSED); ExecutorTestUtils.TestReaderObserver observer = new ExecutorTestUtils.TestReaderObserver(textReader); try (LegacyReaderIterator<String> iterator = textReader.iterator()) { assertArrayEquals("".getBytes("UTF-8"), iterator.next().getBytes("UTF-8")); assertFalse(iterator.hasNext()); assertEquals(Arrays.asList(4), observer.getActualSizes()); } } { TextReader<String> textReader = new TextReader<>(tmpFile.getPath(), true, 2L, null, StringUtf8Coder.of(), TextIO.CompressionType.UNCOMPRESSED); ExecutorTestUtils.TestReaderObserver observer = new ExecutorTestUtils.TestReaderObserver(textReader); try (LegacyReaderIterator<String> iterator = textReader.iterator()) { assertArrayEquals("".getBytes("UTF-8"), iterator.next().getBytes("UTF-8")); assertFalse(iterator.hasNext()); assertEquals(Arrays.asList(3, 3), observer.getActualSizes()); } } }', 'output': 'public void testUtf8Handling() throws Exception { File tmpFile = tmpFolder.newFile(); try (FileOutputStream output = new FileOutputStream(tmpFile)) { output.write( new byte[] {(byte) 0xE2, (byte) 0x82, (byte) 0xAC, \'\\n\', (byte) 0xC2, (byte) 0xA2, \'\\n\'}); } { TextReader<String> textReader = new TextReader<>(tmpFile.getPath(), true, 0L, 3L, StringUtf8Coder.of(), TextIO.CompressionType.UNCOMPRESSED); ExecutorTestUtils.TestReaderObserver observer = new ExecutorTestUtils.TestReaderObserver(textReader); try (Iterator<String> iterator = textReader.iterator()) { assertArrayEquals("".getBytes("UTF-8"), iterator.next().getBytes("UTF-8")); assertFalse(iterator.hasNext()); assertEquals(Arrays.asList(4), observer.getActualSizes()); } } { TextReader<String> textReader = new TextReader<>(tmpFile.getPath(), true, 2L, null, StringUtf8Coder.of(), TextIO.CompressionType.UNCOMPRESSED); ExecutorTestUtils.TestReaderObserver observer = new ExecutorTestUtils.TestReaderObserver(textReader); try (Iterator<String> iterator = textReader.iterator()) { assertArrayEquals("".getBytes("UTF-8"), iterator.next().getBytes("UTF-8")); assertFalse(iterator.hasNext()); assertEquals(Arrays.asList(3, 3), observer.getActualSizes()); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testWrongRegistrarCoding() throws IOException { XXX registrar = (k) -> { /* No-op  */}; KryoCoder<ClassToBeEncoded> coder = KryoCoder.of(registrar); assertEncoding(coder); }', 'ground_truth': 'public void testWrongRegistrarCoding() throws IOException { IdentifiedRegistrar registrar = IdentifiedRegistrar.of((k) -> { /* No-op  */}); KryoCoder<ClassToBeEncoded> coder = KryoCoder.of(registrar); assertEncoding(coder); }', 'output': 'public void testWrongRegistrarCoding() throws IOException { Registrar<ClassToBeEncoded> registrar = (k) -> { /* No-op  */}; KryoCoder<ClassToBeEncoded> coder = KryoCoder.of(registrar); assertEncoding(coder); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void main(String[] args) throws Exception { PipelineOptions options = PipelineOptionsFactory.fromArgs(args).as(PipelineOptions.class); Pipeline p = Pipeline.create(options); List<String> fieldNames = Arrays.asList("c1", "c2", "c3"); List<Integer> fieldTypes = Arrays.asList(Types.INTEGER, Types.VARCHAR, Types.DOUBLE); BeamSqlRowType type = BeamSqlRowType.create(fieldNames, fieldTypes); BeamSqlRow row = new BeamSqlRow(type); row.addField(0, 1); row.addField(1, "row"); row.addField(2, 1.0); PCollection<BeamSqlRow> inputTable = PBegin.in(p).apply(Create.of(row) .withCoder(new BeamSqlRowCoder(type))); PCollection<BeamSqlRow> outputStream = inputTable.apply( BeamSql.simpleQuery("select c1, c2, c3 from PCOLLECTION where c1=1")); outputStream.apply("log_result", MapElements.<BeamSqlRow, Void>via(new SimpleFunction<BeamSqlRow, Void>() { public Void apply(BeamSqlRow input) { System.out.println("PCOLLECTION: " + input); return null; } })); XXX outputStream2 = PCollectionTuple.of(new TupleTag<BeamSqlRow>("CASE1_RESULT"), outputStream) .apply(BeamSql.query("select c2, c3 from CASE1_RESULT where c1=1")); outputStream2.apply("log_result", MapElements.<BeamSqlRow, Void>via(new SimpleFunction<BeamSqlRow, Void>() { @Override public Void apply(BeamSqlRow input) { System.out.println("TABLE_B: " + input); return null; } })); p.run().waitUntilFinish(); }', 'ground_truth': 'public static void main(String[] args) throws Exception { PipelineOptions options = PipelineOptionsFactory.fromArgs(args).as(PipelineOptions.class); Pipeline p = Pipeline.create(options); List<String> fieldNames = Arrays.asList("c1", "c2", "c3"); List<Integer> fieldTypes = Arrays.asList(Types.INTEGER, Types.VARCHAR, Types.DOUBLE); BeamSqlRecordType type = BeamSqlRecordType.create(fieldNames, fieldTypes); BeamRecord row = new BeamRecord(type); row.addField(0, 1); row.addField(1, "row"); row.addField(2, 1.0); PCollection<BeamRecord> inputTable = PBegin.in(p).apply(Create.of(row) .withCoder(type.getRecordCoder())); PCollection<BeamRecord> outputStream = inputTable.apply( BeamSql.simpleQuery("select c1, c2, c3 from PCOLLECTION where c1=1")); outputStream.apply("log_result", MapElements.<BeamRecord, Void>via(new SimpleFunction<BeamRecord, Void>() { public Void apply(BeamRecord input) { System.out.println("PCOLLECTION: " + input); return null; } })); PCollection<BeamRecord> outputStream2 = PCollectionTuple.of(new TupleTag<BeamRecord>("CASE1_RESULT"), outputStream) .apply(BeamSql.query("select c2, c3 from CASE1_RESULT where c1=1")); outputStream2.apply("log_result", MapElements.<BeamRecord, Void>via(new SimpleFunction<BeamRecord, Void>() { @Override public Void apply(BeamRecord input) { System.out.println("TABLE_B: " + input); return null; } })); p.run().waitUntilFinish(); }', 'output': 'public static void main(String[] args) throws Exception { PipelineOptions options = PipelineOptionsFactory.fromArgs(args).as(PipelineOptions.class); Pipeline p = Pipeline.create(options); List<String> fieldNames = Arrays.asList("c1", "c2", "c3"); List<Integer> fieldTypes = Arrays.asList(Types.INTEGER, Types.VARCHAR, Types.DOUBLE); BeamSqlRowType type = BeamSqlRowType.create(fieldNames, fieldTypes); BeamSqlRow row = new BeamSqlRow(type); row.addField(0, 1); row.addField(1, "row"); row.addField(2, 1.0); PCollection<BeamSqlRow> inputTable = PBegin.in(p).apply(Create.of(row) .withCoder(new BeamSqlRowCoder(type))); PCollection<BeamSqlRow> outputStream = inputTable.apply( BeamSql.simpleQuery("select c1, c2, c3 from PCOLLECTION where c1=1")); outputStream.apply("log_result", MapElements.<BeamSqlRow, Void>via(new SimpleFunction<BeamSqlRow, Void>() { public Void apply(BeamSqlRow input) { System.out.println("PCOLLECTION: " + input); return null; } })); PCollection<BeamSqlRow> outputStream2 = PCollectionTuple.of(new TupleTag<BeamSqlRow>("CASE1_RESULT"), outputStream) .apply(BeamSql.query("select c2, c3 from CASE1_RESULT where c1=1")); outputStream2.apply("log_result", MapElements.<BeamSqlRow, Void>via(new SimpleFunction<BeamSqlRow, Void>() { @Override public Void apply(BeamSqlRow input) { System.out.println("TABLE_B: " + input); return null; } })); p.run().waitUntilFinish(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static boolean isIdempotentOrCachable(ActionModel actionModel) { final XXX objectAction = actionModel.getActionMemento() .getAction(actionModel.getSpecificationLoader()); return ObjectAction.Util.isIdempotentOrCachable(objectAction); }', 'ground_truth': 'private static boolean isIdempotentOrCachable(ActionModel actionModel) { val objectAction = actionModel.getAction(); return ObjectAction.Util.isIdempotentOrCachable(objectAction); }', 'output': 'private static boolean isIdempotentOrCachable(ActionModel actionModel) { final ObjectAction objectAction = actionModel.getActionMemento().getAction(actionModel.getSpecificationLoader()); return ObjectAction.Util.isIdempotentOrCachable(objectAction); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void cloneEntries() { if (F.isEmpty(writeEntries)) return; Collection<GridCacheTxEntry<K, V>> cp = new ArrayList<>(writeEntries.size()); for (GridCacheTxEntry<K, V> e : writeEntries) { GridCacheContext<K, V> cacheCtx = e.context(); if (cacheCtx.isNear()) cp.add(e.cleanCopy(cacheCtx.nearTx().dht().context())); else cp.add(e); } writeEntries = cp; }', 'ground_truth': 'public void cloneEntries() { if (F.isEmpty(writeEntries)) return; Collection<IgniteTxEntry<K, V>> cp = new ArrayList<>(writeEntries.size()); for (IgniteTxEntry<K, V> e : writeEntries) { GridCacheContext<K, V> cacheCtx = e.context(); if (cacheCtx.isNear()) cp.add(e.cleanCopy(cacheCtx.nearTx().dht().context())); else cp.add(e); } writeEntries = cp; }', 'output': 'public void cloneEntries() { if (F.isEmpty(writeEntries)) return; Collection<CacheTxEntry<K, V>> cp = new ArrayList<>(writeEntries.size()); for (CacheTxEntry<K, V> e : writeEntries) { CacheContext<K, V> cacheCtx = e.context(); if (cacheCtx.isNear()) cp.add(e.cleanCopy(cacheCtx.nearTx().dht().context())); else cp.add(e); } writeEntries = cp; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testWithContextTeardownCalledAfterExceptionInFinishBundle() { XXX fn = new ExceptionThrowingOldFn(MethodForException.FINISH_BUNDLE); p.apply(Create.of(1, 2, 3)).apply(ParDo.of(fn)); try { p.run(); fail("Pipeline should have failed with an exception"); } catch (Exception e) { assertThat( "Function should have been torn down after exception", ExceptionThrowingOldFn.teardownCalled.get(), is(true)); } }', 'ground_truth': 'public void testWithContextTeardownCalledAfterExceptionInFinishBundle() { ExceptionThrowingFn fn = new ExceptionThrowingFn(MethodForException.FINISH_BUNDLE); p.apply(Create.of(1, 2, 3)).apply(ParDo.of(fn)); try { p.run(); fail("Pipeline should have failed with an exception"); } catch (Exception e) { assertThat( "Function should have been torn down after exception", ExceptionThrowingFn.teardownCalled.get(), is(true)); } }', 'output': 'public void testWithContextTeardownCalledAfterExceptionInFinishBundle() { PCollection<Integer> p = Create.of(1, 2, 3).apply(ParDo.of(new ExceptionThrowingOldFn(MethodForException.FINISH_BUNDLE))); try { p.run(); fail("Pipeline should have failed with an exception"); } catch (Exception e) { assertThat( "Function should have been torn down after exception", ExceptionThrowingOldFn.teardownCalled.get(), is(true)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMissHitPercentage() throws Exception { GridCache<Integer, Integer> cache0 = grid(0).cache(null); int keyCnt = keyCount(); for (int i = 0; i < keyCnt; i++) { cache0.put(i, i); info("Writes: " + cache0.metrics().writes()); for (int j = 0; j < gridCount(); j++) { GridCache<Integer, Integer> cache = grid(j).cache(null); int cacheWrites = cache.metrics().writes(); assertEquals("Wrong cache metrics [i=" + i + ", grid=" + j + \']\', i + 1, cacheWrites); } assertEquals("Wrong value for key: " + i, Integer.valueOf(i), cache0.get(i)); } for (int i = 0; i < gridCount(); i++) { XXX m = grid(i).cache(null).metrics(); assertEquals(m.getCacheHits() * 100f / m.getCacheGets(), m.getCacheHitPercentage(), 0.1f); assertEquals(m.getCacheMisses() * 100f / m.getCacheGets(), m.getCacheMissPercentage(), 0.1f); } }', 'ground_truth': 'public void testMissHitPercentage() throws Exception { GridCache<Integer, Integer> cache0 = grid(0).cache(null); int keyCnt = keyCount(); for (int i = 0; i < keyCnt; i++) { cache0.put(i, i); info("Puts: " + cache0.metrics().getCachePuts()); for (int j = 0; j < gridCount(); j++) { GridCache<Integer, Integer> cache = grid(j).cache(null); long cacheWrites = cache.metrics().getCachePuts(); assertEquals("Wrong cache metrics [i=" + i + ", grid=" + j + \']\', i + 1, cacheWrites); } assertEquals("Wrong value for key: " + i, Integer.valueOf(i), cache0.get(i)); } for (int i = 0; i < gridCount(); i++) { CacheMetricsMxBean m = grid(i).cache(null).metrics(); assertEquals(m.getCacheHits() * 100f / m.getCacheGets(), m.getCacheHitPercentage(), 0.1f); assertEquals(m.getCacheMisses() * 100f / m.getCacheGets(), m.getCacheMissPercentage(), 0.1f); } }', 'output': 'public void testMissHitPercentage() throws Exception { Cache<Integer, Integer> cache0 = grid(0).cache(null); int keyCnt = keyCount(); for (int i = 0; i < keyCnt; i++) { cache0.put(i, i); info("Writes: " + cache0.metrics().writes()); for (int j = 0; j < gridCount(); j++) { Cache<Integer, Integer> cache = grid(j).cache(null); int cacheWrites = cache.metrics().writes(); assertEquals("Wrong cache metrics [i=" + i + ", grid=" + j + \']\', i + 1, cacheWrites); } assertEquals("Wrong value for key: " + i, Integer.valueOf(i), cache0.get(i)); } for (int i = 0; i < gridCount(); i++) { CacheMetrics m = grid(i).cache(null).metrics(); assertEquals(m.getCacheHits() * 100f / m.getCacheGets(), m.getCacheHitPercentage(), 0.1f); assertEquals(m.getCacheMisses() * 100f / m.getCacheGets(), m.getCacheMissPercentage(), 0.1f); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testEnabled() throws Exception { storeValBytes = true; Ignite g0 = startGrid(0); Ignite g1 = startGrid(1); GridCache<Integer, String> c = g0.cache(null); c.put(1, "Cached value"); GridCacheEntryEx<Object, Object> entry = ((GridKernal)g1).internalCache().peekEx(1); assert entry != null; assert entry.valueBytes() != null; }', 'ground_truth': 'public void testEnabled() throws Exception { storeValBytes = true; Ignite g0 = startGrid(0); Ignite g1 = startGrid(1); Cache<Integer, String> c = g0.cache(null); c.put(1, "Cached value"); GridCacheEntryEx<Object, Object> entry = ((GridKernal)g1).internalCache().peekEx(1); assert entry != null; assert entry.valueBytes() != null; }', 'output': 'public void testEnabled() throws Exception { storeValBytes = true; Ignite g0 = startGrid(0); Ignite g1 = startGrid(1); Cache<Integer, String> c = g0.cache(null); c.put(1, "Cached value"); CacheEntryEx<Object, Object> entry = ((GridKernal)g1).internalCache().peekEx(1); assert entry != null; assert entry.valueBytes() != null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static RelDataType deriveUncollectRowType(RelNode rel) { RelDataType inputType = rel.getRowType(); assert inputType.isStruct() : inputType + " is not a struct"; final RelDataTypeField [] fields = inputType.getFields(); assert 1 == fields.length : "expected 1 field"; RelDataType ret = fields[0].getType().getComponentType(); assert null != ret; if (!ret.isStruct()) { ret = rel.getCluster().getTypeFactory().createStructType( new RelDataType[] { ret }, new String[] { SqlUtil.deriveAliasFromOrdinal(0) }); } return ret; }', 'ground_truth': 'public static RelDataType deriveUncollectRowType(RelNode rel) { RelDataType inputType = rel.getRowType(); assert inputType.isStruct() : inputType + " is not a struct"; final List<RelDataTypeField> fields = inputType.getFieldList(); assert 1 == fields.size() : "expected 1 field"; RelDataType ret = fields.get(0).getType().getComponentType(); assert null != ret; if (!ret.isStruct()) { ret = rel.getCluster().getTypeFactory().createStructType( ImmutableList.of( Pair.of(SqlUtil.deriveAliasFromOrdinal(0), ret))); } return ret; }', 'output': 'public static RelDataType deriveUncollectRowType(RelNode rel, RelMetadataQuery mq) { RelDataType inputType = rel.getRowType(); assert inputType.isStruct() : inputType + " is not a struct"; final RelDataTypeField [] fields = inputType.getFields(); assert 1 == fields.length : "expected 1 field"; RelDataType ret = fields[0].getType().getComponentType(); assert null != ret; if (!ret.isStruct()) { ret = mq.getTypeFactory().createStructType( new RelDataType[] { ret }, new String[] { SqlUtil.deriveAliasFromOrdinal(0) }); } return ret; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void waitForLeveling(ColumnFamilyStore cfs) throws InterruptedException { XXX strategy = (LeveledCompactionStrategy) cfs.getCompactionStrategy(); while (strategy.getLevelSize(0) > 1) Thread.sleep(100); }', 'ground_truth': 'private void waitForLeveling(ColumnFamilyStore cfs) throws InterruptedException { WrappingCompactionStrategy strategy = (WrappingCompactionStrategy) cfs.getCompactionStrategy(); while (strategy.getSSTableCountPerLevel()[0] > 1) Thread.sleep(100); }', 'output': 'private void waitForLeveling(ColumnFamilyStore cfs) throws InterruptedException { LeveledCompactionStrategy strategy = (LeveledCompactionStrategy) cfs.getCompactionStrategy(); while (strategy.getLevelSize(0) > 1) Thread.sleep(100); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void notifyListeners(R res, Throwable err) { final Collection<IgniteInClosure<? super GridFuture<R>>> tmp; synchronized (mux) { tmp = new ArrayList<>(lsnrs); } final GridSchedulerFuture<R> snapshot = snapshot(res, err); if (concurNotify) { for (final XXX lsnr : tmp) ctx.closure().runLocalSafe(new GPR() { @Override public void run() { lsnr.apply(snapshot); } }, true); } else { ctx.closure().runLocalSafe(new GPR() { @Override public void run() { for (XXX lsnr : tmp) lsnr.apply(snapshot); } }, true); } }', 'ground_truth': 'private void notifyListeners(R res, Throwable err) { final Collection<IgniteInClosure<? super IgniteFuture<R>>> tmp; synchronized (mux) { tmp = new ArrayList<>(lsnrs); } final GridSchedulerFuture<R> snapshot = snapshot(res, err); if (concurNotify) { for (final IgniteInClosure<? super IgniteFuture<R>> lsnr : tmp) ctx.closure().runLocalSafe(new GPR() { @Override public void run() { lsnr.apply(snapshot); } }, true); } else { ctx.closure().runLocalSafe(new GPR() { @Override public void run() { for (IgniteInClosure<? super IgniteFuture<R>> lsnr : tmp) lsnr.apply(snapshot); } }, true); } }', 'output': 'private void notifyListeners(R res, Throwable err, boolean concurNotify) { final Collection<IgniteInClosure<? super GridFuture<R>>> tmp; synchronized (mux) { tmp = new ArrayList<>(lsnrs); } final GridSchedulerFuture<R> snapshot = snapshot(res, err); if (concurNotify) { for (final IgniteInClosure<? super GridFuture<R>> lsnr : tmp) ctx.closure().runLocalSafe(new GPR() { @Override public void run() { lsnr.apply(snapshot); } }, true); } else { ctx.closure().runLocalSafe(new GPR() { @Override public void run() { for (IgniteInClosure<? super GridFuture<R>> lsnr : tmp) lsnr.apply(snapshot); } }, true); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Clob downloadCommands( @Nullable final UUID interactionId, @Nullable final Integer batchSize, final String filenamePrefix) { final XXX commands = commandServiceRepository.findSince(interactionId, batchSize); if(commands == null) { messageService.informUser("No commands found"); } final CommandsDto commandsDto = contentMappingServiceForCommandsDto.map(commands); final String fileName = String.format( "%s_%s.xml", filenamePrefix, elseDefault(interactionId)); final String xml = jaxbService.toXml(commandsDto); return new Clob(fileName, "application/xml", xml); }', 'ground_truth': 'public Clob downloadCommands( @Nullable final UUID interactionId, @Nullable final Integer batchSize, final String filenamePrefix) { final List<? extends CommandModel> commands = commandModelRepository.findSince(interactionId, batchSize); if(commands == null) { messageService.informUser("No commands found"); } final CommandsDto commandsDto = contentMappingServiceForCommandsDto.map(commands); final String fileName = String.format( "%s_%s.xml", filenamePrefix, elseDefault(interactionId)); final String xml = jaxbService.toXml(commandsDto); return new Clob(fileName, "application/xml", xml); }', 'output': 'public Clob downloadCommands( @Nullable final UUID interactionId, @Nullable final Integer batchSize, final String filenamePrefix) { final List<Command> commands = commandServiceRepository.findSince(interactionId, batchSize); if(commands == null) { messageService.informUser("No commands found"); } final List<CommandDto> commandsDto = contentMappingServiceForCommandsDto.map(commands); final String fileName = String.format( "%s_%s.xml", filenamePrefix, elseDefault(interactionId)); final String xml = jaxbService.toXml(commandsDto); return new Clob(fileName, "application/xml", xml); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void testPutWithFilterTx(final Integer key, final IgniteTxConcurrency txConcurrency) throws Exception { final GridCache<Integer, Integer> cache = grid(0).cache(null); cache.put(key, 0); final int THREADS = 5; final int ITERATIONS_PER_THREAD = iterations(); GridTestUtils.runMultiThreaded(new Callable<Void>() { @Override public Void call() throws Exception { for (int i = 0; i < ITERATIONS_PER_THREAD && !failed; i++) { if (i % 500 == 0) log.info("Iteration " + i); try (IgniteTx tx = cache.txStart(txConcurrency, REPEATABLE_READ)) { cache.putx(key, i, new TestFilter()); tx.commit(); } } return null; } }, THREADS, "putWithFilter"); for (int i = 0; i < gridCount(); i++) { Integer val = (Integer)grid(i).cache(null).get(key); assertNotNull("Unexpected value for grid " + i, val); } assertFalse(failed); }', 'ground_truth': 'private void testPutWithFilterTx(final Integer key, final IgniteTxConcurrency txConcurrency) throws Exception { final Cache<Integer, Integer> cache = grid(0).cache(null); cache.put(key, 0); final int THREADS = 5; final int ITERATIONS_PER_THREAD = iterations(); GridTestUtils.runMultiThreaded(new Callable<Void>() { @Override public Void call() throws Exception { for (int i = 0; i < ITERATIONS_PER_THREAD && !failed; i++) { if (i % 500 == 0) log.info("Iteration " + i); try (IgniteTx tx = cache.txStart(txConcurrency, REPEATABLE_READ)) { cache.putx(key, i, new TestFilter()); tx.commit(); } } return null; } }, THREADS, "putWithFilter"); for (int i = 0; i < gridCount(); i++) { Integer val = (Integer)grid(i).cache(null).get(key); assertNotNull("Unexpected value for grid " + i, val); } assertFalse(failed); }', 'output': 'private void testPutWithFilterTx(final Integer key, final IgniteTxConcurrency txConcurrency) throws Exception { final Cache<Integer, Integer> cache = grid(0).cache(null); cache.put(key, 0); final int THREADS = 5; final int ITERATIONS_PER_THREAD = iterations(); GridTestUtils.runMultiThreaded(new Callable<Void>() { @Override public Void call() throws Exception { for (int i = 0; i < ITERATIONS_PER_THREAD && !failed; i++) { if (i % 500 == 0) log.info("Iteration " + i); try (IgniteTx tx = cache.txStart(txConcurrency, REPEATABLE_READ)) { cache.putx(key, i, new TestFilter()); tx.commit(); } } return null; } }, THREADS, "putWithFilter"); for (int i = 0; i < gridCount(); i++) { Integer val = (Integer)grid(i).cache(null).get(key); assertNotNull("Unexpected value for grid " + i, val); } assertFalse(failed); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPutFromNear() throws Exception { GridCache<Integer, String> cache = cache(0); int key = 0; while (true) { boolean found = false; for (ClusterNode n : grid(0).nodes()) { if (!cache.affinity().isPrimaryOrBackup(n, key)) { found = true; break; } } if (found) break; } assertNull(cache.put(key, "val")); checkStoreUsage(1, 1, 0, 1); }', 'ground_truth': 'public void testPutFromNear() throws Exception { IgniteCache<Integer, String> cache = jcache(0); int key = 0; while (true) { boolean found = false; for (ClusterNode n : grid(0).nodes()) { if (!grid(0).affinity(null).isPrimaryOrBackup(n, key)) { found = true; break; } } if (found) break; } assertNull(cache.getAndPut(key, "val")); checkStoreUsage(1, 1, 0, 1); }', 'output': 'public void testPutFromNear() throws Exception { Cache<Integer, String> cache = cache(0); int key = 0; while (true) { boolean found = false; for (ClusterNode n : grid(0).nodes()) { if (!cache.affinity().isPrimaryOrBackup(n, key)) { found = true; break; } } if (found) break; } assertNull(cache.put(key, "val")); checkStoreUsage(1, 1, 0, 1); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testNonZeroFailoverCount() throws Exception { GridNode rmt = getSpiContext().remoteNodes().iterator().next(); GridTestJobResult failed = new GridTestJobResult(rmt); failed.getJobContext().setAttribute(FAILOVER_ATTEMPT_COUNT_ATTR, getSpi().getMaximumFailoverAttempts() - 1); XXX other = getSpi().failover(new GridFailoverTestContext(new GridTestTaskSession(), failed), new ArrayList<>(getSpiContext().nodes())); assert other != null; assert other != rmt; assert other == getSpiContext().localNode(); checkAttributes(failed.getJobContext(), rmt, getSpi().getMaximumFailoverAttempts()); }', 'ground_truth': 'public void testNonZeroFailoverCount() throws Exception { ClusterNode rmt = getSpiContext().remoteNodes().iterator().next(); GridTestJobResult failed = new GridTestJobResult(rmt); failed.getJobContext().setAttribute(FAILOVER_ATTEMPT_COUNT_ATTR, getSpi().getMaximumFailoverAttempts() - 1); ClusterNode other = getSpi().failover(new GridFailoverTestContext(new GridTestTaskSession(), failed), new ArrayList<>(getSpiContext().nodes())); assert other != null; assert other != rmt; assert other == getSpiContext().localNode(); checkAttributes(failed.getJobContext(), rmt, getSpi().getMaximumFailoverAttempts()); }', 'output': 'public void testNonZeroFailoverCount() throws Exception { ClusterNode rmt = getSpiContext().remoteNodes().iterator().next(); GridTestJobResult failed = new GridTestJobResult(rmt); failed.getJobContext().setAttribute(FAILOVER_ATTEMPT_COUNT_ATTR, getSpi().getMaximumFailoverAttempts() - 1); ClusterNode other = getSpi().failover(new GridFailoverTestContext(new GridTestTaskSession(), failed), new ArrayList<>(getSpiContext().nodes())); assert other != null; assert other != rmt; assert other == getSpiContext().localNode(); checkAttributes(failed.getJobContext(), rmt, getSpi().getMaximumFailoverAttempts()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void cloneArguments(Expression cloneExp, String replacedIdentifier, Expression replacement, ReplacemenetState replacementState) { then_BI clone = (then_BI) cloneExp; clone.whenTrueExp = whenTrueExp.deepCloneWithIdentifierReplaced(replacedIdentifier, replacement, replacementState); clone.whenFalseExp = whenFalseExp.deepCloneWithIdentifierReplaced(replacedIdentifier, replacement, replacementState); } protected void cloneArguments(Expression clone, String replacedIdentifier, Expression replacement,\n                ReplacemenetState replacementState) { ReplacemenetState replacementState) { XXX parametersClone = new ArrayList(parameters.size()); for (int i = 0; i < parameters.size(); i++) { parametersClone.add(((Expression) parameters.get(i)) .deepCloneWithIdentifierReplaced(replacedIdentifier, replacement, replacementState)); } ((switch_BI) clone).parameters = parametersClone; }', 'ground_truth': 'protected void cloneArguments(Expression clone, String replacedIdentifier, Expression replacement, ReplacemenetState replacementState) { List<Expression> parametersClone = new ArrayList<Expression>(parameters.size()); for (Expression parameter : parameters) { parametersClone.add(parameter .deepCloneWithIdentifierReplaced(replacedIdentifier, replacement, replacementState)); } ((switch_BI) clone).parameters = parametersClone; }', 'output': 'protected void cloneArguments(Expression clone, String replacedIdentifier, Expression replacement, ReplacemenetState replacementState) { ReplacemenetState replacementState) { List<Expression> parametersClone = new ArrayList<Expression>(parameters.size()); for (int i = 0; i < parameters.size(); i++) { parametersClone.add(((Expression) parameters.get(i)) .deepCloneWithIdentifierReplaced(replacedIdentifier, replacement, replacementState)); } ((switch_BI) clone).parameters = parametersClone; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void run(Cassandra.Client client) throws IOException { List<String> values  = generateValues(); List<Column> columns = new ArrayList<Column>(); List<SuperColumn> superColumns = new ArrayList<SuperColumn>(); String format = "%0" + session.getTotalKeysLength() + "d"; for (int i = 0; i < session.getColumnsPerKey(); i++) { String columnName  = ("C" + Integer.toString(i)); XXX columnValue = values.get(index % values.size()); columns.add(new Column(ByteBufferUtil.bytes(columnName), ByteBufferUtil.bytes(columnValue), System.currentTimeMillis())); } if (session.getColumnFamilyType() == ColumnFamilyType.Super) { for (int i = 0; i < session.getSuperColumns(); i++) { String superColumnName = "S" + Integer.toString(i); superColumns.add(new SuperColumn(ByteBuffer.wrap(superColumnName.getBytes()), columns)); } } String rawKey = String.format(format, index); Map<ByteBuffer, Map<String, List<Mutation>>> record = new HashMap<ByteBuffer, Map<String, List<Mutation>>>(); record.put(ByteBufferUtil.bytes(rawKey), session.getColumnFamilyType() == ColumnFamilyType.Super ? getSuperColumnsMutationMap(superColumns) : getColumnsMutationMap(columns)); long start = System.currentTimeMillis(); boolean success = false; String exceptionMessage = null; for (int t = 0; t < session.getRetryTimes(); t++) { if (success) break; try { client.batch_mutate(record, session.getConsistencyLevel()); success = true; } catch (Exception e) { exceptionMessage = getExceptionMessage(e); success = false; } } if (!success) { error(String.format("Operation [%d] retried %d times - error inserting key %s %s%n", index, session.getRetryTimes(), rawKey, (exceptionMessage == null) ? "" : "(" + exceptionMessage + ")")); } session.operations.getAndIncrement(); session.keys.getAndIncrement(); session.latency.getAndAdd(System.currentTimeMillis() - start); }', 'ground_truth': 'public void run(Cassandra.Client client) throws IOException { List<String> values  = generateValues(); List<Column> columns = new ArrayList<Column>(); List<SuperColumn> superColumns = new ArrayList<SuperColumn>(); String format = "%0" + session.getTotalKeysLength() + "d"; for (int i = 0; i < session.getColumnsPerKey(); i++) { String columnName = ("C" + Integer.toString(i)); ByteBuffer columnValue = ByteBufferUtil.bytes(values.get(i % values.size())); columns.add(new Column(ByteBufferUtil.bytes(columnName), columnValue, System.currentTimeMillis())); } if (session.getColumnFamilyType() == ColumnFamilyType.Super) { for (int i = 0; i < session.getSuperColumns(); i++) { String superColumnName = "S" + Integer.toString(i); superColumns.add(new SuperColumn(ByteBuffer.wrap(superColumnName.getBytes()), columns)); } } String rawKey = String.format(format, index); Map<ByteBuffer, Map<String, List<Mutation>>> record = new HashMap<ByteBuffer, Map<String, List<Mutation>>>(); record.put(ByteBufferUtil.bytes(rawKey), session.getColumnFamilyType() == ColumnFamilyType.Super ? getSuperColumnsMutationMap(superColumns) : getColumnsMutationMap(columns)); long start = System.currentTimeMillis(); boolean success = false; String exceptionMessage = null; for (int t = 0; t < session.getRetryTimes(); t++) { if (success) break; try { client.batch_mutate(record, session.getConsistencyLevel()); success = true; } catch (Exception e) { exceptionMessage = getExceptionMessage(e); success = false; } } if (!success) { error(String.format("Operation [%d] retried %d times - error inserting key %s %s%n", index, session.getRetryTimes(), rawKey, (exceptionMessage == null) ? "" : "(" + exceptionMessage + ")")); } session.operations.getAndIncrement(); session.keys.getAndIncrement(); session.latency.getAndAdd(System.currentTimeMillis() - start); }', 'output': 'public void run(Cassandra.Client client) throws IOException { List<String> values  = generateValues(); List<Column> columns = new ArrayList<Column>(); List<SuperColumn> superColumns = new ArrayList<SuperColumn>(); String format = "%0" + session.getTotalKeysLength() + "d"; for (int i = 0; i < session.getColumnsPerKey(); i++) { String columnName  = ("C" + Integer.toString(i)); val columnValue = values.get(index % values.size()); columns.add(new Column(ByteBufferUtil.bytes(columnName), ByteBufferUtil.bytes(columnValue), System.currentTimeMillis())); } if (session.getColumnFamilyType() == ColumnFamilyType.Super) { for (int i = 0; i < session.getSuperColumns(); i++) { String superColumnName = "S" + Integer.toString(i); superColumns.add(new SuperColumn(ByteBuffer.wrap(superColumnName.getBytes()), columns)); } } String rawKey = String.format(format, index); Map<ByteBuffer, Map<String, List<Mutation>>> record = new HashMap<ByteBuffer, Map<String, List<Mutation>>>(); record.put(ByteBufferUtil.bytes(rawKey), session.getColumnFamilyType() == ColumnFamilyType.Super ? getSuperColumnsMutationMap(superColumns) : getColumnsMutationMap(columns)); long start = System.currentTimeMillis(); boolean success = false; String exceptionMessage = null; for (int t = 0; t < session.getRetryTimes(); t++) { if (success) break; try { client.batch_mutate(record, session.getConsistencyLevel()); success = true; } catch (Exception e) { exceptionMessage = getExceptionMessage(e); success = false; } } if (!success) { error(String.format("Operation [%d] retried %d times - error inserting key %s %s%n", index, session.getRetryTimes(), rawKey, (exceptionMessage == null) ? "" : "(" + exceptionMessage + ")")); } session.operations.getAndIncrement(); session.keys.getAndIncrement(); session.latency.getAndAdd(System.currentTimeMillis() - start'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void announceKeyspaceDrop(String ksName, boolean announceLocally) throws ConfigurationException { XXX oldKsm = Schema.instance.getKSMetaData(ksName); if (oldKsm == null) throw new ConfigurationException(String.format("Cannot drop non existing keyspace \'%s\'.", ksName)); logger.info(String.format("Drop Keyspace \'%s\'", oldKsm.name)); announce(LegacySchemaTables.makeDropKeyspaceMutation(oldKsm, FBUtilities.timestampMicros()), announceLocally); }', 'ground_truth': 'public static void announceKeyspaceDrop(String ksName, boolean announceLocally) throws ConfigurationException { KeyspaceMetadata oldKsm = Schema.instance.getKSMetaData(ksName); if (oldKsm == null) throw new ConfigurationException(String.format("Cannot drop non existing keyspace \'%s\'.", ksName)); logger.info(String.format("Drop Keyspace \'%s\'", oldKsm.name)); announce(LegacySchemaTables.makeDropKeyspaceMutation(oldKsm, FBUtilities.timestampMicros()), announceLocally); }', 'output': 'public static void announceKeyspaceDrop(String ksName, boolean announceLocally) throws ConfigurationException { KeyspaceMetaData oldKsm = Schema.instance.getKSMetaData(ksName); if (oldKsm == null) throw new ConfigurationException(String.format("Cannot drop non existing keyspace \'%s\'.", ksName)); logger.info(String.format("Drop Keyspace \'%s\'", oldKsm.name)); announce(LegacySchemaTables.makeDropKeyspaceMutation(oldKsm, FBUtilities.timestampMicros()), announceLocally); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testInternalKey() throws Exception { if (atomicityMode() == ATOMIC) return; GridCache<Object, Object> cache = grid(0).cache(null); CacheContinuousQuery<Object, Object> qry = cache.queries().createContinuousQuery(); final Map<Object, Object> map = new ConcurrentHashMap8<>(); final CountDownLatch latch = new CountDownLatch(2); qry.localCallback(new P2<UUID, Collection<CacheContinuousQueryEntry<Object, Object>>>() { @Override public boolean apply(UUID nodeId, Collection<CacheContinuousQueryEntry<Object, Object>> entries) { for (Map.Entry<Object, Object> e : entries) { map.put(e.getKey(), e.getValue()); latch.countDown(); } return true; } }); try { qry.execute(); cache.putx(new GridCacheInternalKeyImpl("test"), 1); cache.putx(1, 1); cache.putx(2, 2); assert latch.await(LATCH_TIMEOUT, MILLISECONDS); assertEquals(2, map.size()); assertEquals(1, (int)map.get(1)); assertEquals(2, (int)map.get(2)); } finally { qry.close(); } }', 'ground_truth': 'public void testInternalKey() throws Exception { if (atomicityMode() == ATOMIC) return; IgniteCache<Object, Object> cache = grid(0).jcache(null); ContinuousQuery<Object, Object> qry = Query.continuous(); final Map<Object, Object> map = new ConcurrentHashMap8<>(); final CountDownLatch latch = new CountDownLatch(2); qry.setLocalListener(new CacheEntryUpdatedListener<Object, Object>() { @Override public void onUpdated(Iterable<CacheEntryEvent<?, ?>> evts) { for (CacheEntryEvent<?, ?> e : evts) { map.put(e.getKey(), e.getValue()); latch.countDown(); } } }); try (QueryCursor<Cache.Entry<Object, Object>> ignored = cache.query(qry)) { cache.put(new GridCacheInternalKeyImpl("test"), 1); cache.put(1, 1); cache.put(2, 2); assert latch.await(LATCH_TIMEOUT, MILLISECONDS); assertEquals(2, map.size()); assertEquals(1, (int)map.get(1)); assertEquals(2, (int)map.get(2)); } }', 'output': 'public void testInternalKey() throws Exception { if (atomicityMode() == ATOMIC) return; IgniteCache<Object, Object> cache = grid(0).jcache(null); CacheContinuousQuery<Object, Object> qry = cache.queries().createContinuousQuery(); final Map<Object, Object> map = new ConcurrentHashMap8<>(); final CountDownLatch latch = new CountDownLatch(2); qry.localCallback(new P2<UUID, Collection<CacheContinuousQueryEntry<Object, Object>>>() { @Override public boolean apply(UUID nodeId, Collection<CacheContinuousQueryEntry<Object, Object>> entries) { for (Map.Entry<Object, Object> e : entries) { map.put(e.getKey(), e.getValue()); latch.countDown(); } return true; } }); try { qry.execute(); cache.putx(new GridCacheInternalKeyImpl("test"), 1); cache.putx(1, 1); cache.putx(2, 2); assert latch.await(LATCH_TIMEOUT, MILLISECONDS); assertEquals(2, map.size()); assertEquals(1, (int)map.get(1)); assertEquals(2, (int)map.get(2)); } finally { qry.close(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void deleteColumnOrSuperColumn(LegacyLayout.LegacyDeletionInfo delInfo, List<LegacyLayout.LegacyCell> cells, CFMetaData cfm, Deletion del, int nowInSec) throws InvalidRequestException { if (del.predicate != null && del.predicate.column_names != null) { for (ByteBuffer c : del.predicate.column_names) { try { if (del.super_column == null && cfm.isSuper()) addRange(cfm, delInfo, Slice.Bound.inclusiveStartOf(c), Slice.Bound.inclusiveEndOf(c), del.timestamp, nowInSec); else if (del.super_column != null) cells.add(toLegacyDeletion(cfm, del.super_column, c, del.timestamp, nowInSec)); else cells.add(toLegacyDeletion(cfm, c, del.timestamp, nowInSec)); } catch (XXX e) { throw new InvalidRequestException(e.getMessage()); } } } else if (del.predicate != null && del.predicate.slice_range != null) { if (del.super_column == null) { LegacyLayout.LegacyBound start = LegacyLayout.decodeTombstoneBound(cfm, del.predicate.getSlice_range().start, true); LegacyLayout.LegacyBound end = LegacyLayout.decodeTombstoneBound(cfm, del.predicate.getSlice_range().finish, false); delInfo.add(cfm, new LegacyLayout.LegacyRangeTombstone(start, end, new DeletionTime(del.timestamp, nowInSec))); } else { throw new InvalidRequestException("Cannot delete a range of subcolumns in a super column"); } } else { if (del.super_column != null) addRange(cfm, delInfo, Slice.Bound.inclusiveStartOf(del.super_column), Slice.Bound.inclusiveEndOf(del.super_column), del.timestamp, nowInSec); else delInfo.add(new DeletionTime(del.timestamp, nowInSec)); } }', 'ground_truth': 'private void deleteColumnOrSuperColumn(LegacyLayout.LegacyDeletionInfo delInfo, List<LegacyLayout.LegacyCell> cells, CFMetaData cfm, Deletion del, int nowInSec) throws InvalidRequestException { if (del.predicate != null && del.predicate.column_names != null) { for (ByteBuffer c : del.predicate.column_names) { try { if (del.super_column == null && cfm.isSuper()) addRange(cfm, delInfo, Slice.Bound.inclusiveStartOf(c), Slice.Bound.inclusiveEndOf(c), del.timestamp, nowInSec); else if (del.super_column != null) cells.add(toLegacyDeletion(cfm, del.super_column, c, del.timestamp, nowInSec)); else cells.add(toLegacyDeletion(cfm, c, del.timestamp, nowInSec)); } catch (UnknownColumnException | IllegalLegacyColumnException e) { throw new InvalidRequestException(e.getMessage()); } } } else if (del.predicate != null && del.predicate.slice_range != null) { if (del.super_column == null) { LegacyLayout.LegacyBound start = LegacyLayout.decodeTombstoneBound(cfm, del.predicate.getSlice_range().start, true); LegacyLayout.LegacyBound end = LegacyLayout.decodeTombstoneBound(cfm, del.predicate.getSlice_range().finish, false); delInfo.add(cfm, new LegacyLayout.LegacyRangeTombstone(start, end, new DeletionTime(del.timestamp, nowInSec))); } else { throw new InvalidRequestException("Cannot delete a range of subcolumns in a super column"); } } else { if (del.super_column != null) addRange(cfm, delInfo, Slice.Bound.inclusiveStartOf(del.super_column), Slice.Bound.inclusiveEndOf(del.super_column), del.timestamp, nowInSec); else delInfo.add(new DeletionTime(del.timestamp, nowInSec)); } }', 'output': 'private void deleteColumnOrSuperColumn(LegacyLayout.LegacyDeletionInfo delInfo, List<LegacyLayout.LegacyCell> cells, CFMetaData cfm, Deletion del, int nowInSec) throws InvalidRequestException { if (del.predicate != null && del.predicate.column_names != null) { for (ByteBuffer c : del.predicate.column_names) { try { if (del.super_column == null && cfm.isSuper()) addRange(cfm, delInfo, Slice.Bound.inclusiveStartOf(c), Slice.Bound.inclusiveEndOf(c), del.timestamp, nowInSec); else if (del.super_column != null) cells.add(toLegacyDeletion(cfm, del.super_column, c, del.timestamp, nowInSec)); else cells.add(toLegacyDeletion(cfm, c, del.timestamp, nowInSec)); } catch (InvalidRequestException e) { throw e; } } } else if (del.predicate != null && del.predicate.slice_range != null) { if (del.super_column == null) { LegacyLayout.LegacyBound start = LegacyLayout.decodeTombstoneBound(cfm, del.predicate.getSlice_range().start, true); LegacyLayout.LegacyBound end = LegacyLayout.decodeTombstoneBound(cfm, del.predicate.getSlice_range().finish, false); delInfo.add(cfm, new LegacyLayout.LegacyRangeTombstone(start, end, new DeletionTime(del.timestamp, nowInSec))); } else { throw new InvalidRequestException("Cannot delete a range of subcolumns in a super column"); } } else { if (del.super_column != null) addRange(cfm, delInfo, Slice.Bound.inclusiveStartOf(del.super_column), Slice.Bound.inclusiveEndOf(del.super_column), del.timestamp, nowInSec); else delInfo.add(new DeletionTime(del.timestamp, nowInSec)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSchemasView() throws Exception { IgniteEx srv = startGrid(getConfiguration().setSqlSchemas("PREDIFINED_SCHEMA_1")); IgniteEx client = startGrid(getConfiguration().setClientMode(true).setIgniteInstanceName("CLIENT").setSqlSchemas("PREDIFINED_SCHEMA_2")); srv.createCache(cacheConfiguration("TST1")); String schemasSql = "SELECT * FROM IGNITE.SCHEMAS"; List<List<?>> srvNodeSchemas = execSql(schemasSql); List<List<?>> clientNodeSchemas = execSql(client, schemasSql); String[] expSchemasSrv = new String[] {"PREDIFINED_SCHEMA_1", "IGNITE", "PUBLIC", "TST1"}; String[] schemasSrv = srvNodeSchemas.stream().map(f -> f.get(0)).map(String.class::cast).toArray(String[]::new); Assert.assertArrayEquals(expSchemasSrv, schemasSrv); XXX expSchemasCli = new String[] {"PREDIFINED_SCHEMA_2", "IGNITE", "PUBLIC", "TST1"}; String[] schemasCli = clientNodeSchemas.stream().map(f -> f.get(0)).map(String.class::cast).toArray(String[]::new); Assert.assertArrayEquals(expSchemasCli, schemasCli); }', 'ground_truth': 'public void testSchemasView() throws Exception { IgniteEx srv = startGrid(getConfiguration().setSqlSchemas("PREDIFINED_SCHEMA_1")); IgniteEx client = startGrid(getConfiguration().setClientMode(true).setIgniteInstanceName("CLIENT").setSqlSchemas("PREDIFINED_SCHEMA_2")); srv.createCache(cacheConfiguration("TST1")); String schemasSql = "SELECT * FROM " + systemSchemaName() + ".SCHEMAS"; List<List<?>> srvNodeSchemas = execSql(schemasSql); List<List<?>> clientNodeSchemas = execSql(client, schemasSql); Set expSchemasSrv = Sets.newHashSet("PREDIFINED_SCHEMA_1", "PUBLIC", "TST1", systemSchemaName()); Set schemasSrv = srvNodeSchemas.stream().map(f -> f.get(0)).map(String.class::cast).collect(toSet()); Assert.assertEquals(expSchemasSrv, schemasSrv); Set expSchemasCli = Sets.newHashSet("PREDIFINED_SCHEMA_2", "PUBLIC", "TST1", systemSchemaName()); Set schemasCli = clientNodeSchemas.stream().map(f -> f.get(0)).map(String.class::cast).collect(toSet()); Assert.assertEquals(expSchemasCli, schemasCli); }', 'output': 'public void testSchemasView() throws Exception { IgniteEx srv = startGrid(getConfiguration().setSqlSchemas("PREDIFINED_SCHEMA_1")); IgniteEx client = startGrid(getConfiguration().setClientMode(true).setIgniteInstanceName("CLIENT").setSqlSchemas("PREDIFINED_SCHEMA_2")); srv.createCache(cacheConfiguration("TST1")); String schemasSql = "SELECT * FROM IGNITE.SCHEMAS"; List<List<?>> srvNodeSchemas = execSql(schemasSql); List<List<?>> clientNodeSchemas = execSql(client, schemasSql); String[] expSchemasSrv = new String[] {"PREDIFINED_SCHEMA_1", "IGNITE", "PUBLIC", "TST1"}; String[] schemasSrv = srvNodeSchemas.stream().map(f -> f.get(0)).map(String.class::cast).toArray(String[]::new); Assert.assertArrayEquals(expSchemasSrv, schemasSrv); String[] expSchemasCli = new String[] {"PREDIFINED_SCHEMA_2", "IGNITE", "PUBLIC", "TST1"}; String[] schemasCli = clientNodeSchemas.stream().map(f -> f.get(0)).map(String.class::cast).toArray(String[]::new); Assert.assertArrayEquals(expSchemasCli, schemasCli); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Component getRenderer() { final JTableOperator table = propertySheetOper.tblSheet(); int row = getRow(); TableCellRenderer renderer = table.getCellRenderer(row, 1); Component comp = renderer.getTableCellRendererComponent( (JTable)table.getSource(), table.getValueAt(row, 1), false, false, row, 1 ); try { XXX clazz = Class.forName("org.openide.explorer.propertysheet.RendererPropertyDisplayer"); Method findInnermostRendererMethod = clazz.getDeclaredMethod("findInnermostRenderer", new Class[] {JComponent.class}); findInnermostRendererMethod.setAccessible(true); comp = (Component)findInnermostRendererMethod.invoke(null, new Object[] {comp}); } catch (Exception e) { throw new JemmyException("RendererPropertyDisplayer.findInnermostRenderer() by reflection failed.", e); } return comp; }', 'ground_truth': 'private Component getRenderer() { final JTableOperator table = propertySheetOper.tblSheet(); int row = getRow(); TableCellRenderer renderer = table.getCellRenderer(row, 1); Component comp = renderer.getTableCellRendererComponent( (JTable)table.getSource(), table.getValueAt(row, 1), false, false, row, 1 ); try { Class<?> clazz = Class.forName("org.openide.explorer.propertysheet.RendererPropertyDisplayer"); Method findInnermostRendererMethod = clazz.getDeclaredMethod("findInnermostRenderer", new Class[] {JComponent.class}); findInnermostRendererMethod.setAccessible(true); comp = (Component)findInnermostRendererMethod.invoke(null, new Object[] {comp}); } catch (Exception e) { throw new JemmyException("RendererPropertyDisplayer.findInnermostRenderer() by reflection failed.", e); } return comp; }', 'output': 'private Component getRenderer() { final JTableOperator table = propertySheetOper.tblSheet(); int row = getRow(); TableCellRenderer renderer = table.getCellRenderer(row, 1); Component comp = renderer.getTableCellRendererComponent( (JTable)table.getSource(), table.getValueAt(row, 1), false, false, row, 1 ); try { Class<?> clazz = Class.forName("org.openide.explorer.propertysheet.RendererPropertyDisplayer"); Method findInnermostRendererMethod = clazz.getDeclaredMethod("findInnermostRenderer", new Class[] {JComponent.class}); findInnermostRendererMethod.setAccessible(true); comp = (Component)findInnermostRendererMethod.invoke(null, new Object[] {comp}); } catch (Exception e) { throw new JemmyException("RendererPropertyDisplayer.findInnermostRenderer() by reflection failed.", e); } return comp; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testImportCacheEnabledWithoutSrcDir() throws Throwable { createTable("create table %s (id int primary key, d int) WITH caching = { \'keys\': \'NONE\', \'rows_per_partition\': \'ALL\' }"); for (int i = 0; i < 10; i++) execute("insert into %s (id, d) values (?, ?)", i, i); getCurrentColumnFamilyStore().forceBlockingFlush(); CacheService.instance.setRowCacheCapacityInMB(1); getCurrentColumnFamilyStore().clearUnsafe(); XXX options = ColumnFamilyStore.ImportOptions.options(null).invalidateCaches(true).build(); getCurrentColumnFamilyStore().importNewSSTables(options); assertEquals(1, getCurrentColumnFamilyStore().getLiveSSTables().size()); }', 'ground_truth': 'public void testImportCacheEnabledWithoutSrcDir() throws Throwable { createTable("create table %s (id int primary key, d int) WITH caching = { \'keys\': \'NONE\', \'rows_per_partition\': \'ALL\' }"); for (int i = 0; i < 10; i++) execute("insert into %s (id, d) values (?, ?)", i, i); getCurrentColumnFamilyStore().forceBlockingFlush(); Set<SSTableReader> sstables = getCurrentColumnFamilyStore().getLiveSSTables(); CacheService.instance.setRowCacheCapacityInMB(1); getCurrentColumnFamilyStore().clearUnsafe(); sstables.forEach(s -> s.selfRef().release()); SSTableImporter.Options options = SSTableImporter.Options.options().invalidateCaches(true).build(); SSTableImporter importer = new SSTableImporter(getCurrentColumnFamilyStore()); importer.importNewSSTables(options); assertEquals(1, getCurrentColumnFamilyStore().getLiveSSTables().size()); }', 'output': 'public void testImportCacheEnabledWithoutSrcDir() throws Throwable { createTable("create table %s (id int primary key, d int) WITH caching = { \'keys\': \'NONE\', \'rows_per_partition\': \'ALL\' }"); for (int i = 0; i < 10; i++) execute("insert into %s (id, d) values (?, ?)", i, i); getCurrentColumnFamilyStore().forceBlockingFlush(); CacheService.instance.setRowCacheCapacityInMB(1); getCurrentColumnFamilyStore().clearUnsafe(); ColumnFamilyStore.ImportOptions options = ColumnFamilyStore.ImportOptions.options(null).invalidateCaches(true).build(); getCurrentColumnFamilyStore().importNewSSTables(options); assertEquals(1, getCurrentColumnFamilyStore().getLiveSSTables().size()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Constructor retrieveConstructor(Class[] arguments) { XXX constructor = (Constructor) chooseMethod("<init>", constructors, arguments, false); if (constructor != null) { return constructor; } constructor = (Constructor) chooseMethod("<init>", constructors, arguments, true); if (constructor != null) { return constructor; } return null; }', 'ground_truth': 'public Constructor retrieveConstructor(Class[] arguments) { CachedConstructor constructor = (CachedConstructor) chooseMethod("<init>", constructors, arguments, false); if (constructor != null) { return constructor.cachedConstructor; } constructor = (CachedConstructor) chooseMethod("<init>", constructors, arguments, true); if (constructor != null) { return constructor.cachedConstructor; } return null; }', 'output': 'public Constructor retrieveConstructor(Class<?>[] arguments) { Constructor constructor = (Constructor) chooseMethod("<init>", constructors, arguments, false); if (constructor != null) { return constructor; } constructor = (Constructor) chooseMethod("<init>", constructors, arguments, true); if (constructor != null) { return constructor; } return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void onMatch(RelOptRuleCall call) { ProjectRel topProject = (ProjectRel) call.rels[0]; ProjectRel bottomProject = (ProjectRel) call.rels[1]; RexBuilder rexBuilder = topProject.getCluster().getRexBuilder(); if (!force) { if (RelOptUtil.checkProjAndChildInputs(topProject, false)) { return; } } RexProgram bottomProgram = RexProgram.create( bottomProject.getChild().getRowType(), bottomProject.getProjectExps(), null, bottomProject.getRowType(), rexBuilder); RexNode [] projExprs = topProject.getProjectExps(); RexProgram topProgram = RexProgram.create( bottomProject.getRowType(), projExprs, null, topProject.getRowType(), rexBuilder); RexProgram mergedProgram = RexProgramBuilder.mergePrograms( topProgram, bottomProgram, rexBuilder); int nProjExprs = projExprs.length; RexNode [] newProjExprs = new RexNode[nProjExprs]; List<RexLocalRef> projList = mergedProgram.getProjectList(); for (int i = 0; i < nProjExprs; i++) { newProjExprs[i] = mergedProgram.expandLocalRef(projList.get(i)); } ProjectRel newProjectRel = (ProjectRel) CalcRel.createProject( bottomProject.getChild(), newProjExprs, RelOptUtil.getFieldNames(topProject.getRowType())); call.transformTo(newProjectRel); }', 'ground_truth': 'public void onMatch(RelOptRuleCall call) { ProjectRel topProject = (ProjectRel) call.rels[0]; ProjectRel bottomProject = (ProjectRel) call.rels[1]; RexBuilder rexBuilder = topProject.getCluster().getRexBuilder(); if (!force) { if (RelOptUtil.checkProjAndChildInputs(topProject, false)) { return; } } RexProgram bottomProgram = RexProgram.create( bottomProject.getChild().getRowType(), bottomProject.getProjectExps(), null, bottomProject.getRowType(), rexBuilder); RexNode [] projExprs = topProject.getProjectExps(); RexProgram topProgram = RexProgram.create( bottomProject.getRowType(), projExprs, null, topProject.getRowType(), rexBuilder); RexProgram mergedProgram = RexProgramBuilder.mergePrograms( topProgram, bottomProgram, rexBuilder); int nProjExprs = projExprs.length; List<RexNode> newProjExprs = new ArrayList<RexNode>(); List<RexLocalRef> projList = mergedProgram.getProjectList(); for (int i = 0; i < nProjExprs; i++) { newProjExprs.add(mergedProgram.expandLocalRef(projList.get(i))); } ProjectRel newProjectRel = (ProjectRel) CalcRel.createProject( bottomProject.getChild(), newProjExprs, topProject.getRowType().getFieldNames()); call.transformTo(newProjectRel); }', 'output': 'public void onMatch(RelOptRuleCall call) { ProjectRel topProject = (ProjectRel) call.rels[0]; ProjectRel bottomProject = (ProjectRel) call.rels[1]; RexBuilder rexBuilder = topProject.getCluster().getRexBuilder(); if (!force) { if (RelOptUtil.checkProjAndChildInputs(topProject, false)) { return; } } RexProgram bottomProgram = RexProgram.create( bottomProject.getChild().getRowType(), bottomProject.getProjectExps(), null, bottomProject.getRowType(), rexBuilder); RexNode [] projExprs = topProject.getProjectExps(); RexProgram topProgram = RexProgram.create( bottomProject.getRowType(), projExprs, null, topProject.getRowType(), rexBuilder); RexProgram mergedProgram = RexProgramBuilder.mergePrograms( topProgram, bottomProgram, rexBuilder); int nProjExprs = projExprs.length; RexNode [] newProjExprs = new RexNode[nProjExprs]; List<RexLocalRef> projList = mergedProgram.getProjectList(); for (int i = 0; i < nProjExprs; i++) { newProjExprs[i] = mergedProgram.expandLocalRef(projList.get(i)); } ProjectRel newProjectRel = (ProjectRel) CalcRel.createProject( bottomProject.getChild(), newProjExprs, RelOptUtil.getFieldNames(topProject.getRowType())); call.transformTo(newProjectRel); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception { IgniteConfiguration c = super.getConfiguration(gridName); c.getTransactionsConfiguration().setDefaultTxTimeout(TIMEOUT); c.getTransactionsConfiguration().setTxSerializableEnabled(true); GridCacheConfiguration cc = defaultCacheConfiguration(); cc.setCacheMode(REPLICATED); c.setCacheConfiguration(cc); XXX spi = new GridTcpDiscoverySpi(); spi.setIpFinder(ipFinder); c.setDiscoverySpi(spi); return c; } } }', 'ground_truth': '@Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception { IgniteConfiguration c = super.getConfiguration(gridName); c.getTransactionsConfiguration().setDefaultTxTimeout(TIMEOUT); c.getTransactionsConfiguration().setTxSerializableEnabled(true); GridCacheConfiguration cc = defaultCacheConfiguration(); cc.setCacheMode(REPLICATED); c.setCacheConfiguration(cc); TcpDiscoverySpi spi = new TcpDiscoverySpi(); spi.setIpFinder(ipFinder); c.setDiscoverySpi(spi); return c; }', 'output': '@Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception { IgniteConfiguration c = super.getConfiguration(gridName); c.getTransactionsConfiguration().setDefaultTxTimeout(TIMEOUT); c.getTransactionsConfiguration().setTxSerializableEnabled(true); GridCacheConfiguration cc = defaultCacheConfiguration(); cc.setCacheMode(REPLICATED); c.setCacheConfiguration(cc); TcpDiscoverySpi spi = new TcpDiscoverySpi(); spi.setIpFinder(ipFinder); c.setDiscoverySpi(spi); return c; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Can<_Either<ManagedObject, InteractionVeto>> parseArguments( final IResourceContext resourceContext, final ObjectAction action, final JsonRepresentation arguments) { val jsonArgList = argListFor(action, arguments); final List<_Either<ManagedObject, InteractionVeto>> argAdapters = _Lists.newArrayList(); val parameters = action.getParameters(); for (int i = 0; i < jsonArgList.size(); i++) { final JsonRepresentation argRepr = jsonArgList.get(i); final int argIndex = i; val paramMeta = parameters.getElseFail(argIndex); val paramSpec = paramMeta.getElementType(); val objectOrVeto = Try.call(()-> (paramMeta.isOptional() && argRepr == null) ? ManagedObject.empty(paramSpec) : new JsonParserHelper(resourceContext, paramSpec) .objectAdapterFor(argRepr)) .<_Either<ManagedObject, InteractionVeto>>fold( success->_Either.left(success.orElseThrow()), exception->_Either.right( InteractionVeto.actionParamInvalid( String.format("exception when parsing paramNr %d [%s]: %s", argIndex, argRepr, exception)))); argAdapters.add(objectOrVeto); } return Can.ofCollection(argAdapters); }', 'ground_truth': 'public static Can<Railway<InteractionVeto, ManagedObject>> parseArguments( final IResourceContext resourceContext, final ObjectAction action, final JsonRepresentation arguments) { val jsonArgList = argListFor(action, arguments); final List<Railway<InteractionVeto, ManagedObject>> argAdapters = _Lists.newArrayList(); val parameters = action.getParameters(); for (int i = 0; i < jsonArgList.size(); i++) { final JsonRepresentation argRepr = jsonArgList.get(i); final int argIndex = i; val paramMeta = parameters.getElseFail(argIndex); val paramSpec = paramMeta.getElementType(); val objectOrVeto = Try.call(()-> (paramMeta.isOptional() && argRepr == null) ? ManagedObject.empty(paramSpec) : new JsonParserHelper(resourceContext, paramSpec) .objectAdapterFor(argRepr)) .<Railway<InteractionVeto, ManagedObject>>fold( exception->Railway.failure( InteractionVeto.actionParamInvalid( String.format("exception when parsing paramNr %d [%s]: %s", argIndex, argRepr, exception))), success->Railway.success(success.orElseThrow())) ; argAdapters.add(objectOrVeto); } return Can.ofCollection(argAdapters); }', 'output': 'public static Can<_Either<ManagedObject, InteractionVeto>> parseArguments( final IResourceContext resourceContext, final ObjectAction action, final JsonRepresentation arguments) { val jsonArgList = argListFor(action, arguments); final List<_Either<ManagedObject, InteractionVeto>> argAdapters = _Lists.newArrayList(); val parameters = action.getParameters(); for (int i = 0; i < jsonArgList.size(); i++) { final JsonRepresentation argRepr = jsonArgList.get(i); final int argIndex = i; val paramMeta = parameters.getElseFail(argIndex); val paramSpec = paramMeta.getElementType(); val objectOrVeto = Try.call(()-> (paramMeta.isOptional() && argRepr == null) ? ManagedObject.empty(paramSpec) : new JsonParserHelper(resourceContext, paramSpec) .objectAdapterFor(argRepr)) .<_Either<ManagedObject, InteractionVeto>>fold( success->_Either.left(success.orElseThrow()), exception->_Either.right( InteractionVeto.actionParamInvalid( String.format("exception when parsing paramNr %d [%s]: %s", argIndex, argRepr, exception)))); argAdapters.add(objectOrVeto); } return Can.ofCollection(argAdapters); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void runConcurrentTest(Ignite g, final int keysCnt, final int batchSize) throws Exception { assert keysCnt % batchSize == 0; final AtomicInteger evictedKeysCnt = new AtomicInteger(); final GridCache<Object, Object> cache = g.cache(null); cache.loadCache(null, 0); info("Finished load cache."); GridFuture<?> evictFut = multithreadedAsync(new Runnable() { @Override public void run() { Collection<Long> keys = new ArrayList<>(batchSize); int evictedBatches = 0; for (long i = 0; i < keysCnt; i++) { keys.add(i); if (keys.size() == batchSize) { cache.evictAll(keys); evictedKeysCnt.addAndGet(batchSize); keys.clear(); evictedBatches++; if (evictedBatches % 100 == 0 && evictedBatches > 0) info("Evicted " + (evictedBatches * batchSize) + " entries."); } } } }, N_THREADS, "evict"); final AtomicInteger unswappedKeys = new AtomicInteger(); XXX unswapFut = multithreadedAsync(new Runnable() { @Override public void run() { try { Collection<Long> keys = new ArrayList<>(batchSize); int unswappedBatches = 0; for (long i = 0; i < keysCnt; i++) { keys.add(i); if (keys.size() == batchSize) { cache.promoteAll(keys); unswappedKeys.addAndGet(batchSize); keys.clear(); unswappedBatches++; if (unswappedBatches % 100 == 0 && unswappedBatches > 0) info("Unswapped " + (unswappedBatches * batchSize) + " entries."); } } } catch (GridException e) { e.printStackTrace(); } } }, N_THREADS, "promote"); evictFut.get(); unswapFut.get(); info("Clearing cache."); for (long i = 0; i < KEYS_CNT; i++) cache.remove(i); }', 'ground_truth': 'private void runConcurrentTest(Ignite g, final int keysCnt, final int batchSize) throws Exception { assert keysCnt % batchSize == 0; final AtomicInteger evictedKeysCnt = new AtomicInteger(); final GridCache<Object, Object> cache = g.cache(null); cache.loadCache(null, 0); info("Finished load cache."); IgniteFuture<?> evictFut = multithreadedAsync(new Runnable() { @Override public void run() { Collection<Long> keys = new ArrayList<>(batchSize); int evictedBatches = 0; for (long i = 0; i < keysCnt; i++) { keys.add(i); if (keys.size() == batchSize) { cache.evictAll(keys); evictedKeysCnt.addAndGet(batchSize); keys.clear(); evictedBatches++; if (evictedBatches % 100 == 0 && evictedBatches > 0) info("Evicted " + (evictedBatches * batchSize) + " entries."); } } } }, N_THREADS, "evict"); final AtomicInteger unswappedKeys = new AtomicInteger(); IgniteFuture<?> unswapFut = multithreadedAsync(new Runnable() { @Override public void run() { try { Collection<Long> keys = new ArrayList<>(batchSize); int unswappedBatches = 0; for (long i = 0; i < keysCnt; i++) { keys.add(i); if (keys.size() == batchSize) { cache.promoteAll(keys); unswappedKeys.addAndGet(batchSize); keys.clear(); unswappedBatches++; if (unswappedBatches % 100 == 0 && unswappedBatches > 0) info("Unswapped " + (unswappedBatches * batchSize) + " entries."); } } } catch (GridException e) { e.printStackTrace(); } } }, N_THREADS, "promote"); evictFut.get(); unswapFut.get(); info("Clearing cache."); for (long i = 0; i < KEYS_CNT; i++) cache.remove(i); }', 'output': 'private void runConcurrentTest(Ignite g, final int keysCnt, final int batchSize) throws Exception { assert keysCnt % batchSize == 0; final AtomicInteger evictedKeysCnt = new AtomicInteger(); final GridCache<Object, Object> cache = g.cache(null); cache.loadCache(null, 0); info("Finished load cache."); IgniteFuture<?> evictFut = multithreadedAsync(new Runnable() { @Override public void run() { Collection<Long> keys = new ArrayList<>(batchSize); int evictedBatches = 0; for (long i = 0; i < keysCnt; i++) { keys.add(i); if (keys.size() == batchSize) { cache.evictAll(keys); evictedKeysCnt.addAndGet(batchSize); keys.clear(); evictedBatches++; if (evictedBatches % 100 == 0 && evictedBatches > 0) info("Evicted " + (evictedBatches * batchSize) + " entries."); } } } }, N_THREADS, "evict"); final AtomicInteger unswappedKeys = new AtomicInteger(); IgniteFuture<?> unswapFut = multithreadedAsync(new Runnable() { @Override public void run() { try { Collection<Long> keys = new ArrayList<>(batchSize); int unswappedBatches = 0; for (long i = 0; i < keysCnt; i++) { keys.add(i); if (keys.size() == batchSize) { cache.promoteAll(keys); unswappedKeys.addAndGet(batchSize); keys.clear(); unswappedBatches++; if (unswappedBatches % 100 == 0 && unswappedBatches > 0) info("Unswapped " + (unswappedBatches * batchSize) + " entries."); } } } catch (IgniteException e) { e.printStackTrace(); } } }, N_THREADS, "promote"); evictFut.get(); unswapFut.get(); info("Clearing cache."); for (long i = 0; i < KEYS_CNT; i++) cache.remove(i); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private LivenessInfo computeLivenessInfoForEntry(Row baseRow) { /* * We need to compute both the timestamp and expiration. * * For the timestamp, it makes sense to use the bigger timestamp for all view PK columns. * * This is more complex for the expiration. We want to maintain consistency between the base and the view, so the * entry should only exist as long as the base row exists _and_ has non-null values for all the columns that are part * of the view PK. * Which means we really have 2 cases: *   1) either the columns for the base and view PKs are exactly the same: in that case, the view entry should live *      as long as the base row lives. This means the view entry should only expire once *everything* in the base row *      has expired. Which means the row TTL should be the max of any other TTL. *   2) or there is a column that is not in the base PK but is in the view PK (we can only have one so far, we\'ll need *      to slightly adapt if we allow more later): in that case, as long as that column lives the entry does too, but *      as soon as it expires (or is deleted for that matter) the entry also should expire. So the expiration for the *      view is the one of that column, irregarding of any other expiration. *      To take an example of that case, if you have: *        CREATE TABLE t (a int, b int, c int, PRIMARY KEY (a, b)) *        CREATE MATERIALIZED VIEW mv AS SELECT * FROM t WHERE c IS NOT NULL AND a IS NOT NULL AND b IS NOT NULL PRIMARY KEY (c, a, b) *        INSERT INTO t(a, b) VALUES (0, 0) USING TTL 3; *        UPDATE t SET c = 0 WHERE a = 0 AND b = 0; *      then even after 3 seconds elapsed, the row will still exist (it just won\'t have a "row marker" anymore) and so *      the MV should still have a corresponding entry. */ assert view.baseNonPKColumnsInViewPK.size() <= 1; LivenessInfo baseLiveness = baseRow.primaryKeyLivenessInfo(); if (view.baseNonPKColumnsInViewPK.isEmpty()) { int ttl = baseLiveness.ttl(); int expirationTime = baseLiveness.localExpirationTime(); for (Cell cell : baseRow.cells()) { if (cell.ttl() > ttl) { ttl = cell.ttl(); expirationTime = cell.localDeletionTime(); } } return ttl == baseLiveness.ttl() ? baseLiveness : LivenessInfo.withExpirationTime(baseLiveness.timestamp(), ttl, expirationTime); } XXX baseColumn = view.baseNonPKColumnsInViewPK.get(0); Cell cell = baseRow.getCell(baseColumn); assert isLive(cell) : "We shouldn\'t have got there if the base row had no associated entry"; long timestamp = Math.max(baseLiveness.timestamp(), cell.timestamp()); return LivenessInfo.withExpirationTime(timestamp, cell.ttl(), cell.localDeletionTime()); }', 'ground_truth': 'private LivenessInfo computeLivenessInfoForEntry(Row baseRow) { /* * We need to compute both the timestamp and expiration. * * For the timestamp, it makes sense to use the bigger timestamp for all view PK columns. * * This is more complex for the expiration. We want to maintain consistency between the base and the view, so the * entry should only exist as long as the base row exists _and_ has non-null values for all the columns that are part * of the view PK. * Which means we really have 2 cases: *   1) either the columns for the base and view PKs are exactly the same: in that case, the view entry should live *      as long as the base row lives. This means the view entry should only expire once *everything* in the base row *      has expired. Which means the row TTL should be the max of any other TTL. *   2) or there is a column that is not in the base PK but is in the view PK (we can only have one so far, we\'ll need *      to slightly adapt if we allow more later): in that case, as long as that column lives the entry does too, but *      as soon as it expires (or is deleted for that matter) the entry also should expire. So the expiration for the *      view is the one of that column, irregarding of any other expiration. *      To take an example of that case, if you have: *        CREATE TABLE t (a int, b int, c int, PRIMARY KEY (a, b)) *        CREATE MATERIALIZED VIEW mv AS SELECT * FROM t WHERE c IS NOT NULL AND a IS NOT NULL AND b IS NOT NULL PRIMARY KEY (c, a, b) *        INSERT INTO t(a, b) VALUES (0, 0) USING TTL 3; *        UPDATE t SET c = 0 WHERE a = 0 AND b = 0; *      then even after 3 seconds elapsed, the row will still exist (it just won\'t have a "row marker" anymore) and so *      the MV should still have a corresponding entry. */ assert view.baseNonPKColumnsInViewPK.size() <= 1; LivenessInfo baseLiveness = baseRow.primaryKeyLivenessInfo(); if (view.baseNonPKColumnsInViewPK.isEmpty()) { int ttl = baseLiveness.ttl(); int expirationTime = baseLiveness.localExpirationTime(); for (Cell cell : baseRow.cells()) { if (cell.ttl() > ttl) { ttl = cell.ttl(); expirationTime = cell.localDeletionTime(); } } return ttl == baseLiveness.ttl() ? baseLiveness : LivenessInfo.withExpirationTime(baseLiveness.timestamp(), ttl, expirationTime); } ColumnMetadata baseColumn = view.baseNonPKColumnsInViewPK.get(0); Cell cell = baseRow.getCell(baseColumn); assert isLive(cell) : "We shouldn\'t have got there if the base row had no associated entry"; long timestamp = Math.max(baseLiveness.timestamp(), cell.timestamp()); return LivenessInfo.withExpirationTime(timestamp, cell.ttl(), cell.localDeletionTime()); }', 'output': 'private LivenessInfo computeLivenessInfoForEntry(Row baseRow) { assert view.baseNonPKColumnsInViewPK.size() <= 1; LivenessInfo baseLiveness = baseRow.primaryKeyLivenessInfo(); if (view.baseNonPKColumnsInViewPK.isEmpty()) { int ttl = baseLiveness.ttl(); int expirationTime = baseLiveness.localExpirationTime(); for (Cell cell : baseRow.cells()) { if (cell.ttl() > ttl) { ttl = cell.ttl(); expirationTime = cell.localDeletionTime(); } } return ttl == baseLiveness.ttl() ? baseLiveness : LivenessInfo.withExpirationTime(baseLiveness.timestamp(), ttl, expirationTime); } String baseColumn = view.baseNonPKColumnsInViewPK.get(0); Cell cell = baseRow.getCell(baseColumn); assert isLive(cell) : "We shouldn\'t have got there if the base row had no associated entry"; long timestamp = Math.max(baseLiveness.timestamp(), cell.timestamp()); return LivenessInfo.withExpirationTime(timestamp, cell.ttl(), cell.localDeletionTime()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void runTest(final Ignite g, int threadCnt, int taskCnt, long dur, final LongAdder iterCntr) { final Semaphore sem = new Semaphore(taskCnt); final XXX lsnr = new CI1<IgniteFuture>() { @Override public void apply(IgniteFuture t) { sem.release(); } }; GridLoadTestUtils.runMultithreadedInLoop(new Callable<Object>() { @Nullable @Override public Object call() throws Exception { sem.acquire(); IgniteCompute comp = g.compute().withAsync(); comp.execute(GridJobExecutionLoadTestTask.class, null); ComputeTaskFuture<Object> f = comp.future(); f.listenAsync(lsnr); iterCntr.increment(); return null; } }, threadCnt, dur > 0 ? dur : Long.MAX_VALUE); }', 'ground_truth': 'private static void runTest(final Ignite g, int threadCnt, int taskCnt, long dur, final LongAdder iterCntr) { final Semaphore sem = new Semaphore(taskCnt); final IgniteInClosure<IgniteInternalFuture> lsnr = new CI1<IgniteInternalFuture>() { @Override public void apply(IgniteInternalFuture t) { sem.release(); } }; GridLoadTestUtils.runMultithreadedInLoop(new Callable<Object>() { @Nullable @Override public Object call() throws Exception { sem.acquire(); IgniteCompute comp = g.compute().withAsync(); comp.execute(GridJobExecutionLoadTestTask.class, null); ComputeTaskFuture<Object> f = comp.future(); f.listenAsync(lsnr); iterCntr.increment(); return null; } }, threadCnt, dur > 0 ? dur : Long.MAX_VALUE); }', 'output': 'private static void runTest(final Ignite g, int threadCnt, int taskCnt, long dur, final LongAdder iterCntr) { final Semaphore sem = new Semaphore(taskCnt); final IgniteInternalFuture<?> lsnr = new CI1<IgniteInternalFuture<?>>() { @Override public void apply(IgniteInternalFuture<?> t) { sem.release(); } }; GridLoadTestUtils.runMultithreadedInLoop(new Callable<Object>() { @Nullable @Override public Object call() throws Exception { sem.acquire(); IgniteCompute comp = g.compute().withAsync(); comp.execute(GridJobExecutionLoadTestTask.class, null); ComputeTaskFuture<Object> f = comp.future(); f.listenAsync(lsnr); iterCntr.increment(); return null; } }, threadCnt, dur > 0 ? dur : Long.MAX_VALUE); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Optional<Class<?>> inferElementType(final @NonNull Parameter param) { final XXX parameterType = param.getType(); final Class<?> declaringType = param.getDeclaringExecutable().getDeclaringClass(); final Type genericParameterType = param.getParameterizedType(); return inferElementType(parameterType, genericParameterType); }', 'ground_truth': 'public static Optional<Class<?>> inferElementType(final @NonNull Parameter param) { val parameterType = param.getType(); if (_Collections.isCollectionType(parameterType) || _Collections.isCanType(parameterType)) { return _Generics.streamGenericTypeArgumentsOfParameter(param) .findFirst(); } return Optional.empty(); }', 'output': 'public static Optional<Class<?>> inferElementType(final @NonNull Parameter param) { final TypeMirror parameterType = param.getType(); final Class<?> declaringType = param.getDeclaringExecutable().getDeclaringClass(); final Type genericParameterType = param.getParameterizedType(); return inferElementType(parameterType, genericParameterType); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void handleEvent(JobEvent event) throws InvalidStateTransitonException { if (LOG.isDebugEnabled()) { LOG.debug("Handling Job event, eventType=" + event.getType().name() + ", event=" + event.toString()); } JobState oldState = getState(); try { writeLock.lock(); try { stateMachine.doTransition(event.getType(), event); } catch (XXX e) { LOG.error("Can\'t handle Job event at current state" + ", jobId=" + this.getId() + ", currentState=" + oldState + ", eventType=" + event.getType() + ", event=" + event); throw e; } } finally { writeLock.unlock(); } if (oldState != getState()) { if (LOG.isDebugEnabled()) { LOG.debug("Job transitioned to a new state" + ", jobId=" + this.getId() + ", oldState=" + oldState + ", currentState=" + getState() + ", eventType=" + event.getType().name() + ", event=" + event); } } }', 'ground_truth': 'public void handleEvent(JobEvent event) throws InvalidStateTransitionException { if (LOG.isDebugEnabled()) { LOG.debug("Handling Job event, eventType=" + event.getType().name() + ", event=" + event.toString()); } JobState oldState = getState(); try { writeLock.lock(); try { stateMachine.doTransition(event.getType(), event); } catch (InvalidStateTransitionException e) { LOG.error("Can\'t handle Job event at current state" + ", jobId=" + this.getId() + ", currentState=" + oldState + ", eventType=" + event.getType() + ", event=" + event); throw e; } } finally { writeLock.unlock(); } if (oldState != getState()) { if (LOG.isDebugEnabled()) { LOG.debug("Job transitioned to a new state" + ", jobId=" + this.getId() + ", oldState=" + oldState + ", currentState=" + getState() + ", eventType=" + event.getType().name() + ", event=" + event); } } }', 'output': 'public void handleEvent(JobEvent event) throws InvalidStateTransitionException { if (LOG.isDebugEnabled()) { LOG.debug("Handling Job event, eventType=" + event.getType().name() + ", event=" + event.toString()); } JobState oldState = getState(); try { writeLock.lock(); try { stateMachine.doTransition(event.getType(), event); } catch (InvalidStateTransitionException e) { LOG.error("Can\'t handle Job event at current state" + ", jobId=" + this.getId() + ", currentState=" + oldState + ", eventType=" + event.getType() + ", event=" + event); throw e; } } finally { writeLock.unlock(); } if (oldState != getState()) { if (LOG.isDebugEnabled()) { LOG.debug("Job transitioned to a new state" + ", jobId=" + this.getId() + ", oldState=" + oldState + ", currentState=" + getState() + ", eventType=" + event.getType().name() + ", event=" + event); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void notifyUpdateAggregate(UDAggregate udf) { for (XXX listener : listeners) listener.onUpdateAggregate(udf.name().keyspace, udf.name().name); }', 'ground_truth': 'public void notifyUpdateAggregate(UDAggregate udf) { for (MigrationListener listener : listeners) listener.onUpdateAggregate(udf.name().keyspace, udf.name().name, udf.argTypes()); }', 'output': 'public void notifyUpdateAggregate(UDAggregate udf, String keyspace) { for (UDAggregateListener listener : listeners) listener.onUpdateAggregate(keyspace, udf.name().name); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void updateLocal(int p, UUID nodeId, GridDhtPartitionState state, long updateSeq) { assert lock.isWriteLockedByCurrentThread(); assert nodeId.equals(cctx.localNodeId()); ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache(); if (oldest.id().equals(cctx.localNodeId())) { long seq = node2part.updateSequence(); if (seq != updateSeq) { if (seq > updateSeq) { if (this.updateSeq.get() < seq) { boolean b = this.updateSeq.compareAndSet(this.updateSeq.get(), seq + 1); assert b : "Invalid update sequence [updateSeq=" + updateSeq + ", seq=" + seq + ", curUpdateSeq=" + this.updateSeq.get() + ", node2part=" + node2part.toFullString() + \']\'; updateSeq = seq + 1; } else updateSeq = seq; } node2part.updateSequence(updateSeq); } } XXX map = node2part.get(nodeId); if (map == null) node2part.put(nodeId, map = new GridDhtPartitionMap2(nodeId, updateSeq, topVer, Collections.<Integer, GridDhtPartitionState>emptyMap(), false)); map.updateSequence(updateSeq, topVer); map.put(p, state); Set<UUID> ids = part2node.get(p); if (ids == null) part2node.put(p, ids = U.newHashSet(3)); ids.add(nodeId); }', 'ground_truth': 'private void updateLocal(int p, UUID nodeId, GridDhtPartitionState state, long updateSeq) { assert lock.isWriteLockedByCurrentThread(); assert nodeId.equals(cctx.localNodeId()); ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache(); if (oldest.id().equals(cctx.localNodeId())) { long seq = node2part.updateSequence(); if (seq != updateSeq) { if (seq > updateSeq) { if (this.updateSeq.get() < seq) { boolean b = this.updateSeq.compareAndSet(this.updateSeq.get(), seq + 1); assert b : "Invalid update sequence [updateSeq=" + updateSeq + ", seq=" + seq + ", curUpdateSeq=" + this.updateSeq.get() + ", node2part=" + node2part.toFullString() + \']\'; updateSeq = seq + 1; } else updateSeq = seq; } node2part.updateSequence(updateSeq); } } GridDhtPartitionMap map = node2part.get(nodeId); if (map == null) node2part.put(nodeId, map = new GridDhtPartitionMap(nodeId, updateSeq, topVer, Collections.<Integer, GridDhtPartitionState>emptyMap(), false)); map.updateSequence(updateSeq, topVer); map.put(p, state); Set<UUID> ids = part2node.get(p); if (ids == null) part2node.put(p, ids = U.newHashSet(3)); ids.add(nodeId); }', 'output': 'private void updateLocal(int p, UUID nodeId, GridDhtPartitionState state, long updateSeq) { assert lock.isWriteLockedByCurrentThread(); assert nodeId.equals(cctx.localNodeId()); ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache(); if (oldest.id().equals(cctx.localNodeId())) { long seq = node2part.updateSequence(); if (seq != updateSeq) { if (seq > updateSeq) { if (this.updateSeq.get() < seq) { boolean b = this.updateSeq.compareAndSet(this.updateSeq.get(), seq + 1); assert b : "Invalid update sequence [updateSeq=" + updateSeq + ", seq=" + seq + ", curUpdateSeq=" + this.updateSeq.get() + ", node2part=" + node2part.toFullString() + \']\'; updateSeq = seq + 1; } else updateSeq = seq; } node2part.updateSequence(updateSeq); } GridDhtPartitionMap2 map = node2part.get(nodeId); if (map == null) node2part.put(nodeId, map = new GridDhtPartitionMap2(nodeId, updateSeq, topVer, Collections.<Integer, GridDhtPartitionState>emptyMap(), false)); map.updateSequence(updateSeq, topVer); map.put(p, state); Set<UUID> ids = part2node.get(p); if (ids == null) part2node.put(p, ids = U.newHashSet(3)); ids.add(nodeId); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void removeElement(Context context, int listIndex) throws Exception { SimpleWeakSet simpleWeakSet = context.getInstance(SimpleWeakSet.class); List removedList = (List) context.getProperty(REMOVED_LIST); XXX list = context.getInstance(List.class); Object element = list.remove(listIndex); removedList.add(element); Object removedElement = simpleWeakSet.remove(element); TestCase.assertEquals(element, removedElement); StringBuilder sb = context.logOpBuilder(); if (sb != null) { sb.append("Remove[").append(listIndex).append("]: ").append(element).append("\\n"); context.logOp(sb); } }', 'ground_truth': 'public static void removeElement(Context context, int listIndex) throws Exception { @SuppressWarnings("unchecked") SimpleWeakSet<Object> simpleWeakSet = context.getInstance(SimpleWeakSet.class); @SuppressWarnings("unchecked") List<Object> removedList = (List<Object>) context.getProperty(REMOVED_LIST); @SuppressWarnings("unchecked") List<Object> list = context.getInstance(List.class); Object element = list.remove(listIndex); removedList.add(element); Object removedElement = simpleWeakSet.remove(element); TestCase.assertEquals(element, removedElement); StringBuilder sb = context.logOpBuilder(); if (sb != null) { sb.append("Remove[").append(listIndex).append("]: ").append(element).append("\\n"); context.logOp(sb); } }', 'output': 'public static void removeElement(Context context, int listIndex) throws Exception { SimpleWeakSet simpleWeakSet = context.getInstance(SimpleWeakSet.class); List removedList = (List) context.getProperty(REMOVED_LIST); List list = context.getInstance(List.class); Object element = list.remove(listIndex); removedList.add(element); Object removedElement = simpleWeakSet.remove(element); TestCase.assertEquals(element, removedElement); StringBuilder sb = context.logOpBuilder(); if (sb != null) { sb.append("Remove[").append(listIndex).append("]: ").append(element).append("\\n"); context.logOp(sb); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public IgniteFuture<IgniteTx> commitAsyncLocal() { if (log.isDebugEnabled()) log.debug("Committing colocated tx locally: " + this); if (pessimistic()) prepareAsync(); IgniteFuture<GridCacheTxEx<K, V>> prep = prepFut.get(); if (F.isEmpty(dhtMap) && F.isEmpty(nearMap)) { if (prep != null) return (IgniteFuture<IgniteTx>)(IgniteFuture)prep; return new GridFinishedFuture<IgniteTx>(cctx.kernalContext(), this); } final GridDhtTxFinishFuture<K, V> fut = new GridDhtTxFinishFuture<>(cctx, this, /*commit*/true); cctx.mvcc().addFuture(fut); if (prep == null || prep.isDone()) { assert prep != null || optimistic(); try { if (prep != null) prep.get(); fut.finish(); } catch (IgniteTxOptimisticException e) { if (log.isDebugEnabled()) log.debug("Failed optimistically to prepare transaction [tx=" + this + ", e=" + e + \']\'); fut.onError(e); } catch (IgniteCheckedException e) { U.error(log, "Failed to prepare transaction: " + this, e); fut.onError(e); } } else prep.listenAsync(new CI1<IgniteFuture<GridCacheTxEx<K, V>>>() { @Override public void apply(IgniteFuture<GridCacheTxEx<K, V>> f) { try { f.get(); fut.finish(); } catch (IgniteTxOptimisticException e) { if (log.isDebugEnabled()) log.debug("Failed optimistically to prepare transaction [tx=" + this + ", e=" + e + \']\'); fut.onError(e); } catch (IgniteCheckedException e) { U.error(log, "Failed to prepare transaction: " + this, e); fut.onError(e); } } }); return fut; }', 'ground_truth': 'public IgniteFuture<IgniteTx> commitAsyncLocal() { if (log.isDebugEnabled()) log.debug("Committing colocated tx locally: " + this); if (pessimistic()) prepareAsync(); IgniteFuture<IgniteTxEx<K, V>> prep = prepFut.get(); if (F.isEmpty(dhtMap) && F.isEmpty(nearMap)) { if (prep != null) return (IgniteFuture<IgniteTx>)(IgniteFuture)prep; return new GridFinishedFuture<IgniteTx>(cctx.kernalContext(), this); } final GridDhtTxFinishFuture<K, V> fut = new GridDhtTxFinishFuture<>(cctx, this, /*commit*/true); cctx.mvcc().addFuture(fut); if (prep == null || prep.isDone()) { assert prep != null || optimistic(); try { if (prep != null) prep.get(); fut.finish(); } catch (IgniteTxOptimisticException e) { if (log.isDebugEnabled()) log.debug("Failed optimistically to prepare transaction [tx=" + this + ", e=" + e + \']\'); fut.onError(e); } catch (IgniteCheckedException e) { U.error(log, "Failed to prepare transaction: " + this, e); fut.onError(e); } } else prep.listenAsync(new CI1<IgniteFuture<IgniteTxEx<K, V>>>() { @Override public void apply(IgniteFuture<IgniteTxEx<K, V>> f) { try { f.get(); fut.finish(); } catch (IgniteTxOptimisticException e) { if (log.isDebugEnabled()) log.debug("Failed optimistically to prepare transaction [tx=" + this + ", e=" + e + \']\'); fut.onError(e); } catch (IgniteCheckedException e) { U.error(log, "Failed to prepare transaction: " + this, e); fut.onError(e); } } }); return fut; }', 'output': 'public IgniteFuture<IgniteTx> commitAsync() { if (log.isDebugEnabled()) log.debug("Committing colocated tx locally: " + this); if (pessimistic()) prepareAsync(); IgniteFuture<GridCacheTxEx<K, V>> prep = prepFut.get(); if (F.isEmpty(dhtMap) && F.isEmpty(nearMap)) { if (prep != null) return (IgniteFuture<IgniteTx>)prep; return new GridFinishedFuture<IgniteTx>(cctx.kernalContext(), this); } final GridDhtTxFinishFuture<K, V> fut = new GridDhtTxFinishFuture<>(cctx, this, /*commit*/true); cctx.mvcc().addFuture(fut); if (prep == null || prep.isDone()) { assert prep != null || optimistic(); try { if (prep != null) prep.get(); fut.finish(); } catch (IgniteTxOptimisticException e) { if (log.isDebugEnabled()) log.debug("Failed optimistically to prepare transaction [tx=" + this + ", e=" + e + \']\'); fut.onError(e); } catch (IgniteCheckedException e) { U.error(log, "Failed to prepare transaction: " + this, e); fut.onError(e); } } else prep.listenAsync(new CI1<IgniteFuture<GridCacheTxEx<K, V>>>() { @Override public void apply(IgniteFuture<GridCacheTxEx<K, V>> f) { try { f.get(); fut.finish(); } catch (IgniteTxOptimisticException e) { if (log.isDebugEnabled()) log.debug("Failed optimistically to prepare transaction [tx=" + this + ", e=" + e + \']\'); fut.onError(e); } catch (IgniteCheckedException e) { U.error(log, "Failed to prepare transaction: " + this, e); fut.onError(e); } } }); return fut; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMarshalUnmarshalCandidate() throws Exception { GridCacheAdapter<String, String> cache = grid.internalCache(); GridCacheTestEntryEx<String, String> parent = new GridCacheTestEntryEx<>(cache.context(), "1"); GridCacheMvccCandidate<String> cand = new GridCacheMvccCandidate<>( parent, UUID.randomUUID(), UUID.randomUUID(), version(1), 123, version(2), 123, /*local*/false, /*reentry*/false, true, false, false, false ); XXX marshaller = getTestResources().getMarshaller(); GridCacheMvccCandidate<String> unmarshalled = marshaller.unmarshal(marshaller.marshal(cand), null); info(unmarshalled.toString()); }', 'ground_truth': 'public void testMarshalUnmarshalCandidate() throws Exception { GridCacheAdapter<String, String> cache = grid.internalCache(); GridCacheTestEntryEx<String, String> parent = new GridCacheTestEntryEx<>(cache.context(), "1"); GridCacheMvccCandidate<String> cand = new GridCacheMvccCandidate<>( parent, UUID.randomUUID(), UUID.randomUUID(), version(1), 123, version(2), 123, /*local*/false, /*reentry*/false, true, false, false, false ); IgniteMarshaller marshaller = getTestResources().getMarshaller(); GridCacheMvccCandidate<String> unmarshalled = marshaller.unmarshal(marshaller.marshal(cand), null); info(unmarshalled.toString()); }', 'output': 'public void testMarshalUnmarshalCandidate() throws Exception { Cache<String, String> cache = internalCache(); CacheTestEntryEx<String, String> parent = new CacheTestEntryEx<>(cache.context(), "1"); CacheMvccCandidate<String> cand = new CacheMvccCandidate<>( parent, UUID.randomUUID(), UUID.randomUUID(), version(1), 123, version(2), 123, /*local*/false, /*reentry*/false, true, false, false, false ); Marshaller marshaller = getTestResources().getMarshaller(); CacheMvccCandidate<String> unmarshalled = marshaller.unmarshal(marshaller.marshal(cand), null); info(unmarshalled.toString()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void main(String[] args) throws Exception { try (Ignite ignite = Ignition.start("examples/config/example-ignite.xml")) { System.out.println(">>> Normalization example started."); IgniteCache<Integer, Person> persons = createCache(ignite); IgniteBiFunction<Integer, Person, double[]> featureExtractor = (k, v) -> new double[] { v.getAge(), v.getSalary() }; IgniteBiFunction<Integer, Person, double[]> preprocessor = new NormalizationTrainer<Integer, Person>() .withP(1) .fit(ignite, persons, featureExtractor); try (SimpleDataset<?> dataset = DatasetFactory.createSimpleDataset(ignite, persons, preprocessor)) { double[] mean = dataset.mean(); System.out.println("Mean \\n\\t" + Arrays.toString(mean)); double[] std = dataset.std(); System.out.println("Standard deviation \\n\\t" + Arrays.toString(std)); double[][] cov = dataset.cov(); System.out.println("Covariance matrix "); for (double[] row : cov) System.out.println("\\t" + Arrays.toString(row)); double[][] corr = dataset.corr(); System.out.println("Correlation matrix "); for (double[] row : corr) System.out.println("\\t" + Arrays.toString(row)); } System.out.println(">>> Normalization example completed."); } }', 'ground_truth': 'public static void main(String[] args) throws Exception { try (Ignite ignite = Ignition.start("examples/config/example-ignite.xml")) { System.out.println(">>> Normalization example started."); IgniteCache<Integer, Person> persons = createCache(ignite); IgniteBiFunction<Integer, Person, Vector> featureExtractor = (k, v) -> VectorUtils.of( v.getAge(), v.getSalary() ); IgniteBiFunction<Integer, Person, Vector> preprocessor = new NormalizationTrainer<Integer, Person>() .withP(1) .fit(ignite, persons, featureExtractor); try (SimpleDataset<?> dataset = DatasetFactory.createSimpleDataset(ignite, persons, preprocessor)) { double[] mean = dataset.mean(); System.out.println("Mean \\n\\t" + Arrays.toString(mean)); double[] std = dataset.std(); System.out.println("Standard deviation \\n\\t" + Arrays.toString(std)); double[][] cov = dataset.cov(); System.out.println("Covariance matrix "); for (double[] row : cov) System.out.println("\\t" + Arrays.toString(row)); double[][] corr = dataset.corr(); System.out.println("Correlation matrix "); for (double[] row : corr) System.out.println("\\t" + Arrays.toString(row)); } System.out.println(">>> Normalization example completed."); } }', 'output': 'public static void main(String[] args) throws Exception { try (Ignite ignite = Ignition.start("examples/config/example-ignite.xml")) { System.out.println(">>> Normalization example started."); IgniteCache<Integer, Person> persons = createCache(ignite); IgniteBiFunction<Integer, Person, double[]> featureExtractor = (k, v) -> new double[] { v.getAge(), v.getSalary() }; IgniteBiFunction<Integer, Person, double[]> preprocessor = new NormalizationTrainer<Integer, Person>() .withP(1) .fit(ignite, persons, featureExtractor); try (SimpleDataset<?> dataset = DatasetFactory.createSimpleDataset(ignite, persons, preprocessor)) { double[] mean = dataset.mean(); System.out.println("Mean \\n\\t" + Arrays.toString(mean)); double[] std = dataset.std(); System.out.println("Standard deviation \\n\\t" + Arrays.toString(std)); double[][] cov = dataset.cov(); System.out.println("Covariance matrix "); for (double[] row : cov) System.out.println("\\t" + Arrays.toString(row)); double[][] corr = dataset.corr(); System.out.println("Correlation matrix "); for (double[] row : corr) System.out.println("\\t" + Arrays.toString(row)); } System.out.println(">>> Normalization example completed."); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void updateScores() { if (!StorageService.instance.isInitialized()) return; if (!registered) { if (MessagingService.instance() != null) { MessagingService.instance().register(this); registered = true; } } double maxLatency = 1; for (Map.Entry<InetAddress, ExponentiallyDecayingSample> entry : samples.entrySet()) { double mean = entry.getValue().getSnapshot().getMedian(); if (mean > maxLatency) maxLatency = mean; } for (Map.Entry<InetAddress, ExponentiallyDecayingSample> entry: samples.entrySet()) { double score = entry.getValue().getSnapshot().getMedian() / maxLatency; score += StorageService.instance.getSeverity(entry.getKey()); scores.put(entry.getKey(), score); } }', 'ground_truth': 'private void updateScores() { if (!StorageService.instance.isInitialized()) return; if (!registered) { if (MessagingService.instance() != null) { MessagingService.instance().register(this); registered = true; } } double maxLatency = 1; for (Map.Entry<InetAddress, ExponentiallyDecayingReservoir> entry : samples.entrySet()) { double mean = entry.getValue().getSnapshot().getMedian(); if (mean > maxLatency) maxLatency = mean; } for (Map.Entry<InetAddress, ExponentiallyDecayingReservoir> entry: samples.entrySet()) { double score = entry.getValue().getSnapshot().getMedian() / maxLatency; score += StorageService.instance.getSeverity(entry.getKey()); scores.put(entry.getKey(), score); } }', 'output': 'private void updateScores() { if (!StorageService.instance.isInitialized()) return; if (!registered) { if (MessagingService.instance() != null) { MessagingService.instance().register(this); registered = true; } } double maxLatency = 1; for (Map.Entry<InetAddress, ExponentiallyDecayingSample> entry : samples.entrySet()) { double mean = entry.getValue().getSnapshot().getMedian(); if (mean > maxLatency) maxLatency = mean; } for (Map.Entry<InetAddress, ExponentiallyDecayingSample> entry: samples.entrySet()) { double score = entry.getValue().getSnapshot().getMedian() / maxLatency; score += StorageService.instance.getSeverity(entry.getKey()); scores.put(entry.getKey(), score); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static EditorContext getContext() { if (editorContext == null) { XXX l = DebuggerManager.getDebuggerManager().lookup(null, EditorContext.class); if (!l.isEmpty()) { editorContext = (EditorContext) l.get(0); } } return editorContext; }', 'ground_truth': 'private static EditorContext getContext() { if (editorContext == null) { List<? extends EditorContext> l = DebuggerManager.getDebuggerManager().lookup(null, EditorContext.class); if (!l.isEmpty()) { editorContext = l.get(0); } } return editorContext; }', 'output': 'private static EditorContext getContext() { if (editorContext == null) { EditorContext context = DebuggerManager.getDebuggerManager().lookup(null, EditorContext.class); if (!context.isEmpty()) { editorContext = context.get(0); } } return editorContext; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testRoundTrip() throws Exception { CompactionManager.instance.disableAutoCompaction(); Keyspace keyspace = Keyspace.open(KEYSPACE_CACHED); String cf = "CachedIntCF"; ColumnFamilyStore cachedStore  = keyspace.getColumnFamilyStore(cf); long startRowCacheHits = cachedStore.metric.partitionCacheHit.getCount(); long startRowCacheOutOfRange = cachedStore.metric.partitionCacheHitOutOfRange.getCount(); CacheService.instance.invalidateRowCache(); CacheService.instance.setRowCacheCapacityInMB(1); ByteBuffer key = ByteBufferUtil.bytes("rowcachekey"); DecoratedKey dk = cachedStore.partitioner.decorateKey(key); RowCacheKey rck = new RowCacheKey(cachedStore.metadata.cfId, dk); RowUpdateBuilder rub = new RowUpdateBuilder(cachedStore.metadata, System.currentTimeMillis(), key); rub.clustering(String.valueOf(0)); rub.add("val", ByteBufferUtil.bytes("val" + 0)); rub.build().applyUnsafe(); Util.getAll(Util.cmd(cachedStore, dk).withLimit(1).build()); assertEquals(startRowCacheHits, cachedStore.metric.partitionCacheHit.getCount()); Util.getAll(Util.cmd(cachedStore, dk).withLimit(1).build()); assertEquals(++startRowCacheHits, cachedStore.metric.partitionCacheHit.getCount()); assertEquals(startRowCacheOutOfRange, cachedStore.metric.partitionCacheHitOutOfRange.getCount()); CachedPartition cachedCf = (CachedPartition)CacheService.instance.rowCache.get(rck); assertEquals(1, cachedCf.rowCount()); for (Unfiltered unfiltered : Util.once(cachedCf.unfilteredIterator(ColumnFilter.selection(cachedCf.columns()), Slices.ALL, false))) { Row r = (Row) unfiltered; for (XXX c : r) { assertEquals(c.value(), ByteBufferUtil.bytes("val" + 0)); } } cachedStore.truncateBlocking(); }', 'ground_truth': 'public void testRoundTrip() throws Exception { CompactionManager.instance.disableAutoCompaction(); Keyspace keyspace = Keyspace.open(KEYSPACE_CACHED); String cf = "CachedIntCF"; ColumnFamilyStore cachedStore  = keyspace.getColumnFamilyStore(cf); long startRowCacheHits = cachedStore.metric.partitionCacheHit.getCount(); long startRowCacheOutOfRange = cachedStore.metric.partitionCacheHitOutOfRange.getCount(); CacheService.instance.invalidateRowCache(); CacheService.instance.setRowCacheCapacityInMB(1); ByteBuffer key = ByteBufferUtil.bytes("rowcachekey"); DecoratedKey dk = cachedStore.partitioner.decorateKey(key); RowCacheKey rck = new RowCacheKey(cachedStore.metadata.cfId, dk); RowUpdateBuilder rub = new RowUpdateBuilder(cachedStore.metadata, System.currentTimeMillis(), key); rub.clustering(String.valueOf(0)); rub.add("val", ByteBufferUtil.bytes("val" + 0)); rub.build().applyUnsafe(); Util.getAll(Util.cmd(cachedStore, dk).withLimit(1).build()); assertEquals(startRowCacheHits, cachedStore.metric.partitionCacheHit.getCount()); Util.getAll(Util.cmd(cachedStore, dk).withLimit(1).build()); assertEquals(++startRowCacheHits, cachedStore.metric.partitionCacheHit.getCount()); assertEquals(startRowCacheOutOfRange, cachedStore.metric.partitionCacheHitOutOfRange.getCount()); CachedPartition cachedCf = (CachedPartition)CacheService.instance.rowCache.get(rck); assertEquals(1, cachedCf.rowCount()); for (Unfiltered unfiltered : Util.once(cachedCf.unfilteredIterator(ColumnFilter.selection(cachedCf.columns()), Slices.ALL, false))) { Row r = (Row) unfiltered; for (ColumnData c : r) { assertEquals(((Cell)c).value(), ByteBufferUtil.bytes("val" + 0)); } } cachedStore.truncateBlocking(); }', 'output': 'public void testRoundTrip() throws Exception { CompactionManager.instance.disableAutoCompaction(); Keyspace keyspace = Keyspace.open(KEYSPACE_CACHED); String cf = "CachedIntCF"; ColumnFamilyStore cachedStore  = keyspace.getColumnFamilyStore(cf); long startRowCacheHits = cachedStore.metric.partitionCacheHit.getCount(); long startRowCacheOutOfRange = cachedStore.metric.partitionCacheHitOutOfRange.getCount(); CacheService.instance.invalidateRowCache(); CacheService.instance.setRowCacheCapacityInMB(1); ByteBuffer key = ByteBufferUtil.bytes("rowcachekey"); DecoratedKey dk = cachedStore.partitioner.decorateKey(key); RowCacheKey rck = new RowCacheKey(cachedStore.metadata.cfId, dk); RowUpdateBuilder rub = new RowUpdateBuilder(cachedStore.metadata, System.currentTimeMillis(), key); rub.clustering(String.valueOf(0)); rub.add("val", ByteBufferUtil.bytes("val" + 0)); rub.build().applyUnsafe(); Util.getAll(Util.cmd(cachedStore, dk).withLimit(1).build()); assertEquals(startRowCacheHits, cachedStore.metric.partitionCacheHit.getCount()); Util.getAll(Util.cmd(cachedStore, dk).withLimit(1).build()); assertEquals(++startRowCacheHits, cachedStore.metric.partitionCacheHit.getCount()); assertEquals(startRowCacheOutOfRange, cachedStore.metric.partitionCacheHitOutOfRange.getCount()); CachedPartition cachedCf = (CachedPartition)CacheService.instance.rowCache.get(rck); assertEquals(1, cachedCf.rowCount()); for (Unfiltered unfiltered : Util.once(cachedCf.unfilteredIterator(ColumnFilter.selection(cachedCf.columns()), Slices.ALL, false))) { Row r = (Row) unfiltered; for (Object c : r) { assertEquals(c.value(), ByteBufferUtil.bytes("val" + 0)); } } cachedStore.truncateBlocking(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void apply(UUID nodeId, M msg) { XXX node = cctx.node(nodeId); if (node == null) { if (log.isDebugEnabled()) log.debug("Received message from failed node [node=" + nodeId + ", msg=" + msg + \']\'); return; } if (log.isDebugEnabled()) log.debug("Received message from node [node=" + nodeId + ", msg=" + msg + \']\'); onMessage(node , msg); } /** * @param node Node. * @param msg Message. */ protected abstract void onMessage(XXX node, M msg); } } }', 'ground_truth': '@Override public void apply(UUID nodeId, M msg) { ClusterNode node = cctx.node(nodeId); if (node == null) { if (log.isDebugEnabled()) log.debug("Received message from failed node [node=" + nodeId + ", msg=" + msg + \']\'); return; } if (log.isDebugEnabled()) log.debug("Received message from node [node=" + nodeId + ", msg=" + msg + \']\'); onMessage(node , msg); }', 'output': '@Override public void apply(UUID nodeId, M msg) { ClusterNode node = cctx.node(nodeId); if (node == null) { if (log.isDebugEnabled()) log.debug("Received message from failed node [node=" + nodeId + ", msg=" + msg + \']\'); return; } if (log.isDebugEnabled()) log.debug("Received message from node [node=" + nodeId + ", msg=" + msg + \']\'); onMessage(node , msg); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void property_whenMethodNameIs_XYyyZzz() throws Exception { final Method method = SomeDomainClass.class.getMethod("getABigDecimal"); final XXX identifierFor = FeatureType.PROPERTY.identifierFor( TypeIdentifier.fqcn(SomeDomainClass.class), method); assertThat(identifierFor.getMemberName(), is("ABigDecimal")); }', 'ground_truth': 'public void property_whenMethodNameIs_XYyyZzz() throws Exception { final Method method = SomeDomainClass.class.getMethod("getABigDecimal"); final Identifier identifierFor = FeatureType.PROPERTY.identifierFor( TypeIdentifier.fqcn(SomeDomainClass.class), method); assertThat(identifierFor.getMemberName(), is("ABigDecimal")); }', 'output': 'public void property_whenMethodNameIs_XYyyZzz() throws Exception { final Method method = SomeDomainClass.class.getMethod("getABigDecimal"); final Identifier identifierFor = FeatureType.PROPERTY.identifierFor( TypeIdentifier.fqcn(SomeDomainClass.class), method); assertThat(identifierFor.getMemberName(), is("ABigDecimal")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private <E extends Tree> E fixGenericTypes(E tree, final TreePath path, final Element member) { final Map<TypeMirror, TypeParameterElement> mappings = new HashMap<TypeMirror, TypeParameterElement>(); DeclaredType declaredType = (DeclaredType) sourceType.asType(); for (TypeMirror typeMirror : declaredType.getTypeArguments()) { DeclaredType currentElement = declaredType; deepSearchTypes(currentElement, typeMirror, typeMirror, mappings); } final Types types = workingCopy.getTypes(); final Map<IdentifierTree, Tree> original2Translated = new HashMap<IdentifierTree, Tree>(); TreeScanner<Void, Void> scanner = new TreeScanner<Void, Void>() { @Override public Void visitIdentifier(IdentifierTree node, Void p) { Element element = workingCopy.getTrees().getElement(new TreePath(path, node)); if (element != null && element.getKind() == ElementKind.TYPE_PARAMETER) { Element typeElement = types.asElement(element.asType()); if (typeElement != null && typeElement.getKind() == ElementKind.TYPE_PARAMETER) { TypeParameterElement parameterElement = (TypeParameterElement) typeElement; Element genericElement = parameterElement.getGenericElement(); if (genericElement != member) { Tree type; TypeParameterElement target = mappings.get(parameterElement.asType()); if (target != null) { type = make.Type(target.asType()); } else { List<? extends TypeMirror> bounds = parameterElement.getBounds(); if (bounds.isEmpty()) { type = make.Type("Object"); } else { type = make.Type(bounds.get(0)); } } original2Translated.put(node, type); } } } return super.visitIdentifier(node, p); } }; scanner.scan(tree, null); E result = (E) workingCopy.getTreeUtilities().translate(tree, original2Translated); return result; }', 'ground_truth': 'private <E extends Tree> E fixGenericTypes(E tree, final TreePath path, final Element member) { final Map<TypeMirror, TypeParameterElement> mappings = new HashMap<TypeMirror, TypeParameterElement>(); DeclaredType declaredType = (DeclaredType) sourceType.asType(); for (TypeMirror typeMirror : declaredType.getTypeArguments()) { DeclaredType currentElement = declaredType; deepSearchTypes(currentElement, typeMirror, typeMirror, mappings); } final Types types = workingCopy.getTypes(); final Map<IdentifierTree, Tree> original2Translated = new HashMap<IdentifierTree, Tree>(); ErrorAwareTreeScanner<Void, Void> scanner = new ErrorAwareTreeScanner<Void, Void>() { @Override public Void visitIdentifier(IdentifierTree node, Void p) { Element element = workingCopy.getTrees().getElement(new TreePath(path, node)); if (element != null && element.getKind() == ElementKind.TYPE_PARAMETER) { Element typeElement = types.asElement(element.asType()); if (typeElement != null && typeElement.getKind() == ElementKind.TYPE_PARAMETER) { TypeParameterElement parameterElement = (TypeParameterElement) typeElement; Element genericElement = parameterElement.getGenericElement(); if (genericElement != member) { Tree type; TypeParameterElement target = mappings.get(parameterElement.asType()); if (target != null) { type = make.Type(target.asType()); } else { List<? extends TypeMirror> bounds = parameterElement.getBounds(); if (bounds.isEmpty()) { type = make.Type("Object"); } else { type = make.Type(bounds.get(0)); } } original2Translated.put(node, type); } } } return super.visitIdentifier(node, p); } }; scanner.scan(tree, null); E result = (E) workingCopy.getTreeUtilities().translate(tree, original2Translated); return result; }', 'output': 'private <E extends Tree> E fixGenericTypes(E tree, final TreePath path, final Element member) { final Map<TypeMirror, TypeParameterElement> mappings = new HashMap<TypeMirror, TypeParameterElement>(); DeclaredType declaredType = (DeclaredType) sourceType.asType(); for (TypeMirror typeMirror : declaredType.getTypeArguments()) { DeclaredType currentElement = declaredType; deepSearchTypes(currentElement, typeMirror, typeMirror, mappings); } final Types types = workingCopy.getTypes(); final Map<IdentifierTree, Tree> original2Translated = new HashMap<IdentifierTree, Tree>(); TreeScanner<Void, Void> scanner = new TreeScanner<Void, Void>() { @Override public Void visitIdentifier(IdentifierTree node, Void p) { Element element = workingCopy.getTrees().getElement(new TreePath(path, node)); if (element != null && element.getKind() == ElementKind.TYPE_PARAMETER) { Element typeElement = types.asElement(element.asType()); if (typeElement != null && typeElement.getKind() == ElementKind.TYPE_PARAMETER) { TypeParameterElement parameterElement = (TypeParameterElement) typeElement; Element genericElement = parameterElement.getGenericElement(); if (genericElement != member) { Tree type; TypeParameterElement target = mappings.get(parameterElement.asType()); if (target != null) { type = make.Type(target.asType()); } else { List<? extends TypeMirror> bounds = parameterElement.getBounds(); if (bounds.isEmpty()) { type = make.Type("Object"); } else { type = make.Type(bounds.get(0)); } } original2Translated.put(node, type); } } } return super.visitIdentifier(node, p); } }; scanner.scan(tree, null); E result = (E) workingCopy.getTreeUtilities().translate(tree, original2Translated); return result; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testStopWhenDisconnected() throws Exception { clientMode = true; Ignite client = startGrid(serverCount()); assertTrue(client.cluster().localNode().isClient()); Ignite srv = clientRouter(client); TestTcpDiscoverySpi srvSpi = spi(srv); final CountDownLatch disconnectLatch = new CountDownLatch(1); final CountDownLatch reconnectLatch = new CountDownLatch(1); final XXX clientSpi = spi(client); log.info("Block reconnect."); clientSpi.writeLatch = new CountDownLatch(1); client.events().localListen(new IgnitePredicate<Event>() { @Override public boolean apply(Event evt) { if (evt.type() == EVT_CLIENT_NODE_DISCONNECTED) { info("Disconnected: " + evt); disconnectLatch.countDown(); } else if (evt.type() == EVT_CLIENT_NODE_RECONNECTED) { info("Reconnected: " + evt); reconnectLatch.countDown(); } return true; } }, EVT_CLIENT_NODE_DISCONNECTED, EVT_CLIENT_NODE_RECONNECTED); srvSpi.failNode(client.cluster().localNode().id(), null); waitReconnectEvent(disconnectLatch); IgniteFuture<?> reconnectFut = null; try { client.getOrCreateCache(new CacheConfiguration<>(DEFAULT_CACHE_NAME)); fail(); } catch (IgniteClientDisconnectedException e) { log.info("Expected operation exception: " + e); reconnectFut = e.reconnectFuture(); } assertNotNull(reconnectFut); client.close(); try { reconnectFut.get(); fail(); } catch (IgniteException e) { log.info("Expected reconnect exception: " + e); } }', 'ground_truth': 'public void testStopWhenDisconnected() throws Exception { clientMode = true; Ignite client = startGrid(serverCount()); assertTrue(client.cluster().localNode().isClient()); Ignite srv = clientRouter(client); DiscoverySpi srvSpi = spi0(srv); final CountDownLatch disconnectLatch = new CountDownLatch(1); final CountDownLatch reconnectLatch = new CountDownLatch(1); final IgniteDiscoverySpi clientSpi = spi0(client); DiscoverySpiTestListener lsnr = new DiscoverySpiTestListener(); clientSpi.setInternalListener(lsnr); log.info("Block reconnect."); lsnr.startBlockJoin(); client.events().localListen(new IgnitePredicate<Event>() { @Override public boolean apply(Event evt) { if (evt.type() == EVT_CLIENT_NODE_DISCONNECTED) { info("Disconnected: " + evt); disconnectLatch.countDown(); } else if (evt.type() == EVT_CLIENT_NODE_RECONNECTED) { info("Reconnected: " + evt); reconnectLatch.countDown(); } return true; } }, EVT_CLIENT_NODE_DISCONNECTED, EVT_CLIENT_NODE_RECONNECTED); srvSpi.failNode(client.cluster().localNode().id(), null); waitReconnectEvent(disconnectLatch); IgniteFuture<?> reconnectFut = null; try { client.getOrCreateCache(new CacheConfiguration<>(DEFAULT_CACHE_NAME)); fail(); } catch (IgniteClientDisconnectedException e) { log.info("Expected operation exception: " + e); reconnectFut = e.reconnectFuture(); } assertNotNull(reconnectFut); client.close(); try { reconnectFut.get(); fail(); } catch (IgniteException e) { log.info("Expected reconnect exception: " + e); } }', 'output': 'public void testStopWhenDisconnected() throws Exception { clientMode = true; Ignite client = startGrid(serverCount()); assertTrue(client.cluster().localNode().isClient()); Ignite srv = clientRouter(client); TestTcpDiscoverySpi srvSpi = spi(srv); final CountDownLatch disconnectLatch = new CountDownLatch(1); final CountDownLatch reconnectLatch = new CountDownLatch(1); final IgniteClientSpi clientSpi = spi(client); log.info("Block reconnect."); clientSpi.writeLatch = new CountDownLatch(1); client.events().localListen(new IgnitePredicate<Event>() { @Override public boolean apply(Event evt) { if (evt.type() == EVT_CLIENT_NODE_DISCONNECTED) { info("Disconnected: " + evt); disconnectLatch.countDown(); } else if (evt.type() == EVT_CLIENT_NODE_RECONNECTED) { info("Reconnected: " + evt); reconnectLatch.countDown(); } return true; } }, EVT_CLIENT_NODE_DISCONNECTED, EVT_CLIENT_NODE_RECONNECTED); srvSpi.failNode(client.cluster().localNode().id(), null); waitReconnectEvent(disconnectLatch); IgniteFuture<?> reconnectFut = null; try { client.getOrCreateCache(new CacheConfiguration<>(DEFAULT_CACHE_NAME)); fail(); } catch (IgniteClientDisconnectedException e) { log.info("Expected operation exception: " + e); reconnectFut = e.reconnectFuture(); } assertNotNull(reconnectFut); client.close(); try { reconnectFut.get(); fail(); } catch (IgniteException e) { log.info("Expected reconnect exception: " + e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static String createMessage(int seconds) { XXX debuggerPort = PhpOptions.getInstance().getDebuggerPort(); final String entry1 = "<li>"+NbBundle.getMessage(ConnectionErrMessage.class, "MSG_ErrDebugSessionEntry1")+"</li>"; final String entry2 = "<li>"+NbBundle.getMessage(ConnectionErrMessage.class, "MSG_ErrDebugSessionEntry2")+"</li>"; final String entry3 = "<li>"+NbBundle.getMessage(ConnectionErrMessage.class, "MSG_ErrDebugSessionEntry3")+"</li>"; final String entry4 = "<li>"+NbBundle.getMessage(ConnectionErrMessage.class, "MSG_ErrDebugSessionEntry4", String.valueOf(debuggerPort))+"</li>"; final String entries = "<ul>"+entry1+entry2+entry3+entry4+"</ul>"; return "<html>"+NbBundle.getMessage(ConnectionErrMessage.class, "MSG_ErrDebugSession", seconds,entries)+"</html>"; }', 'ground_truth': 'private static String createMessage(int seconds) { String debuggerPort = String.valueOf(PhpOptions.getInstance().getDebuggerPort()); StringBuilder sb = new StringBuilder(); sb.append("<h4>Xdebug 2</h4>"); sb.append("<ul>"); sb.append("<li>").append("xdebug.remote_enable=on").append("</li>"); sb.append("<li>").append("xdebug.remote_handler=dbgp").append("</li>"); sb.append("<li>").append(Bundle.ConnectionErrMessage_xdebug_remote_host()).append("</li>"); sb.append("<li>").append(Bundle.ConnectionErrMessage_xdebug_remote_port(debuggerPort)).append("</li>"); sb.append("</ul>"); sb.append("<h4>Xdebug 3</h4>"); sb.append("<ul>"); sb.append("<li>").append("xdebug.mode=debug").append("</li>"); sb.append("<li>").append(Bundle.ConnectionErrMessage_xdebug_client_host()).append("</li>"); sb.append("<li>").append(Bundle.ConnectionErrMessage_xdebug_client_port(debuggerPort)).append("</li>"); sb.append("</ul>"); return "<html>" + NbBundle.getMessage(ConnectionErrMessage.class, "MSG_ErrDebugSession", seconds, sb.toString()) + "</html>"; }', 'output': 'private static String createMessage(int seconds) { int debuggerPort = PhpOptions.getInstance().getDebuggerPort(); final String entry1 = "<li>"+NbBundle.getMessage(ConnectionErrMessage.class, "MSG_ErrDebugSessionEntry1")+"</li>"; final String entry2 = "<li>"+NbBundle.getMessage(ConnectionErrMessage.class, "MSG_ErrDebugSessionEntry2")+"</li>"; final String entry3 = "<li>"+NbBundle.getMessage(ConnectionErrMessage.class, "MSG_ErrDebugSessionEntry3")+"</li>"; final String entry4 = "<li>"+NbBundle.getMessage(ConnectionErrMessage.class, "MSG_ErrDebugSessionEntry4", String.valueOf(debuggerPort))+"</li>"; final String entries = "<ul>"+entry1+entry2+entry3+entry4+"</ul>"; return "<html>"+NbBundle.getMessage(ConnectionErrMessage.class, "MSG_ErrDebugSession", seconds,entries)+"</html>"; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private long testBody() { int MAX = 20000000; Collection<Integer> l = new ArrayList<>(MAX); for (int i = 0; i < MAX / 10; i++) l.add(i); IgniteClosure<Integer, Integer> c = new IgniteClosure<Integer, Integer>() { @Override public Integer apply(Integer e) { return e; } }; GridPredicate<Integer> p1 = new GridPredicate<Integer>() { @Override public boolean apply(Integer e) { return e % 2 == 0; } }; XXX p2 = new GridPredicate<Integer>() { @Override public boolean apply(Integer e) { return e % 2 != 0; } }; GridIterator<Integer> iter = F.iterator(l, c, true, p1, p2); long n = 0; long start = System.currentTimeMillis(); for (Integer i : iter) n += i; long duration = System.currentTimeMillis() - start; System.out.println("Duration: " + duration + "msec."); return duration; }', 'ground_truth': 'private long testBody() { int MAX = 20000000; Collection<Integer> l = new ArrayList<>(MAX); for (int i = 0; i < MAX / 10; i++) l.add(i); IgniteClosure<Integer, Integer> c = new IgniteClosure<Integer, Integer>() { @Override public Integer apply(Integer e) { return e; } }; IgnitePredicate<Integer> p1 = new IgnitePredicate<Integer>() { @Override public boolean apply(Integer e) { return e % 2 == 0; } }; IgnitePredicate<Integer> p2 = new IgnitePredicate<Integer>() { @Override public boolean apply(Integer e) { return e % 2 != 0; } }; GridIterator<Integer> iter = F.iterator(l, c, true, p1, p2); long n = 0; long start = System.currentTimeMillis(); for (Integer i : iter) n += i; long duration = System.currentTimeMillis() - start; System.out.println("Duration: " + duration + "msec."); return duration; }', 'output': 'private long testBody() { int MAX = 20000000; Collection<Integer> l = new ArrayList<>(MAX); for (int i = 0; i < MAX / 10; i++) l.add(i); IgniteClosure<Integer, Integer> c = new IgniteClosure<Integer, Integer>() { @Override public Integer apply(Integer e) { return e; } }; GridPredicate<Integer> p1 = new GridPredicate<Integer>() { @Override public boolean apply(Integer e) { return e % 2 == 0; } }; GridPredicate<Integer> p2 = new GridPredicate<Integer>() { @Override public boolean apply(Integer e) { return e % 2 != 0; } }; GridIterator<Integer> iter = F.iterator(l, c, true, p1, p2); long n = 0; long start = System.currentTimeMillis(); for (Integer i : iter) n += i; long duration = System.currentTimeMillis() - start; System.out.println("Duration: " + duration + "msec."); return duration; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected String computeDigest() { StringWriter sw = new StringWriter(); XXX pw = new RelOptPlanWriter( new PrintWriter(sw), SqlExplainLevel.DIGEST_ATTRIBUTES) { protected void explain_( RelNode rel, List<Pair<String, Object>> values) { pw.write(getRelTypeName()); for (RelTrait trait : traitSet) { pw.write("."); pw.write(trait.toString()); } pw.write("("); int j = 0; for (Pair<String, Object> value : values) { if (j++ > 0) { pw.write(","); } pw.write(value.left + "=" + value.right); } pw.write(")"); } }; explain(pw); return sw.toString(); }', 'ground_truth': 'protected String computeDigest() { StringWriter sw = new StringWriter(); RelWriter pw = new RelWriterImpl( new PrintWriter(sw), SqlExplainLevel.DIGEST_ATTRIBUTES, false) { protected void explain_( RelNode rel, List<Pair<String, Object>> values) { pw.write(getRelTypeName()); for (RelTrait trait : traitSet) { pw.write("."); pw.write(trait.toString()); } pw.write("("); int j = 0; for (Pair<String, Object> value : values) { if (j++ > 0) { pw.write(","); } pw.write(value.left + "=" + value.right); } pw.write(")"); } }; explain(pw); return sw.toString(); }', 'output': 'protected String computeDigest() { StringWriter sw = new StringWriter(); RelWriter rw = new RelWriter(new PrintWriter(sw), SqlExplainLevel.DIGEST_ATTRIBUTES) { protected void explain_( RelNode rel, List<Pair<String, Object>> values) { rw.write(getRelTypeName()); for (RelTrait trait : traitSet) { rw.write("."); rw.write(trait.toString()); } rw.write("("); int j = 0; for (Pair<String, Object> value : values) { if (j++ > 0) { rw.write(","); } rw.write(value.left + "=" + value.right); } rw.write(")"); } }; explain(rw); return sw.toString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void requestTime(UUID rmtNodeId) { XXX node = ctx.discovery().node(rmtNodeId); if (node != null) { InetAddress addr = node.attribute(ATTR_TIME_SERVER_HOST); int port = node.attribute(ATTR_TIME_SERVER_PORT); try { GridClockMessage req = new GridClockMessage(ctx.localNodeId(), rmtNodeId, currentTime(), 0); srv.sendPacket(req, addr, port); } catch (GridException e) { LT.warn(log, e, "Failed to send time request to remote node [rmtNodeId=" + rmtNodeId + ", addr=" + addr + ", port=" + port + \']\'); } } else onNodeLeft(rmtNodeId); }', 'ground_truth': 'private void requestTime(UUID rmtNodeId) { ClusterNode node = ctx.discovery().node(rmtNodeId); if (node != null) { InetAddress addr = node.attribute(ATTR_TIME_SERVER_HOST); int port = node.attribute(ATTR_TIME_SERVER_PORT); try { GridClockMessage req = new GridClockMessage(ctx.localNodeId(), rmtNodeId, currentTime(), 0); srv.sendPacket(req, addr, port); } catch (GridException e) { LT.warn(log, e, "Failed to send time request to remote node [rmtNodeId=" + rmtNodeId + ", addr=" + addr + ", port=" + port + \']\'); } } else onNodeLeft(rmtNodeId); }', 'output': 'private void requestTime(UUID rmtNodeId) { ClusterNode node = ctx.discovery().node(rmtNodeId); if (node != null) { InetAddress addr = node.attribute(ATTR_TIME_SERVER_HOST); int port = node.attribute(ATTR_TIME_SERVER_PORT); try { GridClockMessage req = new GridClockMessage(ctx.localNodeId(), rmtNodeId, currentTime(), 0); srv.sendPacket(req, addr, port); } catch (GridException e) { LT.warn(log, e, "Failed to send time request to remote node [rmtNodeId=" + rmtNodeId + ", addr=" + addr + ", port=" + port + \']\'); } } else onNodeLeft(rmtNodeId); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void rebuildDiff(AffinityAssignment affAssignment) { assert lock.isWriteLockedByCurrentThread(); if (node2part == null) return; if (FAST_DIFF_REBUILD) { Collection<UUID> affNodes = F.nodeIds(ctx.discovery().cacheGroupAffinityNodes(grp.groupId(), affAssignment.topologyVersion())); for (Map.Entry<Integer, Set<UUID>> e : diffFromAffinity.entrySet()) { int p = e.getKey(); Iterator<UUID> iter = e.getValue().iterator(); while (iter.hasNext()) { UUID nodeId = iter.next(); if (!affNodes.contains(nodeId) || affAssignment.getIds(p).contains(nodeId)) iter.remove(); } } } else { for (Map.Entry<UUID, GridDhtPartitionMap> e : node2part.entrySet()) { UUID nodeId = e.getKey(); for (Map.Entry<Integer, GridDhtPartitionState> e0 : e.getValue().entrySet()) { XXX p0 = e0.getKey(); GridDhtPartitionState state = e0.getValue(); Set<UUID> ids = diffFromAffinity.get(p0); if ((state == MOVING || state == OWNING) && !affAssignment.getIds(p0).contains(nodeId)) { if (ids == null) diffFromAffinity.put(p0, ids = U.newHashSet(3)); ids.add(nodeId); } else { if (ids != null) ids.remove(nodeId); } } } } diffFromAffinityVer = affAssignment.topologyVersion(); }', 'ground_truth': 'private void rebuildDiff(AffinityAssignment affAssignment) { assert lock.isWriteLockedByCurrentThread(); if (node2part == null) return; if (FAST_DIFF_REBUILD) { Collection<UUID> affNodes = F.nodeIds(ctx.discovery().cacheGroupAffinityNodes(grp.groupId(), affAssignment.topologyVersion())); for (Map.Entry<Integer, Set<UUID>> e : diffFromAffinity.entrySet()) { int p = e.getKey(); Iterator<UUID> iter = e.getValue().iterator(); while (iter.hasNext()) { UUID nodeId = iter.next(); if (!affNodes.contains(nodeId) || affAssignment.getIds(p).contains(nodeId)) iter.remove(); } } } else { for (Map.Entry<UUID, GridDhtPartitionMap> e : node2part.entrySet()) { UUID nodeId = e.getKey(); for (Map.Entry<Integer, GridDhtPartitionState> e0 : e.getValue().entrySet()) { Integer p0 = e0.getKey(); GridDhtPartitionState state = e0.getValue(); Set<UUID> ids = diffFromAffinity.get(p0); if ((state == MOVING || state == OWNING || state == RENTING) && !affAssignment.getIds(p0).contains(nodeId)) { if (ids == null) diffFromAffinity.put(p0, ids = U.newHashSet(3)); ids.add(nodeId); } else { if (ids != null) ids.remove(nodeId); } } } } diffFromAffinityVer = affAssignment.topologyVersion(); }', 'output': 'private void rebuildDiff(AffinityAssignment affAssignment) { assert lock.isWriteLockedByCurrentThread(); if (node2part == null) return; if (FAST_DIFF_REBUILD) { Collection<UUID> affNodes = F.nodeIds(ctx.discovery().cacheGroupAffinityNodes(grp.groupId(), affAssignment.topologyVersion())); for (Map.Entry<Integer, Set<UUID>> e : diffFromAffinity.entrySet()) { int p = e.getKey(); Iterator<UUID> iter = e.getValue().iterator(); while (iter.hasNext()) { UUID nodeId = iter.next(); if (!affNodes.contains(nodeId) || affAssignment.getIds(p).contains(nodeId)) iter.remove(); } } } else { for (Map.Entry<UUID, GridDhtPartitionMap> e : node2part.entrySet()) { UUID nodeId = e.getKey(); for (Map.Entry<Integer, GridDhtPartitionState> e0 : e.getValue().entrySet()) { int p0 = e0.getKey(); GridDhtPartitionState state = e0.getValue(); Set<UUID> ids = diffFromAffinity.get(p0); if ((state == MOVING || state == OWNING) && !affAssignment.getIds(p0).contains(nodeId)) { if (ids == null) diffFromAffinity.put(p0, ids = U.newHashSet(3)); ids.add(nodeId); } else { if (ids != null) ids.remove(nodeId); } } } } diffFromAffinityVer = affAssignment.topologyVersion(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': "public void disabledTestStartSingleInstanceSpi() throws Exception { GridConfiguration cfg1 = getConfiguration(); XXX cfg2 = getConfiguration(); cfg1.setCollisionSpi(new TestSingleInstancesCollisionSpi()); cfg2.setCollisionSpi(new TestSingleInstancesCollisionSpi()); G.start(cfg1); assert G.state(cfg1.getGridName()) == STARTED; assert G.state(getTestGridName() + '1') == STOPPED; G.stop(cfg1.getGridName(), false); assert G.state(cfg1.getGridName()) == STOPPED; assert G.state(getTestGridName() + '1') == STOPPED; cfg2.setGridName(getTestGridName() + '1'); G.start(cfg2); assert G.state(cfg1.getGridName()) == STOPPED; assert G.state(getTestGridName() + '1') == STARTED; G.stop(getTestGridName() + '1', false); assert G.state(cfg1.getGridName()) == STOPPED; assert G.state(getTestGridName() + '1') == STOPPED; cfg2.setGridName(getTestGridName() + '1'); G.start(cfg2); assert G.state(getTestGridName() + '1') == STARTED; assert G.state(getTestGridName()) == STOPPED; G.stop(getTestGridName() + '1', false); G.stop(getTestGridName(), false); assert G.state(getTestGridName() + '1') == STOPPED; assert G.state(getTestGridName()) == STOPPED; }", 'ground_truth': "public void disabledTestStartSingleInstanceSpi() throws Exception { IgniteConfiguration cfg1 = getConfiguration(); IgniteConfiguration cfg2 = getConfiguration(); cfg1.setCollisionSpi(new TestSingleInstancesCollisionSpi()); cfg2.setCollisionSpi(new TestSingleInstancesCollisionSpi()); G.start(cfg1); assert G.state(cfg1.getGridName()) == STARTED; assert G.state(getTestGridName() + '1') == STOPPED; G.stop(cfg1.getGridName(), false); assert G.state(cfg1.getGridName()) == STOPPED; assert G.state(getTestGridName() + '1') == STOPPED; cfg2.setGridName(getTestGridName() + '1'); G.start(cfg2); assert G.state(cfg1.getGridName()) == STOPPED; assert G.state(getTestGridName() + '1') == STARTED; G.stop(getTestGridName() + '1', false); assert G.state(cfg1.getGridName()) == STOPPED; assert G.state(getTestGridName() + '1') == STOPPED; cfg2.setGridName(getTestGridName() + '1'); G.start(cfg2); assert G.state(getTestGridName() + '1') == STARTED; assert G.state(getTestGridName()) == STOPPED; G.stop(getTestGridName() + '1', false); G.stop(getTestGridName(), false); assert G.state(getTestGridName() + '1') == STOPPED; assert G.state(getTestGridName()) == STOPPED; }", 'output': "public void disabledTestStartSingleInstanceSpi() throws Exception { IgniteConfiguration cfg1 = getConfiguration(); IgniteConfiguration cfg2 = getConfiguration(); cfg1.setCollisionSpi(new TestSingleInstancesCollisionSpi()); cfg2.setCollisionSpi(new TestSingleInstancesCollisionSpi()); G.start(cfg1); assert G.state(cfg1.getGridName()) == STARTED; assert G.state(getTestGridName() + '1') == STOPPED; G.stop(cfg1.getGridName(), false); assert G.state(cfg1.getGridName()) == STOPPED; assert G.state(getTestGridName() + '1') == STOPPED; cfg2.setGridName(getTestGridName() + '1'); G.start(cfg2); assert G.state(getTestGridName() + '1') == STARTED; assert G.state(getTestGridName()) == STOPPED; G.stop(getTestGridName() + '1', false); assert G.state(cfg1.getGridName()) == STOPPED; assert G.state(getTestGridName() + '1') == STOPPED; cfg2.setGridName(getTestGridName() + '1'); G.start(cfg2); assert G.state(getTestGridName() + '1') == STARTED; assert G.state(getTestGridName()) == STOPPED; G.stop(getTestGridName() + '1', false); G.stop(getTestGridName(), false); assert G.state(getTestGridName() + '1') == STOPPED; assert G.state(getTestGridName()) == STOPPED; }</s>"}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean sampling(Boolean val) throws IgniteCheckedException { if (busyLock.enterBusy()) { try { validTxState(false); XXX tx = metaCache.txStart(PESSIMISTIC, REPEATABLE_READ); try { Object prev = val != null ? metaCache.put(sampling, val) : metaCache.remove(sampling); tx.commit(); return !F.eq(prev, val); } finally { tx.close(); } } finally { busyLock.leaveBusy(); } } else throw new IllegalStateException("Failed to set sampling flag because Grid is stopping."); }', 'ground_truth': 'public boolean sampling(Boolean val) throws IgniteCheckedException { if (busyLock.enterBusy()) { try { validTxState(false); IgniteTxEx tx = metaCache.txStartEx(PESSIMISTIC, REPEATABLE_READ); try { Object prev = val != null ? metaCache.put(sampling, val) : metaCache.remove(sampling); tx.commit(); return !F.eq(prev, val); } finally { tx.close(); } } finally { busyLock.leaveBusy(); } } else throw new IllegalStateException("Failed to set sampling flag because Grid is stopping."); }', 'output': 'public boolean sampling(Boolean val) throws IgniteCheckedException { if (busyLock.enterBusy()) { try { validTxState(false); IgniteTx tx = metaCache.txStart(PESSIMISTIC, REPEATABLE_READ); try { Object prev = val != null ? metaCache.put(sampling, val) : metaCache.remove(sampling); tx.commit(); return !F.eq(prev, val); } finally { tx.close(); } } finally { busyLock.leaveBusy(); } } else throw new IllegalStateException("Failed to set sampling flag because Grid is stopping."); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Consent isUsable( final ManagedObject mixedInAdapter, final InteractionInitiatedBy interactionInitiatedBy, final Where where) { final ManagedObject mixinAdapter = mixinAdapterFor(mixinType, mixedInAdapter); final XXX ic = mixinAction.createUsableInteractionContext(mixinAdapter, interactionInitiatedBy, where); ic.setMixedIn(mixedInAdapter); return InteractionUtils.isUsableResult(this, ic).createConsent(); }', 'ground_truth': 'public Consent isUsable( final ManagedObject mixedInAdapter, final InteractionInitiatedBy interactionInitiatedBy, final Where where) { final ManagedObject mixinAdapter = mixinAdapterFor(mixinType, mixedInAdapter); final UsabilityContext ic = mixinAction.createUsableInteractionContext(mixinAdapter, interactionInitiatedBy, where); ic.setMixedIn(mixedInAdapter); return InteractionUtils.isUsableResult(this, ic).createConsent(); }', 'output': 'public Consent isUsable( final ManagedObject mixedInAdapter, final InteractionInitiatedBy interactionInitiatedBy, final Where where) { final ManagedObject mixinAdapter = mixinAdapterFor(mixinType, mixedInAdapter); final UsableInteractionContext ic = mixinAction.createUsableInteractionContext(mixinAdapter, interactionInitiatedBy, where); ic.setMixedIn(mixedInAdapter); return InteractionUtils.isUsableResult(this, ic).createConsent(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void removeQueryResult(@Nullable UUID sndId, long reqId) { if (sndId == null) return; Map<Long, GridFutureAdapter<QueryResult<K, V>>> futs = qryIters.get(sndId); if (futs != null) { GridFuture<QueryResult<K, V>> fut; synchronized (futs) { fut = futs.remove(reqId); } if (fut != null) { try { fut.get().closeIfNotShared(recipient(sndId, reqId)); } catch (GridException e) { U.error(log, "Failed to close iterator.", e); } } } }', 'ground_truth': 'protected void removeQueryResult(@Nullable UUID sndId, long reqId) { if (sndId == null) return; Map<Long, GridFutureAdapter<QueryResult<K, V>>> futs = qryIters.get(sndId); if (futs != null) { IgniteFuture<QueryResult<K, V>> fut; synchronized (futs) { fut = futs.remove(reqId); } if (fut != null) { try { fut.get().closeIfNotShared(recipient(sndId, reqId)); } catch (GridException e) { U.error(log, "Failed to close iterator.", e); } } } }', 'output': 'protected void removeQueryResult(@Nullable UUID sndId, long reqId) { if (sndId == null) return; Map<Long, GridFutureAdapter<QueryResult<K, V>>> futs = qryIters.get(sndId); if (futs != null) { GridFuture<QueryResult<K, V>> fut; synchronized (futs) { fut = futs.remove(reqId); } if (fut != null) { try { fut.get().closeIfNotShared(recipient(sndId, reqId)); } catch (GridException e) { U.error(log, "Failed to close iterator.", e); } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void printThreadPoolStats(PrintStream outs) { ObjectName query; try { query = new ObjectName("org.apache.cassandra.concurrent:type=*"); Set<ObjectName> result = mbeanServerConn.queryNames(query, null); for (ObjectName objectName : result) { String poolName = objectName.getKeyProperty("type"); XXX threadPoolProxy = JMX.newMBeanProxy(mbeanServerConn, objectName, DebuggableThreadPoolExecutorMBean.class); outs.println(poolName + ", pending tasks=" + threadPoolProxy.getPendingTasks()); } } catch (MalformedObjectNameException e) { throw new RuntimeException("Invalid ObjectName? Please report this as a bug.", e); } catch (IOException e) { throw new RuntimeException("Could not retrieve list of stat mbeans.", e); } }', 'ground_truth': 'public void printThreadPoolStats(PrintStream outs) { ObjectName query; try { query = new ObjectName("org.apache.cassandra.concurrent:type=*"); Set<ObjectName> result = mbeanServerConn.queryNames(query, null); for (ObjectName objectName : result) { String poolName = objectName.getKeyProperty("type"); IExecutorMBean threadPoolProxy = JMX.newMBeanProxy(mbeanServerConn, objectName, IExecutorMBean.class); outs.println(poolName + ", pending tasks=" + threadPoolProxy.getPendingTasks()); } } catch (MalformedObjectNameException e) { throw new RuntimeException("Invalid ObjectName? Please report this as a bug.", e); } catch (IOException e) { throw new RuntimeException("Could not retrieve list of stat mbeans.", e); } }', 'output': 'public void printThreadPoolStats(PrintStream outs) { ObjectName query; try { query = new ObjectName("org.apache.cassandra.concurrent:type=*"); Set<ObjectName> result = mbeanServerConn.queryNames(query, null); for (ObjectName objectName : result) { String poolName = objectName.getKeyProperty("type"); DebuggableThreadPoolExecutorMBean threadPoolProxy = JMX.newMBeanProxy(mbeanServerConn, objectName, DebuggableThreadPoolExecutorMBean.class); outs.println(poolName + ", pending tasks=" + threadPoolProxy.getPendingTasks()); } } catch (MalformedObjectNameException e) { throw new RuntimeException("Invalid ObjectName? Please report this as a bug.", e); } catch (IOException e) { throw new RuntimeException("Could not retrieve list of stat mbeans.", e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override protected IgniteAtomicConfiguration atomicConfiguration() { XXX atomicCfg = super.atomicConfiguration(); atomicCfg.setBackups(1); return atomicCfg; } } }', 'ground_truth': '@Override protected AtomicConfiguration atomicConfiguration() { AtomicConfiguration atomicCfg = super.atomicConfiguration(); atomicCfg.setBackups(1); return atomicCfg; }', 'output': '@Override protected IgniteConfiguration configuration() { IgniteConfiguration cfg = super.configuration(); cfg.setBackups(1); return cfg; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void zipFile(ZipOutputStream zip, String path) throws InterruptedException, IOException { try { zip.putNextEntry(new ZipEntry(path.substring(1))); FSDataInputStream in = getApi(context).open(path); byte[] chunk = new byte[1024]; while (in.read(chunk) != -1) { zip.write(chunk); } } catch (XXX ex) { String msg = "Error zipping file " + path.substring(1) + ": " + ex.getMessage(); logger.error(msg); zip.write(ex.getMessage().getBytes()); throw new ServiceFormattedException(ex.getMessage(), ex); } finally { zip.closeEntry(); } }', 'ground_truth': 'private void zipFile(ZipOutputStream zip, String path) { try { FSDataInputStream in = getApi(context).open(path); zip.putNextEntry(new ZipEntry(path.substring(1))); byte[] chunk = new byte[1024]; while (in.read(chunk) != -1) { zip.write(chunk); } } catch (IOException ex) { logger.error("Error zipping file " + path.substring(1) + " (file ignored): " + ex.getMessage()); } catch (InterruptedException ex) { String msg = "Error zipping file " + path.substring(1) + " (file ignored): " + ex.getMessage(); logger.error(msg); } finally { try { zip.closeEntry(); } catch (IOException ex) { logger.error("Error closing entry " + path.substring(1) + " (file ignored): " + ex.getMessage()); } } }', 'output': 'private void zipFile(ZipOutputStream zip, String path) throws InterruptedException, IOException { try { zip.putNextEntry(new ZipEntry(path.substring(1))); FSDataInputStream in = getApi(context).open(path); byte[] chunk = new byte[1024]; while (in.read(chunk) != -1) { zip.write(chunk); } } catch (IOException ex) { String msg = "Error zipping file " + path.substring(1) + ": " + ex.getMessage(); logger.error(msg); zip.write(ex.getMessage().getBytes()); throw new ServiceFormattedException(ex.getMessage(), ex); } finally { zip.closeEntry(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public PartitionUpdate deserialize(DataInputPlus in, int version, DeserializationHelper.Flag flag) throws IOException { TableMetadata metadata = Schema.instance.getExistingTableMetadata(TableId.deserialize(in)); UnfilteredRowIteratorSerializer.Header header = UnfilteredRowIteratorSerializer.serializer.deserializeHeader(metadata, null, in, version, flag); if (header.isEmpty) return emptyUpdate(metadata, header.key); assert !header.isReversed; assert header.rowEstimate >= 0; MutableDeletionInfo.Builder deletionBuilder = MutableDeletionInfo.builder(header.partitionDeletion, metadata.comparator, false); XXX rows = BTree.builder(metadata.comparator, header.rowEstimate); rows.auto(false); try (UnfilteredRowIterator partition = UnfilteredRowIteratorSerializer.serializer.deserialize(in, version, metadata, flag, header)) { while (partition.hasNext()) { Unfiltered unfiltered = partition.next(); if (unfiltered.kind() == Unfiltered.Kind.ROW) rows.add((Row)unfiltered); else deletionBuilder.add((RangeTombstoneMarker)unfiltered); } } MutableDeletionInfo deletionInfo = deletionBuilder.build(); return new PartitionUpdate(metadata, header.key, new Holder(header.sHeader.columns(), rows.build(), deletionInfo, header.staticRow, header.sHeader.stats()), deletionInfo, false); }', 'ground_truth': 'public PartitionUpdate deserialize(DataInputPlus in, int version, DeserializationHelper.Flag flag) throws IOException { TableMetadata metadata = Schema.instance.getExistingTableMetadata(TableId.deserialize(in)); UnfilteredRowIteratorSerializer.Header header = UnfilteredRowIteratorSerializer.serializer.deserializeHeader(metadata, null, in, version, flag); if (header.isEmpty) return emptyUpdate(metadata, header.key); assert !header.isReversed; assert header.rowEstimate >= 0; MutableDeletionInfo.Builder deletionBuilder = MutableDeletionInfo.builder(header.partitionDeletion, metadata.comparator, false); Object[] rows; try (BTree.FastBuilder<Row> builder = BTree.fastBuilder(); UnfilteredRowIterator partition = UnfilteredRowIteratorSerializer.serializer.deserialize(in, version, metadata, flag, header)) { while (partition.hasNext()) { Unfiltered unfiltered = partition.next(); if (unfiltered.kind() == Unfiltered.Kind.ROW) builder.add((Row)unfiltered); else deletionBuilder.add((RangeTombstoneMarker)unfiltered); } rows = builder.build(); } MutableDeletionInfo deletionInfo = deletionBuilder.build(); return new PartitionUpdate(metadata, header.key, new Holder(header.sHeader.columns(), rows, deletionInfo, header.staticRow, header.sHeader.stats()), deletionInfo, false); }', 'output': 'public PartitionUpdate deserialize(DataInputPlus in, int version, DeserializationHelper.Flag flag) throws IOException { TableMetadata metadata = Schema.instance.getExistingTableMetadata(TableId.deserialize(in)); UnfilteredRowIteratorSerializer.Header header = UnfilteredRowIteratorSerializer.serializer.deserializeHeader(metadata, null, in, version, flag); if (header.isEmpty) return emptyUpdate(metadata, header.key); assert !header.isReversed; assert header.rowEstimate >= 0; MutableDeletionInfo.Builder deletionBuilder = MutableDeletionInfo.builder(header.partitionDeletion, metadata.comparator, false); BTree.Builder rows = BTree.builder(metadata.comparator, header.rowEstimate); rows.auto(false); try (UnfilteredRowIterator partition = UnfilteredRowIteratorSerializer.serializer.deserialize(in, version, metadata, flag, header)) { while (partition.hasNext()) { Unfiltered unfiltered = partition.next(); if (unfiltered.kind() == Unfiltered.Kind.ROW) rows.add((Row)unfiltered); else deletionBuilder.add((RangeTombstoneMarker)unfiltered); } } MutableDeletionInfo deletionInfo = deletionBuilder.build(); return new PartitionUpdate(metadata, header.key, new Holder(header.sHeader.columns(), rows.build(), deletionInfo, header.staticRow, header.sHeader.stats()), deletionInfo, false); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void digest(Digest digest) { if (!complexDeletion.isLive()) complexDeletion.digest(digest); for (XXX cell : this) cell.digest(digest); }', 'ground_truth': 'public void digest(Digest digest) { if (!complexDeletion.isLive()) complexDeletion.digest(digest); for (Cell<?> cell : this) cell.digest(digest); }', 'output': 'public void digest(Digest digest) { if (!complexDeletion.isLive()) complexDeletion.digest(digest); for (Cell<K, V> cell : this) cell.digest(digest); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testServices() throws Exception { IgniteConfiguration cfg = optimize(getConfiguration("g1")); try (Ignite g1 = G.start(cfg)) { XXX services = grid().services(grid().forNode(g1.cluster().localNode())); services.deployNodeSingleton("test", new DummyService()); GridMarshallerTestBean inBean = newTestBean(services); byte[] buf = marshal(inBean); GridMarshallerTestBean outBean = unmarshal(buf); assert inBean.getObjectField() != null; assert outBean.getObjectField() != null; assert inBean.getObjectField().getClass().equals(GridServicesImpl.class); assert outBean.getObjectField().getClass().equals(GridServicesImpl.class); assert inBean != outBean; assert inBean.equals(outBean); ClusterGroup inPrj = services.projection(); ClusterGroup outPrj = ((GridServices)outBean.getObjectField()).projection(); assert inPrj.getClass().equals(outPrj.getClass()); assert F.eqNotOrdered(inPrj.nodes(), outPrj.nodes()); outBean.checkNullResources(); } }', 'ground_truth': 'public void testServices() throws Exception { IgniteConfiguration cfg = optimize(getConfiguration("g1")); try (Ignite g1 = G.start(cfg)) { IgniteManaged services = grid().services(grid().forNode(g1.cluster().localNode())); services.deployNodeSingleton("test", new DummyService()); GridMarshallerTestBean inBean = newTestBean(services); byte[] buf = marshal(inBean); GridMarshallerTestBean outBean = unmarshal(buf); assert inBean.getObjectField() != null; assert outBean.getObjectField() != null; assert inBean.getObjectField().getClass().equals(IgniteManagedImpl.class); assert outBean.getObjectField().getClass().equals(IgniteManagedImpl.class); assert inBean != outBean; assert inBean.equals(outBean); ClusterGroup inPrj = services.projection(); ClusterGroup outPrj = ((IgniteManaged)outBean.getObjectField()).projection(); assert inPrj.getClass().equals(outPrj.getClass()); assert F.eqNotOrdered(inPrj.nodes(), outPrj.nodes()); outBean.checkNullResources(); } }', 'output': 'public void testServices() throws Exception { IgniteConfiguration cfg = optimize(getConfiguration("g1")); try (Ignite g1 = G.start(cfg)) { Services services = grid().services(grid().forNode(g1.cluster().localNode())); services.deployNodeSingleton("test", new DummyService()); GridMarshallerTestBean inBean = newTestBean(services); byte[] buf = marshal(inBean); GridMarshallerTestBean outBean = unmarshal(buf); assert inBean.getObjectField() != null; assert outBean.getObjectField() != null; assert inBean.getObjectField().getClass().equals(GridServicesImpl.class); assert outBean.getObjectField().getClass().equals(GridServicesImpl.class); assert inBean != outBean; assert inBean.equals(outBean); ClusterGroup inPrj = services.projection(); ClusterGroup outPrj = ((GridServices)outBean.getObjectField()).projection(); assert inPrj.getClass().equals(outPrj.getClass()); assert F.eqNotOrdered(inPrj.nodes(), outPrj.nodes()); outBean.checkNullResources(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void defaultEventType() throws Exception { final SomeDomainObject sdo = new SomeDomainObject(); final XXX identifier = FeatureIdentifier.actionIdentifier(TypeIdentifier.fqcn(SomeDomainObject.class), "foo", new Class[]{int.class, String.class}); Utils.domainEventHelper(); final ActionDomainEvent<Object> ev = DomainEventHelper.newActionDomainEvent( ActionDomainEvent.Default.class, identifier, sdo, new Object[]{1, "bar"}); assertSame(ev.getSource(), sdo); assertThat(ev.getIdentifier(), is(identifier)); assertThat(ev.getArguments(), is(not(nullValue()))); assertEquals(ev.getArguments().get(0), Integer.valueOf(1)); assertEquals(ev.getArguments().get(1), "bar"); }', 'ground_truth': 'public void defaultEventType() throws Exception { final SomeDomainObject sdo = new SomeDomainObject(); final Identifier identifier = Identifier.actionIdentifier(TypeIdentifier.fqcn(SomeDomainObject.class), "foo", new Class[]{int.class, String.class}); Utils.domainEventHelper(); final ActionDomainEvent<Object> ev = DomainEventHelper.newActionDomainEvent( ActionDomainEvent.Default.class, identifier, sdo, new Object[]{1, "bar"}); assertSame(ev.getSource(), sdo); assertThat(ev.getIdentifier(), is(identifier)); assertThat(ev.getArguments(), is(not(nullValue()))); assertEquals(ev.getArguments().get(0), Integer.valueOf(1)); assertEquals(ev.getArguments().get(1), "bar"); }', 'output': 'public void defaultEventType() throws Exception { final SomeDomainObject sdo = new SomeDomainObject(); final Identifier identifier = FeatureIdentifier.actionIdentifier(TypeIdentifier.fqcn(SomeDomainObject.class), "foo", new Class[]{int.class, String.class}); Utils.domainEventHelper(); final ActionDomainEvent<Object> ev = DomainEventHelper.newActionDomainEvent( ActionDomainEvent.Default.class, identifier, sdo, new Object[]{1, "bar"}); assertSame(ev.getSource(), sdo); assertThat(ev.getIdentifier(), is(identifier)); assertThat(ev.getArguments(), is(not(nullValue()))); assertEquals(ev.getArguments().get(0), Integer.valueOf(1)); assertEquals(ev.getArguments().get(1), "bar"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testBoundsAsClusteringWithSingleEqAndSliceRestrictions() { TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC); ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); ByteBuffer value4 = ByteBufferUtil.bytes(4); ByteBuffer value5 = ByteBufferUtil.bytes(5); Restriction singleEq = newSingleEq(tableMetadata, 0, value1); Restriction multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(singleEq).mergeWith(multiSlice); XXX bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1); singleEq = newSingleEq(tableMetadata, 0, value1); multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3); Restriction multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, false, value4); restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(multiSlice2).mergeWith(singleEq).mergeWith(multiSlice); bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), false, value1, value4); singleEq = newSingleEq(tableMetadata, 0, value1); multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, true, value2, value3); multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, true, value4, value5); restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(multiSlice2).mergeWith(singleEq).mergeWith(multiSlice); bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), true, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value4, value5); }', 'ground_truth': 'public void testBoundsAsClusteringWithSingleEqAndSliceRestrictions() { TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC); ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); ByteBuffer value4 = ByteBufferUtil.bytes(4); ByteBuffer value5 = ByteBufferUtil.bytes(5); Restriction singleEq = newSingleEq(tableMetadata, 0, value1); Restriction multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(singleEq).mergeWith(multiSlice); SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1); singleEq = newSingleEq(tableMetadata, 0, value1); multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3); Restriction multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, false, value4); restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(multiSlice2).mergeWith(singleEq).mergeWith(multiSlice); bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), false, value1, value4); singleEq = newSingleEq(tableMetadata, 0, value1); multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, true, value2, value3); multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, true, value4, value5); restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(multiSlice2).mergeWith(singleEq).mergeWith(multiSlice); bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), true, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value4, value5); }', 'output': 'public void testBoundsAsClusteringWithSingleEqAndSliceRestrictions() { TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC); ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); ByteBuffer value4 = ByteBufferUtil.bytes(4); ByteBuffer value5 = ByteBufferUtil.bytes(5); Restriction singleEq = newSingleEq(tableMetadata, 0, value1); Restriction multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(singleEq).mergeWith(multiSlice); Bounds bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1); singleEq = newSingleEq(tableMetadata, 0, value1); multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3); Restriction multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, false, value4, value5); restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(multiSlice2).mergeWith(singleEq).mergeWith(multiSlice); Bounds bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), false, value1, value4); singleEq = newSingleEq(tableMetadata, 0'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCustomIdMapperWithGlobal() throws Exception { PortableMarshaller marsh = new PortableMarshaller(); marsh.setIdMapper(new PortableIdMapper() { @Override public int typeId(String clsName) { return 11111; } @Override public int fieldId(int typeId, String fieldName) { assert typeId == 11111; if ("val1".equals(fieldName)) return 22222; else if ("val2".equals(fieldName)) return 33333; assert false : "Unknown field: " + fieldName; return 0; } }); PortableTypeConfiguration type1 = new PortableTypeConfiguration(CustomMappedObject1.class.getName()); PortableTypeConfiguration type2 = new PortableTypeConfiguration(CustomMappedObject2.class.getName()); type2.setIdMapper(new PortableIdMapper() { @Override public int typeId(String clsName) { return 44444; } @Override public int fieldId(int typeId, String fieldName) { assert typeId == 44444; if ("val1".equals(fieldName)) return 55555; else if ("val2".equals(fieldName)) return 66666; assert false : "Unknown field: " + fieldName; return 0; } }); marsh.setTypeConfigurations(Arrays.asList(type1, type2)); CustomMappedObject1 obj1 = new CustomMappedObject1(10, "str1"); PortableObjectEx po1 = marshal(obj1, marsh); assertEquals(11111, po1.typeId()); assertEquals(10, po1.field(22222)); assertEquals("str1", po1.field(33333)); assertEquals(10, po1.<CustomMappedObject1>deserialize().val1); assertEquals("str1", po1.<CustomMappedObject1>deserialize().val2); CustomMappedObject2 obj2 = new CustomMappedObject2(20, "str2"); XXX po2 = marshal(obj2, marsh); assertEquals(44444, po2.typeId()); assertEquals(20, po2.field(55555)); assertEquals("str2", po2.field(66666)); assertEquals(20, po2.<CustomMappedObject2>deserialize().val1); assertEquals("str2", po2.<CustomMappedObject2>deserialize().val2); }', 'ground_truth': 'public void testCustomIdMapperWithGlobal() throws Exception { PortableMarshaller marsh = new PortableMarshaller(); marsh.setIdMapper(new BinaryTypeIdMapper() { @Override public int typeId(String clsName) { return 11111; } @Override public int fieldId(int typeId, String fieldName) { assert typeId == 11111; if ("val1".equals(fieldName)) return 22222; else if ("val2".equals(fieldName)) return 33333; assert false : "Unknown field: " + fieldName; return 0; } }); BinaryTypeConfiguration type1 = new BinaryTypeConfiguration(CustomMappedObject1.class.getName()); BinaryTypeConfiguration type2 = new BinaryTypeConfiguration(CustomMappedObject2.class.getName()); type2.setIdMapper(new BinaryTypeIdMapper() { @Override public int typeId(String clsName) { return 44444; } @Override public int fieldId(int typeId, String fieldName) { assert typeId == 44444; if ("val1".equals(fieldName)) return 55555; else if ("val2".equals(fieldName)) return 66666; assert false : "Unknown field: " + fieldName; return 0; } }); marsh.setTypeConfigurations(Arrays.asList(type1, type2)); CustomMappedObject1 obj1 = new CustomMappedObject1(10, "str1"); BinaryObjectEx po1 = marshal(obj1, marsh); assertEquals(11111, po1.typeId()); assertEquals(10, po1.field(22222)); assertEquals("str1", po1.field(33333)); assertEquals(10, po1.<CustomMappedObject1>deserialize().val1); assertEquals("str1", po1.<CustomMappedObject1>deserialize().val2); CustomMappedObject2 obj2 = new CustomMappedObject2(20, "str2"); BinaryObjectEx po2 = marshal(obj2, marsh); assertEquals(44444, po2.typeId()); assertEquals(20, po2.field(55555)); assertEquals("str2", po2.field(66666)); assertEquals(20, po2.<CustomMappedObject2>deserialize().val1); assertEquals("str2", po2.<CustomMappedObject2>deserialize().val2); }', 'output': 'public void testCustomIdMapperWithGlobal() throws Exception { PortableMarshaller marsh = new PortableMarshaller(); marsh.setIdMapper(new PortableIdMapper() { @Override public int typeId(String clsName) { return 11111; } @Override public int fieldId(int typeId, String fieldName) { assert typeId == 11111; if ("val1".equals(fieldName)) return 22222; else if ("val2".equals(fieldName)) return 33333; assert false : "Unknown field: " + fieldName; return 0; } }); PortableTypeConfiguration type1 = new PortableTypeConfiguration(CustomMappedObject1.class.getName()); PortableTypeConfiguration type2 = new PortableTypeConfiguration(CustomMappedObject2.class.getName()); type2.setIdMapper(new PortableIdMapper() { @Override public int typeId(String clsName) { return 44444; } @Override public int fieldId(int typeId, String fieldName) { assert typeId == 44444; if ("val1".equals(fieldName)) return 55555; else if ("val2".equals(fieldName)) return 66666; assert false : "Unknown field: " + fieldName; return 0; } }); marsh.setTypeConfigurations(Arrays.asList(type1, type2)); CustomMappedObject1 obj1 = new CustomMappedObject1(10, "str1"); PortableObjectEx po1 = marshal(obj1, marsh); assertEquals(11111, po1.typeId()); assertEquals(10, po1.field(22222)); assertEquals("str1", po1.field(33333)); assertEquals(10, po1.<CustomMappedObject1>deserialize().val1); assertEquals("str1", po1.<CustomMappedObject1>deserialize().val2); CustomMappedObject2 obj2 = new CustomMappedObject2(20, "str2"); PortableObjectEx po2 = marshal(obj2, marsh); assertEquals(44444, po2.type'}
{'instruction': 'Please refactor the following code:', 'input': 'public Variable deriveBoxedVersion() { if (getType().isPrimitiveType()) { XXX t = getType().getWrapper(); return new Variable(index, t, name); } else { return this; } }', 'ground_truth': 'public Variable deriveBoxedVersion() { if (ClassHelper.isPrimitiveType(getType())) { ClassNode t = ClassHelper.getWrapper(getType()); return new Variable(index, t, name); } else { return this; } }', 'output': 'public Variable deriveBoxedVersion() { if (getType().isPrimitiveType()) { Class<?> t = getType().getWrapper(); return new Variable(index, t, name); } else { return this; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void assertSplit(long suggestedStart, long suggestedLen, long expStart, long expLen, byte[] data, byte[]... delims) throws Exception { write(data); GridGgfsByteDelimiterRecordResolver rslvr = resolver(delims); XXX split; try (IgniteFsInputStream is = read()) { split = rslvr.resolveRecords(ggfs, is, split(suggestedStart, suggestedLen)); } assert split != null : "Split is null."; assert split.start() == expStart : "Incorrect start [expected=" + expStart + ", actual=" + split.start() + \']\'; assert split.length() == expLen : "Incorrect length [expected=" + expLen + ", actual=" + split.length() + \']\'; }', 'ground_truth': 'public void assertSplit(long suggestedStart, long suggestedLen, long expStart, long expLen, byte[] data, byte[]... delims) throws Exception { write(data); GridGgfsByteDelimiterRecordResolver rslvr = resolver(delims); IgniteFsFileRange split; try (IgniteFsInputStream is = read()) { split = rslvr.resolveRecords(ggfs, is, split(suggestedStart, suggestedLen)); } assert split != null : "Split is null."; assert split.start() == expStart : "Incorrect start [expected=" + expStart + ", actual=" + split.start() + \']\'; assert split.length() == expLen : "Incorrect length [expected=" + expLen + ", actual=" + split.length() + \']\'; }', 'output': 'public void assertSplit(long suggestedStart, long suggestedLen, long expStart, long expLen, byte[] data, byte[]... delims) throws Exception { write(data); GridGgfsByteDelimiterRecordResolver rslvr = resolver(delims); IgniteFsInputStream is = read(); IgniteFsOutputStream os = new IgniteFsOutputStream(ggfs, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,'}
{'instruction': 'Please refactor the following code:', 'input': 'public EndPoint[] getNStorageEndPoint(String key) { XXX token = hash(key); return nodePicker_.getStorageEndPoints(token); } public Map<EndPoint, EndPoint> getNStorageEndPointMap(String key)\n    { { XXX token = hash(key); return nodePicker_.getHintedStorageEndPoints(token); }', 'ground_truth': 'public EndPoint[] getNStorageEndPoint(String key) { Token token = token(key); return nodePicker_.getStorageEndPoints(token); }', 'output': 'public List<EndPoint> getNStorageEndPoint(String key) { XXX token = hash(key); return nodePicker_.getHintedStorageEndPoints(token); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public <IN, OUT, T extends Operator<IN, OUT>> T add(T operator) { T ret = super.add(operator); List<Operator<?, ?>> inputOperators = operator .listInputs() .stream() .map(d -> (Operator<?, ?>) new WrappedPCollectionOperator(this, unwrapped(d), d)) .collect(Collectors.toList()); final DAG<Operator<?, ?>> dag; if (inputOperators.isEmpty()) { dag = DAG.of(operator); } else { dag = DAG.of(inputOperators); dag.add(operator, inputOperators); } DAG<Operator<?, ?>> unfolded = FlowTranslator.unfold(dag); context.setTranslationDAG(unfolded); FlowTranslator.updateContextBy(unfolded, context); Dataset<OUT> output = operator.output(); XXX dagOutput = (Dataset) Iterables.getOnlyElement(unfolded.getLeafs()).get().output(); if (output != dagOutput) { context.setPCollection(output, unwrapped(dagOutput)); } return ret; }', 'ground_truth': 'public <InputT, OutputT, T extends Operator<InputT, OutputT>> T add(T operator) { T ret = super.add(operator); List<Operator<?, ?>> inputOperators = operator .listInputs() .stream() .map(d -> (Operator<?, ?>) new WrappedPCollectionOperator(this, unwrapped(d), d)) .collect(Collectors.toList()); final DAG<Operator<?, ?>> dag; if (inputOperators.isEmpty()) { dag = DAG.of(operator); } else { dag = DAG.of(inputOperators); dag.add(operator, inputOperators); } DAG<Operator<?, ?>> unfolded = FlowTranslator.unfold(dag); context.setTranslationDAG(unfolded); FlowTranslator.updateContextBy(unfolded, context); Dataset<OutputT> output = operator.output(); Dataset<OutputT> dagOutput = (Dataset) Iterables.getOnlyElement(unfolded.getLeafs()).get().output(); if (output != dagOutput) { context.setPCollection(output, unwrapped(dagOutput)); } return ret; }', 'output': 'public <IN, OUT, T extends Operator<IN, OUT>> T add(T operator) { T ret = super.add(operator); List<Operator<?, ?>> inputOperators = operator .listInputs() .stream() .map(d -> (Operator<?, ?>) new WrappedPCollectionOperator(this, unwrapped(d), d)) .collect(Collectors.toList()); final DAG<Operator<?, ?>> dag; if (inputOperators.isEmpty()) { dag = DAG.of(operator); } else { dag = DAG.of(inputOperators); dag.add(operator, inputOperators); } DAG<Operator<?, ?>> unfolded = FlowTranslator.unfold(dag); context.setTranslationDAG(unfolded); FlowTranslator.updateContextBy(unfolded, context); Dataset<OUT> output = operator.output(); PCollection<OUT> dagOutput = (PCollection<OUT>) Iterables.getOnlyElement(unfolded.getLeafs()).get(); if (output != dagOutput) { context.setPCollection(output, unwrapped(dagOutput)); } return ret; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void awaitIndexing(UUID nodeId) { T2<CountDownLatch, AtomicBoolean> blocker = BLOCKS.get(nodeId); if (blocker != null) { assertTrue(blocker.get2().compareAndSet(false, true)); while (true) { try { blocker.get1().await(); break; } catch (InterruptedException e) { } } } }', 'ground_truth': 'private static void awaitIndexing(UUID nodeId) { T3<CountDownLatch, AtomicBoolean, CountDownLatch> blocker = BLOCKS.get(nodeId); if (blocker != null) { assertTrue(blocker.get2().compareAndSet(false, true)); blocker.get3().countDown(); while (true) { try { blocker.get1().await(); break; } catch (InterruptedException e) { } } } }', 'output': 'private static void awaitIndexing(UUID nodeId) { T2<CountDownLatch, AtomicBoolean> blocker = BLOCKS.get(nodeId); if (blocker != null) { assertTrue(blocker.get2().compareAndSet(false, true)); while (true) { try { blocker.get1().await(); break; } catch (InterruptedException e) { } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean isAlive(InetAddress ep) { /* If the endpoint in question is the local endpoint return true. */ InetAddress localHost = FBUtilities.getLocalAddress(); if (localHost.equals(ep)) return true; /* Incoming port is assumed to be the Storage port. We need to change it to the control port */ XXX epState = Gossiper.instance.getEndpointStateForEndpoint(ep); return epState.isAlive(); }', 'ground_truth': 'public boolean isAlive(InetAddress ep) { /* If the endpoint in question is the local endpoint return true. */ InetAddress localHost = FBUtilities.getLocalAddress(); if (localHost.equals(ep)) return true; /* Incoming port is assumed to be the Storage port. We need to change it to the control port */ EndpointState epState = Gossiper.instance.getEndpointStateForEndpoint(ep); return epState.isAlive(); }', 'output': 'public boolean isAlive(InetAddress ep) { InetAddress localHost = FBUtilities.getLocalAddress(); if (localHost.equals(ep)) return true; EndpointState epState = Gossiper.instance.getEndpointStateForEndpoint(ep); return epState.isAlive(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private <R> R callInternal(final @NonNull Callable<R> callable) { serviceInjector.injectServicesInto(callable); try { return callable.call(); } catch (XXX e) { requestRollback(); throw e; } }', 'ground_truth': 'private <R> R callInternal(final @NonNull Callable<R> callable) { serviceInjector.injectServicesInto(callable); try { return callable.call(); } catch (Throwable e) { requestRollback(e); throw e; } }', 'output': 'private <R> R callInternal(final @NonNull Callable<R> callable) { serviceInjector.injectServicesInto(callable); try { return callable.call(); } catch (Throwable e) { requestRollback(); throw e; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void buildIndexBlocking() { logger.info(String.format("Submitting index build of %s for data in %s", getIndexName(), StringUtils.join(baseCfs.getSSTables(), ", "))); XXX sstables = baseCfs.markCurrentSSTablesReferenced(); try { SecondaryIndexBuilder builder = new SecondaryIndexBuilder(baseCfs, Collections.singleton(getIndexName()), new ReducingKeyIterator(sstables)); Future<?> future = CompactionManager.instance.submitIndexBuild(builder); FBUtilities.waitOnFuture(future); forceBlockingFlush(); setIndexBuilt(); } finally { SSTableReader.releaseReferences(sstables); } logger.info("Index build of {} complete", getIndexName()); }', 'ground_truth': 'protected void buildIndexBlocking() { logger.info(String.format("Submitting index build of %s for data in %s", getIndexName(), StringUtils.join(baseCfs.getSSTables(), ", "))); try (Refs<SSTableReader> sstables = baseCfs.selectAndReference(ColumnFamilyStore.ALL_SSTABLES).refs) { SecondaryIndexBuilder builder = new SecondaryIndexBuilder(baseCfs, Collections.singleton(getIndexName()), new ReducingKeyIterator(sstables)); Future<?> future = CompactionManager.instance.submitIndexBuild(builder); FBUtilities.waitOnFuture(future); forceBlockingFlush(); setIndexBuilt(); } logger.info("Index build of {} complete", getIndexName()); }', 'output': 'protected void buildIndexBlocking() { logger.info(String.format("Submitting index build of %s for data in %s", getIndexName(), StringUtils.join(baseCfs.getSSTables(), ", "))); Collection<SSTableReader> sstables = baseCfs.markCurrentSSTablesReferenced(); try { SecondaryIndexBuilder builder = new SecondaryIndexBuilder(baseCfs, Collections.singleton(getIndexName()), new ReducingKeyIterator(sstables)); Future<?> future = CompactionManager.instance.submitIndexBuild(builder); FBUtilities.waitOnFuture(future); forceBlockingFlush(); setIndexBuilt(); } finally { SSTableReader.releaseReferences(sstables); } logger.info("Index build of {} complete", getIndexName()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSerialization() { MyOptionsWrapper wrapper = new MyOptionsWrapper(PipelineOptionsTest.options); ByteArrayOutputStream bos = new ByteArrayOutputStream(); FSStorageAgent.store(bos, wrapper); ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); XXX wrapperCopy = (MyOptionsWrapper) FSStorageAgent.retrieve(bis); assertNotNull(wrapperCopy.options); assertEquals("nothing", wrapperCopy.options.get().as(MyOptions.class).getTestOption()); }', 'ground_truth': 'public void testSerialization() { OptionsWrapper wrapper = new OptionsWrapper( PipelineOptionsFactory.fromArgs("--testOption=nothing").as(MyOptions.class)); ByteArrayOutputStream bos = new ByteArrayOutputStream(); FSStorageAgent.store(bos, wrapper); ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); OptionsWrapper wrapperCopy = (OptionsWrapper) FSStorageAgent.retrieve(bis); assertNotNull(wrapperCopy.options); assertEquals("nothing", wrapperCopy.options.get().as(MyOptions.class).getTestOption()); }', 'output': 'public void testSerialization() { MyOptionsWrapper wrapper = new MyOptionsWrapper(PipelineOptionsTest.options); ByteArrayOutputStream bos = new ByteArrayOutputStream(); FSStorageAgent.store(bos, wrapper); ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); MyOptionsWrapper wrapperCopy = (MyOptionsWrapper) FSStorageAgent.retrieve(bis); assertNotNull(wrapperCopy.options); assertEquals("nothing", wrapperCopy.options.get().as(MyOptions.class).getTestOption()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPartitionGroupLock() throws Exception { plc = new GridCacheFifoEvictionPolicy<>(100); try { Ignite g = startGrid(1); Integer affKey = 1; GridCache<Object, Integer> cache = g.cache(null); IgniteTx tx = cache.txStartPartition(cache.affinity().partition(affKey), PESSIMISTIC, REPEATABLE_READ, 0, 5); try { for (int i = 0; i < 5; i++) cache.put(new GridCacheAffinityKey<Object>(i, affKey), i); tx.commit(); } finally { tx.close(); } assertEquals(5, ((GridCacheFifoEvictionPolicy)plc).queue().size()); tx = cache.txStartPartition(cache.affinity().partition(affKey), PESSIMISTIC, REPEATABLE_READ, 0, 5); try { for (int i = 0; i < 5; i++) cache.remove(new GridCacheAffinityKey<Object>(i, affKey)); tx.commit(); } finally { tx.close(); } assertEquals(0, ((GridCacheFifoEvictionPolicy)plc).queue().size()); } finally { stopAllGrids(); } }', 'ground_truth': 'public void testPartitionGroupLock() throws Exception { plc = new CacheFifoEvictionPolicy<>(100); try { Ignite g = startGrid(1); Integer affKey = 1; Cache<Object, Integer> cache = g.cache(null); IgniteTx tx = cache.txStartPartition(cache.affinity().partition(affKey), PESSIMISTIC, REPEATABLE_READ, 0, 5); try { for (int i = 0; i < 5; i++) cache.put(new CacheAffinityKey<Object>(i, affKey), i); tx.commit(); } finally { tx.close(); } assertEquals(5, ((CacheFifoEvictionPolicy)plc).queue().size()); tx = cache.txStartPartition(cache.affinity().partition(affKey), PESSIMISTIC, REPEATABLE_READ, 0, 5); try { for (int i = 0; i < 5; i++) cache.remove(new CacheAffinityKey<Object>(i, affKey)); tx.commit(); } finally { tx.close(); } assertEquals(0, ((CacheFifoEvictionPolicy)plc).queue().size()); } finally { stopAllGrids(); } }', 'output': 'public void testPartitionGroupLock() throws Exception { plc = new GridCacheFifoEvictionPolicy<>(100); try { Ignite g = startGrid(1); Integer affKey = 1; GridCache<Object, Integer> cache = g.cache(null); IgniteTx tx = cache.txStartPartition(cache.affinity().partition(affKey), PESSIMISTIC, REPEATABLE_READ, 0, 5); try { for (int i = 0; i < 5; i++) cache.put(new GridCacheAffinityKey<Object>(i, affKey), i); tx.commit(); } finally { tx.close(); } assertEquals(5, ((GridCacheFifoEvictionPolicy)plc).queue().size()); tx = cache.txStartPartition(cache.affinity().partition(affKey), PESSIMISTIC, REPEATABLE_READ, 0, 5); try { for (int i = 0; i < 5; i++) cache.remove(new GridCacheAffinityKey<Object>(i, affKey)); tx.commit(); } finally { tx.close(); } assertEquals(0, ((GridCacheFifoEvictionPolicy)plc).queue().size()); } finally { stopAllGrids(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected IgniteConfiguration commonConfiguration(int idx) throws Exception { IgniteConfiguration cfg = super.getConfiguration(getTestIgniteInstanceName(idx)); cfg.setDiscoverySpi(new TcpDiscoverySpi().setIpFinder(IP_FINDER)); cfg.setMarshaller(new BinaryMarshaller()); XXX memCfg = new MemoryConfiguration() .setDefaultMemoryPolicyName("default") .setMemoryPolicies( new MemoryPolicyConfiguration() .setName("default") .setMaxSize(128 * 1024 * 1024L) .setInitialSize(128 * 1024 * 1024L) ); cfg.setMemoryConfiguration(memCfg); return optimize(cfg); }', 'ground_truth': 'protected IgniteConfiguration commonConfiguration(int idx) throws Exception { IgniteConfiguration cfg = super.getConfiguration(getTestIgniteInstanceName(idx)); cfg.setDiscoverySpi(new TcpDiscoverySpi().setIpFinder(IP_FINDER)); cfg.setMarshaller(new BinaryMarshaller()); DataStorageConfiguration memCfg = new DataStorageConfiguration().setDefaultDataRegionConfiguration( new DataRegionConfiguration().setMaxSize(128 * 1024 * 1024)); cfg.setDataStorageConfiguration(memCfg); return optimize(cfg); }', 'output': 'protected IgniteConfiguration commonConfiguration(int idx) throws Exception { IgniteConfiguration cfg = super.getConfiguration(getTestIgniteInstanceName(idx)); cfg.setDiscoverySpi(new TcpDiscoverySpi().setIpFinder(IP_FINDER)); cfg.setMarshaller(new BinaryMarshaller()); IgniteConfiguration memCfg = new MemoryConfiguration() .setDefaultMemoryPolicyName("default") .setMemoryPolicies( new MemoryPolicyConfiguration() .setName("default") .setMaxSize(128 * 1024 * 1024L) .setInitialSize(128 * 1024 * 1024L) ); cfg.setMemoryConfiguration(memCfg); return optimize(cfg); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ReplayPosition add(Mutation mutation) { assert mutation != null; long size = Mutation.serializer.serializedSize(mutation, MessagingService.current_version); long totalSize = size + ENTRY_OVERHEAD_SIZE; if (totalSize > MAX_MUTATION_SIZE) { throw new IllegalArgumentException(String.format("Mutation of %s bytes is too large for the maxiumum size of %s", totalSize, MAX_MUTATION_SIZE)); } Allocation alloc = allocator.allocate(mutation, (int) totalSize); try { ICRC32 checksum = CRC32Factory.instance.create(); final ByteBuffer buffer = alloc.getBuffer(); XXX dos = new DataOutputByteBuffer(buffer); dos.writeInt((int) size); checksum.update(buffer, buffer.position() - 4, 4); buffer.putInt(checksum.getCrc()); int start = buffer.position(); Mutation.serializer.serialize(mutation, dos, MessagingService.current_version); checksum.update(buffer, start, (int) size); buffer.putInt(checksum.getCrc()); } catch (IOException e) { throw new FSWriteError(e, alloc.getSegment().getPath()); } finally { alloc.markWritten(); } executor.finishWriteFor(alloc); return alloc.getReplayPosition(); }', 'ground_truth': 'public ReplayPosition add(Mutation mutation) { assert mutation != null; long size = Mutation.serializer.serializedSize(mutation, MessagingService.current_version); long totalSize = size + ENTRY_OVERHEAD_SIZE; if (totalSize > MAX_MUTATION_SIZE) { throw new IllegalArgumentException(String.format("Mutation of %s bytes is too large for the maxiumum size of %s", totalSize, MAX_MUTATION_SIZE)); } Allocation alloc = allocator.allocate(mutation, (int) totalSize); try { ICRC32 checksum = CRC32Factory.instance.create(); final ByteBuffer buffer = alloc.getBuffer(); BufferedDataOutputStreamPlus dos = new BufferedDataOutputStreamPlus(null, buffer); dos.writeInt((int) size); checksum.update(buffer, buffer.position() - 4, 4); buffer.putInt(checksum.getCrc()); int start = buffer.position(); Mutation.serializer.serialize(mutation, dos, MessagingService.current_version); checksum.update(buffer, start, (int) size); buffer.putInt(checksum.getCrc()); } catch (IOException e) { throw new FSWriteError(e, alloc.getSegment().getPath()); } finally { alloc.markWritten(); } executor.finishWriteFor(alloc); return alloc.getReplayPosition(); }', 'output': 'public ReplayPosition add(Mutation mutation) { assert mutation != null; long size = Mutation.serializer.serializedSize(mutation, MessagingService.current_version); long totalSize = size + ENTRY_OVERHEAD_SIZE; if (totalSize > MAX_MUTATION_SIZE) { throw new IllegalArgumentException(String.format("Mutation of %s bytes is too large for the maxiumum size of %s", totalSize, MAX_MUTATION_SIZE)); } Allocation alloc = allocator.allocate(mutation, (int) totalSize); try { ICRC32 checksum = CRC32Factory.instance.create(); final ByteBuffer buffer = alloc.getBuffer(); DataOutputByteBuffer dos = new DataOutputByteBuffer(buffer); dos.writeInt((int) size); checksum.update(buffer, buffer.position() - 4, 4); buffer.putInt(checksum.getCrc()); int start = buffer.position(); Mutation.serializer.serialize(mutation, dos, MessagingService.current_version); checksum.update(buffer, start, (int) size); buffer.putInt(checksum.getCrc()); } catch (IOException e) { throw new FSWriteError(e, alloc.getSegment().getPath()); } finally { alloc.markWritten(); } executor.finishWriteFor(alloc); return alloc.getReplayPosition(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testReloadAllAsyncFiltered() throws Exception { GridCache<Integer, ValueObject> cache = cache(); for (int i = 0; i < PUT_CNT; i++) assert cache.putx(i, new ValueObject(i)); assert cache.size() == PUT_CNT; Integer[] keys = new Integer[PUT_CNT]; for (int i = 0; i < PUT_CNT; i++) keys[i] = i; for (Integer key : keys) cache.clear(key); assert cache.isEmpty(); assertEquals(0, cache.size()); cache.projection(new P1<GridCacheEntry<Integer, ValueObject>>() { @Override public boolean apply(GridCacheEntry<Integer, ValueObject> e) { return e.getKey() >= 5; } }).reloadAllAsync(Arrays.asList(keys)).get(); assertEquals(5, cache.size()); Collection<Map.Entry<Integer, ValueObject>> res = cache.queries().createSqlQuery(ValueObject.class, "val >= 0").execute().get(); assert res != null; assert res.size() == PUT_CNT - 5; assert size(ValueObject.class) == PUT_CNT - 5; }', 'ground_truth': 'public void testReloadAllAsyncFiltered() throws Exception { Cache<Integer, ValueObject> cache = cache(); for (int i = 0; i < PUT_CNT; i++) assert cache.putx(i, new ValueObject(i)); assert cache.size() == PUT_CNT; Integer[] keys = new Integer[PUT_CNT]; for (int i = 0; i < PUT_CNT; i++) keys[i] = i; for (Integer key : keys) cache.clear(key); assert cache.isEmpty(); assertEquals(0, cache.size()); cache.projection(new P1<CacheEntry<Integer, ValueObject>>() { @Override public boolean apply(CacheEntry<Integer, ValueObject> e) { return e.getKey() >= 5; } }).reloadAllAsync(Arrays.asList(keys)).get(); assertEquals(5, cache.size()); Collection<Map.Entry<Integer, ValueObject>> res = cache.queries().createSqlQuery(ValueObject.class, "val >= 0").execute().get(); assert res != null; assert res.size() == PUT_CNT - 5; assert size(ValueObject.class) == PUT_CNT - 5; }', 'output': 'public void testReloadAllAsyncFiltered() throws Exception { Cache<Integer, ValueObject> cache = cache(); for (int i = 0; i < PUT_CNT; i++) assert cache.putx(i, new ValueObject(i)); assert cache.size() == PUT_CNT; Integer[] keys = new Integer[PUT_CNT]; for (int i = 0; i < PUT_CNT; i++) keys[i] = i; for (Integer key : keys) cache.clear(key); assert cache.isEmpty(); assertEquals(0, cache.size()); cache.projection(new P1<GridCacheEntry<Integer, ValueObject>>() { @Override public boolean apply(GridCacheEntry<Integer, ValueObject> e) { return e.getKey() >= 5; } }).reloadAllAsync(Arrays.asList(keys)).get(); assertEquals(5, cache.size()); Collection<Map.Entry<Integer, ValueObject>> res = cache.queries().createSqlQuery(ValueObject.class, "val >= 0").execute().get(); assert res != null; assert res.size() == PUT_CNT - 5; assert size(ValueObject.class) == PUT_CNT - 5; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Collection getFilteredData(Collection<JPACompletionItem> data, String prefix) { if (prefix.length() == 0) { return data; } XXX ret = new ArrayList(); for (Iterator<JPACompletionItem> it = data.iterator(); it.hasNext();) { CompletionItem itm = it.next(); if (itm.getInsertPrefix().toString().startsWith(prefix)) { ret.add(itm); } } return ret; }', 'ground_truth': 'private Collection getFilteredData(Collection<JPACompletionItem> data, String prefix) { if (prefix.length() == 0) { return data; } List<CompletionItem> ret = new ArrayList<>(); for (Iterator<JPACompletionItem> it = data.iterator(); it.hasNext();) { CompletionItem itm = it.next(); if (itm.getInsertPrefix().toString().startsWith(prefix)) { ret.add(itm); } } return ret; }', 'output': 'private Collection<JPACompletionItem> getFilteredData(Collection<JPACompletionItem> data, String prefix) { if (prefix.length() == 0) { return data; } List<JPACompletionItem> ret = new ArrayList<>(); for (Iterator<JPACompletionItem> it = data.iterator(); it.hasNext();) { JPACompletionItem itm = it.next(); if (itm.getInsertPrefix().toString().startsWith(prefix)) { ret.add(itm); } } return ret; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkPutGetRemoveObjectTx(Integer key, IgniteTxConcurrency txConcurrency) throws Exception { GridCache<Integer, TestValue> c = grid(0).cache(null); TestValue val = new TestValue(new byte[10]); IgniteTx tx = c.txStart(txConcurrency, REPEATABLE_READ); assertNull(c.put(key, val)); tx.commit(); checkValue(key, val); tx = c.txStart(txConcurrency, REPEATABLE_READ); assertEquals(val, c.remove(key)); tx.commit(); checkValue(key, null); }', 'ground_truth': 'private void checkPutGetRemoveObjectTx(Integer key, IgniteTxConcurrency txConcurrency) throws Exception { Cache<Integer, TestValue> c = grid(0).cache(null); TestValue val = new TestValue(new byte[10]); IgniteTx tx = c.txStart(txConcurrency, REPEATABLE_READ); assertNull(c.put(key, val)); tx.commit(); checkValue(key, val); tx = c.txStart(txConcurrency, REPEATABLE_READ); assertEquals(val, c.remove(key)); tx.commit(); checkValue(key, null); }', 'output': 'private void checkPutGetRemoveObjectTx(Integer key, IgniteTxConcurrency txConcurrency) throws Exception { Cache<Integer, TestValue> c = grid(0).cache(null); TestValue val = new TestValue(new byte[10]); IgniteTx tx = c.txStart(txConcurrency, REPEATABLE_READ); assertNull(c.put(key, val)); tx.commit(); checkValue(key, val); tx = c.txStart(txConcurrency, REPEATABLE_READ); assertEquals(val, c.remove(key)); tx.commit(); checkValue(key, null); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private GridComputeTaskFuture<?> generateEvents(@Nullable Long timeout, GridComputeJob job) throws Exception { XXX comp = ignite.compute().enableAsync(); if (timeout == null) comp.execute(GridAllEventsTestTask.class.getName(), job); else comp.withTimeout(timeout).execute(GridAllEventsTestTask.class.getName(), job); return comp.future(); }', 'ground_truth': 'private GridComputeTaskFuture<?> generateEvents(@Nullable Long timeout, GridComputeJob job) throws Exception { IgniteCompute comp = ignite.compute().enableAsync(); if (timeout == null) comp.execute(GridAllEventsTestTask.class.getName(), job); else comp.withTimeout(timeout).execute(GridAllEventsTestTask.class.getName(), job); return comp.future(); }', 'output': 'private GridComputeTaskFuture<?> generateEvents(@Nullable Long timeout, GridComputeJob job) throws Exception { IgniteCompute comp = ignite.compute(); if (timeout == null) comp.execute(GridAllEventsTestTask.class.getName(), job); else comp.withTimeout(timeout).execute(GridAllEventsTestTask.class.getName(), job); return comp.future(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testKeySetIteratorMultithreaded() throws Exception { for (int i = 0; i < gridCount(); i++) cache(i).removeAll(); final XXX putFut = GridTestUtils.runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { for (int i = 0; i < entryCount(); i++) cache().put(KEY_PREFIX + i, i); } }, 1, "put-thread"); GridTestUtils.runMultiThreaded(new CA() { @Override public void apply() { while (!putFut.isDone()) { for (String key : cache().keySet()) { assert key != null; assert key.contains(KEY_PREFIX); } } } }, 3, "iterator-thread"); }', 'ground_truth': 'public void testKeySetIteratorMultithreaded() throws Exception { for (int i = 0; i < gridCount(); i++) cache(i).removeAll(); final IgniteFuture<?> putFut = GridTestUtils.runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { for (int i = 0; i < entryCount(); i++) cache().put(KEY_PREFIX + i, i); } }, 1, "put-thread"); GridTestUtils.runMultiThreaded(new CA() { @Override public void apply() { while (!putFut.isDone()) { for (String key : cache().keySet()) { assert key != null; assert key.contains(KEY_PREFIX); } } } }, 3, "iterator-thread"); }', 'output': 'public void testKeySetIteratorMultithreaded() throws Exception { for (int i = 0; i < gridCount(); i++) cache(i).removeAll(); final IgniteInternalFuture<?> putFut = GridTestUtils.runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { for (int i = 0; i < entryCount(); i++) cache().put(KEY_PREFIX + i, i); } }, 1, "put-thread"); GridTestUtils.runMultiThreaded(new CA() { @Override public void apply() { while (!putFut.isDone()) { for (String key : cache().keySet()) { assert key != null; assert key.contains(KEY_PREFIX); } } } }, 3, "iterator-thread"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void runBenchmark(IndexConfiguration idxCfg) throws Exception { int thrCnt = getIntProperty(THREADS_CNT, 1); int dur = getIntProperty(TEST_DUR_SEC, 60); int winSize = getIntProperty("GG_WIN_SIZE", 5000); dumpProperties(System.out); final XXX win = new GridStreamerBoundedSizeWindow<>(); win.setMaximumSize(winSize); win.setIndexes(idxCfg.indexProvider()); win.start(); final AtomicLong enqueueCntr = new AtomicLong(); IgniteFuture<Long> enqueueFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { Random rnd = new Random(); while (!Thread.currentThread().isInterrupted()) { win.enqueue(rnd.nextInt()); enqueueCntr.incrementAndGet(); } } }, thrCnt, "generator"); final AtomicLong evictCntr = new AtomicLong(); IgniteFuture<Long> evictFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { while (!Thread.currentThread().isInterrupted()) { win.pollEvicted(); evictCntr.incrementAndGet(); } } }, thrCnt, "evictor"); IgniteFuture<Long> collFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() { int nSec = 0; long prevEnqueue = enqueueCntr.get(); long prevEvict = evictCntr.get(); try { while (!Thread.currentThread().isInterrupted()) { U.sleep(1000); nSec++; long curEnqueue = enqueueCntr.get(); long curEvict = evictCntr.get(); X.println("Stats [enqueuePerSec=" + (curEnqueue - prevEnqueue) + ", evictPerSec=" + (curEvict - prevEvict) + \']\'); prevEnqueue = curEnqueue; prevEvict = curEvict; } } catch (GridInterruptedException ignored) { } X.println("Final results [enqueuePerSec=" + (enqueueCntr.get() / nSec) + ", evictPerSec=" + (evictCntr.get() / nSec) + \']\'); } }, 1, "collector"); U.sleep(dur * 1000); X.println("Finishing test."); collFut.cancel(); enqueueFut.cancel(); evictFut.cancel(); }', 'ground_truth': 'public static void runBenchmark(IndexConfiguration idxCfg) throws Exception { int thrCnt = getIntProperty(THREADS_CNT, 1); int dur = getIntProperty(TEST_DUR_SEC, 60); int winSize = getIntProperty("GG_WIN_SIZE", 5000); dumpProperties(System.out); final StreamerBoundedSizeWindow<Integer> win = new StreamerBoundedSizeWindow<>(); win.setMaximumSize(winSize); win.setIndexes(idxCfg.indexProvider()); win.start(); final AtomicLong enqueueCntr = new AtomicLong(); IgniteFuture<Long> enqueueFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { Random rnd = new Random(); while (!Thread.currentThread().isInterrupted()) { win.enqueue(rnd.nextInt()); enqueueCntr.incrementAndGet(); } } }, thrCnt, "generator"); final AtomicLong evictCntr = new AtomicLong(); IgniteFuture<Long> evictFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { while (!Thread.currentThread().isInterrupted()) { win.pollEvicted(); evictCntr.incrementAndGet(); } } }, thrCnt, "evictor"); IgniteFuture<Long> collFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() { int nSec = 0; long prevEnqueue = enqueueCntr.get(); long prevEvict = evictCntr.get(); try { while (!Thread.currentThread().isInterrupted()) { U.sleep(1000); nSec++; long curEnqueue = enqueueCntr.get(); long curEvict = evictCntr.get(); X.println("Stats [enqueuePerSec=" + (curEnqueue - prevEnqueue) + ", evictPerSec=" + (curEvict - prevEvict) + \']\'); prevEnqueue = curEnqueue; prevEvict = curEvict; } } catch (GridInterruptedException ignored) { } X.println("Final results [enqueuePerSec=" + (enqueueCntr.get() / nSec) + ", evictPerSec=" + (evictCntr.get() / nSec) + \']\'); } }, 1, "collector"); U.sleep(dur * 1000); X.println("Finishing test."); collFut.cancel(); enqueueFut.cancel(); evictFut.cancel(); }', 'output': 'public static void runBenchmark(IndexConfiguration idxCfg) throws Exception { int thrCnt = getIntProperty(THREADS_CNT, 1); int dur = getIntProperty(TEST_DUR_SEC, 60); int winSize = getIntProperty("GG_WIN_SIZE", 5000); dumpProperties(System.out); final GridStreamerBoundedSizeWindow win = new GridStreamerBoundedSizeWindow<>(); win.setMaximumSize(winSize); win.setIndexes(idxCfg.indexProvider()); win.start(); final AtomicLong enqueueCntr = new AtomicLong(); IgniteFuture<Long> enqueueFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { Random rnd = new Random(); while (!Thread.currentThread().isInterrupted()) { win.enqueue(rnd.nextInt()); enqueueCntr.incrementAndGet(); } } }, thrCnt, "generator"); final AtomicLong evictCntr = new AtomicLong(); IgniteFuture<Long> evictFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { while (!Thread.currentThread().isInterrupted()) { win.pollEvicted(); evictCntr.incrementAndGet(); } } }, thrCnt, "evictor"); IgniteFuture<Long> collFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() { int nSec = 0; long prevEnqueue = enqueueCntr.get(); long prevEvict = evictCntr.get(); try { while (!Thread.currentThread().isInterrupted()) { U.sleep(1000); nSec++; long curEnqueue = enqueueCntr.get(); long curEvict = evictCntr.get(); X.println("Stats [enqueuePerSec=" + (curEnqueue - prevEnqueue) + ", evictPerSec=" + (curEvict - prevEvict) + \']\'); prevEnqueue = curEnqueue; prevEvict = curEvict; } } catch (GridInterruptedException ignored) { } X.println("Final results ['}
{'instruction': 'Please refactor the following code:', 'input': 'public String getString(ByteBuffer bytes) { try { return TimeUUIDTerm.instance.getString(bytes); } catch (XXX e) { throw new MarshalException(e.getMessage()); } }', 'ground_truth': 'public String getString(ByteBuffer bytes) { try { return JdbcTimeUUID.instance.getString(bytes); } catch (org.apache.cassandra.cql.jdbc.MarshalException e) { throw new MarshalException(e.getMessage()); } }', 'output': 'public String getString(ByteBuffer bytes) { try { return TimeUUIDTerm.instance.getString(bytes); } catch (IOException e) { throw new MarshalException(e.getMessage()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testTypeProjection() throws Exception { GridCache<String, Integer> cache = cache(); cache.putAll(F.asMap("k1", 1 , "k2", 2, "k3", 3)); GridCache<Double, Boolean> anotherCache = grid(0).cache(null); assert anotherCache != null; anotherCache.put(3.14, true); GridCacheProjection<String, Integer> prj = cache.projection(String.class, Integer.class); List<String> keys = F.asList("k1", "k2", "k3"); for (String key : keys) assert prj.containsKey(key); }', 'ground_truth': 'public void testTypeProjection() throws Exception { Cache<String, Integer> cache = cache(); cache.putAll(F.asMap("k1", 1 , "k2", 2, "k3", 3)); Cache<Double, Boolean> anotherCache = grid(0).cache(null); assert anotherCache != null; anotherCache.put(3.14, true); CacheProjection<String, Integer> prj = cache.projection(String.class, Integer.class); List<String> keys = F.asList("k1", "k2", "k3"); for (String key : keys) assert prj.containsKey(key); }', 'output': 'public void testTypeProjection() throws Exception { Cache<String, Integer> cache = cache(); cache.putAll(F.asMap("k1", 1 , "k2", 2, "k3", 3)); Cache<Double, Boolean> anotherCache = grid(0).cache(null); assert anotherCache != null; anotherCache.put(3.14, true); CacheProjection<String, Integer> prj = cache.projection(String.class, Integer.class); List<String> keys = F.asList("k1", "k2", "k3"); for (String key : keys) assert prj.containsKey(key); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Collection<GridCacheSqlMetadata> sqlMetadata() throws GridException { if (!enterBusy()) throw new IllegalStateException("Failed to get metadata (grid is stopping)."); try { Callable<Collection<CacheSqlMetadata>> job = new MetadataJob(spi); XXX nodes = F.view(cctx.discovery().remoteNodes(), new P1<GridNode>() { @Override public boolean apply(GridNode n) { return U.hasCache(n, space); } }); Collection<Collection<CacheSqlMetadata>> res = new ArrayList<>(nodes.size() + 1); GridFuture<Collection<Collection<CacheSqlMetadata>>> rmtFut = null; if (!nodes.isEmpty()) rmtFut = cctx.closures().callAsyncNoFailover(BROADCAST, F.asSet(job), nodes, true); GridFuture<Collection<CacheSqlMetadata>> locFut = cctx.closures().callLocalSafe(job, true); if (rmtFut != null) res.addAll(rmtFut.get()); res.add(locFut.get()); Map<String, Collection<CacheSqlMetadata>> map = new HashMap<>(); for (Collection<CacheSqlMetadata> col : res) { for (CacheSqlMetadata meta : col) { String name = meta.cacheName(); Collection<CacheSqlMetadata> cacheMetas = map.get(name); if (cacheMetas == null) map.put(name, cacheMetas = new LinkedList<>()); cacheMetas.add(meta); } } Collection<GridCacheSqlMetadata> col = new ArrayList<>(map.size()); col.add(new CacheSqlMetadata(map.remove(space))); for (Collection<CacheSqlMetadata> metas : map.values()) col.add(new CacheSqlMetadata(metas)); return col; } finally { leaveBusy(); } }', 'ground_truth': 'public Collection<GridCacheSqlMetadata> sqlMetadata() throws GridException { if (!enterBusy()) throw new IllegalStateException("Failed to get metadata (grid is stopping)."); try { Callable<Collection<CacheSqlMetadata>> job = new MetadataJob(spi); Collection<ClusterNode> nodes = F.view(cctx.discovery().remoteNodes(), new P1<ClusterNode>() { @Override public boolean apply(ClusterNode n) { return U.hasCache(n, space); } }); Collection<Collection<CacheSqlMetadata>> res = new ArrayList<>(nodes.size() + 1); GridFuture<Collection<Collection<CacheSqlMetadata>>> rmtFut = null; if (!nodes.isEmpty()) rmtFut = cctx.closures().callAsyncNoFailover(BROADCAST, F.asSet(job), nodes, true); GridFuture<Collection<CacheSqlMetadata>> locFut = cctx.closures().callLocalSafe(job, true); if (rmtFut != null) res.addAll(rmtFut.get()); res.add(locFut.get()); Map<String, Collection<CacheSqlMetadata>> map = new HashMap<>(); for (Collection<CacheSqlMetadata> col : res) { for (CacheSqlMetadata meta : col) { String name = meta.cacheName(); Collection<CacheSqlMetadata> cacheMetas = map.get(name); if (cacheMetas == null) map.put(name, cacheMetas = new LinkedList<>()); cacheMetas.add(meta); } } Collection<GridCacheSqlMetadata> col = new ArrayList<>(map.size()); col.add(new CacheSqlMetadata(map.remove(space))); for (Collection<CacheSqlMetadata> metas : map.values()) col.add(new CacheSqlMetadata(metas)); return col; } finally { leaveBusy(); } }', 'output': 'public Collection<GridCacheSqlMetadata> sqlMetadata() throws GridException { if (!enterBusy()) throw new IllegalStateException("Failed to get metadata (grid is stopping)."); try { Callable<Collection<CacheSqlMetadata>> job = new MetadataJob(spi); Collection<ClusterNode> nodes = F.view(cctx.discovery().remoteNodes(), new P1<ClusterNode>() { @Override public boolean apply(ClusterNode n) { return U.hasCache(n, space); } }); Collection<Collection<CacheSqlMetadata>> res = new ArrayList<>(nodes.size() + 1); GridFuture<Collection<Collection<CacheSqlMetadata>>> rmtFut = null; if (!nodes.isEmpty()) rmtFut = cctx.closures().callAsyncNoFailover(BROADCAST, F.asSet(job), nodes, true); GridFuture<Collection<CacheSqlMetadata>> locFut = cctx.closures().callLocalSafe(job, true); if (rmtFut != null) res.addAll(rmtFut.get()); res.add(locFut.get()); Map<String, Collection<CacheSqlMetadata>> map = new HashMap<>(); for (Collection<CacheSqlMetadata> col : res) { for (CacheSqlMetadata meta : col) { String name = meta.cacheName(); Collection<CacheSqlMetadata> cacheMetas = map.get(name); if (cacheMetas == null) map.put(name, cacheMetas = new LinkedList<>()); cacheMetas.add(meta); } } Collection<GridCacheSqlMetadata> col = new ArrayList<>(map.size()); col.add(new CacheSqlMetadata(map.remove(space))); for (Collection<CacheSqlMetadata> metas : map.values()) col.add(new CacheSqlMetadata(metas)); return col; } finally { leaveBusy(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void main(String[] args) throws FileNotFoundException { System.out.println(); System.out.println(">>> Logistic regression model over partitioned dataset usage example started."); try (Ignite ignite = Ignition.start("examples/config/example-ignite.xml")) { System.out.println(">>> Ignite grid started."); IgniteCache<Integer, Vector> dataCache = new SandboxMLCache(ignite) .fillCacheWith(MLSandboxDatasets.TWO_CLASSED_IRIS); System.out.println(">>> Create new logistic regression trainer object."); XXX trainer = new LogisticRegressionSGDTrainer<>() .withUpdatesStgy(new UpdatesStrategy<>( new SimpleGDUpdateCalculator(0.2), SimpleGDParameterUpdate::sumLocal, SimpleGDParameterUpdate::avg )) .withMaxIterations(100000) .withLocIterations(100) .withBatchSize(10) .withSeed(123L); System.out.println(">>> Perform the training to get the model."); DatasetTrainer< ModelsComposition, Double> baggedTrainer = TrainerTransformers.makeBagged( trainer, 10, 0.6, 4, 3, new OnMajorityPredictionsAggregator()); System.out.println(">>> Perform evaluation of the model."); double[] score = new CrossValidation<ModelsComposition, Double, Integer, Vector>().score( baggedTrainer, new Accuracy<>(), ignite, dataCache, (k, v) -> v.copyOfRange(1, v.size()), (k, v) -> v.get(0), 3 ); System.out.println(">>> ---------------------------------"); Arrays.stream(score).forEach(sc -> System.out.println("\\n>>> Accuracy " + sc)); System.out.println(">>> Bagged logistic regression model over partitioned dataset usage example completed."); } }', 'ground_truth': 'public static void main(String[] args) throws FileNotFoundException { System.out.println(); System.out.println(">>> Logistic regression model over partitioned dataset usage example started."); try (Ignite ignite = Ignition.start("examples/config/example-ignite.xml")) { System.out.println(">>> Ignite grid started."); IgniteCache<Integer, Vector> dataCache = new SandboxMLCache(ignite) .fillCacheWith(MLSandboxDatasets.TWO_CLASSED_IRIS); System.out.println(">>> Create new logistic regression trainer object."); LogisticRegressionSGDTrainer trainer = new LogisticRegressionSGDTrainer() .withUpdatesStgy(new UpdatesStrategy<>( new SimpleGDUpdateCalculator(0.2), SimpleGDParameterUpdate::sumLocal, SimpleGDParameterUpdate::avg )) .withMaxIterations(100000) .withLocIterations(100) .withBatchSize(10) .withSeed(123L); System.out.println(">>> Perform the training to get the model."); DatasetTrainer< ModelsComposition, Double> baggedTrainer = TrainerTransformers.makeBagged( trainer, 10, 0.6, 4, 3, new OnMajorityPredictionsAggregator()); System.out.println(">>> Perform evaluation of the model."); double[] score = new CrossValidation<ModelsComposition, Double, Integer, Vector>().score( baggedTrainer, new Accuracy<>(), ignite, dataCache, (k, v) -> v.copyOfRange(1, v.size()), (k, v) -> v.get(0), 3 ); System.out.println(">>> ---------------------------------"); Arrays.stream(score).forEach(sc -> System.out.println("\\n>>> Accuracy " + sc)); System.out.println(">>> Bagged logistic regression model over partitioned dataset usage example completed."); } }', 'output': 'public static void main(String[] args) throws FileNotFoundException { System.out.println(); System.out.println(">>> Logistic regression model over partitioned dataset usage example started."); try (Ignite ignite = Ignition.start("examples/config/example-ignite.xml")) { System.out.println(">>> Ignite grid started."); IgniteCache<Integer, Vector> dataCache = new SandboxMLCache(ignite) .fillCacheWith(MLSandboxDatasets.TWO_CLASSED_IRIS); System.out.println(">>> Create new logistic regression trainer object."); LogisticRegressionTrainer<Vector> trainer = new LogisticRegressionTrainer<>() .withUpdatesStgy(new UpdatesStrategy<>( new SimpleGDUpdateCalculator(0.2), SimpleGDParameterUpdate::sumLocal, SimpleGDParameterUpdate::avg )) .withMaxIterations(100000) .withLocIterations(100) .withBatchSize(10) .withSeed(123L); System.out.println(">>> Perform the training to get the model."); DatasetTrainer< ModelsComposition, Double> baggedTrainer = TrainerTransformers.makeBagged( trainer, 10, 0.6, 4, 3, new OnMajorityPredictionsAggregator()); System.out.println(">>> Perform evaluation of the model."); double[] score = new CrossValidation<ModelsComposition, Double, Integer, Vector>().score( baggedTrainer, new Accuracy<>(), ignite, dataCache, (k, v) -> v.copyOfRange(1, v.size()), (k, v) -> v.get(0), 3 ); System.out.println(">>> ---------------------------------"); Arrays.stream(score).forEach(sc -> System.out.println("\\n>>> Accuracy " + sc)); System.out.println(">>> Bagged logistic regression model over partitioned dataset usage example completed."); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testDurationsTable() throws Throwable { createTable("CREATE TABLE %s (" + "k int PRIMARY KEY, " + "result duration)"); execute("USE " + keyspace()); executeNet("USE " + keyspace()); try { createView("mv_duration", "CREATE MATERIALIZED VIEW %s AS SELECT * FROM %%s WHERE result IS NOT NULL AND k IS NOT NULL PRIMARY KEY (result,k)"); Assert.fail("MV on duration should fail"); } catch (XXX e) { Assert.assertEquals("duration type is not supported for PRIMARY KEY column \'result\'", e.getMessage()); } }', 'ground_truth': 'public void testDurationsTable() { createTable("CREATE TABLE %s (" + "k int PRIMARY KEY, " + "result duration)"); try { createView("CREATE MATERIALIZED VIEW %s AS SELECT * FROM %s WHERE result IS NOT NULL AND k IS NOT NULL PRIMARY KEY (result,k)"); Assert.fail("MV on duration should fail"); } catch (Exception e) { Throwable cause = e.getCause(); Assert.assertEquals("duration type is not supported for PRIMARY KEY column \'result\'", cause.getMessage()); } }', 'output': 'public void testDurationsTable() throws Throwable { createTable("CREATE TABLE %s (" + "k int PRIMARY KEY, " + "result duration)"); execute("USE " + keyspace()); executeNet("USE " + keyspace()); try { createView("mv_duration", "CREATE MATERIALIZED VIEW %s AS SELECT * FROM %%s WHERE result IS NOT NULL AND k IS NOT NULL PRIMARY KEY (result,k)"); Assert.fail("MV on duration should fail"); } catch (UnsupportedOperationException e) { Assert.assertEquals("duration type is not supported for PRIMARY KEY column \'result\'", e.getMessage()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testOutputKafka_uploadData() throws Exception { LOG.info("testOutputKafka_uploadData()"); Map<String, Object> config = new HashMap<String, Object>(); config.put("broker_list", "some broker list"); config.put("topic", TEST_TOPIC); outputKafka.loadConfig(config); outputKafka.init(new LogFeederProps()); @SuppressWarnings("unchecked") Future<RecordMetadata> mockFuture = EasyMock.mock(Future.class); EasyMock.expect(mockKafkaProducer.send(new ProducerRecord<String, String>(TEST_TOPIC, "value0"))) .andReturn(mockFuture); EasyMock.expect(mockFuture.get()).andReturn(null); for (int i = 1; i < 10; i++) EasyMock.expect(mockKafkaProducer.send(EasyMock.eq(new ProducerRecord<String, String>(TEST_TOPIC, "value" + i)), EasyMock.anyObject(KafkaCallBack.class))).andReturn(null); EasyMock.replay(mockKafkaProducer); for (int i = 0; i < 10; i++) { XXX inputMarker = new InputMarker(EasyMock.mock(Input.class), null, 0); outputKafka.write("value" + i, inputMarker); } EasyMock.verify(mockKafkaProducer); }', 'ground_truth': 'public void testOutputKafka_uploadData() throws Exception { LOG.info("testOutputKafka_uploadData()"); Map<String, Object> config = new HashMap<String, Object>(); config.put("broker_list", "some broker list"); config.put("topic", TEST_TOPIC); outputKafka.loadConfig(config); outputKafka.init(new LogFeederProps()); @SuppressWarnings("unchecked") Future<RecordMetadata> mockFuture = EasyMock.mock(Future.class); EasyMock.expect(mockKafkaProducer.send(new ProducerRecord<String, String>(TEST_TOPIC, "value0"))) .andReturn(mockFuture); EasyMock.expect(mockFuture.get()).andReturn(null); for (int i = 1; i < 10; i++) EasyMock.expect(mockKafkaProducer.send(EasyMock.eq(new ProducerRecord<String, String>(TEST_TOPIC, "value" + i)), EasyMock.anyObject(KafkaCallBack.class))).andReturn(null); EasyMock.replay(mockKafkaProducer); for (int i = 0; i < 10; i++) { InputFileMarker inputMarker = new InputFileMarker(EasyMock.mock(Input.class), null, 0); outputKafka.write("value" + i, inputMarker); } EasyMock.verify(mockKafkaProducer); }', 'output': 'public void testOutputKafka_uploadData() throws Exception { LOG.info("testOutputKafka_uploadData()"); Map<String, Object> config = new HashMap<String, Object>(); config.put("broker_list", "some broker list"); config.put("topic", TEST_TOPIC); outputKafka.loadConfig(config); outputKafka.init(new LogFeederProps()); @SuppressWarnings("unchecked") Future<RecordMetadata> mockFuture = EasyMock.mock(Future.class); EasyMock.expect(mockKafkaProducer.send(new ProducerRecord<String, String>(TEST_TOPIC, "value0"))) .andReturn(mockFuture); EasyMock.expect(mockFuture.get()).andReturn(null); for (int i = 1; i < 10; i++) EasyMock.expect(mockKafkaProducer.send(EasyMock.eq(new ProducerRecord<String, String>(TEST_TOPIC, "value" + i)), EasyMock.anyObject(KafkaCallBack.class))).andReturn(null); EasyMock.replay(mockKafkaProducer); for (int i = 0; i < 10; i++) { InputMarker inputMarker = new InputMarker(EasyMock.mock(Input.class), null, 0); outputKafka.write("value" + i, inputMarker); } EasyMock.verify(mockKafkaProducer); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void close() { clock.updateStamp(Long.MAX_VALUE, this::onTimerCallback); for (XXX window : new ArrayList<>(states.keySet())) { processTriggerResult( window, new ElementTriggerContext(window), Trigger.TriggerResult.FLUSH_AND_PURGE); } }', 'ground_truth': 'public void close() { clock.updateStamp(Long.MAX_VALUE, this::onTimerCallback); for (WidT window : new ArrayList<>(states.keySet())) { processTriggerResult( window, new ElementTriggerContext(window), Trigger.TriggerResult.FLUSH_AND_PURGE); } }', 'output': 'public void close() { clock.updateStamp(Long.MAX_VALUE, this::onTimerCallback); for (String window : new ArrayList<>(states.keySet())) { processTriggerResult( window, new ElementTriggerContext(window), Trigger.TriggerResult.FLUSH_AND_PURGE); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public int getParsletUsageCount(String parslet) { int toret = 0; XXX it = values().iterator(); while (it.hasNext()) { Entry next = (Entry) it.next(); if (parslet != null && parslet.equals(next.parslet)) { toret++; } } return toret; }', 'ground_truth': 'public int getParsletUsageCount(String parslet) { int toret = 0; Iterator<Entry> it = values().iterator(); while (it.hasNext()) { Entry next = it.next(); if (parslet != null && parslet.equals(next.parslet)) { toret++; } } return toret; }', 'output': 'public int getParsletUsageCount(String parslet) { int toret = 0; Iterator<Entry> it = values().iterator(); while (it.hasNext()) { Entry next = it.next(); if (parslet != null && parslet.equals(next.parslet)) { toret++; } } return toret; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPut() throws Exception { try { Ignite g = G.start("examples/config/example-cache.xml"); if (g.cluster().nodes().size() < 5) U.warn(log, "Topology is too small for this test. " + "Run with 4 remote nodes or more having large number of backup nodes."); g.compute().run(new CAX() { @IgniteInstanceResource private Ignite g; @Override public void applyx() throws IgniteCheckedException { X.println("Checking whether cache is empty."); GridCache<SampleKey, SampleValue> cache = g.cache("partitioned"); assert cache.isEmpty(); } } ); GridCache<SampleKey, SampleValue> cache = g.cache("partitioned"); int cnt = 0; for (int key = 0; key < 1000; key++) { SampleKey key1 = new SampleKey(key); if (!g.cluster().localNode().id().equals(g.cluster().mapKeyToNode("partitioned", key1).id())) { cache.put(key1, new SampleValue(key)); cnt++; } } X.println(">>> Put count: " + cnt); } finally { G.stopAll(false); } }', 'ground_truth': 'public void testPut() throws Exception { try { Ignite g = G.start("examples/config/example-cache.xml"); if (g.cluster().nodes().size() < 5) U.warn(log, "Topology is too small for this test. " + "Run with 4 remote nodes or more having large number of backup nodes."); g.compute().run(new CAX() { @IgniteInstanceResource private Ignite g; @Override public void applyx() throws IgniteCheckedException { X.println("Checking whether cache is empty."); IgniteCache<SampleKey, SampleValue> cache = g.jcache("partitioned"); assert cache.isEmpty(); } } ); IgniteCache<SampleKey, SampleValue> cache = g.jcache("partitioned"); int cnt = 0; for (int key = 0; key < 1000; key++) { SampleKey key1 = new SampleKey(key); if (!g.cluster().localNode().id().equals(g.cluster().mapKeyToNode("partitioned", key1).id())) { cache.put(key1, new SampleValue(key)); cnt++; } } X.println(">>> Put count: " + cnt); } finally { G.stopAll(false); } }', 'output': 'public void testPut() throws Exception { try { Ignite g = G.start("examples/config/example-cache.xml"); if (g.cluster().nodes().size() < 5) U.warn(log, "Topology is too small for this test. " + "Run with 4 remote nodes or more having large number of backup nodes."); g.compute().run(new CAX() { @IgniteInstanceResource private Ignite g; @Override public void applyx() throws IgniteCheckedException { X.println("Checking whether cache is empty."); GridCache<SampleKey, SampleValue> cache = g.cache("partitioned"); assert cache.isEmpty(); } } ); GridCache<SampleKey, SampleValue> cache = g.cache("partitioned"); int cnt = 0; for (int key = 0; key < 1000; key++) { SampleKey key1 = new SampleKey(key); if (!g.cluster().localNode().id().equals(g.cluster().mapKeyToNode("partitioned", key1).id())) { cache.put(key1, new SampleValue(key)); cnt++; } } X.println(">>> Put count: " + cnt); } finally { G.stopAll(false); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void populateResource(Resource resource, Set<String> requestedIds) throws AmbariException { AlertSummaryDTO summary = null; AlertHostSummaryDTO hostSummary = null; String clusterName = (String) resource.getPropertyValue(m_clusterPropertyId); if (null == clusterName) { return; } String typeId = null == m_typeIdPropertyId ? null : (String) resource.getPropertyValue(m_typeIdPropertyId); Cluster cluster = s_clusters.get().getCluster(clusterName); switch (m_resourceType.getInternalType()) { case Cluster: long clusterId = cluster.getClusterId(); if (BaseProvider.isPropertyRequested(ALERTS_SUMMARY, requestedIds)) { summary = s_dao.findCurrentCounts(cluster.getClusterId(), null, null); } if (BaseProvider.isPropertyRequested(ALERTS_SUMMARY_HOSTS, requestedIds)) { hostSummary = s_dao.findCurrentHostCounts(clusterId); } break; case Service: summary = s_dao.findCurrentCounts(cluster.getClusterId(), typeId, null); break; case Host: summary = s_dao.findCurrentCounts(cluster.getClusterId(), null, typeId); break; default: break; } if (null != summary) { Map<AlertState, Integer> map = new HashMap<AlertState, Integer>(); map.put(AlertState.OK, Integer.valueOf(summary.getOkCount())); map.put(AlertState.WARNING, Integer.valueOf(summary.getWarningCount())); map.put(AlertState.CRITICAL, Integer.valueOf(summary.getCriticalCount())); map.put(AlertState.UNKNOWN, Integer.valueOf(summary.getUnknownCount())); setResourceProperty(resource, ALERTS_SUMMARY, map, requestedIds); } if (null != hostSummary) { Map<AlertState, Integer> map = new HashMap<AlertState, Integer>(); map.put(AlertState.OK, Integer.valueOf(hostSummary.getOkCount())); map.put(AlertState.WARNING, Integer.valueOf(hostSummary.getWarningCount())); map.put(AlertState.CRITICAL, Integer.valueOf(hostSummary.getCriticalCount())); map.put(AlertState.UNKNOWN, Integer.valueOf(hostSummary.getUnknownCount())); setResourceProperty(resource, ALERTS_SUMMARY_HOSTS, map, requestedIds); } }', 'ground_truth': 'private void populateResource(Resource resource, Set<String> requestedIds) throws AmbariException { AlertSummaryDTO summary = null; AlertHostSummaryDTO hostSummary = null; String clusterName = (String) resource.getPropertyValue(m_clusterPropertyId); if (null == clusterName) { return; } String typeId = null == m_typeIdPropertyId ? null : (String) resource.getPropertyValue(m_typeIdPropertyId); Cluster cluster = s_clusters.get().getCluster(clusterName); switch (m_resourceType.getInternalType()) { case Cluster: long clusterId = cluster.getClusterId(); if (BaseProvider.isPropertyRequested(ALERTS_SUMMARY, requestedIds)) { summary = s_dao.findCurrentCounts(cluster.getClusterId(), null, null); } if (BaseProvider.isPropertyRequested(ALERTS_SUMMARY_HOSTS, requestedIds)) { hostSummary = s_dao.findCurrentHostCounts(clusterId); } break; case Service: summary = s_dao.findCurrentCounts(cluster.getClusterId(), typeId, null); break; case Host: summary = s_dao.findCurrentCounts(cluster.getClusterId(), null, typeId); break; default: break; } if (null != summary) { Map<String, Integer> map = new HashMap<String, Integer>(); map.put(AlertState.OK.name(), Integer.valueOf(summary.getOkCount())); map.put(AlertState.WARNING.name(), Integer.valueOf(summary.getWarningCount())); map.put(AlertState.CRITICAL.name(), Integer.valueOf(summary.getCriticalCount())); map.put(AlertState.UNKNOWN.name(), Integer.valueOf(summary.getUnknownCount())); map.put("MAINTENANCE", Integer.valueOf(summary.getMaintenanceCount())); setResourceProperty(resource, ALERTS_SUMMARY, map, requestedIds); } if (null != hostSummary) { Map<AlertState, Integer> map = new HashMap<AlertState, Integer>(); map.put(AlertState.OK, Integer.valueOf(hostSummary.getOkCount())); map.put(AlertState.WARNING, Integer.valueOf(hostSummary.getWarningCount())); map.put(AlertState.CRITICAL, Integer.valueOf(hostSummary.getCriticalCount())); map.put(AlertState.UNKNOWN, Integer.valueOf(hostSummary.getUnknownCount())); setResourceProperty(resource, ALERTS_SUMMARY_HOSTS, map, requestedIds); } }', 'output': 'private void populateResource(Resource resource, Set<String> requestedIds) throws AmbariException { AlertSummaryDTO summary = null; AlertHostSummaryDTO hostSummary = null; String clusterName = (String) resource.getPropertyValue(m_clusterPropertyId); if (null == clusterName) { return; } String typeId = null == m_typeIdPropertyId ? null : (String) resource.getPropertyValue(m_typeIdPropertyId); Cluster cluster = s_clusters.get().getCluster(clusterName); switch (m_resourceType.getInternalType()) { case Cluster: long clusterId = cluster.getClusterId(); if (BaseProvider.isPropertyRequested(ALERTS_SUMMARY, requestedIds)) { summary = s_dao.findCurrentCounts(cluster.getClusterId(), null, null); } if (BaseProvider.isPropertyRequested(ALERTS_SUMMARY_HOSTS, requestedIds)) { hostSummary = s_dao.findCurrentHostCounts(clusterId); } break; case Service: summary = s_dao.findCurrentCounts(cluster.getClusterId(), typeId, null); break; case Host: summary = s_dao.findCurrentCounts(cluster.getClusterId(), null, typeId); break; default: break; } if (null != summary) { Map<AlertState, Integer> map = new HashMap<AlertState, Integer>(); map.put(AlertState.OK, Integer.valueOf(summary.getOkCount())); map.put(AlertState.WARNING, Integer.valueOf(summary.getWarningCount())); map.put(AlertState.CRITICAL, Integer.valueOf(summary.getCriticalCount())); map.put(AlertState.UNKNOWN, Integer.valueOf(summary.getUnknownCount())); setResourceProperty(resource, ALERTS_SUMMARY, map, requestedIds); } if (null != hostSummary) { Map<AlertState, Integer> map = new HashMap<AlertState, Integer>(); map.put(AlertState.OK, Integer.valueOf(hostSummary.getOkCount())); map.put(AlertState.WARNING, Integer.valueOf(hostSummary.getWarningCount())); map.put(AlertState.CRITICAL, Integer.valueOf(hostSummary'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCacheCheckpoint() throws Exception { XXX cfg = getConfiguration(); String cacheName = "test-checkpoints"; GridCacheConfiguration cacheCfg = defaultCacheConfiguration(); cacheCfg.setName(cacheName); GridCacheCheckpointSpi spi = new GridCacheCheckpointSpi(); spi.setCacheName(cacheName); cfg.setCacheConfiguration(cacheCfg); cfg.setCheckpointSpi(spi); GridSessionCheckpointSelfTest.spi = spi; checkCheckpoints(cfg); }', 'ground_truth': 'public void testCacheCheckpoint() throws Exception { IgniteConfiguration cfg = getConfiguration(); String cacheName = "test-checkpoints"; GridCacheConfiguration cacheCfg = defaultCacheConfiguration(); cacheCfg.setName(cacheName); GridCacheCheckpointSpi spi = new GridCacheCheckpointSpi(); spi.setCacheName(cacheName); cfg.setCacheConfiguration(cacheCfg); cfg.setCheckpointSpi(spi); GridSessionCheckpointSelfTest.spi = spi; checkCheckpoints(cfg); }', 'output': 'public void testCacheCheckpoint() throws Exception { IgniteConfiguration cfg = getConfiguration(); String cacheName = "test-checkpoints"; GridCacheConfiguration cacheCfg = defaultCacheConfiguration(); cacheCfg.setName(cacheName); GridCacheCheckpointSpi spi = new GridCacheCheckpointSpi(); spi.setCacheName(cacheName); cfg.setCacheConfiguration(cacheCfg); cfg.setCheckpointSpi(spi); GridSessionCheckpointSelfTest.spi = spi; checkCheckpoints(cfg); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ColumnFamilyStore createColumnFamilyStore(String table, String columnFamily) throws IOException { /* * Get all data files associated with old Memtables for this table. * These files are named as follows <Table>-1.db, ..., <Table>-n.db. Get * the max which in this case is n and increment it to use it for next * index. */ List<Integer> generations = new ArrayList<Integer>(); String[] dataFileDirectories = DatabaseDescriptor.getAllDataFileLocationsForTable(table); for (String directory : dataFileDirectories) { File fileDir = new File(directory); File[] files = fileDir.listFiles(); for (File file : files) { String filename = file.getAbsolutePath(); String cfName = getColumnFamilyFromFileName(filename); if (cfName.equals(columnFamily)) { generations.add(getGenerationFromFileName(filename)); } } } Collections.sort(generations); int value = (generations.size() > 0) ? (generations.get(generations.size() - 1)) : 0; ColumnFamilyStore cfs = new ColumnFamilyStore(table, columnFamily, "Super".equals(DatabaseDescriptor.getColumnType(table, columnFamily)), value); MBeanServer mbs = ManagementFactory.getPlatformMBeanServer(); try { XXX mbeanName = "org.apache.cassandra.db:type=ColumnFamilyStores,keyspace=" + table + ",columnfamily=" + columnFamily; mbs.registerMBean(cfs, new ObjectName(mbeanName)); } catch (Exception e) { throw new RuntimeException(e); } return cfs; }', 'ground_truth': 'public static ColumnFamilyStore createColumnFamilyStore(String table, String columnFamily) throws IOException { /* * Get all data files associated with old Memtables for this table. * These files are named as follows <Table>-1.db, ..., <Table>-n.db. Get * the max which in this case is n and increment it to use it for next * index. */ List<Integer> generations = new ArrayList<Integer>(); String[] dataFileDirectories = DatabaseDescriptor.getAllDataFileLocationsForTable(table); for (String directory : dataFileDirectories) { File fileDir = new File(directory); File[] files = fileDir.listFiles(); for (File file : files) { String filename = file.getAbsolutePath(); String cfName = getColumnFamilyFromFileName(filename); if (cfName.equals(columnFamily)) { generations.add(getGenerationFromFileName(filename)); } } } Collections.sort(generations); int value = (generations.size() > 0) ? (generations.get(generations.size() - 1)) : 0; ColumnFamilyStore cfs = new ColumnFamilyStore(table, columnFamily, "Super".equals(DatabaseDescriptor.getColumnType(table, columnFamily)), value); MBeanServer mbs = ManagementFactory.getPlatformMBeanServer(); try { ObjectName mbeanName = new ObjectName("org.apache.cassandra.db:type=ColumnFamilyStores,keyspace=" + table + ",columnfamily=" + columnFamily); if (mbs.isRegistered(mbeanName)) mbs.unregisterMBean(mbeanName); mbs.registerMBean(cfs, mbeanName); } catch (Exception e) { throw new RuntimeException(e); } return cfs; }', 'output': 'public static ColumnFamilyStore createColumnFamilyStore(String table, String columnFamily) throws IOException { /* * Get all data files associated with old Memtables for this table. * These files are named as follows <Table>-1.db, ..., <Table>-n.db. Get * the max which in this case is n and increment it to use it for next * index. */ List<Integer> generations = new ArrayList<Integer>(); String[] dataFileDirectories = DatabaseDescriptor.getAllDataFileLocationsForTable(table); for (String directory : dataFileDirectories) { File fileDir = new File(directory); File[] files = fileDir.listFiles(); for (File file : files) { String filename = file.getAbsolutePath(); String cfName = getColumnFamilyFromFileName(filename); if (cfName.equals(columnFamily)) { generations.add(getGenerationFromFileName(filename)); } } } Collections.sort(generations); int value = (generations.size() > 0) ? (generations.get(generations.size() - 1)) : 0; ColumnFamilyStore cfs = new ColumnFamilyStore(table, columnFamily, "Super".equals(DatabaseDescriptor.getColumnType(table, columnFamily)), value); MBeanServer mbs = ManagementFactory.getPlatformMBeanServer(); try { String mbeanName = "org.apache.cassandra.db:type=ColumnFamilyStores,keyspace=" + table + ",columnfamily=" + columnFamily; mbs.registerMBean(cfs, new ObjectName(mbeanName)); } catch (Exception e) { throw new RuntimeException(e); } return cfs; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ColumnFamily getColumnFamily(DecoratedKey key, XXX path, ByteBuffer start, ByteBuffer finish, boolean reversed, int limit) { return getColumnFamily(QueryFilter.getSliceFilter(key, path, start, finish, reversed, limit)); }', 'ground_truth': 'public ColumnFamily getColumnFamily(DecoratedKey key, ByteBuffer start, ByteBuffer finish, boolean reversed, int limit) { return getColumnFamily(QueryFilter.getSliceFilter(key, name, start, finish, reversed, limit)); }', 'output': 'public ColumnFamily getColumnFamily(DecoratedKey key, String path, ByteBuffer start, ByteBuffer finish, boolean reversed, int limit) { return getColumnFamily(QueryFilter.getSliceFilter(key, path, start, finish, reversed, limit)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testDontBrokeCyclicDependency() throws IgniteCheckedException { GridPortableTestClasses.TestObjectOuter outer = new GridPortableTestClasses.TestObjectOuter(); outer.inner = new GridPortableTestClasses.TestObjectInner(); outer.inner.outer = outer; outer.foo = "a"; BinaryObjectBuilder builder = builder(toPortable(outer)); builder.setField("foo", "b"); XXX res = builder.build().deserialize(); assertEquals("b", res.foo); assertSame(res, res.inner.outer); }', 'ground_truth': 'public void testDontBrokeCyclicDependency() throws IgniteCheckedException { GridBinaryTestClasses.TestObjectOuter outer = new GridBinaryTestClasses.TestObjectOuter(); outer.inner = new GridBinaryTestClasses.TestObjectInner(); outer.inner.outer = outer; outer.foo = "a"; BinaryObjectBuilder builder = builder(toPortable(outer)); builder.setField("foo", "b"); GridBinaryTestClasses.TestObjectOuter res = builder.build().deserialize(); assertEquals("b", res.foo); assertSame(res, res.inner.outer); }', 'output': 'public void testDontBrokeCyclicDependency() throws IgniteCheckedException { GridPortableTestClasses.TestObjectOuter outer = new GridPortableTestClasses.TestObjectOuter(); outer.inner = new GridPortableTestClasses.TestObjectInner(); outer.inner.outer = outer; outer.foo = "a"; BinaryObjectBuilder builder = builder(toPortable(outer)); builder.setField("foo", "b"); BinaryObject res = builder.build().deserialize(); assertEquals("b", res.foo); assertSame(res, res.inner.outer); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPromote() throws Exception { GridCache<Integer, TestValue> c = grid(0).cache(null); TestValue val = new TestValue(new byte[100 * 1024]); List<Integer> keys = primaryKeys(c, 200); for (Integer key : keys) c.putx(key, val); for (int i = 0; i < 50; i++) { TestValue val0 = c.promote(keys.get(i)); Assert.assertArrayEquals(val.val, val0.val); } List<Integer> keys0 = keys.subList(50, 100); c.promoteAll(keys0); for (Integer key : keys0) { TestValue val0 = c.get(key); Assert.assertArrayEquals(val.val, val0.val); } }', 'ground_truth': 'public void testPromote() throws Exception { IgniteCache<Integer, TestValue> c = grid(0).jcache(null); TestValue val = new TestValue(new byte[100 * 1024]); List<Integer> keys = primaryKeys(c, 200); for (Integer key : keys) c.put(key, val); c.localPromote(new HashSet<>(keys)); for (Integer key : keys) { TestValue val0 = c.get(key); Assert.assertArrayEquals(val.val, val0.val); } }', 'output': 'public void testPromote() throws Exception { Cache<Integer, TestValue> c = grid(0).cache(null); TestValue val = new TestValue(new byte[100 * 1024]); List<Integer> keys = primaryKeys(c, 200); for (Integer key : keys) c.putx(key, val); for (int i = 0; i < 50; i++) { TestValue val0 = c.promote(keys.get(i)); Assert.assertArrayEquals(val.val, val0.val); } List<Integer> keys0 = keys.subList(50, 100); c.promoteAll(keys0); for (Integer key : keys0) { TestValue val0 = c.get(key); Assert.assertArrayEquals(val.val, val0.val); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ByteBuffer getValueOf(ColumnMetadata column, Row row, int nowInSecs) { if (row == null) return null; switch (column.kind) { case CLUSTERING: if (row.isStatic()) return null; return row.clustering().get(column.position()); case STATIC: if (!row.isStatic()) return null; case REGULAR: XXX cell = row.getCell(column); return cell == null || !cell.isLive(nowInSecs) ? null : cell.value(); default: return null; } }', 'ground_truth': 'public static ByteBuffer getValueOf(ColumnMetadata column, Row row, int nowInSecs) { if (row == null) return null; switch (column.kind) { case CLUSTERING: if (row.isStatic()) return null; return row.clustering().bufferAt(column.position()); case STATIC: if (!row.isStatic()) return null; case REGULAR: Cell<?> cell = row.getCell(column); return cell == null || !cell.isLive(nowInSecs) ? null : cell.buffer(); default: return null; } }', 'output': 'public static ByteBuffer getValueOf(ColumnMetadata column, Row row, long nowInSecs) { if (row == null) return null; switch (column.kind) { case CLUSTERING: if (row.isStatic()) return null; return row.clustering().get(column.position()); case STATIC: if (!row.isStatic()) return null; case REGULAR: Cell cell = row.getCell(column); return cell == null || !cell.isLive(nowInSecs) ? null : cell.value(); default: return null; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkLockLocked(boolean loc) throws Exception { storeEnabled = false; startGridsMultiThreaded(3); try { final Ignite g0 = grid(0); Ignite g1 = grid(1); final Integer key = forPrimary(loc ? g0 : g1); final CountDownLatch lockLatch = new CountDownLatch(1); final CountDownLatch unlockLatch = new CountDownLatch(1); final Lock lock = g0.jcache(null).lock(key); XXX unlockFut = multithreadedAsync(new Runnable() { @Override public void run() { try { lock.lock(); try { lockLatch.countDown(); U.await(unlockLatch); } finally { lock.unlock(); } } catch (IgniteCheckedException e) { fail("Unexpected exception: " + e); } } }, 1); U.await(lockLatch); assert g0.jcache(null).isLocalLocked(key, false); assert !g0.jcache(null).isLocalLocked(key, true) : "Key can not be locked by current thread."; assert !lock.tryLock(); assert g0.cache(null).isLocked(key); assert !g0.cache(null).isLockedByThread(key) : "Key can not be locked by current thread."; unlockLatch.countDown(); unlockFut.get(); assert lock.tryLock(); lock.unlock(); } finally { stopAllGrids(); } }', 'ground_truth': 'private void checkLockLocked(boolean loc) throws Exception { storeEnabled = false; startGridsMultiThreaded(3); try { final Ignite g0 = grid(0); Ignite g1 = grid(1); final Integer key = forPrimary(loc ? g0 : g1); final CountDownLatch lockLatch = new CountDownLatch(1); final CountDownLatch unlockLatch = new CountDownLatch(1); final Lock lock = g0.jcache(null).lock(key); IgniteInternalFuture<?> unlockFut = multithreadedAsync(new Runnable() { @Override public void run() { try { lock.lock(); try { lockLatch.countDown(); U.await(unlockLatch); } finally { lock.unlock(); } } catch (IgniteCheckedException e) { fail("Unexpected exception: " + e); } } }, 1); U.await(lockLatch); assert g0.jcache(null).isLocalLocked(key, false); assert !g0.jcache(null).isLocalLocked(key, true) : "Key can not be locked by current thread."; assert !lock.tryLock(); assert g0.cache(null).isLocked(key); assert !g0.cache(null).isLockedByThread(key) : "Key can not be locked by current thread."; unlockLatch.countDown(); unlockFut.get(); assert lock.tryLock(); lock.unlock(); } finally { stopAllGrids(); } }', 'output': 'private void checkLockLocked(boolean loc) throws Exception { storeEnabled = false; startGridsMultiThreaded(3); try { final Ignite g0 = grid(0); Ignite g1 = grid(1); final Integer key = forPrimary(loc ? g0 : g1); final CountDownLatch lockLatch = new CountDownLatch(1); final CountDownLatch unlockLatch = new CountDownLatch(1); final Lock lock = g0.jcache(null).lock(key); IgniteInternalFuture<?> unlockFut = multithreadedAsync(new Runnable() { @Override public void run() { try { lock.lock(); try { lockLatch.countDown(); U.await(unlockLatch); } finally { lock.unlock(); } } catch (IgniteCheckedException e) { fail("Unexpected exception: " + e); } } }, 1); U.await(lockLatch); assert g0.jcache(null).isLocalLocked(key, false); assert !g0.jcache(null).isLocalLocked(key, true) : "Key can not be locked by current thread."; assert !lock.tryLock(); assert g0.cache(null).isLocked(key); assert !g0.cache(null).isLockedByThread(key) : "Key can not be locked by current thread."; unlockLatch.countDown(); unlockFut.get(); assert lock.tryLock(); lock.unlock(); } finally { stopAllGrids(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private SqlTypeName getFieldType(Row row, String fieldName) { XXX rowType = row.getRowType(); int fieldIndex = rowType.indexOf(fieldName); if (fieldIndex < 0) { throw new IllegalArgumentException( "Cannot find field \'" + fieldName + "\' in " + row.getRowType()); } SqlTypeCoder fieldCoder = (SqlTypeCoder) row.getRowType().getFieldCoder(fieldIndex); return CalciteUtils.toCalciteType(fieldCoder); }', 'ground_truth': 'private SqlTypeName getFieldType(Row row, String fieldName) { Schema rowType = row.getRowType(); int fieldIndex = rowType.indexOf(fieldName); if (fieldIndex < 0) { throw new IllegalArgumentException( "Cannot find field \'" + fieldName + "\' in " + row.getRowType()); } SqlTypeCoder fieldCoder = (SqlTypeCoder) row.getRowType().getFieldCoder(fieldIndex); return CalciteUtils.toCalciteType(fieldCoder); }', 'output': 'private SqlTypeName getFieldType(Row row, String fieldName) { RowType rowType = row.getRowType(); int fieldIndex = rowType.indexOf(fieldName); if (fieldIndex < 0) { throw new IllegalArgumentException( "Cannot find field \'" + fieldName + "\' in " + row.getRowType()); } SqlTypeCoder fieldCoder = (SqlTypeCoder) rowType.getFieldCoder(fieldIndex); return CalciteUtils.toCalciteType(fieldCoder); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private InteractionResult isVisibleResult( final ManagedObject target, final InteractionInitiatedBy interactionInitiatedBy, final Where where) { final XXX ic = createVisibleInteractionContext(target, interactionInitiatedBy, where); return InteractionUtils.isVisibleResult(this, ic); }', 'ground_truth': 'private InteractionResult isVisibleResult( final ManagedObject target, final InteractionInitiatedBy interactionInitiatedBy, final Where where) { final VisibilityContext ic = createVisibleInteractionContext(target, interactionInitiatedBy, where); return InteractionUtils.isVisibleResult(this, ic); }', 'output': 'private InteractionResult isVisibleResult( final ManagedObject target, final InteractionInitiatedBy interactionInitiatedBy, final Where where, final Object context) { final VisibleInteractionContext ic = createVisibleInteractionContext(target, interactionInitiatedBy, where, context); return InteractionUtils.isVisibleResult(this, ic); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception { IgniteConfiguration c = super.getConfiguration(gridName); GridTransactionsConfiguration txCfg = c.getTransactionsConfiguration(); txCfg.setTxSerializableEnabled(true); txCfg.setDefaultTxTimeout(TIMEOUT); XXX spi = new GridTcpDiscoverySpi(); spi.setIpFinder(ipFinder); c.setDiscoverySpi(spi); GridCacheConfiguration cc = defaultCacheConfiguration(); cc.setCacheMode(PARTITIONED); cc.setBackups(1); cc.setAtomicityMode(TRANSACTIONAL); c.setCacheConfiguration(cc); return c; } } }', 'ground_truth': '@Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception { IgniteConfiguration c = super.getConfiguration(gridName); GridTransactionsConfiguration txCfg = c.getTransactionsConfiguration(); txCfg.setTxSerializableEnabled(true); txCfg.setDefaultTxTimeout(TIMEOUT); TcpDiscoverySpi spi = new TcpDiscoverySpi(); spi.setIpFinder(ipFinder); c.setDiscoverySpi(spi); GridCacheConfiguration cc = defaultCacheConfiguration(); cc.setCacheMode(PARTITIONED); cc.setBackups(1); cc.setAtomicityMode(TRANSACTIONAL); c.setCacheConfiguration(cc); return c; }', 'output': '@Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception { IgniteConfiguration c = super.getConfiguration(gridName); GridTransactionsConfiguration txCfg = c.getTransactionsConfiguration(); txCfg.setTxSerializableEnabled(true); txCfg.setDefaultTxTimeout(TIMEOUT); TcpDiscoverySpi spi = new TcpDiscoverySpi(); spi.setIpFinder(ipFinder); c.setDiscoverySpi(spi); GridCacheConfiguration cc = defaultCacheConfiguration(); cc.setCacheMode(PARTITIONED); cc.setBackups(1); cc.setAtomicityMode(TRANSACTIONAL); c.setCacheConfiguration(cc); return c; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void call2(AtomicInteger cnt) throws Exception { IgniteCompute comp = compute(prj).withAsync(); Collection<Callable<String>> jobs = F.asList(calJob); comp.call(jobs); XXX fut = comp.future(); waitForExecution(fut); cnt.set(0); compute(prj).call(jobs); waitForValue(cnt, jobs.size()); }', 'ground_truth': 'private void call2(AtomicInteger cnt) throws Exception { IgniteCompute comp = compute(prj).withAsync(); Collection<Callable<String>> jobs = F.asList(calJob); comp.call(jobs); IgniteInternalFuture fut = comp.future(); waitForExecution(fut); cnt.set(0); compute(prj).call(jobs); waitForValue(cnt, jobs.size()); }', 'output': 'private void call2(AtomicInteger cnt) throws Exception { IgniteCompute comp = compute(prj).withAsync(); Collection<Callable<String>> jobs = F.asList(calJob); comp.call(jobs); IgniteFuture<?> fut = comp.future(); waitForExecution(fut); cnt.set(0); compute(prj).call(jobs); waitForValue(cnt, jobs.size()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testReloadAllAsyncFiltered() throws Exception { GridCache<String, Integer> cache = cache(); Collection<String> keys = primaryKeysForCache(cache, 3); for (String key : keys) assertNull(cache.get(key)); Map<String, Integer> vals = new HashMap<>(keys.size()); int i = 0; for (String key : keys) { cache.put(key, i); vals.put(key, i); i++; } for (String key : keys) assertEquals(vals.get(key), cache.peek(key)); cache.clearAll(); for (String key : keys) assertNull(cache.peek(key)); String first = F.first(keys); cache.put(first, 0); assertEquals((Integer)0, cache.peek(first)); cache.projection(F.<String, Integer>cacheHasPeekValue()).reloadAllAsync(keys).get(); assertEquals((Integer)0, cache.peek(first)); for (String key : keys) { if (!first.equals(key)) assertNull(cache.peek(key)); } }', 'ground_truth': 'public void testReloadAllAsyncFiltered() throws Exception { Cache<String, Integer> cache = cache(); Collection<String> keys = primaryKeysForCache(cache, 3); for (String key : keys) assertNull(cache.get(key)); Map<String, Integer> vals = new HashMap<>(keys.size()); int i = 0; for (String key : keys) { cache.put(key, i); vals.put(key, i); i++; } for (String key : keys) assertEquals(vals.get(key), cache.peek(key)); cache.clearAll(); for (String key : keys) assertNull(cache.peek(key)); String first = F.first(keys); cache.put(first, 0); assertEquals((Integer)0, cache.peek(first)); cache.projection(F.<String, Integer>cacheHasPeekValue()).reloadAllAsync(keys).get(); assertEquals((Integer)0, cache.peek(first)); for (String key : keys) { if (!first.equals(key)) assertNull(cache.peek(key)); } }', 'output': 'public void testReloadAllAsyncFiltered() throws Exception { Cache<String, Integer> cache = cache(); Collection<String> keys = primaryKeysForCache(cache, 3); for (String key : keys) assertNull(cache.get(key)); Map<String, Integer> vals = new HashMap<>(keys.size()); int i = 0; for (String key : keys) { cache.put(key, i); vals.put(key, i); i++; } for (String key : keys) assertEquals(vals.get(key), cache.peek(key)); cache.clearAll(); for (String key : keys) assertNull(cache.peek(key)); String first = F.first(keys); cache.put(first, 0); assertEquals((Integer)0, cache.peek(first)); cache.projection(F.<String, Integer>cacheHasPeekValue()).reloadAllAsync(keys).get(); assertEquals((Integer)0, cache.peek(first)); for (String key : keys) { if (!first.equals(key)) assertNull(cache.peek(key)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void onUpdate(AjaxRequestTarget target) { for (XXX subscriber : subscribers) { subscriber.onUpdate(target, ScalarPanelAbstract.this); } target.appendJavaScript( String.format("Wicket.Event.publish(Isis.Topic.FOCUS_FIRST_ACTION_PARAMETER, \'%s\')", getMarkupId())); }', 'ground_truth': 'protected void onUpdate(AjaxRequestTarget target) { for (ScalarModelSubscriberLegacy subscriber : subscribers) { subscriber.onUpdate(target, ScalarPanelAbstractLegacy.this); } target.appendJavaScript( String.format("Wicket.Event.publish(Isis.Topic.FOCUS_FIRST_ACTION_PARAMETER, \'%s\')", getMarkupId())); }', 'output': 'protected void onUpdate(AjaxRequestTarget target) { for (Subscriber subscriber : subscribers) { subscriber.onUpdate(target, this); } target.appendJavaScript( String.format("Wicket.Event.publish(Isis.Topic.FOCUS_FIRST_ACTION_PARAMETER, \'%s\')", getMarkupId())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean isUnitTestCase(Class scriptClass) { boolean isUnitTestCase = false; try { XXX ctxtClassLoader = getClassLoader(); try { Class testCaseClass = ctxtClassLoader.loadClass("groovy.util.GroovyTestCase"); if (testCaseClass.isAssignableFrom(scriptClass)) { isUnitTestCase = true; } } catch (ClassNotFoundException e) { } } catch (Exception e) { } return isUnitTestCase; }', 'ground_truth': 'private boolean isUnitTestCase(Class scriptClass) { boolean isUnitTestCase = false; try { try { Class testCaseClass = this.loader.loadClass("groovy.util.GroovyTestCase"); if (testCaseClass.isAssignableFrom(scriptClass)) { isUnitTestCase = true; } } catch (ClassNotFoundException e) { } } catch (Exception e) { } return isUnitTestCase; }', 'output': 'private boolean isUnitTestCase(Class<?> scriptClass) { boolean isUnitTestCase = false; try { ClassLoader ctxtClassLoader = getClassLoader(); try { Class testCaseClass = ctxtClassLoader.loadClass("groovy.util.GroovyTestCase"); if (testCaseClass.isAssignableFrom(scriptClass)) { isUnitTestCase = true; } } catch (ClassNotFoundException e) { } } catch (Exception e) { } return isUnitTestCase; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void removeDisplayNameForLocale(String locale) throws VersionNotSupportedException { XXX map = new java.util.HashMap(); for (int i=0;i<sizeDisplayName();i++) { String desc=getDisplayName(i); String loc=getDisplayNameXmlLang(i); if ((locale==null && loc!=null) || (locale!=null && !locale.equalsIgnoreCase(loc))) map.put(loc, desc); } setAllDisplayNames(map); }', 'ground_truth': 'public void removeDisplayNameForLocale(String locale) throws VersionNotSupportedException { Map<String, String> map = new HashMap<>(); for (int i=0;i<sizeDisplayName();i++) { String desc=getDisplayName(i); String loc=getDisplayNameXmlLang(i); if ((locale==null && loc!=null) || (locale!=null && !locale.equalsIgnoreCase(loc))) map.put(loc, desc); } setAllDisplayNames(map); }', 'output': 'public void removeDisplayNameForLocale(String locale) throws VersionNotSupportedException { Map<String, String> map = new HashMap<>(); for (int i=0;i<sizeDisplayName();i++) { String desc=getDisplayName(i); String loc=getDisplayNameXmlLang(i); if ((locale==null && loc!=null) || (locale!=null && !locale.equalsIgnoreCase(loc))) map.put(loc, desc); } setAllDisplayNames(map); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public synchronized void mapHostToCluster(String hostname, String clusterName) throws AmbariException { Cluster cluster = getCluster(clusterName); XXX host = (HostImpl) getHost(hostname); if (!hostClusterMap.containsKey(hostname)) { throw new HostNotFoundException(hostname); } for (Cluster c : hostClusterMap.get(hostname)) { if (c.getClusterName().equals(clusterName)) { throw new DuplicateResourceException("Attempted to create a host which already exists: clusterName=" + clusterName + ", hostName=" + hostname); } } if (!isOsSupportedByClusterStack(cluster, host)) { String message = "Trying to map host to cluster where stack does not" + " support host\'s os type" + ", clusterName=" + clusterName + ", clusterStackId=" + cluster.getDesiredStackVersion().getStackId() + ", hostname=" + hostname + ", hostOsType=" + host.getOsType(); LOG.warn(message); throw new AmbariException(message); } mapHostClusterEntities(hostname, cluster.getClusterId()); hostClusterMap.get(hostname).add(cluster); clusterHostMap.get(clusterName).add(host); cluster.refresh(); host.refresh(); if (LOG.isDebugEnabled()) { LOG.debug("Mapping a host to a cluster" + ", clusterName=" + clusterName + ", clusterId=" + cluster.getClusterId() + ", hostname=" + hostname); } }', 'ground_truth': 'public void mapHostToCluster(String hostname, String clusterName) throws AmbariException { loadClustersAndHosts(); w.lock(); try { Host host = getHost(hostname); Cluster cluster = getCluster(clusterName); for (Cluster c : hostClusterMap.get(hostname)) { if (c.getClusterName().equals(clusterName)) { throw new DuplicateResourceException("Attempted to create a host which already exists: clusterName=" + clusterName + ", hostName=" + hostname); } } if (!isOsSupportedByClusterStack(cluster, host)) { String message = "Trying to map host to cluster where stack does not" + " support host\'s os type" + ", clusterName=" + clusterName + ", clusterStackId=" + cluster.getDesiredStackVersion().getStackId() + ", hostname=" + hostname + ", hostOsType=" + host.getOsType(); LOG.warn(message); throw new AmbariException(message); } mapHostClusterEntities(hostname, cluster.getClusterId()); hostClusterMap.get(hostname).add(cluster); clusterHostMap.get(clusterName).add(host); if (LOG.isDebugEnabled()) { LOG.debug("Mapping a host to a cluster" + ", clusterName=" + clusterName + ", clusterId=" + cluster.getClusterId() + ", hostname=" + hostname); } } finally { w.unlock(); } }', 'output': 'public synchronized void mapHostToCluster(String hostname, String clusterName) throws AmbariException { Cluster cluster = getCluster(clusterName); Host host = getHost(hostname); if (!hostClusterMap.containsKey(hostname)) { throw new HostNotFoundException(hostname); } for (Cluster c : hostClusterMap.get(hostname)) { if (c.getClusterName().equals(clusterName)) { throw new DuplicateResourceException("Attempted to create a host which already exists: clusterName=" + clusterName + ", hostName=" + hostname); } } if (!isOsSupportedByClusterStack(cluster, host)) { String message = "Trying to map host to cluster where stack does not" + " support host\'s os type" + ", clusterName=" + clusterName + ", clusterStackId=" + cluster.getDesiredStackVersion().getStackId() + ", hostname=" + hostname + ", hostOsType=" + host.getOsType(); LOG.warn(message); throw new AmbariException(message); } mapHostClusterEntities(hostname, cluster.getClusterId()); hostClusterMap.get(hostname).add(cluster); clusterHostMap.get(clusterName).add(host); cluster.refresh(); host.refresh(); if (LOG.isDebugEnabled()) { LOG.debug("Mapping a host to a cluster" + ", clusterName=" + clusterName + ", clusterId=" + cluster.getClusterId() + ", hostname=" + hostname); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void startAdditionalNodes(int cnt, String queueName) throws Exception { GridCacheAffinityFunction aff = cache(0).configuration().getAffinity(); GridCacheAffinityKeyMapper mapper = cache(0).configuration().getAffinityMapper(); assertNotNull(aff); assertNotNull(mapper); int part = aff.partition(mapper.affinityKey(queueName)); Collection<GridNode> nodes = grid(0).nodes(); Collection<GridNode> aff0 = cache(0).affinity().mapKeyToPrimaryAndBackups(queueName); Collection<GridNode> aff1 = nodes(aff, part, nodes); assertEquals(new ArrayList<>(aff0), new ArrayList<>(aff1)); Collection<GridNode> aff2; XXX tmpNodes; int retries = 10000; do { tmpNodes = new ArrayList<>(cnt); for (int i = 0; i < cnt; i++) tmpNodes.add(new GridTestNode(UUID.randomUUID())); aff2 = nodes(aff, part, F.concat(true, tmpNodes, nodes)); if (retries-- < 0) throw new GridException("Failed to find node IDs to change current affinity mapping."); } while (F.containsAny(aff1, aff2)); int i = GRID_CNT; for (UUID id : F.nodeIds(tmpNodes)) { nodeId = id; startGrid(i++); } aff2 = cache(0).affinity().mapKeyToPrimaryAndBackups(queueName); assertFalse("Unexpected affinity [aff1=" + aff1 + ", aff2=" + aff2 + \']\', F.containsAny(aff1, aff2)); }', 'ground_truth': 'private void startAdditionalNodes(int cnt, String queueName) throws Exception { GridCacheAffinityFunction aff = cache(0).configuration().getAffinity(); GridCacheAffinityKeyMapper mapper = cache(0).configuration().getAffinityMapper(); assertNotNull(aff); assertNotNull(mapper); int part = aff.partition(mapper.affinityKey(queueName)); Collection<ClusterNode> nodes = grid(0).nodes(); Collection<ClusterNode> aff0 = cache(0).affinity().mapKeyToPrimaryAndBackups(queueName); Collection<ClusterNode> aff1 = nodes(aff, part, nodes); assertEquals(new ArrayList<>(aff0), new ArrayList<>(aff1)); Collection<ClusterNode> aff2; Collection<ClusterNode> tmpNodes; int retries = 10000; do { tmpNodes = new ArrayList<>(cnt); for (int i = 0; i < cnt; i++) tmpNodes.add(new GridTestNode(UUID.randomUUID())); aff2 = nodes(aff, part, F.concat(true, tmpNodes, nodes)); if (retries-- < 0) throw new GridException("Failed to find node IDs to change current affinity mapping."); } while (F.containsAny(aff1, aff2)); int i = GRID_CNT; for (UUID id : F.nodeIds(tmpNodes)) { nodeId = id; startGrid(i++); } aff2 = cache(0).affinity().mapKeyToPrimaryAndBackups(queueName); assertFalse("Unexpected affinity [aff1=" + aff1 + ", aff2=" + aff2 + \']\', F.containsAny(aff1, aff2)); }', 'output': 'private void startAdditionalNodes(int cnt, String queueName) throws Exception { GridCacheAffinityFunction aff = cache(0).configuration().getAffinity(); GridCacheAffinityKeyMapper mapper = cache(0).configuration().getAffinityMapper(); assertNotNull(aff); assertNotNull(mapper); int part = aff.partition(mapper.affinityKey(queueName)); Collection<ClusterNode> nodes = grid(0).nodes(); Collection<ClusterNode> aff0 = cache(0).affinity().mapKeyToPrimaryAndBackups(queueName); Collection<ClusterNode> aff1 = nodes(aff, part, nodes); assertEquals(new ArrayList<>(aff0), new ArrayList<>(aff1)); Collection<ClusterNode> aff2; List<ClusterNode> tmpNodes; int retries = 10000; do { tmpNodes = new ArrayList<>(cnt); for (int i = 0; i < cnt; i++) tmpNodes.add(new GridTestNode(UUID.randomUUID())); aff2 = nodes(aff, part, F.concat(true, tmpNodes, nodes)); if (retries-- < 0) throw new GridException("Failed to find node IDs to change current affinity mapping."); } while (F.containsAny(aff1, aff2)); int i = GRID_CNT; for (ClusterNode id : F.nodeIds(tmpNodes)) { nodeId = id; startGrid(i++); } aff2 = cache(0).affinity().mapKeyToPrimaryAndBackups(queueName); assertFalse("Unexpected affinity [aff1=" + aff1 + ", aff2=" + aff2 + \']\', F.containsAny(aff1, aff2)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testTextQueryOnProjection() throws Exception { GridCache<Integer, ObjectValue> cache = ignite.cache(null); cache.putx(1, new ObjectValue("test", 1)); cache.putx(2, new ObjectValue("test", 2)); P2<Integer, ObjectValue> p = new P2<Integer, ObjectValue>() { @Override public boolean apply(Integer key, ObjectValue val) { return val.intVal == 1; } }; GridCacheProjection<Integer, ObjectValue> cachePrj = grid(0).<Integer, ObjectValue>cache(null).projection(p); GridCacheQuery<Map.Entry<Integer, ObjectValue>> qry = cachePrj.queries().createFullTextQuery(ObjectValue.class, "test"); GridCacheQueryFuture<Map.Entry<Integer, ObjectValue>> iter = qry.execute(); assert iter != null; int expCnt = 1; for (int i = 0; i < expCnt; i++) assert iter.next() != null; assert iter.next() == null; }', 'ground_truth': 'public void testTextQueryOnProjection() throws Exception { Cache<Integer, ObjectValue> cache = ignite.cache(null); cache.putx(1, new ObjectValue("test", 1)); cache.putx(2, new ObjectValue("test", 2)); P2<Integer, ObjectValue> p = new P2<Integer, ObjectValue>() { @Override public boolean apply(Integer key, ObjectValue val) { return val.intVal == 1; } }; CacheProjection<Integer, ObjectValue> cachePrj = grid(0).<Integer, ObjectValue>cache(null).projection(p); CacheQuery<Map.Entry<Integer, ObjectValue>> qry = cachePrj.queries().createFullTextQuery(ObjectValue.class, "test"); CacheQueryFuture<Map.Entry<Integer, ObjectValue>> iter = qry.execute(); assert iter != null; int expCnt = 1; for (int i = 0; i < expCnt; i++) assert iter.next() != null; assert iter.next() == null; }', 'output': 'public void testTextQueryOnProjection() throws Exception { Cache<Integer, ObjectValue> cache = ignite.cache(null); cache.putx(1, new ObjectValue("test", 1)); cache.putx(2, new ObjectValue("test", 2)); P2<Integer, ObjectValue> p = new P2<Integer, ObjectValue>() { @Override public boolean apply(Integer key, ObjectValue val) { return val.intVal == 1; } }; CacheProjection<Integer, ObjectValue> cachePrj = grid(0).<Integer, ObjectValue>cache(null).projection(p); CacheQuery<Map.Entry<Integer, ObjectValue>> qry = cachePrj.queries().createFullTextQuery(ObjectValue.class, "test"); CacheQueryFuture<Map.Entry<Integer, ObjectValue>> iter = qry.execute(); assert iter != null; int expCnt = 1; for (int i = 0; i < expCnt; i++) assert iter.next() != null; assert iter.next() == null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private IgniteInternalTx txStart0(IgniteTxConcurrency concurrency, IgniteTxIsolation isolation, long timeout, int txSize, boolean sys) { XXX cfg = cctx.gridConfig().getTransactionsConfiguration(); if (!cfg.isTxSerializableEnabled() && isolation == SERIALIZABLE) throw new IllegalArgumentException("SERIALIZABLE isolation level is disabled (to enable change " + "\'txSerializableEnabled\' configuration property)"); IgniteInternalTx<K, V> tx = (IgniteInternalTx<K, V>)cctx.tm().userTx(); if (tx != null) throw new IllegalStateException("Failed to start new transaction " + "(current thread already has a transaction): " + tx); tx = cctx.tm().newTx( false, false, sys, concurrency, isolation, timeout, false, true, txSize, /** group lock keys */null, /** partition lock */false ); assert tx != null; return tx; }', 'ground_truth': 'private IgniteInternalTx txStart0(IgniteTxConcurrency concurrency, IgniteTxIsolation isolation, long timeout, int txSize, boolean sys) { TransactionConfiguration cfg = cctx.gridConfig().getTransactionsConfiguration(); if (!cfg.isTxSerializableEnabled() && isolation == SERIALIZABLE) throw new IllegalArgumentException("SERIALIZABLE isolation level is disabled (to enable change " + "\'txSerializableEnabled\' configuration property)"); IgniteInternalTx<K, V> tx = (IgniteInternalTx<K, V>)cctx.tm().userTx(); if (tx != null) throw new IllegalStateException("Failed to start new transaction " + "(current thread already has a transaction): " + tx); tx = cctx.tm().newTx( false, false, sys, concurrency, isolation, timeout, false, true, txSize, /** group lock keys */null, /** partition lock */false ); assert tx != null; return tx; }', 'output': 'private IgniteInternalTx<K, V> txStart0(IgniteTxConcurrency concurrency, IgniteTxIsolation isolation, long timeout, int txSize, boolean sys) { TransactionConfiguration cfg = cctx.gridConfig().getTransactionsConfiguration(); if (!cfg.isTxSerializableEnabled() && isolation == SERIALIZABLE) throw new IllegalArgumentException("SERIALIZABLE isolation level is disabled (to enable change " + "\'txSerializableEnabled\' configuration property)"); IgniteInternalTx<K, V> tx = (IgniteInternalTx<K, V>)cctx.tm().userTx(); if (tx != null) throw new IllegalStateException("Failed to start new transaction " + "(current thread already has a transaction): " + tx); tx = cctx.tm().newTx( false, false, sys, concurrency, isolation, timeout, false, true, txSize, /** group lock keys */null, /** partition lock */false ); assert tx != null; return tx; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected String dump(boolean canonical) { XXX buf = new StringBuffer(); String dn = getParentElement() instanceof AssignmentInstruction ? null : getNodeTypeSymbol(); if (dn != null) { if (canonical) buf.append("<"); buf.append(dn); buf.append(\' \'); } buf.append(_CoreStringUtils.toFTLTopLevelTragetIdentifier(variableName)); if (valueExp != null) { buf.append(\' \'); } buf.append(getOperatorTypeAsString()); if (valueExp != null) { buf.append(\' \'); buf.append(valueExp.getCanonicalForm()); } if (dn != null) { if (namespaceExp != null) { buf.append(" in "); buf.append(namespaceExp.getCanonicalForm()); } if (canonical) buf.append(">"); } String result = buf.toString(); return result; }', 'ground_truth': 'protected String dump(boolean canonical) { StringBuilder buf = new StringBuilder(); String dn = getParentElement() instanceof AssignmentInstruction ? null : getNodeTypeSymbol(); if (dn != null) { if (canonical) buf.append("<"); buf.append(dn); buf.append(\' \'); } buf.append(_CoreStringUtils.toFTLTopLevelTragetIdentifier(variableName)); if (valueExp != null) { buf.append(\' \'); } buf.append(getOperatorTypeAsString()); if (valueExp != null) { buf.append(\' \'); buf.append(valueExp.getCanonicalForm()); } if (dn != null) { if (namespaceExp != null) { buf.append(" in "); buf.append(namespaceExp.getCanonicalForm()); } if (canonical) buf.append(">"); } String result = buf.toString(); return result; }', 'output': 'protected String dump(boolean canonical) { StringBuilder buf = new StringBuilder(); String dn = getParentElement() instanceof AssignmentInstruction ? null : getNodeTypeSymbol(); if (dn != null) { if (canonical) buf.append("<"); buf.append(dn); buf.append(\' \'); } buf.append(_CoreStringUtils.toFTLTopLevelTragetIdentifier(variableName)); if (valueExp != null) { buf.append(\' \'); } buf.append(getOperatorTypeAsString()); if (valueExp != null) { buf.append(\' \'); buf.append(valueExp.getCanonicalForm()); } if (dn != null) { if (namespaceExp != null) { buf.append(" in "); buf.append(namespaceExp.getCanonicalForm()); } if (canonical) buf.append(">"); } String result = buf.toString(); return result; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Output<InputT> output() { Dataset<InputT> positiveOutput = Filter.named(name + POSITIVE_FILTER_SUFFIX).of(input).by(predicate).output(); XXX negativeOutput = Filter.named(name + NEGATIVE_FILTER_SUFFIX) .of(input) .by((UnaryPredicate<InputT>) what -> !predicate.apply(what)) .output(); return new Output<>(positiveOutput, negativeOutput); }', 'ground_truth': 'public Output<InputT> output() { PCollection<InputT> positiveOutput = Filter.named(name + POSITIVE_FILTER_SUFFIX).of(input).by(predicate).output(); PCollection<InputT> negativeOutput = Filter.named(name + NEGATIVE_FILTER_SUFFIX) .of(input) .by((UnaryPredicate<InputT>) what -> !predicate.apply(what)) .output(); return new Output<>(positiveOutput, negativeOutput); }', 'output': 'public Output<InputT> output() { Dataset<InputT> positiveOutput = Filter.named(name + POSITIVE_FILTER_SUFFIX).of(input).by(predicate).output(); Dataset<InputT> negativeOutput = Filter.named(name + NEGATIVE_FILTER_SUFFIX) .of(input) .by((UnaryPredicate<InputT>) what -> !predicate.apply(what)) .output(); return new Output<>(positiveOutput, negativeOutput); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Map<UUID, Integer> serviceTopology(String name) { for (XXX desc : ctx.service().deployedServices()) { if (desc.name().equals(name)) return desc.topologySnapshot(); } return null; }', 'ground_truth': 'private Map<UUID, Integer> serviceTopology(String name) { for (ManagedServiceDescriptor desc : ctx.service().deployedServices()) { if (desc.name().equals(name)) return desc.topologySnapshot(); } return null; }', 'output': 'private Map<UUID, Deployment> serviceTopology(String name) { for (Deployment desc : ctx.service().deployedServices()) { if (desc.name().equals(name)) return desc.topologySnapshot(); } return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void process(final ProcessMethodContext processMethodContext) { final Class<?> declaringClass = processMethodContext.getMethod().getDeclaringClass(); final XXX spec = getSpecificationLoader().loadSpecification( declaringClass); if (spec.containsNonFallbackFacet(ImmutableFacet.class)) { final FacetedMethod facetHolder = processMethodContext.getFacetHolder(); DisabledFacet facet = facetHolder.getFacet(DisabledFacet.class); if(facet != null && facet.isInvertedSemantics()) { return; } super.addFacet(new DisabledFacetOnPropertyDerivedFromImmutable(facetHolder)); } }', 'ground_truth': 'public void process(final ProcessMethodContext processMethodContext) { val declaringClass = processMethodContext.getMethod().getDeclaringClass(); val spec = getSpecificationLoader().loadSpecification(declaringClass); val immutableFacet = spec.getFacet(ImmutableFacet.class); if (immutableFacet!=null && !immutableFacet.isFallback()) { final FacetedMethod facetHolder = processMethodContext.getFacetHolder(); DisabledFacet facet = facetHolder.getFacet(DisabledFacet.class); if(facet != null && facet.isInvertedSemantics()) { return; } super.addFacet(DisabledFacetOnPropertyDerivedFromImmutable .forImmutable(facetHolder, immutableFacet)); } }', 'output': 'public void process(final ProcessMethodContext processMethodContext) { final Class<?> declaringClass = processMethodContext.getMethod().getDeclaringClass(); final Specification spec = getSpecificationLoader().loadSpecification( declaringClass); if (spec.containsNonFallbackFacet(ImmutableFacet.class)) { final FacetedMethod facetHolder = processMethodContext.getFacetHolder(); DisabledFacet facet = facetHolder.getFacet(DisabledFacet.class); if(facet != null && facet.isInvertedSemantics()) { return; } super.addFacet(new DisabledFacetOnPropertyDerivedFromImmutable(facetHolder)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void conflictingSessionAcquisitionFailure() throws Exception { cfs.disableAutoCompaction(); makeSSTables(2); List<SSTableReader> sstables = new ArrayList<>(cfs.getLiveSSTables()); assertEquals(2, sstables.size()); SSTableReader repaired = sstables.get(0); SSTableReader unrepaired = sstables.get(1); assertTrue(repaired.intersects(FULL_RANGE)); assertTrue(unrepaired.intersects(FULL_RANGE)); XXX sessionId = prepareSession(); repaired.descriptor.getMetadataSerializer().mutateRepairMetadata(repaired.descriptor, 0, sessionId, false); repaired.reloadSSTableMetadata(); assertTrue(repaired.isPendingRepair()); PendingAntiCompaction.AcquisitionCallable acquisitionCallable = new PendingAntiCompaction.AcquisitionCallable(cfs, FULL_RANGE, UUIDGen.getTimeUUID(), 0, 0); PendingAntiCompaction.AcquireResult result = acquisitionCallable.call(); assertNull(result); }', 'ground_truth': 'public void conflictingSessionAcquisitionFailure() throws Exception { cfs.disableAutoCompaction(); makeSSTables(2); List<SSTableReader> sstables = new ArrayList<>(cfs.getLiveSSTables()); assertEquals(2, sstables.size()); SSTableReader repaired = sstables.get(0); SSTableReader unrepaired = sstables.get(1); assertTrue(repaired.intersects(FULL_RANGE)); assertTrue(unrepaired.intersects(FULL_RANGE)); TimeUUID sessionId = prepareSession(); repaired.descriptor.getMetadataSerializer().mutateRepairMetadata(repaired.descriptor, 0, sessionId, false); repaired.reloadSSTableMetadata(); assertTrue(repaired.isPendingRepair()); PendingAntiCompaction.AcquisitionCallable acquisitionCallable = new PendingAntiCompaction.AcquisitionCallable(cfs, FULL_RANGE, nextTimeUUID(), 0, 0); PendingAntiCompaction.AcquireResult result = acquisitionCallable.call(); assertNull(result); }', 'output': 'public void conflictingSessionAcquisitionFailure() throws Exception { cfs.disableAutoCompaction(); makeSSTables(2); List<SSTableReader> sstables = new ArrayList<>(cfs.getLiveSSTables()); assertEquals(2, sstables.size()); SSTableReader repaired = sstables.get(0); SSTableReader unrepaired = sstables.get(1); assertTrue(repaired.intersects(FULL_RANGE)); assertTrue(unrepaired.intersects(FULL_RANGE)); SessionId sessionId = prepareSession(); repaired.descriptor.getMetadataSerializer().mutateRepairMetadata(repaired.descriptor, 0, sessionId, false); repaired.reloadSSTableMetadata(); assertTrue(repaired.isPendingRepair()); PendingAntiCompaction.AcquisitionCallable acquisitionCallable = new PendingAntiCompaction.AcquisitionCallable(cfs, FULL_RANGE, UUIDGen.getTimeUUID(), 0, 0); PendingAntiCompaction.AcquireResult result = acquisitionCallable.call(); assertNull(result); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public scala.Option<RDD<Tuple2<Source<T>, CheckpointMarkT>>> compute(Time validTime) { RDD<scala.Tuple2<Source<T>, CheckpointMarkT>> rdd = new SourceRDD.Unbounded<>( ssc().sparkContext(), options, createMicrobatchSource(), numPartitions); return scala.Option.apply(rdd); }', 'ground_truth': 'public scala.Option<RDD<Tuple2<Source<T>, CheckpointMarkT>>> compute(Time validTime) { RDD<Tuple2<Source<T>, CheckpointMarkT>> rdd = new SourceRDD.Unbounded<>( ssc().sparkContext(), options, createMicrobatchSource(), numPartitions); return scala.Option.apply(rdd); }', 'output': 'public RDD<Tuple2<Source<T>, CheckpointMarkT>> compute(Time validTime) { RDD<Tuple2<Source<T>, CheckpointMarkT>> rdd = new SourceRDD.Unbounded<>( ssc().sparkContext(), options, createMicrobatchSource(), numPartitions); return rdd; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkPutsConcurrentMap(int threadCnt, long duration) throws Exception { final Map<GridByteArrayWrapper, byte[]> map = new ConcurrentHashMap8<>(); final AtomicLong opCnt = new AtomicLong(); final AtomicLong totalOpCnt = new AtomicLong(); final AtomicBoolean done = new AtomicBoolean(); long start = System.currentTimeMillis(); XXX fut = multithreadedAsync(new Callable<Object>() { @Override public Object call() throws Exception { Random rnd = new Random(); long locTotalOpCnt = 0; while (!done.get()) { for (int i = 0; i < 500; i++) { GridByteArrayWrapper key = randomKeyWrapper(rnd); map.put(key, new byte[1024]); } locTotalOpCnt += 500; opCnt.addAndGet(500); } totalOpCnt.addAndGet(locTotalOpCnt); return null; } }, threadCnt); final int step = 2000; while (System.currentTimeMillis() - start < duration) { U.sleep(step); long ops = opCnt.getAndSet(0); info("Putting " + (ops * 1000) / step + " ops/sec"); } done.set(true); fut.get(); long end = System.currentTimeMillis(); info("Average put performance: " + (totalOpCnt.get() * 1000) / (end - start) + " ops/sec"); }', 'ground_truth': 'private void checkPutsConcurrentMap(int threadCnt, long duration) throws Exception { final Map<GridByteArrayWrapper, byte[]> map = new ConcurrentHashMap8<>(); final AtomicLong opCnt = new AtomicLong(); final AtomicLong totalOpCnt = new AtomicLong(); final AtomicBoolean done = new AtomicBoolean(); long start = System.currentTimeMillis(); IgniteFuture<?> fut = multithreadedAsync(new Callable<Object>() { @Override public Object call() throws Exception { Random rnd = new Random(); long locTotalOpCnt = 0; while (!done.get()) { for (int i = 0; i < 500; i++) { GridByteArrayWrapper key = randomKeyWrapper(rnd); map.put(key, new byte[1024]); } locTotalOpCnt += 500; opCnt.addAndGet(500); } totalOpCnt.addAndGet(locTotalOpCnt); return null; } }, threadCnt); final int step = 2000; while (System.currentTimeMillis() - start < duration) { U.sleep(step); long ops = opCnt.getAndSet(0); info("Putting " + (ops * 1000) / step + " ops/sec"); } done.set(true); fut.get(); long end = System.currentTimeMillis(); info("Average put performance: " + (totalOpCnt.get() * 1000) / (end - start) + " ops/sec"); }', 'output': 'private void checkPutsConcurrentMap(int threadCnt, long duration) throws Exception { final Map<GridByteArrayWrapper, byte[]> map = new ConcurrentHashMap8<>(); final AtomicLong opCnt = new AtomicLong(); final AtomicLong totalOpCnt = new AtomicLong(); final AtomicBoolean done = new AtomicBoolean(); long start = System.currentTimeMillis(); IgniteFuture<?> fut = multithreadedAsync(new Callable<Object>() { @Override public Object call() throws Exception { Random rnd = new Random(); long locTotalOpCnt = 0; while (!done.get()) { for (int i = 0; i < 500; i++) { GridByteArrayWrapper key = randomKeyWrapper(rnd); map.put(key, new byte[1024]); } locTotalOpCnt += 500; opCnt.addAndGet(500); } totalOpCnt.addAndGet(locTotalOpCnt); return null; } }, threadCnt); final int step = 2000; while (System.currentTimeMillis() - start < duration) { U.sleep(step); long ops = opCnt.getAndSet(0); info("Putting " + (ops * 1000) / step + " ops/sec"); } done.set(true); fut.get(); long end = System.currentTimeMillis(); info("Average put performance: " + (totalOpCnt.get() * 1000) / (end - start) + " ops/sec"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void putMapKeyValue(final MapDto mapDto, final String key, final String value) { if(mapDto == null) { return; } final Optional<MapDto.Entry> entryIfAny = entryIfAnyFor(mapDto, key); if(entryIfAny.isPresent()) { entryIfAny.get().setValue(value); } else { final XXX entry = new MapDto.Entry(); entry.setKey(key); entry.setValue(value); mapDto.getEntry().add(entry); } }', 'ground_truth': 'public static void putMapKeyValue(final MapDto mapDto, final String key, final String value) { if(mapDto == null) { return; } final Optional<MapDto.Entry> entryIfAny = entryIfAnyFor(mapDto, key); if(entryIfAny.isPresent()) { entryIfAny.get().setValue(value); } else { val entry = new MapDto.Entry(); entry.setKey(key); entry.setValue(value); mapDto.getEntry().add(entry); } }', 'output': 'public static void putMapKeyValue(final MapDto mapDto, final String key, final String value) { if(mapDto == null) { return; } final Optional<MapDto.Entry> entryIfAny = entryIfAnyFor(mapDto, key); if(entryIfAny.isPresent()) { entryIfAny.get().setValue(value); } else { final MapDto.Entry entry = new MapDto.Entry(); entry.setKey(key); entry.setValue(value); mapDto.getEntries().add(entry); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public TransformResult<KeyedWorkItem<K, V>> finishBundle() throws Exception { XXX state = stepContext.commitState(); return StepTransformResult.<KeyedWorkItem<K, V>>withHold( application, state.getEarliestWatermarkHold()) .withState(state) .addOutput(outputBundles) .withTimerUpdate(stepContext.getTimerUpdate()) .withAggregatorChanges(aggregatorChanges) .addUnprocessedElements(unprocessedElements.build()) .build(); }', 'ground_truth': 'public TransformResult<KeyedWorkItem<K, V>> finishBundle() throws Exception { CopyOnAccessInMemoryStateInternals state = stepContext.commitState(); return StepTransformResult.<KeyedWorkItem<K, V>>withHold( application, state.getEarliestWatermarkHold()) .withState(state) .addOutput(outputBundles) .withTimerUpdate(stepContext.getTimerUpdate()) .withAggregatorChanges(aggregatorChanges) .addUnprocessedElements(unprocessedElements.build()) .build(); }', 'output': 'public TransformResult<KeyedWorkItem<K, V>> finishBundle() throws Exception { State state = stepContext.commitState(); return StepTransformResult.<KeyedWorkItem<K, V>>withHold( application, state.getEarliestWatermarkHold()) .withState(state) .addOutput(outputBundles) .withTimerUpdate(stepContext.getTimerUpdate()) .withAggregatorChanges(aggregatorChanges) .addUnprocessedElements(unprocessedElements.build()) .build(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ViewFragment markReferenced(RowPosition startWith, RowPosition stopAt) { DataTracker.View view; List<SSTableReader> sstables; while (true) { view = data.getView(); XXX stopInTree = stopAt.isMinimum() ? view.intervalTree.max() : stopAt; sstables = view.intervalTree.search(new Interval(startWith, stopInTree)); if (SSTableReader.acquireReferences(sstables)) break; } return new ViewFragment(sstables, Iterables.concat(Collections.singleton(view.memtable), view.memtablesPendingFlush)); }', 'ground_truth': 'public ViewFragment markReferenced(RowPosition startWith, RowPosition stopAt) { DataTracker.View view; List<SSTableReader> sstables; while (true) { view = data.getView(); if (view.intervalTree.isEmpty()) { sstables = Collections.<SSTableReader>emptyList(); break; } RowPosition stopInTree = stopAt.isMinimum() ? view.intervalTree.max() : stopAt; sstables = view.intervalTree.search(Interval.<RowPosition, SSTableReader>create(startWith, stopInTree)); if (SSTableReader.acquireReferences(sstables)) break; } return new ViewFragment(sstables, Iterables.concat(Collections.singleton(view.memtable), view.memtablesPendingFlush)); }', 'output': 'public ViewFragment markReferenced(RowPosition startWith, RowPosition stopAt) { DataTracker.View view; List<SSTableReader> sstables; while (true) { view = data.getView(); StopInTree stopInTree = stopAt.isMinimum() ? view.intervalTree.max() : stopAt; sstables = view.intervalTree.search(new Interval(startWith, stopInTree)); if (SSTableReader.acquireReferences(sstables)) break; } return new ViewFragment(sstables, Iterables.concat(Collections.singleton(view.memtable), view.memtablesPendingFlush)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testLongArrayModification() { TestObjectAllTypes obj = new TestObjectAllTypes(); obj.lArr = new long[] {1, 1, 1}; BinaryObjectBuilderImpl mutObj = wrap(obj); long[] arr = mutObj.getField("lArr"); arr[0] = 2; XXX res = mutObj.build().deserialize(); Assert.assertArrayEquals(new long[] {2, 1, 1}, res.lArr); }', 'ground_truth': 'public void testLongArrayModification() { GridPortableTestClasses.TestObjectAllTypes obj = new GridPortableTestClasses.TestObjectAllTypes(); obj.lArr = new long[] {1, 1, 1}; BinaryObjectBuilderImpl mutObj = wrap(obj); long[] arr = mutObj.getField("lArr"); arr[0] = 2; GridPortableTestClasses.TestObjectAllTypes res = mutObj.build().deserialize(); Assert.assertArrayEquals(new long[] {2, 1, 1}, res.lArr); }', 'output': 'public void testLongArrayModification() { TestObjectAllTypes obj = new TestObjectAllTypes(); obj.lArr = new long[] {1, 1, 1}; BinaryObjectBuilderImpl mutObj = wrap(obj); long[] arr = mutObj.getField("lArr"); arr[0] = 2; BinaryObject res = mutObj.build().deserialize(); Assert.assertArrayEquals(new long[] {2, 1, 1}, res.lArr); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public Serializable execute() throws IgniteCheckedException { if (log.isInfoEnabled()) log.info("Execute GridTestGridifyJob.execute(" + argument(0) + \')\'); GridTestAopTarget target = new GridTestAopTarget(); try { if ("1".equals(argument(0))) return target.gridifyNonDefaultClass("10"); else if ("2".equals(argument(0))) return target.gridifyNonDefaultName("20"); else if ("3".equals(argument(0))) return target.gridifyNonDefaultClassResource("30"); else if ("4".equals(argument(0))) return target.gridifyNonDefaultNameResource("40"); } catch (XXX e) { throw new RuntimeException("Failed to execute target method.", e); } assert false : "Argument must be equals to \\"0\\" [gridifyArg=" + argument(0) + \']\'; return null; } } }', 'ground_truth': '@Override public Serializable execute() throws IgniteCheckedException { if (log.isInfoEnabled()) log.info("Execute TestGridifyJob.execute(" + argument(0) + \')\'); TestAopTarget target = new TestAopTarget(); try { if ("1".equals(argument(0))) return target.gridifyNonDefaultClass("10"); else if ("2".equals(argument(0))) return target.gridifyNonDefaultName("20"); else if ("3".equals(argument(0))) return target.gridifyNonDefaultClassResource("30"); else if ("4".equals(argument(0))) return target.gridifyNonDefaultNameResource("40"); } catch (TestGridifyException e) { throw new RuntimeException("Failed to execute target method.", e); } assert false : "Argument must be equals to \\"0\\" [gridifyArg=" + argument(0) + \']\'; return null; }', 'output': '@Override public Serializable execute() throws IgniteCheckedException { if (log.isInfoEnabled()) log.info("Execute GridTestGridifyJob.execute(" + argument(0) + \')\'); GridTestAopTarget target = new GridTestAopTarget(); try { if ("1".equals(argument(0))) return target.gridifyNonDefaultClass("10"); else if ("2".equals(argument(0))) return target.gridifyNonDefaultName("20"); else if ("3".equals(argument(0))) return target.gridifyNonDefaultClassResource("30"); else if ("4".equals(argument(0))) return target.gridifyNonDefaultNameResource("40"); } catch (IgniteCheckedException e) { throw new RuntimeException("Failed to execute target method.", e); } assert false : "Argument must be equals to \\"0\\" [gridifyArg=" + argument(0) + \']\'; return null; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSimpleRBK() { final Flow flow = Flow.create(); final ListDataSource<Integer> input = ListDataSource.unbounded(Arrays.asList(1, 2, 3), Arrays.asList(2, 3, 4)); final ListDataSink<Pair<Integer, Integer>> output = ListDataSink.get(); ReduceByKey.of(flow.createInput(input, e -> 1000L * e)) .keyBy(i -> i % 2) .reduceBy(Sums.ofInts()) .windowBy(FixedWindows.of(org.joda.time.Duration.standardHours(1))) .triggeredBy(AfterWatermark.pastEndOfWindow()) .discardingFiredPanes() .output() .persist(output); XXX executor = TestUtils.createExecutor(); executor.execute(flow); DatasetAssert.unorderedEquals(output.getOutputs(), Pair.of(0, 8), Pair.of(1, 7)); }', 'ground_truth': 'public void testSimpleRBK() { final Flow flow = Flow.create(); final ListDataSource<Integer> input = ListDataSource.unbounded(Arrays.asList(1, 2, 3), Arrays.asList(2, 3, 4)); final ListDataSink<Pair<Integer, Integer>> output = ListDataSink.get(); ReduceByKey.of(flow.createInput(input, e -> 1000L * e)) .keyBy(i -> i % 2) .reduceBy(Sums.ofInts()) .windowBy(FixedWindows.of(org.joda.time.Duration.standardHours(1))) .triggeredBy(AfterWatermark.pastEndOfWindow()) .discardingFiredPanes() .output() .persist(output); BeamRunnerWrapper executor = BeamRunnerWrapper.ofDirect(); executor.executeSync(flow); DatasetAssert.unorderedEquals(output.getOutputs(), Pair.of(0, 8), Pair.of(1, 7)); }', 'output': 'public void testSimpleRBK() { final Flow flow = Flow.create(); final ListDataSource<Integer> input = ListDataSource.unbounded(Arrays.asList(1, 2, 3), Arrays.asList(2, 3, 4)); final ListDataSink<Pair<Integer, Integer>> output = ListDataSink.get(); ReduceByKey.of(flow.createInput(input, e -> 1000L * e)) .keyBy(i -> i % 2) .reduceBy(Sums.ofInts()) .windowBy(FixedWindows.of(org.joda.time.Duration.standardHours(1))) .triggeredBy(AfterWatermark.pastEndOfWindow()) .discardingFiredPanes() .output() .persist(output); ExecutorService executor = TestUtils.createExecutor(); executor.execute(flow); DatasetAssert.unorderedEquals(output.getOutputs(), Pair.of(0, 8), Pair.of(1, 7)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void append(IgniteFsPath path, int len) throws Exception { XXX os = ggfsPrimary.append(path, false); os.write(new byte[len]); os.close(); }', 'ground_truth': 'private void append(IgniteFsPath path, int len) throws Exception { IgniteFsOutputStream os = ggfsPrimary.append(path, false); os.write(new byte[len]); os.close(); }', 'output': 'private void append(IgniteFsPath path, int len) throws Exception { IgniteFsOutputStream os = ggfsPrimary.append(path, false); os.write(new byte[len]); os.close(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected Expression asExpression(AST node) { AST leftNode = node.getFirstChild(); Expression leftExpression = expression(leftNode); AST rightNode = leftNode.getNextSibling(); XXX type = resolvedName(rightNode); return CastExpression.asExpression(type, leftExpression); }', 'ground_truth': 'protected Expression asExpression(AST node) { AST leftNode = node.getFirstChild(); Expression leftExpression = expression(leftNode); AST rightNode = leftNode.getNextSibling(); ClassNode type = buildName(rightNode); return CastExpression.asExpression(type, leftExpression); }', 'output': 'protected Expression asExpression(AST node) { AST leftNode = node.getFirstChild(); Expression leftExpression = expression(leftNode); AST rightNode = leftNode.getNextSibling(); Class<?> type = resolvedName(rightNode); return CastExpression.asExpression(type, leftExpression); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private SortedMap<String, SortedSet<String>> gatherHostGroupBindings(ClusterTopology clusterTopology) { SortedMap<String, SortedSet<String>> hgBindngs = Maps.newTreeMap(); for (Map.Entry<String, HostGroupInfo> hgEnrty: clusterTopology.getHostGroupInfo().entrySet()) { hgBindngs.put(hgEnrty.getKey(), new TreeSet<>(hgEnrty.getValue().getHostNames())); } return hgBindngs; }', 'ground_truth': 'private Map<String, Set<String>> gatherHostGroupBindings(ClusterTopology clusterTopology) { Map<String, Set<String>> hgBindngs = Maps.newHashMap(); for (Map.Entry<String, HostGroupInfo> hgEnrty: clusterTopology.getHostGroupInfo().entrySet()) { hgBindngs.put(hgEnrty.getKey(), Sets.newCopyOnWriteArraySet(hgEnrty.getValue().getHostNames())); } return hgBindngs; }', 'output': 'private SortedMap<String, Set<String>> gatherHostGroupBindings(ClusterTopology clusterTopology) { SortedMap<String, Set<String>> hgBindngs = Maps.newTreeMap(); for (Map.Entry<String, HostGroupInfo> hgEnrty: clusterTopology.getHostGroupInfo().entrySet()) { hgBindngs.put(hgEnrty.getKey(), new TreeSet<>(hgEnrty.getValue().getHostNames())); } return hgBindngs; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGetFromBackupNode() throws Exception { for (int i = 0; i < GRID_CNT; i++) { GridCache<String, Integer> c = grid(i).cache(null); GridCacheEntry<String, Integer> e = c.entry(KEY); if (e.backup()) { info("Backup node: " + grid(i).localNode().id()); Integer val = c.get(KEY); assert val != null && val == 1; assert !await(); assert c.evict(KEY); assert c.peek(KEY) == null; val = c.get(KEY); assert val != null && val == 1; assert !await(); break; } } }', 'ground_truth': 'public void testGetFromBackupNode() throws Exception { for (int i = 0; i < GRID_CNT; i++) { Cache<String, Integer> c = grid(i).cache(null); CacheEntry<String, Integer> e = c.entry(KEY); if (e.backup()) { info("Backup node: " + grid(i).localNode().id()); Integer val = c.get(KEY); assert val != null && val == 1; assert !await(); assert c.evict(KEY); assert c.peek(KEY) == null; val = c.get(KEY); assert val != null && val == 1; assert !await(); break; } } }', 'output': 'public void testGetFromBackupNode() throws Exception { for (int i = 0; i < GRID_CNT; i++) { Cache<String, Integer> c = grid(i).cache(null); CacheEntry<String, Integer> e = c.entry(KEY); if (e.backup()) { info("Backup node: " + grid(i).localNode().id()); Integer val = c.get(KEY); assert val != null && val == 1; assert !await(); assert c.evict(KEY); assert c.peek(KEY) == null; val = c.get(KEY); assert val != null && val == 1; assert !await(); break; } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPutDebug() throws Exception { for (int i = 0; i < gridCount(); i++) info(">>>>> Grid" + i + ": " + grid(i).localNode().id()); int size = 10; GridCache<Object, Object> prj0 = grid(0).cache(null); for (int i = 0; i < size; i++) { info("Putting value [i=" + i + \']\'); assertNull(prj0.put(i, i)); info("Finished putting value [i=" + i + \']\'); } for (int i = 0; i < gridCount(); i++) { assertEquals(0, context(i).tm().idMapSize()); GridCache<Object, Object> cache = grid(i).cache(null); ClusterNode node = grid(i).localNode(); for (int k = 0; k < size; k++) { if (cache.affinity().isPrimaryOrBackup(node, k)) assertEquals("Check failed for node: " + node.id(), k, cache.peek(k)); } } for (int i = 0; i < size; i++) { info("Putting value 2 [i=" + i + \']\'); assertEquals(i, prj0.putIfAbsent(i, i * i)); info("Finished putting value 2 [i=" + i + \']\'); } for (int i = 0; i < size; i++) assertEquals(i, prj0.get(i)); }', 'ground_truth': 'public void testPutDebug() throws Exception { for (int i = 0; i < gridCount(); i++) info(">>>>> Grid" + i + ": " + grid(i).localNode().id()); int size = 10; IgniteCache<Object, Object> prj0 = grid(0).jcache(null); for (int i = 0; i < size; i++) { info("Putting value [i=" + i + \']\'); prj0.put(i, i); info("Finished putting value [i=" + i + \']\'); } for (int i = 0; i < gridCount(); i++) { assertEquals(0, context(i).tm().idMapSize()); IgniteCache<Object, Object> cache = grid(i).jcache(null); ClusterNode node = grid(i).localNode(); for (int k = 0; k < size; k++) { if (affinity(cache).isPrimaryOrBackup(node, k)) assertEquals("Check failed for node: " + node.id(), k, cache.peek(k)); } } for (int i = 0; i < size; i++) { info("Putting value 2 [i=" + i + \']\'); assertEquals(i, prj0.putIfAbsent(i, i * i)); info("Finished putting value 2 [i=" + i + \']\'); } for (int i = 0; i < size; i++) assertEquals(i, prj0.get(i)); }', 'output': 'public void testPutDebug() throws Exception { for (int i = 0; i < gridCount(); i++) info(">>>>> Grid" + i + ": " + grid(i).localNode().id()); int size = 10; Cache<Object, Object> prj0 = grid(0).cache(null); for (int i = 0; i < size; i++) { info("Putting value [i=" + i + \']\'); assertNull(prj0.put(i, i)); info("Finished putting value [i=" + i + \']\'); } for (int i = 0; i < gridCount(); i++) { assertEquals(0, context(i).tm().idMapSize()); Cache<Object, Object> cache = grid(i).cache(null); ClusterNode node = grid(i).localNode(); for (int k = 0; k < size; k++) { if (cache.affinity().isPrimaryOrBackup(node, k)) assertEquals("Check failed for node: " + node.id(), k, cache.peek(k)); } } for (int i = 0; i < size; i++) { info("Putting value 2 [i=" + i + \']\'); assertEquals(i, prj0.putIfAbsent(i, i * i)); info("Finished putting value 2 [i=" + i + \']\'); } for (int i = 0; i < size; i++) assertEquals(i, prj0.get(i)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean findFieldByName(String name) { if (raw) throw new BinaryObjectException("Failed to read named field because reader is in raw mode."); assert dataStart != start; if (footerLen == 0) return false; if (userType) { int order; if (matching) { int expOrder = matchingOrder++; XXX confirm = schema.confirmOrder(expOrder, name); switch (confirm) { case CONFIRMED: if (expOrder == 0) streamPosition(dataStart); return true; case REJECTED: matching = false; order = schema.order(fieldId(name)); break; default: assert confirm == PortableSchema.Confirmation.CLARIFY; int id = fieldId(name); int realId = schema.fieldId(expOrder); if (id == realId) { schema.clarifyFieldName(expOrder, name); if (expOrder == 0) streamPosition(dataStart); return true; } else { matching = false; order = schema.order(id); } break; } } else order = schema.order(fieldId(name)); return trySetUserFieldPosition(order); } else return trySetSystemFieldPosition(fieldId(name)); }', 'ground_truth': 'public boolean findFieldByName(String name) { if (raw) throw new BinaryObjectException("Failed to read named field because reader is in raw mode."); assert dataStart != start; if (footerLen == 0) return false; if (userType) { int order; if (matching) { int expOrder = matchingOrder++; BinarySchema.Confirmation confirm = schema.confirmOrder(expOrder, name); switch (confirm) { case CONFIRMED: if (expOrder == 0) streamPosition(dataStart); return true; case REJECTED: matching = false; order = schema.order(fieldId(name)); break; default: assert confirm == BinarySchema.Confirmation.CLARIFY; int id = fieldId(name); int realId = schema.fieldId(expOrder); if (id == realId) { schema.clarifyFieldName(expOrder, name); if (expOrder == 0) streamPosition(dataStart); return true; } else { matching = false; order = schema.order(id); } break; } } else order = schema.order(fieldId(name)); return trySetUserFieldPosition(order); } else return trySetSystemFieldPosition(fieldId(name)); }', 'output': 'public boolean findFieldByName(String name) { if (raw) throw new BinaryObjectException("Failed to read named field because reader is in raw mode."); assert dataStart != start; if (footerLen == 0) return false; if (userType) { int order; if (matching) { int expOrder = matchingOrder++; schema.confirmOrder(expOrder, name); switch (schema.confirmOrder(expOrder, name)) { case CONFIRMED: if (expOrder == 0) streamPosition(dataStart); return true; case REJECTED: matching = false; order = schema.order(fieldId(name)); break; default: assert confirm == PortableSchema.Confirmation.CLARIFY; int id = fieldId(name); int realId = schema.fieldId(expOrder); if (id == realId) { schema.clarifyFieldName(expOrder, name); if (expOrder == 0) streamPosition(dataStart); return true; } else { matching = false; order = schema.order(id); } break; } } else order = schema.order(fieldId(name)); return trySetUserFieldPosition(order); } else return trySetSystemFieldPosition(fieldId(name)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testNonSerializable1() throws GridException { IgniteOptimizedMarshaller marsh = new IgniteOptimizedMarshaller(); marsh.setRequireSerializable(false); byte[] bytes = marsh.marshal(new GridTcpDiscoveryVmIpFinder()); XXX ipFinder = marsh.unmarshal(bytes, null); assertFalse(ipFinder.isShared()); ipFinder = marsh.unmarshal(marsh.marshal(new GridTcpDiscoveryVmIpFinder(true)), null); assertTrue(ipFinder.isShared()); }', 'ground_truth': 'public void testNonSerializable1() throws GridException { IgniteOptimizedMarshaller marsh = new IgniteOptimizedMarshaller(); marsh.setRequireSerializable(false); byte[] bytes = marsh.marshal(new GridTcpDiscoveryVmIpFinder()); TcpDiscoveryIpFinder ipFinder = marsh.unmarshal(bytes, null); assertFalse(ipFinder.isShared()); ipFinder = marsh.unmarshal(marsh.marshal(new GridTcpDiscoveryVmIpFinder(true)), null); assertTrue(ipFinder.isShared()); }', 'output': 'public void testNonSerializable1() throws GridException { IgniteOptimizedMarshaller marsh = new IgniteOptimizedMarshaller(); marsh.setRequireSerializable(false); byte[] bytes = marsh.marshal(new GridTcpDiscoveryVmIpFinder()); GridTcpDiscoveryVmIpFinder ipFinder = marsh.unmarshal(bytes, null); assertFalse(ipFinder.isShared()); ipFinder = marsh.unmarshal(marsh.marshal(new GridTcpDiscoveryVmIpFinder(true)), null); assertTrue(ipFinder.isShared()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void defaultEventType() throws Exception { SomeDomainObject sdo = new SomeDomainObject(); SomeReferencedObject other = new SomeReferencedObject(); XXX identifier = FeatureIdentifier.propertyOrCollectionIdentifier( TypeIdentifier.fqcn(SomeDomainObject.class), "references"); final CollectionDomainEvent<Object, Object> ev = Utils.domainEventHelper().newCollectionDomainEvent( CollectionDomainEvent.Default.class, AbstractDomainEvent.Phase.EXECUTED, identifier, sdo, CollectionDomainEvent.Of.REMOVE_FROM, other); assertSame(ev.getSource(), sdo); assertThat(ev.getIdentifier(), is(identifier)); assertThat(ev.getOf(), is(CollectionDomainEvent.Of.REMOVE_FROM)); assertSame(ev.getValue(), other); }', 'ground_truth': 'public void defaultEventType() throws Exception { SomeDomainObject sdo = new SomeDomainObject(); SomeReferencedObject other = new SomeReferencedObject(); Identifier identifier = Identifier.propertyOrCollectionIdentifier( TypeIdentifier.fqcn(SomeDomainObject.class), "references"); final CollectionDomainEvent<Object, Object> ev = Utils.domainEventHelper().newCollectionDomainEvent( CollectionDomainEvent.Default.class, AbstractDomainEvent.Phase.EXECUTED, identifier, sdo, CollectionDomainEvent.Of.REMOVE_FROM, other); assertSame(ev.getSource(), sdo); assertThat(ev.getIdentifier(), is(identifier)); assertThat(ev.getOf(), is(CollectionDomainEvent.Of.REMOVE_FROM)); assertSame(ev.getValue(), other); }', 'output': 'public void defaultEventType() throws Exception { SomeDomainObject sdo = new SomeDomainObject(); SomeReferencedObject other = new SomeReferencedObject(); Identifier identifier = FeatureIdentifier.propertyOrCollectionIdentifier( TypeIdentifier.fqcn(SomeDomainObject.class), "references"); final CollectionDomainEvent<Object, Object> ev = Utils.domainEventHelper().newCollectionDomainEvent( CollectionDomainEvent.Default.class, AbstractDomainEvent.Phase.EXECUTED, identifier, sdo, CollectionDomainEvent.Of.REMOVE_FROM, other); assertSame(ev.getSource(), sdo); assertThat(ev.getIdentifier(), is(identifier)); assertThat(ev.getOf(), is(CollectionDomainEvent.Of.REMOVE_FROM)); assertSame(ev.getValue(), other); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testApplyWithUnknownGategorialValues() { Vector[] data = new Vector[]{ new DenseVector(new Serializable[] {"1", "Moscow", "A"}), new DenseVector(new Serializable[] {"2", "Moscow", "A"}), new DenseVector(new Serializable[] {"2", "Moscow", "B"}), }; Vectorizer<Integer, Vector, Integer, Double> vectorizer = new DummyVectorizer<>(0, 1, 2); OneHotEncoderPreprocessor<Integer, Vector> preprocessor = new OneHotEncoderPreprocessor<Integer, Vector>( new HashMap[]{new HashMap() { { put("2", 0); } }, new HashMap() { { put("Moscow", 0); } }, new HashMap() { { put("A", 0); put("B", 1); } }}, vectorizer, new HashSet() { { add(0); add(1); add(2); } }); double[][] postProcessedData = new double[][]{ {0.0, 1.0, 1.0, 1.0, 0.0}, {1.0, 0.0, 1.0, 1.0, 0.0}, {1.0, 0.0, 1.0, 0.0, 1.0}, }; try { for (int i = 0; i < data.length; i++) assertArrayEquals(postProcessedData[i], preprocessor.apply(i, data[i]).features().asArray(), 1e-8); fail("UnknownCategorialFeatureValue"); } catch (XXX e) { return; } fail("UnknownCategorialFeatureValue"); }', 'ground_truth': 'public void testApplyWithUnknownGategorialValues() { Vector[] data = new Vector[] { new DenseVector(new Serializable[] {"1", "Moscow", "A"}), new DenseVector(new Serializable[] {"2", "Moscow", "A"}), new DenseVector(new Serializable[] {"2", "Moscow", "B"}), }; Vectorizer<Integer, Vector, Integer, Double> vectorizer = new DummyVectorizer<>(0, 1, 2); OneHotEncoderPreprocessor<Integer, Vector> preprocessor = new OneHotEncoderPreprocessor<Integer, Vector>( new HashMap[] { new HashMap() { { put("2", 0); } }, new HashMap() { { put("Moscow", 0); } }, new HashMap() { { put("A", 0); put("B", 1); } }}, vectorizer, new HashSet() { { add(0); add(1); add(2); } }); double[][] postProcessedData = new double[][] { {0.0, 1.0, 1.0, 1.0, 0.0}, {1.0, 0.0, 1.0, 1.0, 0.0}, {1.0, 0.0, 1.0, 0.0, 1.0}, }; try { for (int i = 0; i < data.length; i++) assertArrayEquals(postProcessedData[i], preprocessor.apply(i, data[i]).features().asArray(), 1e-8); fail("UnknownCategorialFeatureValue"); } catch (UnknownCategorialValueException e) { return; } fail("UnknownCategorialFeatureValue"); }', 'output': 'public void testApplyWithUnknownGategorialValues() { Vector[] data = new Vector[]{ new DenseVector(new Serializable[] {"1", "Moscow", "A"}), new DenseVector(new Serializable[] {"2", "Moscow", "A"}), new DenseVector(new Serializable[] {"2", "Moscow", "B"}), }; Vectorizer<Integer, Vector, Integer, Double> vectorizer = new DummyVectorizer<>(0, 1, 2); OneHotEncoderPreprocessor<Integer, Vector> preprocessor = new OneHotEncoderPreprocessor<Integer, Vector>( new HashMap[]{new HashMap() { { put("2", 0); } }, new HashMap() { { put("Moscow", 0); } }, new HashMap() { { put("A", 0); put("B", 1); } }}, vectorizer, new HashSet() { { add(0); add(1); add(2); } }); double[][] postProcessedData = new double[][]{ {0.0, 1.0, 1.0, 1.0, 0.0}, {1.0, 0.0, 1.0, 1.0, 0.0}, {1.0, 0.0, 1.0, 0.0, 1.0}, }; try { for (int i = 0; i < data.length; i++) assertArrayEquals(postProcessedData[i], preprocessor.apply(i, data[i]).features().asArray(), 1e-8); fail("UnknownCategorialFeatureValue"); } catch (IOException e) { return; } fail("UnknownCategorialFeatureValue"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected Object createInstance(final InstanceCookie[] cookies) throws IOException, ClassNotFoundException { Toolbar.this.removeAll(); for (int i = 0; i < cookies.length; i++) { try { Object obj = cookies[i].instanceCreate(); DataObject file = cookiesToObjects.get(obj); if (obj instanceof Presenter.Toolbar) { if (obj instanceof Action && file != null) { AcceleratorBinding.setAccelerator((Action)obj, file.getPrimaryFile()); } obj = ((Presenter.Toolbar) obj).getToolbarPresenter(); } if (obj instanceof Component) { if ((obj instanceof JComponent) && "Fixed".equals(((JComponent) obj).getClientProperty("Toolbar"))) { org.openide.awt.Toolbar.this.removeAll(); setBorder(null); } if (obj instanceof javax.swing.JComponent) { if (ToolbarPool.getDefault().getPreferredIconSize() == 24) { ((JComponent) obj).putClientProperty("PreferredIconSize", new Integer(24)); } ((JComponent) obj).putClientProperty("file", file); } Toolbar.this.add((Component) obj); continue; } if (obj instanceof Action) { Action a = (Action) obj; XXX b = new DefaultIconButton(); if (ToolbarPool.getDefault().getPreferredIconSize() == 24) { b.putClientProperty("PreferredIconSize", new Integer(24)); } if (null == a.getValue(Action.SMALL_ICON) && (null == a.getValue(Action.NAME) || a.getValue(Action.NAME).toString().length() == 0)) { a.putValue(Action.SMALL_ICON, ImageUtilities.loadImageIcon("org/openide/loaders/unknown.gif", false)); } org.openide.awt.Actions.connect(b, a); b.putClientProperty("file", file); org.openide.awt.Toolbar.this.add(b); if (file != null) { AcceleratorBinding.setAccelerator(a, file.getPrimaryFile()); } continue; } } catch (java.io.IOException ex) { LOG.log(Level.WARNING, null, ex); } catch (java.lang.ClassNotFoundException ex) { LOG.log(Level.WARNING, null, ex); } finally { cookiesToObjects.clear(); } } if( cookies.length == 0 ) { label = new JButton("<"+Actions.cutAmpersand(getDisplayName())+">"); Toolbar.this.add(label); } Toolbar.this.invalidate (); return Toolbar.this; }', 'ground_truth': 'protected Object createInstance(final InstanceCookie[] cookies) throws IOException, ClassNotFoundException { Toolbar.this.removeAll(); for (int i = 0; i < cookies.length; i++) { try { Object obj = cookies[i].instanceCreate(); DataObject file = cookiesToObjects.get(obj); if (obj instanceof Presenter.Toolbar) { if (obj instanceof Action && file != null) { AcceleratorBinding.setAccelerator((Action)obj, file.getPrimaryFile()); } obj = ((Presenter.Toolbar) obj).getToolbarPresenter(); } if (obj instanceof Component) { if ((obj instanceof JComponent) && "Fixed".equals(((JComponent) obj).getClientProperty("Toolbar"))) { org.openide.awt.Toolbar.this.removeAll(); setBorder(null); } if (obj instanceof javax.swing.JComponent) { if (ToolbarPool.getDefault().getPreferredIconSize() == 24) { ((JComponent) obj).putClientProperty("PreferredIconSize", new Integer(24)); } ((JComponent) obj).putClientProperty("file", file); } Toolbar.this.add((Component) obj); continue; } if (obj instanceof Action) { Action a = (Action) obj; AbstractButton b; if (a.getValue(Actions.ACTION_VALUE_TOGGLE) != null) { b = new DefaultIconToggleButton(); } else { b = new DefaultIconButton(); } if (ToolbarPool.getDefault().getPreferredIconSize() == 24) { b.putClientProperty("PreferredIconSize", new Integer(24)); } if (null == a.getValue(Action.SMALL_ICON) && (null == a.getValue(Action.NAME) || a.getValue(Action.NAME).toString().length() == 0)) { a.putValue(Action.SMALL_ICON, ImageUtilities.loadImageIcon("org/openide/loaders/unknown.gif", false)); } org.openide.awt.Actions.connect(b, a); b.putClientProperty("file", file); org.openide.awt.Toolbar.this.add(b); if (file != null) { AcceleratorBinding.setAccelerator(a, file.getPrimaryFile()); } continue; } } catch (java.io.IOException ex) { LOG.log(Level.WARNING, null, ex); } catch (java.lang.ClassNotFoundException ex) { LOG.log(Level.WARNING, null, ex); } finally { cookiesToObjects.clear(); } } if( cookies.length == 0 ) { label = new JButton("<"+Actions.cutAmpersand(getDisplayName())+">"); Toolbar.this.add(label); } Toolbar.this.invalidate (); return Toolbar.this; }', 'output': 'protected Object createInstance(final InstanceCookie[] cookies) throws IOException, ClassNotFoundException { Toolbar.this.removeAll(); for (int i = 0; i < cookies.length; i++) { try { Object obj = cookies[i].instanceCreate(); DataObject file = cookiesToObjects.get(obj); if (obj instanceof Presenter.Toolbar) { if (obj instanceof Action && file != null) { AcceleratorBinding.setAccelerator((Action)obj, file.getPrimaryFile()); } obj = ((Presenter.Toolbar) obj).getToolbarPresenter(); } if (obj instanceof Component) { if ((obj instanceof JComponent) && "Fixed".equals(((JComponent) obj).getClientProperty("Toolbar"))) { org.openide.awt.Toolbar.this.removeAll(); setBorder(null); } if (obj instanceof javax.swing.JComponent) { if (ToolbarPool.getDefault().getPreferredIconSize() == 24) { ((JComponent) obj).putClientProperty("PreferredIconSize", new Integer(24)); } ((JComponent) obj).putClientProperty("file", file); } Toolbar.this.add((Component) obj); continue; } if (obj instanceof Action) { Action a = (Action) obj; IconButton b = new DefaultIconButton(); if (ToolbarPool.getDefault().getPreferredIconSize() == 24) { b.putClientProperty("PreferredIconSize", new Integer(24)); } if (null == a.getValue(Action.SMALL_ICON) && (null == a.getValue(Action.NAME) || a.getValue(Action.NAME).toString().length() == 0)) { a.putValue(Action.SMALL_ICON, ImageUtilities.loadImageIcon("org/openide/loaders/unknown.gif", false)); } org.openide.awt.Actions.connect(b, a); b.putClientProperty("file", file); org.openide.awt.Toolbar.this.add(b); if (file != null) { AcceleratorBinding.setAccelerator(a, file.getPrimaryFile()); } continue; } } catch (java.io.IOException ex) { LOG.log(Level.WARNING, null, ex); } catch'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void rebuildSecondaryIndex(String ksName, String cfName, String... idxNames) { ColumnFamilyStore cfs = Table.open(ksName).getColumnFamilyStore(cfName); XXX indexes = new TreeSet<ByteBuffer>(cfs.metadata.comparator); if (idxNames.length == 0) indexes.addAll(cfs.indexManager.getIndexedColumns()); for (String idxName : idxNames) indexes.add(cfs.indexManager.getColumnByIdxName(idxName)); Collection<SSTableReader> sstables = cfs.getSSTables(); try { cfs.indexManager.setIndexRemoved(indexes); SSTableReader.acquireReferences(sstables); logger.info(String.format("User Requested secondary index re-build for %s/%s indexes", ksName, cfName)); cfs.indexManager.maybeBuildSecondaryIndexes(sstables, indexes); cfs.indexManager.setIndexBuilt(indexes); } finally { SSTableReader.releaseReferences(sstables); } }', 'ground_truth': 'public static void rebuildSecondaryIndex(String ksName, String cfName, String... idxNames) { ColumnFamilyStore cfs = Table.open(ksName).getColumnFamilyStore(cfName); Set<String> indexes = new HashSet<String>(Arrays.asList(idxNames)); Collection<SSTableReader> sstables = cfs.getSSTables(); try { cfs.indexManager.setIndexRemoved(indexes); SSTableReader.acquireReferences(sstables); logger.info(String.format("User Requested secondary index re-build for %s/%s indexes", ksName, cfName)); cfs.indexManager.maybeBuildSecondaryIndexes(sstables, indexes); cfs.indexManager.setIndexBuilt(indexes); } finally { SSTableReader.releaseReferences(sstables); } }', 'output': 'public static void rebuildSecondaryIndex(String ksName, String cfName, String... idxNames) { ColumnFamilyStore cfs = Table.open(ksName).getColumnFamilyStore(cfName); Set<ByteBuffer> indexes = new TreeSet<ByteBuffer>(cfs.metadata.comparator); if (idxNames.length == 0) indexes.addAll(cfs.indexManager.getIndexedColumns()); for (String idxName : idxNames) indexes.add(cfs.indexManager.getColumnByIdxName(idxName)); Collection<SSTableReader> sstables = cfs.getSSTables(); try { cfs.indexManager.setIndexRemoved(indexes); SSTableReader.acquireReferences(sstables); logger.info(String.format("User Requested secondary index re-build for %s/%s indexes", ksName, cfName)); cfs.indexManager.maybeBuildSecondaryIndexes(sstables, indexes); cfs.indexManager.setIndexBuilt(indexes); } finally { SSTableReader.releaseReferences(sstables); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private ColumnConditions prepareColumnConditions(CFMetaData metadata, VariableSpecifications boundNames) { checkNull(attrs.timestamp, "Cannot provide custom timestamp for conditional updates"); ColumnConditions.Builder builder = ColumnConditions.newBuilder(); for (Pair<ColumnIdentifier.Raw, ColumnCondition.Raw> entry : conditions) { ColumnIdentifier id = entry.left.prepare(metadata); ColumnDefinition def = metadata.getColumnDefinition(id); checkNotNull(metadata.getColumnDefinition(id), "Unknown identifier %s in IF conditions", id); ColumnCondition condition = entry.right.prepare(keyspace(), def, metadata); condition.collectMarkerSpecification(boundNames); checkFalse(def.isPrimaryKeyColumn(), "PRIMARY KEY column \'%s\' cannot have IF conditions", id); builder.add(condition); } return builder.build(); }', 'ground_truth': 'private ColumnConditions prepareColumnConditions(CFMetaData metadata, VariableSpecifications boundNames) { checkNull(attrs.timestamp, "Cannot provide custom timestamp for conditional updates"); ColumnConditions.Builder builder = ColumnConditions.newBuilder(); for (Pair<ColumnDefinition.Raw, ColumnCondition.Raw> entry : conditions) { ColumnDefinition def = entry.left.prepare(metadata); ColumnCondition condition = entry.right.prepare(keyspace(), def, metadata); condition.collectMarkerSpecification(boundNames); checkFalse(def.isPrimaryKeyColumn(), "PRIMARY KEY column \'%s\' cannot have IF conditions", def.name); builder.add(condition); } return builder.build(); }', 'output': 'private ColumnConditions prepareColumnConditions(CFMetaData metadata, List<VariableSpecifications> boundNames) { checkNull(attrs.timestamp, "Cannot provide custom timestamp for conditional updates"); ColumnConditions.Builder builder = ColumnConditions.newBuilder(); for (Pair<ColumnIdentifier.Raw, ColumnCondition.Raw> entry : conditions) { ColumnIdentifier id = entry.left.prepare(metadata); ColumnDefinition def = metadata.getColumnDefinition(id); checkNotNull(metadata.getColumnDefinition(id), "Unknown identifier %s in IF conditions", id); ColumnCondition condition = entry.right.prepare(keyspace(), def, metadata); condition.collectMarkerSpecification(boundNames); checkFalse(def.isPrimaryKeyColumn(), "PRIMARY KEY column \'%s\' cannot have IF conditions", id); builder.add(condition); } return builder.build(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void serialize(ColumnSubselection subSel, DataOutputPlus out, int version) throws IOException { XXX column = subSel.column(); ByteBufferUtil.writeWithShortLength(column.name.bytes, out); out.writeByte(subSel.kind().ordinal()); switch (subSel.kind()) { case SLICE: Slice slice = (Slice)subSel; column.cellPathSerializer().serialize(slice.from, out); column.cellPathSerializer().serialize(slice.to, out); break; case ELEMENT: Element eltSelection = (Element)subSel; column.cellPathSerializer().serialize(eltSelection.element, out); break; default: throw new AssertionError(); } }', 'ground_truth': 'public void serialize(ColumnSubselection subSel, DataOutputPlus out, int version) throws IOException { ColumnMetadata column = subSel.column(); ByteBufferUtil.writeWithShortLength(column.name.bytes, out); out.writeByte(subSel.kind().ordinal()); switch (subSel.kind()) { case SLICE: Slice slice = (Slice)subSel; column.cellPathSerializer().serialize(slice.from, out); column.cellPathSerializer().serialize(slice.to, out); break; case ELEMENT: Element eltSelection = (Element)subSel; column.cellPathSerializer().serialize(eltSelection.element, out); break; default: throw new AssertionError(); } }', 'output': 'public void serialize(ColumnSubselection subSel, DataOutputPlus out, int version) throws IOException { Column<?> column = subSel.column(); ByteBufferUtil.writeWithShortLength(column.name.bytes, out); out.writeByte(subSel.kind().ordinal()); switch (subSel.kind()) { case SLICE: Slice slice = (Slice)subSel; column.cellPathSerializer().serialize(slice.from, out); column.cellPathSerializer().serialize(slice.to, out); break; case ELEMENT: Element eltSelection = (Element)subSel; column.cellPathSerializer().serialize(eltSelection.element, out); break; default: throw new AssertionError(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static List<String> deriveNaturalJoinColumnList( RelDataType leftRowType, RelDataType rightRowType) { List<String> naturalColumnNames = new ArrayList<String>(); final String [] leftNames = SqlTypeUtil.getFieldNames(leftRowType); final String [] rightNames = SqlTypeUtil.getFieldNames(rightRowType); for (String name : leftNames) { if ((countOccurrences(name, leftNames) == 1) && (countOccurrences(name, rightNames) == 1)) { naturalColumnNames.add(name); } } return naturalColumnNames; }', 'ground_truth': 'public static List<String> deriveNaturalJoinColumnList( RelDataType leftRowType, RelDataType rightRowType) { List<String> naturalColumnNames = new ArrayList<String>(); final List<String> leftNames = leftRowType.getFieldNames(); final List<String> rightNames = rightRowType.getFieldNames(); for (String name : leftNames) { if ((Collections.frequency(leftNames, name) == 1) && (Collections.frequency(rightNames, name) == 1)) { naturalColumnNames.add(name); } } return naturalColumnNames; }', 'output': 'public static List<String> deriveNaturalJoinColumnList( RelDataType leftRowType, RelDataType rightRowType) { List<String> naturalColumnNames = new ArrayList<String>(); final String [] leftNames = SqlTypeUtil.getFieldNames(leftRowType); final String [] rightNames = SqlTypeUtil.getFieldNames(rightRowType); for (String name : leftNames) { if ((countOccurrences(name, leftNames) == 1) && (countOccurrences(name, rightNames) == 1)) { naturalColumnNames.add(name); } } return naturalColumnNames; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testEntryParent() throws Exception { cache().put("key", 1); GridCacheProxyImpl<String, Integer> prj = (GridCacheProxyImpl<String, Integer>)cache(). flagsOn(CLONE, INVALIDATE); GridCacheEntry<String, Integer> entry = prj.entry("key"); assert entry != null; GridCacheProxyImpl<String, Integer> entryPrj = (GridCacheProxyImpl<String, Integer>)entry.projection(); assert entryPrj.delegate() == prj.delegate(); }', 'ground_truth': 'public void testEntryParent() throws Exception { cache().put("key", 1); GridCacheProxyImpl<String, Integer> prj = (GridCacheProxyImpl<String, Integer>)cache(). flagsOn(CLONE, INVALIDATE); CacheEntry<String, Integer> entry = prj.entry("key"); assert entry != null; GridCacheProxyImpl<String, Integer> entryPrj = (GridCacheProxyImpl<String, Integer>)entry.projection(); assert entryPrj.delegate() == prj.delegate(); }', 'output': 'public void testEntryParent() throws Exception { cache().put("key", 1); IgniteCache<String, Integer> prj = (IgniteCache<String, Integer>)cache(). flagsOn(CLONE, INVALIDATE); IgniteCacheEntry<String, Integer> entry = prj.entry("key"); assert entry != null; IgniteCacheProxyImpl<String, Integer> entryPrj = (IgniteCacheProxyImpl<String, Integer>)entry.projection(); assert entryPrj.delegate() == prj.delegate(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void submitDataChunk(final Ignite master, UUID preferredNodeId, final Collection<Integer> dataChunk) throws GridException { ClusterGroup prj = master.cluster().forPredicate(workerNodesFilter); XXX comp = master.compute(prj).enableAsync(); comp.execute(new GridCacheGroupLockPutTask(preferredNodeId, CACHE_NAME, optimisticTx()), dataChunk); GridComputeTaskFuture<Void> fut = comp.future(); fut.listenAsync(new CI1<IgniteFuture<Void>>() { @Override public void apply(IgniteFuture<Void> f) { GridComputeTaskFuture taskFut = (GridComputeTaskFuture)f; boolean fail = false; try { f.get(); } catch (GridException ignore) { info("Put task failed, going to remap keys: " + dataChunk.size()); fail = true; } finally { resQueue.remove(taskFut); try { if (fail) remap(master, dataChunk); } catch (GridException e) { info("Failed to remap task [data=" + dataChunk.size() + ", e=" + e + \']\'); } } } }); try { resQueue.put(fut); if (fut.isDone()) resQueue.remove(fut); } catch (InterruptedException ignored) { info(">>>> Failed to wait for future submission: " + fut); Thread.currentThread().interrupt(); } }', 'ground_truth': 'private void submitDataChunk(final Ignite master, UUID preferredNodeId, final Collection<Integer> dataChunk) throws GridException { ClusterGroup prj = master.cluster().forPredicate(workerNodesFilter); IgniteCompute comp = master.compute(prj).enableAsync(); comp.execute(new GridCacheGroupLockPutTask(preferredNodeId, CACHE_NAME, optimisticTx()), dataChunk); GridComputeTaskFuture<Void> fut = comp.future(); fut.listenAsync(new CI1<IgniteFuture<Void>>() { @Override public void apply(IgniteFuture<Void> f) { GridComputeTaskFuture taskFut = (GridComputeTaskFuture)f; boolean fail = false; try { f.get(); } catch (GridException ignore) { info("Put task failed, going to remap keys: " + dataChunk.size()); fail = true; } finally { resQueue.remove(taskFut); try { if (fail) remap(master, dataChunk); } catch (GridException e) { info("Failed to remap task [data=" + dataChunk.size() + ", e=" + e + \']\'); } } } }); try { resQueue.put(fut); if (fut.isDone()) resQueue.remove(fut); } catch (InterruptedException ignored) { info(">>>> Failed to wait for future submission: " + fut); Thread.currentThread().interrupt(); } }', 'output': 'private void submitDataChunk(final Ignite master, UUID preferredNodeId, final Collection<Integer> dataChunk) throws GridException { ClusterGroup prj = master.cluster().forPredicate(workerNodesFilter); IgniteCompute comp = master.compute(prj).enableAsync(); comp.execute(new GridCacheGroupLockPutTask(preferredNodeId, CACHE_NAME, optimisticTx()), dataChunk); GridComputeTaskFuture<Void> fut = comp.future(); fut.listenAsync(new CI1<IgniteFuture<Void>>() { @Override public void apply(IgniteFuture<Void> f) { GridComputeTaskFuture taskFut = (GridComputeTaskFuture)f; boolean fail = false; try { f.get(); } catch (GridException ignore) { info("Put task failed, going to remap keys: " + dataChunk.size()); fail = true; } finally { resQueue.remove(taskFut); try { if (fail) remap(master, dataChunk); } catch (GridException e) { info("Failed to remap task [data=" + dataChunk.size() + ", e=" + e + \']\'); } } } }); try { resQueue.put(fut); if (fut.isDone()) resQueue.remove(fut); } catch (InterruptedException ignored) { info(">>>> Failed to wait for future submission: " + fut); Thread.currentThread().interrupt(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void notifyCustomEvent(final ZkDiscoveryCustomEventData evtData, final DiscoverySpiCustomMessage msg) { assert !(msg instanceof ZkInternalMessage) : msg; if (log.isDebugEnabled()) log.debug(" [topVer=" + evtData.topologyVersion() + ", msg=" + msg + \']\'); final ZookeeperClusterNode sndNode = rtState.top.nodesById.get(evtData.sndNodeId); assert sndNode != null : evtData; final List<ClusterNode> topSnapshot = rtState.top.topologySnapshot(); XXX fut = lsnr.onDiscovery( DiscoveryCustomEvent.EVT_DISCOVERY_CUSTOM_EVT, evtData.topologyVersion(), sndNode, topSnapshot, Collections.emptyMap(), msg ); if (msg != null && msg.isMutable()) { try { fut.get(); } catch (IgniteCheckedException e) { throw new IgniteException("Failed to wait for discovery listener notification", e); } } }', 'ground_truth': 'private void notifyCustomEvent(final ZkDiscoveryCustomEventData evtData, final DiscoverySpiCustomMessage msg) { assert !(msg instanceof ZkInternalMessage) : msg; if (log.isDebugEnabled()) log.debug(" [topVer=" + evtData.topologyVersion() + ", msg=" + msg + \']\'); final ZookeeperClusterNode sndNode = rtState.top.nodesById.get(evtData.sndNodeId); assert sndNode != null : evtData; final List<ClusterNode> topSnapshot = rtState.top.topologySnapshot(); IgniteFuture<?> fut = lsnr.onDiscovery( DiscoveryCustomEvent.EVT_DISCOVERY_CUSTOM_EVT, evtData.topologyVersion(), sndNode, topSnapshot, Collections.emptyMap(), msg ); if (msg != null && msg.isMutable()) fut.get(); }', 'output': 'private void notifyCustomEvent(final ZkDiscoveryCustomEventData evtData, final DiscoverySpiCustomMessage msg) { assert !(msg instanceof ZkInternalMessage) : msg; if (log.isDebugEnabled()) log.debug(" [topVer=" + evtData.topologyVersion() + ", msg=" + msg + \']\'); final ZookeeperClusterNode sndNode = rtState.top.nodesById.get(evtData.sndNodeId); assert sndNode != null : evtData; final List<ClusterNode> topSnapshot = rtState.top.topologySnapshot(); IgniteFuture<?> fut = lsnr.onDiscovery( DiscoveryCustomEvent.EVT_DISCOVERY_CUSTOM_EVT, evtData.topologyVersion(), sndNode, topSnapshot, Collections.emptyMap(), msg ); if (msg != null && msg.isMutable()) { try { fut.get(); } catch (IgniteCheckedException e) { throw new IgniteException("Failed to wait for discovery listener notification", e); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkTask(int num) throws InterruptedException, GridException { Ignite ignite = G.grid(getTestGridName()); XXX fut = executeAsync(ignite.compute(), GridTaskSessionTestTask.class, num); assert fut != null; try { boolean await = startSignal[num].await(WAIT_TIME, TimeUnit.MILLISECONDS); assert await : "Jobs did not start."; Object res = fut.get(); assert "interrupt-task-data".equals(res) : "Invalid task result: " + res; await = stopSignal[num].await(WAIT_TIME, TimeUnit.MILLISECONDS); assert await : "Jobs did not cancel [interruptCount=" + Arrays.toString(interruptCnt) + \']\'; int cnt = interruptCnt[num].get(); assert cnt == SPLIT_COUNT - 1 : "Invalid interrupt count value: " + cnt; } finally { fut.get(); } }', 'ground_truth': 'private void checkTask(int num) throws InterruptedException, GridException { Ignite ignite = G.grid(getTestGridName()); ComputeTaskFuture<?> fut = executeAsync(ignite.compute(), GridTaskSessionTestTask.class, num); assert fut != null; try { boolean await = startSignal[num].await(WAIT_TIME, TimeUnit.MILLISECONDS); assert await : "Jobs did not start."; Object res = fut.get(); assert "interrupt-task-data".equals(res) : "Invalid task result: " + res; await = stopSignal[num].await(WAIT_TIME, TimeUnit.MILLISECONDS); assert await : "Jobs did not cancel [interruptCount=" + Arrays.toString(interruptCnt) + \']\'; int cnt = interruptCnt[num].get(); assert cnt == SPLIT_COUNT - 1 : "Invalid interrupt count value: " + cnt; } finally { fut.get(); } }', 'output': 'private void checkTask(int num) throws InterruptedException, GridException { Ignite ignite = G.grid(getTestGridName()); ComputeTaskFuture<?> fut = executeAsync(ignite.compute(), GridTaskSessionTestTask.class, num); assert fut != null; try { boolean await = startSignal[num].await(WAIT_TIME, TimeUnit.MILLISECONDS); assert await : "Jobs did not start."; Object res = fut.get(); assert "interrupt-task-data".equals(res) : "Invalid task result: " + res; await = stopSignal[num].await(WAIT_TIME, TimeUnit.MILLISECONDS); assert await : "Jobs did not cancel [interruptCount=" + Arrays.toString(interruptCnt) + \']\'; int cnt = interruptCnt[num].get(); assert cnt == SPLIT_COUNT - 1 : "Invalid interrupt count value: " + cnt; } finally { fut.get(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private ElementHandle<ExecutableElement> generateServiceLocatorJNDI(FileObject referencingFO, final String referencingClassName, String homeName, String refName, boolean narrow, String componentName, boolean throwCheckedExceptions, String serviceLocatorName) throws IOException { String name = "lookup"+refName.substring(refName.lastIndexOf(\'/\')+1); String body = null; List<String> exceptions = new ArrayList<String>(3); exceptions.add(NamingException.class.getName()); String returnType = isSimplified ? componentName : homeName; boolean genCreate = isSession; if (genCreate) { returnType = componentName; exceptions.add("javax.ejb.CreateException"); if (narrow) { exceptions.add("java.rmi.RemoteException"); } } Project enterpriseProject = FileOwnerQuery.getOwner(referencingFO); ServiceLocatorStrategy sls = ServiceLocatorStrategy.create(enterpriseProject, referencingFO, serviceLocatorName); if (narrow) { body = sls.genRemoteEjbStringLookup(refName, homeName, referencingFO, referencingClassName, genCreate); } else { body = sls.genLocalEjbStringLookup(refName, homeName, referencingFO, referencingClassName, genCreate); } if (!throwCheckedExceptions) { XXX exIt = exceptions.iterator(); StringBuffer catchBody = new StringBuffer("try {\\n" + body + "\\n}"); while (exIt.hasNext()) { String exceptionName = (String) exIt.next(); catchBody.append(" catch("); catchBody.append(exceptionName); catchBody.append(\' \'); String capitalLetters = extractAllCapitalLetters(exceptionName); catchBody.append(capitalLetters); catchBody.append(") {\\n"); catchBody.append(MessageFormat.format(LOG_STATEMENT, new Object[] {capitalLetters})); catchBody.append("throw new RuntimeException("+capitalLetters+");\\n"); catchBody.append(\'}\'); body = catchBody.toString(); exceptions = Collections.<String>emptyList(); } } final MethodModel methodModel = MethodModel.create( _RetoucheUtil.uniqueMemberName(referencingFO, referencingClassName, name, "ejb"), returnType, body, Collections.<MethodModel.Variable>emptyList(), exceptions, Collections.singleton(Modifier.PRIVATE) ); JavaSource javaSource = JavaSource.forFileObject(referencingFO); javaSource.runModificationTask(new Task<WorkingCopy>() { public void run(WorkingCopy workingCopy) throws IOException { workingCopy.toPhase(JavaSource.Phase.ELEMENTS_RESOLVED); TypeElement typeElement = workingCopy.getElements().getTypeElement(referencingClassName); MethodTree methodTree = MethodModelSupport.createMethodTree(workingCopy, methodModel); ClassTree classTree = workingCopy.getTrees().getTree(typeElement); ClassTree newClassTree = workingCopy.getTreeMaker().addClassMember(classTree, methodTree); workingCopy.rewrite(classTree, newClassTree); } }).commit(); return _RetoucheUtil.getMethodHandle(javaSource, methodModel, referencingClassName); }', 'ground_truth': 'private ElementHandle<ExecutableElement> generateServiceLocatorJNDI(FileObject referencingFO, final String referencingClassName, String homeName, String refName, boolean narrow, String componentName, boolean throwCheckedExceptions, String serviceLocatorName) throws IOException { String name = "lookup"+refName.substring(refName.lastIndexOf(\'/\')+1); String body = null; List<String> exceptions = new ArrayList<String>(3); exceptions.add(NamingException.class.getName()); String returnType = isSimplified ? componentName : homeName; boolean genCreate = isSession; if (genCreate) { returnType = componentName; exceptions.add("javax.ejb.CreateException"); if (narrow) { exceptions.add("java.rmi.RemoteException"); } } Project enterpriseProject = FileOwnerQuery.getOwner(referencingFO); ServiceLocatorStrategy sls = ServiceLocatorStrategy.create(enterpriseProject, referencingFO, serviceLocatorName); if (narrow) { body = sls.genRemoteEjbStringLookup(refName, homeName, referencingFO, referencingClassName, genCreate); } else { body = sls.genLocalEjbStringLookup(refName, homeName, referencingFO, referencingClassName, genCreate); } if (!throwCheckedExceptions) { Iterator<String> exIt = exceptions.iterator(); StringBuffer catchBody = new StringBuffer("try {\\n" + body + "\\n}"); while (exIt.hasNext()) { String exceptionName = (String) exIt.next(); catchBody.append(" catch("); catchBody.append(exceptionName); catchBody.append(\' \'); String capitalLetters = extractAllCapitalLetters(exceptionName); catchBody.append(capitalLetters); catchBody.append(") {\\n"); catchBody.append(MessageFormat.format(LOG_STATEMENT, new Object[] {capitalLetters})); catchBody.append("throw new RuntimeException("+capitalLetters+");\\n"); catchBody.append(\'}\'); body = catchBody.toString(); exceptions = Collections.<String>emptyList(); } } final MethodModel methodModel = MethodModel.create( _RetoucheUtil.uniqueMemberName(referencingFO, referencingClassName, name, "ejb"), returnType, body, Collections.<MethodModel.Variable>emptyList(), exceptions, Collections.singleton(Modifier.PRIVATE) ); JavaSource javaSource = JavaSource.forFileObject(referencingFO); javaSource.runModificationTask(new Task<WorkingCopy>() { public void run(WorkingCopy workingCopy) throws IOException { workingCopy.toPhase(JavaSource.Phase.ELEMENTS_RESOLVED); TypeElement typeElement = workingCopy.getElements().getTypeElement(referencingClassName); MethodTree methodTree = MethodModelSupport.createMethodTree(workingCopy, methodModel); ClassTree classTree = workingCopy.getTrees().getTree(typeElement); ClassTree newClassTree = workingCopy.getTreeMaker().addClassMember(classTree, methodTree); workingCopy.rewrite(classTree, newClassTree); } }).commit(); return _RetoucheUtil.getMethodHandle(javaSource, methodModel, referencingClassName); }', 'output': 'private ElementHandle<ExecutableElement> generateServiceLocatorJNDI(FileObject referencingFO, final String referencingClassName, String homeName, String refName, boolean narrow, String componentName, boolean throwCheckedExceptions, String serviceLocatorName) throws IOException { String name = "lookup"+refName.substring(refName.lastIndexOf(\'/\')+1); String body = null; List<String> exceptions = new ArrayList<String>(3); exceptions.add(NamingException.class.getName()); String returnType = isSimplified ? componentName : homeName; boolean genCreate = isSession; if (genCreate) { returnType = componentName; exceptions.add("javax.ejb.CreateException"); if (narrow) { exceptions.add("java.rmi.RemoteException"); } } Project enterpriseProject = FileOwnerQuery.getOwner(referencingFO); ServiceLocatorStrategy sls = ServiceLocatorStrategy.create(enterpriseProject, referencingFO, serviceLocatorName); if (narrow) { body = sls.genRemoteEjbStringLookup(refName, homeName, referencingFO, referencingClassName, genCreate); } else { body = sls.genLocalEjbStringLookup(refName, homeName, referencingFO, referencingClassName, genCreate); } if (!throwCheckedExceptions) { Exceptions exIt = exceptions.iterator(); StringBuffer catchBody = new StringBuffer("try {\\n" + body + "\\n}"); while (exIt.hasNext()) { String exceptionName = (String) exIt.next(); catchBody.append(" catch("); catchBody.append(exceptionName); catchBody.append(\' \'); String capitalLetters = extractAllCapitalLetters(exceptionName); catchBody.append(capitalLetters); catchBody.append(") {\\n"); catchBody.append(MessageFormat.format(LOG_STATEMENT, new Object[] {capitalLetters})); catchBody.append("throw new RuntimeException("+capitalLetters+");\\n"); catchBody.append(\'}\'); body = catchBody.toString(); exceptions = Collections.<String>emptyList(); } } final MethodModel methodModel = MethodModel.create( _RetoucheUtil.uniqueMemberName(referencingFO, referencingClassName, name, "ejb"), returnType, body, Collections.<MethodModel'}
{'instruction': 'Please refactor the following code:', 'input': 'public DAG<Operator<?, ?>> getBasicOps() { final ReduceByKey rbk; final DAG<Operator<?, ?>> dag = DAG.empty(); if (windowing != null) { rbk = new ReduceByKey<>( getName() + "::ReduceByKey", getFlow(), input, getKeyExtractor(), valueExtractor, windowing, reducer, valueComparator, getHints()); dag.add(rbk); } else { FlatMap<IN, Pair<Window<?>, IN>> map = new FlatMap<>( getName() + "::window-to-key", getFlow(), input, (IN in, Collector<Pair<Window<?>, IN>> c) -> { c.collect(Pair.of(c.getWindow(), in)); }, null); rbk = new ReduceByKey<>( getName() + "::ReduceByKey::attached", getFlow(), map.output(), Pair::getFirst, p -> valueExtractor.apply(p.getSecond()), null, reducer, valueComparator, getHints()); dag.add(map); dag.add(rbk); } MapElements<Pair<Object, OUT>, OUT> format = new MapElements<Pair<Object, OUT>, OUT>( getName() + "::MapElements", getFlow(), (Dataset) rbk.output(), Pair::getSecond); dag.add(format); return dag; }', 'ground_truth': 'public DAG<Operator<?, ?>> getBasicOps() { final ReduceByKey rbk; final DAG<Operator<?, ?>> dag = DAG.empty(); if (windowing != null) { rbk = new ReduceByKey<>( getName() + "::ReduceByKey", getFlow(), input, getKeyExtractor(), valueExtractor, windowing, reducer, valueComparator, getHints()); dag.add(rbk); } else { FlatMap<InputT, Pair<Window<?>, InputT>> map = new FlatMap<>( getName() + "::window-to-key", getFlow(), input, (InputT in, Collector<Pair<Window<?>, InputT>> c) -> { c.collect(Pair.of(c.getWindow(), in)); }, null); rbk = new ReduceByKey<>( getName() + "::ReduceByKey::attached", getFlow(), map.output(), Pair::getFirst, p -> valueExtractor.apply(p.getSecond()), null, reducer, valueComparator, getHints()); dag.add(map); dag.add(rbk); } MapElements<Pair<Object, OutputT>, OutputT> format = new MapElements<Pair<Object, OutputT>, OutputT>( getName() + "::MapElements", getFlow(), (Dataset) rbk.output(), Pair::getSecond); dag.add(format); return dag; }', 'output': 'public DAG<Operator<?, ?>> getBasicOps() { final ReduceByKey rbk; final DAG<Operator<?, ?>> dag = DAG.empty(); if (windowing != null) { rbk = new ReduceByKey<>( getName() + "::ReduceByKey", getFlow(), input, getKeyExtractor(), valueExtractor, windowing, reducer, valueComparator, getHints()); dag.add(rbk); } else { FlatMap<IN, Pair<Window<?>, IN>> map = new FlatMap<>( getName() + "::window-to_key", getFlow(), input, (IN in, Collector<Pair<Window<?>, IN>> c) -> { c.collect(Pair.of(c.getWindow(), in)); }, null); rbk = new ReduceByKey<>( getName() + "::ReduceByKey::attached", getFlow(), map.output(), Pair::getFirst, p -> valueExtractor.apply(p.getSecond()), null, reducer, valueComparator, getHints()); dag.add(map); dag.add(rbk); } MapElements<Pair<Object, OUT>, OUT> format = new MapElements<Pair<Object, OUT>, OUT>( getName() + "::MapElements", getFlow(), (Dataset) rbk.output(), Pair::getSecond); dag.add(format); return dag; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testReset() throws IOException { File f = File.createTempFile("testMark", "1"); final String expected = "The quick brown fox jumps over the lazy dog"; final int numIterations = 10; try(SequentialWriter writer = SequentialWriter.open(f)) { for (int i = 0; i < numIterations; i++) writer.write(expected.getBytes()); writer.finish(); } assert f.exists(); try(ChannelProxy channel = new ChannelProxy(f); RandomAccessReader reader = new RandomAccessReader.Builder(channel).build()) { assertEquals(expected.length() * numIterations, reader.length()); ByteBuffer b = ByteBufferUtil.read(reader, expected.length()); assertEquals(expected, new String(b.array(), Charset.forName("UTF-8"))); assertFalse(reader.isEOF()); assertEquals((numIterations - 1) * expected.length(), reader.bytesRemaining()); XXX mark = reader.mark(); assertEquals(0, reader.bytesPastMark()); assertEquals(0, reader.bytesPastMark(mark)); for (int i = 0; i < (numIterations - 1); i++) { b = ByteBufferUtil.read(reader, expected.length()); assertEquals(expected, new String(b.array(), Charset.forName("UTF-8"))); } assertTrue(reader.isEOF()); assertEquals(expected.length() * (numIterations - 1), reader.bytesPastMark()); assertEquals(expected.length() * (numIterations - 1), reader.bytesPastMark(mark)); reader.reset(mark); assertEquals(0, reader.bytesPastMark()); assertEquals(0, reader.bytesPastMark(mark)); assertFalse(reader.isEOF()); for (int i = 0; i < (numIterations - 1); i++) { b = ByteBufferUtil.read(reader, expected.length()); assertEquals(expected, new String(b.array(), Charset.forName("UTF-8"))); } reader.reset(); assertEquals(0, reader.bytesPastMark()); assertEquals(0, reader.bytesPastMark(mark)); assertFalse(reader.isEOF()); for (int i = 0; i < (numIterations - 1); i++) { b = ByteBufferUtil.read(reader, expected.length()); assertEquals(expected, new String(b.array(), Charset.forName("UTF-8"))); } assertTrue(reader.isEOF()); } }', 'ground_truth': 'public void testReset() throws IOException { File f = File.createTempFile("testMark", "1"); final String expected = "The quick brown fox jumps over the lazy dog"; final int numIterations = 10; try(SequentialWriter writer = SequentialWriter.open(f)) { for (int i = 0; i < numIterations; i++) writer.write(expected.getBytes()); writer.finish(); } assert f.exists(); try(ChannelProxy channel = new ChannelProxy(f); RandomAccessReader reader = new RandomAccessReader.Builder(channel).build()) { assertEquals(expected.length() * numIterations, reader.length()); ByteBuffer b = ByteBufferUtil.read(reader, expected.length()); assertEquals(expected, new String(b.array(), Charset.forName("UTF-8"))); assertFalse(reader.isEOF()); assertEquals((numIterations - 1) * expected.length(), reader.bytesRemaining()); DataPosition mark = reader.mark(); assertEquals(0, reader.bytesPastMark()); assertEquals(0, reader.bytesPastMark(mark)); for (int i = 0; i < (numIterations - 1); i++) { b = ByteBufferUtil.read(reader, expected.length()); assertEquals(expected, new String(b.array(), Charset.forName("UTF-8"))); } assertTrue(reader.isEOF()); assertEquals(expected.length() * (numIterations - 1), reader.bytesPastMark()); assertEquals(expected.length() * (numIterations - 1), reader.bytesPastMark(mark)); reader.reset(mark); assertEquals(0, reader.bytesPastMark()); assertEquals(0, reader.bytesPastMark(mark)); assertFalse(reader.isEOF()); for (int i = 0; i < (numIterations - 1); i++) { b = ByteBufferUtil.read(reader, expected.length()); assertEquals(expected, new String(b.array(), Charset.forName("UTF-8"))); } reader.reset(); assertEquals(0, reader.bytesPastMark()); assertEquals(0, reader.bytesPastMark(mark)); assertFalse(reader.isEOF()); for (int i = 0; i < (numIterations - 1); i++) { b = ByteBufferUtil.read(reader, expected.length()); assertEquals(expected, new String(b.array(), Charset.forName("UTF-8"))); } assertTrue(reader.isEOF()); } }', 'output': 'public void testReset() throws IOException { File f = File.createTempFile("testMark", "1"); final String expected = "The quick brown fox jumps over the lazy dog"; final int numIterations = 10; try(SequentialWriter writer = SequentialWriter.open(f)) { for (int i = 0; i < numIterations; i++) writer.write(expected.getBytes()); writer.finish(); } assert f.exists(); try(ChannelProxy channel = new ChannelProxy(f); RandomAccessReader reader = new RandomAccessReader.Builder(channel).build()) { assertEquals(expected.length() * numIterations, reader.length()); ByteBuffer b = ByteBufferUtil.read(reader, expected.length()); assertEquals(expected, new String(b.array(), Charset.forName("UTF-8"))); assertFalse(reader.isEOF()); assertEquals((numIterations - 1) * expected.length(), reader.bytesRemaining()); long mark = reader.mark(); assertEquals(0, reader.bytesPastMark()); assertEquals(0, reader.bytesPastMark(mark)); for (int i = 0; i < (numIterations - 1); i++) { b = ByteBufferUtil.read(reader, expected.length()); assertEquals(expected, new String(b.array(), Charset.forName("UTF-8"))); } assertTrue(reader.isEOF()); assertEquals(expected.length() * (numIterations - 1), reader.bytesPastMark()); assertEquals(expected.length() * (numIterations - 1), reader.bytesPastMark(mark)); reader.reset(mark); assertEquals(0, reader.bytesPastMark()); assertEquals(0, reader.bytesPastMark(mark)); assertFalse(reader.isEOF()); for (int i = 0; i < (numIterations - 1); i++) { b = ByteBufferUtil.read(reader, expected.length()); assertEquals(expected, new String(b.array(), Charset.forName("UTF-8"))); } reader.reset(); assertEquals(0, reader.bytesPastMark()); assertEquals(0, reader.bytesPastMark(mark)); assertFalse(reader.isEOF()); for (int i = 0'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMaxAttempts() throws Exception { GridAlwaysFailoverSpi spi = getSpi(); spi.setMaximumFailoverAttempts(1); List<GridNode> nodes = new ArrayList<>(); nodes.add(new GridTestNode(UUID.randomUUID())); nodes.add(new GridTestNode(UUID.randomUUID())); GridComputeJobResult jobRes = new GridTestJobResult(nodes.get(0)); XXX node = spi.failover(new GridFailoverTestContext(new GridTestTaskSession(), jobRes), nodes); assert node != null; assert node.equals(nodes.get(1)); checkFailedNodes(jobRes, 1); node = spi.failover(new GridFailoverTestContext(new GridTestTaskSession(), jobRes), nodes); assert node == null; checkFailedNodes(jobRes, 1); }', 'ground_truth': 'public void testMaxAttempts() throws Exception { GridAlwaysFailoverSpi spi = getSpi(); spi.setMaximumFailoverAttempts(1); List<ClusterNode> nodes = new ArrayList<>(); nodes.add(new GridTestNode(UUID.randomUUID())); nodes.add(new GridTestNode(UUID.randomUUID())); GridComputeJobResult jobRes = new GridTestJobResult(nodes.get(0)); ClusterNode node = spi.failover(new GridFailoverTestContext(new GridTestTaskSession(), jobRes), nodes); assert node != null; assert node.equals(nodes.get(1)); checkFailedNodes(jobRes, 1); node = spi.failover(new GridFailoverTestContext(new GridTestTaskSession(), jobRes), nodes); assert node == null; checkFailedNodes(jobRes, 1); }', 'output': 'public void testMaxAttempts() throws Exception { GridAlwaysFailoverSpi spi = getSpi(); spi.setMaximumFailoverAttempts(1); List<ClusterNode> nodes = new ArrayList<>(); nodes.add(new GridTestNode(UUID.randomUUID())); nodes.add(new GridTestNode(UUID.randomUUID())); GridComputeJobResult jobRes = new GridTestJobResult(nodes.get(0)); ClusterNode node = spi.failover(new GridFailoverTestContext(new GridTestTaskSession(), jobRes), nodes); assert node != null; assert node.equals(nodes.get(1)); checkFailedNodes(jobRes, 1); node = spi.failover(new GridFailoverTestContext(new GridTestTaskSession(), jobRes), nodes); assert node == null; checkFailedNodes(jobRes, 1); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private List<SecondaryIndexSearcher> getIndexSearchersForQuery(IndexClause clause) { List<SecondaryIndexSearcher> indexSearchers = new ArrayList<SecondaryIndexSearcher>(); Map<IndexType, Set<ByteBuffer>> groupByIndexType = new HashMap<IndexType, Set<ByteBuffer>>(); for(IndexExpression ix : clause.expressions) { SecondaryIndex index = getIndexForColumn(ix.column_name); if(index == null) continue; Set<ByteBuffer> columns = groupByIndexType.get(index.type()); if (columns == null) { columns = new HashSet<ByteBuffer>(); groupByIndexType.put(index.type(), columns); } columns.add(ix.column_name); } for (Map.Entry<IndexType, Set<ByteBuffer>> entry : groupByIndexType.entrySet()) { indexSearchers.add( getIndexForColumn(entry.getValue().iterator().next()).createSecondaryIndexSearcher(entry.getValue()) ); } return indexSearchers; }', 'ground_truth': 'private List<SecondaryIndexSearcher> getIndexSearchersForQuery(IndexClause clause) { List<SecondaryIndexSearcher> indexSearchers = new ArrayList<SecondaryIndexSearcher>(); Map<String, Set<ByteBuffer>> groupByIndexType = new HashMap<String, Set<ByteBuffer>>(); for(IndexExpression ix : clause.expressions) { SecondaryIndex index = getIndexForColumn(ix.column_name); if(index == null) continue; Set<ByteBuffer> columns = groupByIndexType.get(index.getClass().getCanonicalName()); if (columns == null) { columns = new HashSet<ByteBuffer>(); groupByIndexType.put(index.getClass().getCanonicalName(), columns); } columns.add(ix.column_name); } for (Map.Entry<String, Set<ByteBuffer>> entry : groupByIndexType.entrySet()) { indexSearchers.add( getIndexForColumn(entry.getValue().iterator().next()).createSecondaryIndexSearcher(entry.getValue()) ); } return indexSearchers; }', 'output': 'private List<SecondaryIndexSearcher> getIndexSearchersForQuery(IndexClause clause) { List<SecondaryIndexSearcher> indexSearchers = new ArrayList<SecondaryIndexSearcher>(); Map<IndexType, Set<ByteBuffer>> groupByIndexType = new HashMap<IndexType, Set<ByteBuffer>>(); for(IndexExpression ix : clause.expressions) { SecondaryIndex index = getIndexForColumn(ix.column_name); if(index == null) continue; Set<ByteBuffer> columns = groupByIndexType.get(index.type()); if (columns == null) { columns = new HashSet<ByteBuffer>(); groupByIndexType.put(index.type(), columns); } columns.add(ix.column_name); } for (Map.Entry<IndexType, Set<ByteBuffer>> entry : groupByIndexType.entrySet()) { indexSearchers.add( getIndexForColumn(entry.getValue().iterator().next()).createSecondaryIndexSearcher(entry.getValue()) ); } return indexSearchers; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ObjectObjectOpenHashMap<String, ReplicaCount> countPerDc(Collection<String> dataCenters, Iterable<Replica> replicas) { ObjectObjectOpenHashMap<String, ReplicaCount> perDc = new ObjectObjectOpenHashMap<>(dataCenters.size()); for (String dc: dataCenters) perDc.put(dc, new ReplicaCount()); IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch(); for (Replica replica : replicas) { String dc = snitch.getDatacenter(replica); perDc.get(dc).increment(replica); } return perDc; }', 'ground_truth': 'public static ObjectObjectHashMap<String, ReplicaCount> countPerDc(Collection<String> dataCenters, Iterable<Replica> replicas) { ObjectObjectHashMap<String, ReplicaCount> perDc = new ObjectObjectHashMap<>(dataCenters.size()); for (String dc: dataCenters) perDc.put(dc, new ReplicaCount()); IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch(); for (Replica replica : replicas) { String dc = snitch.getDatacenter(replica); perDc.get(dc).increment(replica); } return perDc; }', 'output': 'public static ObjectOpenHashMap<String, ReplicaCount> countPerDc(Collection<String> dataCenters, Iterable<Replica> replicas) { ObjectOpenHashMap<String, ReplicaCount> perDc = new ObjectOpenHashMap<>(dataCenters.size()); for (String dc: dataCenters) perDc.put(dc, new ReplicaCount()); IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch(); for (Replica replica : replicas) { String dc = snitch.getDatacenter(replica); perDc.get(dc).increment(replica); } return perDc; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected Map<ByteBuffer, CQL3Row> readRequiredRows(Collection<ByteBuffer> partitionKeys, Composite clusteringPrefix, boolean local, ConsistencyLevel cl) throws RequestExecutionException, RequestValidationException { if (!requiresRead()) return null; try { cl.validateForRead(keyspace()); } catch (InvalidRequestException e) { throw new InvalidRequestException(String.format("Write operation require a read but consistency %s is not supported on reads", cl)); } ColumnSlice[] slices = new ColumnSlice[]{ clusteringPrefix.slice() }; List<ReadCommand> commands = new ArrayList<ReadCommand>(partitionKeys.size()); long now = System.currentTimeMillis(); for (ByteBuffer key : partitionKeys) commands.add(new SliceFromReadCommand(keyspace(), key, columnFamily(), now, new SliceQueryFilter(slices, false, Integer.MAX_VALUE))); List<Row> rows = local ? SelectStatement.readLocally(keyspace(), commands) : StorageProxy.read(commands, cl); Map<ByteBuffer, CQL3Row> map = new HashMap<ByteBuffer, CQL3Row>(); for (Row row : rows) { if (row.cf == null || row.cf.isEmpty()) continue; XXX iter = cfm.comparator.CQL3RowBuilder(cfm, now).group(row.cf.getSortedColumns().iterator()); if (iter.hasNext()) { map.put(row.key.getKey(), iter.next()); assert !iter.hasNext(); } } return map; }', 'ground_truth': 'protected Map<ByteBuffer, CQL3Row> readRequiredRows(Collection<ByteBuffer> partitionKeys, Composite clusteringPrefix, boolean local, ConsistencyLevel cl) throws RequestExecutionException, RequestValidationException { if (!requiresRead()) return null; try { cl.validateForRead(keyspace()); } catch (InvalidRequestException e) { throw new InvalidRequestException(String.format("Write operation require a read but consistency %s is not supported on reads", cl)); } ColumnSlice[] slices = new ColumnSlice[]{ clusteringPrefix.slice() }; List<ReadCommand> commands = new ArrayList<ReadCommand>(partitionKeys.size()); long now = System.currentTimeMillis(); for (ByteBuffer key : partitionKeys) commands.add(new SliceFromReadCommand(keyspace(), key, columnFamily(), now, new SliceQueryFilter(slices, false, Integer.MAX_VALUE))); List<Row> rows = local ? SelectStatement.readLocally(keyspace(), commands) : StorageProxy.read(commands, cl); Map<ByteBuffer, CQL3Row> map = new HashMap<ByteBuffer, CQL3Row>(); for (Row row : rows) { if (row.cf == null || row.cf.isEmpty()) continue; CQL3Row.RowIterator iter = cfm.comparator.CQL3RowBuilder(cfm, now).group(row.cf.getSortedColumns().iterator()); if(iter.getStaticRow() != null) { map.put(row.key.getKey(), iter.getStaticRow()); } if (iter.hasNext()) { map.put(row.key.getKey(), iter.next()); assert !iter.hasNext(); } } return map; }', 'output': 'protected Map<ByteBuffer, CQL3Row> readRequiredRows(Collection<ByteBuffer> partitionKeys, Composite clusteringPrefix, boolean local, ConsistencyLevel cl) throws RequestExecutionException, RequestValidationException { if (!requiresRead()) return null; try { cl.validateForRead(keyspace()); } catch (InvalidRequestException e) { throw new InvalidRequestException(String.format("Write operation require a read but consistency %s is not supported on reads", cl)); } ColumnSlice[] slices = new ColumnSlice[]{ clusteringPrefix.slice() }; List<ReadCommand> commands = new ArrayList<ReadCommand>(partitionKeys.size()); long now = System.currentTimeMillis(); for (ByteBuffer key : partitionKeys) commands.add(new SliceFromReadCommand(keyspace(), key, columnFamily(), now, new SliceQueryFilter(slices, false, Integer.MAX_VALUE))); List<Row> rows = local ? SelectStatement.readLocally(keyspace(), commands) : StorageProxy.read(commands, cl); Map<ByteBuffer, CQL3Row> map = new HashMap<ByteBuffer, CQL3Row>(); for (Row row : rows) { if (row.cf == null || row.cf.isEmpty()) continue; Iterator<Column> iter = cfm.comparator.CQL3RowBuilder(cfm, now).group(row.cf.getSortedColumns().iterator()); if (iter.hasNext()) { map.put(row.key.getKey(), iter.next()); assert !iter.hasNext(); } } return map; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCreateConsistencyMultithreaded() throws Exception { final AtomicBoolean stop = new AtomicBoolean(); final AtomicInteger createCtr = new AtomicInteger(); final AtomicReference<Exception> err = new AtomicReference<>(); ggfs.create(FILE, false).close(); int threadCnt = 5; XXX fut = multithreadedAsync(new Runnable() { @Override public void run() { while (!stop.get()) { IgniteFsOutputStream os = null; try { os = ggfs.create(FILE, true); os.write(chunk); U.sleep(50); os.close(); createCtr.incrementAndGet(); } catch (IgniteCheckedException ignore) { try { U.sleep(10); } catch (IgniteInterruptedException ignored) { } } catch (IOException e) { if (!e.getMessage().startsWith("File was concurrently deleted")) err.compareAndSet(null, e); } finally { U.closeQuiet(os); } } } }, threadCnt); long startTime = U.currentTimeMillis(); while (createCtr.get() < 50 && U.currentTimeMillis() - startTime < 60 * 1000) U.sleep(100); stop.set(true); fut.get(); awaitFileClose(ggfs, FILE); if (err.get() != null) throw err.get(); checkFileContent(ggfs, FILE, chunk); }', 'ground_truth': 'public void testCreateConsistencyMultithreaded() throws Exception { final AtomicBoolean stop = new AtomicBoolean(); final AtomicInteger createCtr = new AtomicInteger(); final AtomicReference<Exception> err = new AtomicReference<>(); ggfs.create(FILE, false).close(); int threadCnt = 5; IgniteInternalFuture<?> fut = multithreadedAsync(new Runnable() { @Override public void run() { while (!stop.get()) { IgniteFsOutputStream os = null; try { os = ggfs.create(FILE, true); os.write(chunk); U.sleep(50); os.close(); createCtr.incrementAndGet(); } catch (IgniteCheckedException ignore) { try { U.sleep(10); } catch (IgniteInterruptedException ignored) { } } catch (IOException e) { if (!e.getMessage().startsWith("File was concurrently deleted")) err.compareAndSet(null, e); } finally { U.closeQuiet(os); } } } }, threadCnt); long startTime = U.currentTimeMillis(); while (createCtr.get() < 50 && U.currentTimeMillis() - startTime < 60 * 1000) U.sleep(100); stop.set(true); fut.get(); awaitFileClose(ggfs, FILE); if (err.get() != null) throw err.get(); checkFileContent(ggfs, FILE, chunk); }', 'output': 'public void testCreateConsistencyMultithreaded() throws Exception { final AtomicBoolean stop = new AtomicBoolean(); final AtomicInteger createCtr = new AtomicInteger(); final AtomicReference<Exception> err = new AtomicReference<>(); ggfs.create(FILE, false).close(); int threadCnt = 5; IgniteFuture<?> fut = multithreadedAsync(new Runnable() { @Override public void run() { while (!stop.get()) { IgniteFsOutputStream os = null; try { os = ggfs.create(FILE, true); os.write(chunk); U.sleep(50); os.close(); createCtr.incrementAndGet(); } catch (IgniteCheckedException ignore) { try { U.sleep(10); } catch (IgniteInterruptedException ignored) { } } catch (IOException e) { if (!e.getMessage().startsWith("File was concurrently deleted")) err.compareAndSet(null, e); } finally { U.closeQuiet(os); } } } }, threadCnt); long startTime = U.currentTimeMillis(); while (createCtr.get() < 50 && U.currentTimeMillis() - startTime < 60 * 1000) U.sleep(100); stop.set(true); fut.get(); awaitFileClose(ggfs, FILE); if (err.get() != null) throw err.get(); checkFileContent(ggfs, FILE, chunk); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Latch getOrCreate(String id, AffinityTopologyVersion topVer) { lock.lock(); try { ClusterNode coordinator = getLatchCoordinator(topVer); if (coordinator == null) { XXX latch = new ClientLatch(id, AffinityTopologyVersion.NONE, null, Collections.emptyList()); latch.complete(); return latch; } Collection<ClusterNode> participants = getLatchParticipants(topVer); return coordinator.isLocal() ? createServerLatch(id, topVer, participants) : createClientLatch(id, topVer, coordinator, participants); } finally { lock.unlock(); } }', 'ground_truth': 'public Latch getOrCreate(String id, AffinityTopologyVersion topVer) { lock.lock(); try { final CompletableLatchUid latchUid = new CompletableLatchUid(id, topVer); CompletableLatch latch = clientLatches.containsKey(latchUid) ? clientLatches.get(latchUid) : serverLatches.get(latchUid); if (latch != null) return latch; ClusterNode coordinator = getLatchCoordinator(topVer); if (coordinator == null) return null; Collection<ClusterNode> participants = getLatchParticipants(topVer); return coordinator.isLocal() ? createServerLatch(latchUid, participants) : createClientLatch(latchUid, coordinator, participants); } finally { lock.unlock(); } }', 'output': 'public Latch getOrCreate(String id, AffinityTopologyVersion topVer) { lock.lock(); try { ClusterNode coordinator = getLatchCoordinator(topVer); if (coordinator == null) { ClientLatch latch = new ClientLatch(id, AffinityTopologyVersion.NONE, null, Collections.emptyList()); latch.complete(); return latch; } Collection<ClusterNode> participants = getLatchParticipants(topVer); return coordinator.isLocal() ? createServerLatch(id, topVer, participants) : createClientLatch(id, topVer, coordinator, participants); } finally { lock.unlock(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testExceptionPropagation() throws Exception { try (XXX srv = startGrid("server")) { GridStringLogger log = new GridStringLogger(); try (Ignite client = startGrid("client", getConfiguration("client").setGridLogger(log).setClientMode(true))) { try { client.services().deployClusterSingleton("my-service", new ServiceImpl()); } catch (IgniteException ignored) { assertTrue(log.toString().contains("ServiceImpl init exception")); return; } fail("https: } } }', 'ground_truth': 'public void testExceptionPropagation() throws Exception { try (IgniteEx srv = startGrid("server")) { try (Ignite client = startGrid("client", getConfiguration("client").setClientMode(true))) { final String srvcName = "my-service"; try { client.services().deployClusterSingleton(srvcName, new ServiceImpl()); fail("Deployment exception has been expected."); } catch (ServiceDeploymentException ex) { String errMsg = ex.getSuppressed()[0].getMessage(); assertTrue(errMsg.contains(srv.cluster().localNode().id().toString())); assertTrue(errMsg.contains(srvcName)); Throwable cause = ex.getSuppressed()[0].getCause(); assertTrue(cause.getMessage().contains("ServiceImpl init exception")); } } } }', 'output': 'public void testExceptionPropagation() throws Exception { try (IgniteEx srv = startGrid("server")) { GridStringLogger log = new GridStringLogger(); try (IgniteEx client = startGrid("client", getConfiguration("client").setGridLogger(log).setClientMode(true))) { try { client.services().deployClusterSingleton("my-service", new ServiceImpl()); } catch (IgniteException ignored) { assertTrue(log.toString().contains("ServiceImpl init exception")); return; } fail("https: } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public int getChildrenCount(TreeModel original, Object node) throws UnknownTypeException { EvalWatch ew; synchronized (evalWatches) { ew = evalWatches.get(node); } if (ew != null) { switch (ew.getStatus()) { case READY: XXX result = ew.getResult(); return result.getNumChildren(); } } return original.getChildrenCount(node); }', 'ground_truth': 'public int getChildrenCount(TreeModel original, Object node) throws UnknownTypeException { EvalWatch ew; synchronized (evalWatches) { ew = evalWatches.get(node); } if (ew != null) { switch (ew.getStatus()) { case READY: NIVariable result = ew.getResult(); return result.getNumChildren(); } } return original.getChildrenCount(node); }', 'output': 'public int getChildrenCount(TreeModel original, Object node) throws UnknownTypeException { EvalWatch ew; synchronized (evalWatches) { ew = evalWatches.get(node); } if (ew != null) { switch (ew.getStatus()) { case READY: TreeEvalResult result = ew.getResult(); return result.getNumChildren(); } } return original.getChildrenCount(node); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testDenormalize2() { StreamFromOptions dummy = new StreamFromOptions(null, range(40, 50)); Map<Range<Token>, StreamFromOptions> incoming = new HashMap<>(); incoming.put(range(40, 50), dummy); Set<Range<Token>> newInput = RangeDenormalizer.denormalize(range(0, 100), incoming); assertEquals(1, incoming.size()); assertTrue(incoming.containsKey(range(40, 50))); assertEquals(3, newInput.size()); assertTrue(newInput.contains(range(0, 40))); assertTrue(newInput.contains(range(40, 50))); assertTrue(newInput.contains(range(50, 100))); }', 'ground_truth': 'public void testDenormalize2() { StreamFromOptions dummy = new StreamFromOptions(null, range(40, 50)); RangeMap<StreamFromOptions> incoming = new RangeMap<>(); incoming.put(range(40, 50), dummy); Set<Range<Token>> newInput = RangeDenormalizer.denormalize(range(0, 100), incoming); assertEquals(1, incoming.size()); assertTrue(incoming.containsKey(range(40, 50))); assertEquals(3, newInput.size()); assertTrue(newInput.contains(range(0, 40))); assertTrue(newInput.contains(range(40, 50))); assertTrue(newInput.contains(range(50, 100))); }', 'output': 'public void testDenormalize2() { StreamFromOptions dummy = new StreamFromOptions(null, range(40, 50)); Map<Range<Token>, StreamFromOptions> incoming = new HashMap<>(); incoming.put(range(40, 50), dummy); Set<Range<Token>> newInput = RangeDenormalizer.denormalize(range(0, 100), incoming); assertEquals(1, incoming.size()); assertTrue(incoming.containsKey(range(40, 50))); assertEquals(3, newInput.size()); assertTrue(newInput.contains(range(0, 40))); assertTrue(newInput.contains(range(40, 50))); assertTrue(newInput.contains(range(50, 100))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GridFutureAdapter<Void> opportunisticRestart() { XXX restartFut = new GridFutureAdapter<>(); while (true) { if (this.restartFut.compareAndSet(null, restartFut)) return null; GridFutureAdapter<Void> curFut = this.restartFut.get(); if (curFut != null) return curFut; } }', 'ground_truth': 'public GridFutureAdapter<Void> opportunisticRestart() { RestartFuture restartFut = new RestartFuture(ctx.name()); while (true) { if (this.restartFut.compareAndSet(null, restartFut)) return null; GridFutureAdapter<Void> curFut = this.restartFut.get(); if (curFut != null) return curFut; } }', 'output': 'public GridFutureAdapter<Void> opportunisticRestart() { GridFutureAdapter<Void> restartFut = new GridFutureAdapter<>(); while (true) { if (this.restartFut.compareAndSet(null, restartFut)) return null; GridFutureAdapter<Void> curFut = this.restartFut.get(); if (curFut != null) return curFut; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void visitMethod(MethodNode node) { XXX oldInsideScriptBody = insideScriptBody; if (node.isScriptBody()) insideScriptBody = true; super.visitMethod(node); insideScriptBody = oldInsideScriptBody; }', 'ground_truth': 'public void visitMethod(MethodNode node) { boolean oldInsideScriptBody = insideScriptBody; if (node.isScriptBody()) insideScriptBody = true; super.visitMethod(node); insideScriptBody = oldInsideScriptBody; }', 'output': 'public void visitMethod(MethodNode node) { boolean oldInsideScriptBody = insideScriptBody; if (node.isScriptBody()) insideScriptBody = true; super.visitMethod(node); insideScriptBody = oldInsideScriptBody; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testJoinManyToOneMapping() { leftListOfKv.add(KV.of("Key2", 4L)); leftListOfKv.add(KV.of("Key2", 6L)); PCollection<KV<String, Long>> leftCollection = p.apply(Create.of(leftListOfKv)); listRightOfKv.add(KV.of("Key2", "bar")); PCollection<KV<String, String>> rightCollection = p.apply(Create.of(listRightOfKv)); PCollection<KV<Long, String>> output = Join.rightOuterJoin( leftCollection, rightCollection, -1L); expectedResult.add(KV.of(4L, "bar")); expectedResult.add(KV.of(6L, "bar")); DataflowAssert.that(output).containsInAnyOrder(expectedResult); p.run(); }', 'ground_truth': 'public void testJoinManyToOneMapping() { leftListOfKv.add(KV.of("Key2", 4L)); leftListOfKv.add(KV.of("Key2", 6L)); PCollection<KV<String, Long>> leftCollection = p.apply(Create.of(leftListOfKv)); listRightOfKv.add(KV.of("Key2", "bar")); PCollection<KV<String, String>> rightCollection = p.apply(Create.of(listRightOfKv)); PCollection<KV<String, KV<Long, String>>> output = Join.rightOuterJoin( leftCollection, rightCollection, -1L); expectedResult.add(KV.of("Key2", KV.of(4L, "bar"))); expectedResult.add(KV.of("Key2", KV.of(6L, "bar"))); DataflowAssert.that(output).containsInAnyOrder(expectedResult); p.run(); }', 'output': 'public void testJoinManyToOneMapping() { leftListOfKv.add(KV.of("Key2", 4L)); leftListOfKv.add(KV.of("Key2", 6L)); PCollection<KV<String, Long>> leftCollection = p.apply(Create.of(leftListOfKv)); listRightOfKv.add(KV.of("Key2", "bar")); PCollection<KV<String, String>> rightCollection = p.apply(Create.of(listRightOfKv)); PCollection<KV<Long, String>> output = Join.rightOuterJoin( leftCollection, rightCollection, -1L); expectedResult.add(KV.of(4L, "bar")); expectedResult.add(KV.of(6L, "bar")); DataflowAssert.that(output).containsInAnyOrder(expectedResult); p.run(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCreateFileColocated() throws Exception { XXX path = new IgniteFsPath("/colocated"); UUID uuid = UUID.randomUUID(); IgniteUuid affKey; long idx = 0; while (true) { affKey = new IgniteUuid(uuid, idx); if (grid(0).mapKeyToNode(DATA_CACHE_NAME, affKey).id().equals(grid(0).localNode().id())) break; idx++; } try (IgniteFsOutputStream out = fs.create(path, 1024, true, affKey, 0, 1024, null)) { for (int i = 0; i < 15; i++) out.write(new byte[1024 * 1024]); } IgniteFsFile info = fs.info(path); Collection<IgniteFsBlockLocation> affNodes = fs.affinity(path, 0, info.length()); assertEquals(1, affNodes.size()); Collection<UUID> nodeIds = F.first(affNodes).nodeIds(); assertEquals(1, nodeIds.size()); assertEquals(grid(0).localNode().id(), F.first(nodeIds)); }', 'ground_truth': 'public void testCreateFileColocated() throws Exception { IgfsPath path = new IgfsPath("/colocated"); UUID uuid = UUID.randomUUID(); IgniteUuid affKey; long idx = 0; while (true) { affKey = new IgniteUuid(uuid, idx); if (grid(0).mapKeyToNode(DATA_CACHE_NAME, affKey).id().equals(grid(0).localNode().id())) break; idx++; } try (IgniteFsOutputStream out = fs.create(path, 1024, true, affKey, 0, 1024, null)) { for (int i = 0; i < 15; i++) out.write(new byte[1024 * 1024]); } IgniteFsFile info = fs.info(path); Collection<IgniteFsBlockLocation> affNodes = fs.affinity(path, 0, info.length()); assertEquals(1, affNodes.size()); Collection<UUID> nodeIds = F.first(affNodes).nodeIds(); assertEquals(1, nodeIds.size()); assertEquals(grid(0).localNode().id(), F.first(nodeIds)); }', 'output': 'public void testCreateFileColocated() throws Exception { IgniteFsPath path = new IgniteFsPath("/colocated"); UUID uuid = UUID.randomUUID(); IgniteUuid affKey; long idx = 0; while (true) { affKey = new IgniteUuid(uuid, idx); if (grid(0).mapKeyToNode(DATA_CACHE_NAME, affKey).id().equals(grid(0).localNode().id())) break; idx++; } try (IgniteFsOutputStream out = fs.create(path, 1024, true, affKey, 0, 1024, null)) { for (int i = 0; i < 15; i++) out.write(new byte[1024 * 1024]); } IgniteFsFile info = fs.info(path); Collection<IgniteFsBlockLocation> affNodes = fs.affinity(path, 0, info.length()); assertEquals(1, affNodes.size()); Collection<UUID> nodeIds = F.first(affNodes).nodeIds(); assertEquals(1, nodeIds.size()); assertEquals(grid(0).localNode().id(), F.first(nodeIds)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String valueInString() { XXX sb = new StringBuffer(); for (int idx = 0; idx < size(); ++idx) { sb.append(String.format(",%s=%s", dataType.getFieldsName().get(idx), getFieldValue(idx))); } return sb.substring(1); }', 'ground_truth': 'public String valueInString() { StringBuilder sb = new StringBuilder(); for (int idx = 0; idx < size(); ++idx) { sb.append(String.format(",%s=%s", dataType.getFieldsName().get(idx), getFieldValue(idx))); } return sb.substring(1); }', 'output': 'public String valueInString() { StringBuilder sb = new StringBuilder(); for (int idx = 0; idx < size(); ++idx) { sb.append(String.format(",%s=%s", dataType.getFieldsName().get(idx), getFieldValue(idx))); } return sb.toString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static BaseBeamTable getTable() { final RelProtoDataType protoRowType = new RelProtoDataType() { @Override public RelDataType apply(RelDataTypeFactory a0) { return a0.builder().add("order_id", SqlTypeName.BIGINT).add("site_id", SqlTypeName.INTEGER) .add("price", SqlTypeName.DOUBLE).add("order_time", SqlTypeName.TIMESTAMP).build(); } }; BeamSQLRecordType dataType = BeamSQLRecordType.from( protoRowType.apply(BeamQueryPlanner.TYPE_FACTORY)); BeamSQLRow row1 = new BeamSQLRow(dataType); row1.addField(0, 12345L); row1.addField(1, 0); row1.addField(2, 10.5); row1.addField(3, new Date()); BeamSQLRow row2 = new BeamSQLRow(dataType); row2.addField(0, 12345L); row2.addField(1, 1); row2.addField(2, 20.5); row2.addField(3, new Date()); BeamSQLRow row3 = new BeamSQLRow(dataType); row3.addField(0, 12345L); row3.addField(1, 0); row3.addField(2, 20.5); row3.addField(3, new Date()); XXX row4 = new BeamSQLRow(dataType); row4.addField(0, null); row4.addField(1, null); row4.addField(2, 20.5); row4.addField(3, new Date()); return new MockedBeamSQLTable(protoRowType).withInputRecords( Arrays.asList(row1, row2, row3, row4)); }', 'ground_truth': 'private static BaseBeamTable getTable() { final RelProtoDataType protoRowType = new RelProtoDataType() { @Override public RelDataType apply(RelDataTypeFactory a0) { return a0.builder().add("order_id", SqlTypeName.BIGINT).add("site_id", SqlTypeName.INTEGER) .add("price", SqlTypeName.DOUBLE).add("order_time", SqlTypeName.TIMESTAMP).build(); } }; BeamSqlRecordType dataType = BeamSqlRecordType.from( protoRowType.apply(BeamQueryPlanner.TYPE_FACTORY)); BeamSqlRow row1 = new BeamSqlRow(dataType); row1.addField(0, 12345L); row1.addField(1, 0); row1.addField(2, 10.5); row1.addField(3, new Date()); BeamSqlRow row2 = new BeamSqlRow(dataType); row2.addField(0, 12345L); row2.addField(1, 1); row2.addField(2, 20.5); row2.addField(3, new Date()); BeamSqlRow row3 = new BeamSqlRow(dataType); row3.addField(0, 12345L); row3.addField(1, 0); row3.addField(2, 20.5); row3.addField(3, new Date()); BeamSqlRow row4 = new BeamSqlRow(dataType); row4.addField(0, null); row4.addField(1, null); row4.addField(2, 20.5); row4.addField(3, new Date()); return new MockedBeamSqlTable(protoRowType).withInputRecords( Arrays.asList(row1, row2, row3, row4)); }', 'output': 'private static BeamTable getTable() { final RelProtoDataType protoRowType = new RelProtoDataType() { @Override public RelDataType apply(RelDataTypeFactory a0) { return a0.builder().add("order_id", SqlTypeName.BIGINT).add("site_id", SqlTypeName.INTEGER) .add("price", SqlTypeName.DOUBLE).add("order_time", SqlTypeName.TIMESTAMP).build(); } }; BeamSQLRecordType dataType = BeamSQLRecordType.from( protoRowType.apply(BeamQueryPlanner.TYPE_FACTORY)); BeamSQLRow row1 = new BeamSQLRow(dataType); row1.addField(0, 12345L); row1.addField(1, 0); row1.addField(2, 10.5); row1.addField(3, new Date()); BeamSQLRow row2 = new BeamSQLRow(dataType); row2.addField(0, 12345L); row2.addField(1, 1); row2.addField(2, 20.5); row2.addField(3, new Date()); BeamSQLRow row3 = new BeamSQLRow(dataType); row3.addField(0, 12345L); row3.addField(1, 0); row3.addField(2, 20.5); row3.addField(3, new Date()); BeamSQLRow row4 = new BeamSQLRow(dataType); row4.addField(0, null); row4.addField(1, null); row4.addField(2, 20.5); row4.addField(3, new Date()); return new MockedBeamSQLTable(protoRowType).withInputRecords( Arrays.asList(row1, row2, row3, row4)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean isStale(Row data, ByteBuffer indexValue, int nowInSec) { ByteBuffer[] components = ((CompositeType)functions.getIndexedValueType(indexedColumn)).split(indexValue); ByteBuffer mapKey = components[0]; ByteBuffer mapValue = components[1]; ColumnMetadata columnDef = indexedColumn; XXX cell = data.getCell(columnDef, CellPath.create(mapKey)); if (cell == null || !cell.isLive(nowInSec)) return true; AbstractType<?> valueComparator = ((CollectionType)columnDef.type).valueComparator(); return valueComparator.compare(mapValue, cell.value()) != 0; }', 'ground_truth': 'public boolean isStale(Row data, ByteBuffer indexValue, int nowInSec) { ByteBuffer[] components = ((CompositeType)functions.getIndexedValueType(indexedColumn)).split(indexValue); ByteBuffer mapKey = components[0]; ByteBuffer mapValue = components[1]; ColumnMetadata columnDef = indexedColumn; Cell<?> cell = data.getCell(columnDef, CellPath.create(mapKey)); if (cell == null || !cell.isLive(nowInSec)) return true; AbstractType<?> valueComparator = ((CollectionType)columnDef.type).valueComparator(); return valueComparator.compare(mapValue, cell.buffer()) != 0; }', 'output': 'public boolean isStale(Row data, ByteBuffer indexValue, int nowInSec) { ByteBuffer[] components = ((CompositeType)functions.getIndexedValueType(indexedColumn)).split(indexValue); ByteBuffer mapKey = components[0]; ByteBuffer mapValue = components[1]; ColumnMetadata columnDef = indexedColumn; DataCell cell = data.getCell(columnDef, CellPath.create(mapKey)); if (cell == null || !cell.isLive(nowInSec)) return true; AbstractType<?> valueComparator = ((CollectionType)columnDef.type).valueComparator(); return valueComparator.compare(mapValue, cell.value()) != 0; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void publish(GridClockDeltaSnapshot snapshot, GridDiscoveryTopologySnapshot top) { if (!rw.tryReadLock()) return; try { timeSyncHist.put(snapshot.version(), snapshot); for (XXX n : top.topologyNodes()) { GridClockDeltaSnapshotMessage msg = new GridClockDeltaSnapshotMessage( snapshot.version(), snapshot.deltas()); try { ctx.io().send(n, TOPIC_TIME_SYNC, msg, SYSTEM_POOL); } catch (GridException e) { if (ctx.discovery().pingNode(n.id())) U.error(log, "Failed to send time sync snapshot to remote node (did not leave grid?) " + "[nodeId=" + n.id() + ", msg=" + msg + ", err=" + e.getMessage() + \']\'); else if (log.isDebugEnabled()) log.debug("Failed to send time sync snapshot to remote node (did not leave grid?) " + "[nodeId=" + n.id() + ", msg=" + msg + ", err=" + e.getMessage() + \']\'); } } } finally { rw.readUnlock(); } }', 'ground_truth': 'private void publish(GridClockDeltaSnapshot snapshot, GridDiscoveryTopologySnapshot top) { if (!rw.tryReadLock()) return; try { timeSyncHist.put(snapshot.version(), snapshot); for (ClusterNode n : top.topologyNodes()) { GridClockDeltaSnapshotMessage msg = new GridClockDeltaSnapshotMessage( snapshot.version(), snapshot.deltas()); try { ctx.io().send(n, TOPIC_TIME_SYNC, msg, SYSTEM_POOL); } catch (GridException e) { if (ctx.discovery().pingNode(n.id())) U.error(log, "Failed to send time sync snapshot to remote node (did not leave grid?) " + "[nodeId=" + n.id() + ", msg=" + msg + ", err=" + e.getMessage() + \']\'); else if (log.isDebugEnabled()) log.debug("Failed to send time sync snapshot to remote node (did not leave grid?) " + "[nodeId=" + n.id() + ", msg=" + msg + ", err=" + e.getMessage() + \']\'); } } } finally { rw.readUnlock(); } }', 'output': 'private void publish(GridClockDeltaSnapshot snapshot, GridDiscoveryTopologySnapshot top) { if (!rw.tryReadLock()) return; try { timeSyncHist.put(snapshot.version(), snapshot); for (ClusterNode n : top.topologyNodes()) { GridClockDeltaSnapshotMessage msg = new GridClockDeltaSnapshotMessage( snapshot.version(), snapshot.deltas()); try { ctx.io().send(n, TOPIC_TIME_SYNC, msg, SYSTEM_POOL); } catch (GridException e) { if (ctx.discovery().pingNode(n.id())) U.error(log, "Failed to send time sync snapshot to remote node (did not leave grid?) " + "[nodeId=" + n.id() + ", msg=" + msg + ", err=" + e.getMessage() + \']\'); else if (log.isDebugEnabled()) log.debug("Failed to send time sync snapshot to remote node (did not leave grid?) " + "[nodeId=" + n.id() + ", msg=" + msg + ", err=" + e.getMessage() + \']\'); } } } finally { rw.readUnlock(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean containsAll(Collection<?> other) { if (other == this) return true; if (other.size() > this.size()) return false; BTreeSearchIterator<ColumnDefinition, ColumnDefinition> iter = BTree.slice(columns, Comparator.naturalOrder(), BTree.Dir.ASC); for (Object def : other) if (iter.next((ColumnDefinition) def) == null) return false; return true; }', 'ground_truth': 'public boolean containsAll(Collection<?> other) { if (other == this) return true; if (other.size() > this.size()) return false; BTreeSearchIterator<ColumnMetadata, ColumnMetadata> iter = BTree.slice(columns, Comparator.naturalOrder(), BTree.Dir.ASC); for (Object def : other) if (iter.next((ColumnMetadata) def) == null) return false; return true; }', 'output': 'public boolean containsAll(Collection<?> other) { if (other == this) return true; if (other.size() > columns.size()) return false; BTreeSearchIterator<ColumnDefinition, ColumnDefinition> iter = BTree.slice(columns, Comparator.naturalOrder(), BTree.Dir.ASC); for (Object def : other) if (iter.next((ColumnDefinition) def) == null) return false; return true; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testRunnableWithSuccessfulCall() throws Exception { final String expectedHostName = "c6401.ambari.apache.org"; final String expectedComponentName = "DATANODE"; final String expectedClusterName = "clusterone"; final String expectedComponentAndHostName = expectedComponentName + "+" + expectedHostName; EasyMockSupport mockSupport = new EasyMockSupport(); LoggingRequestHelperFactory helperFactoryMock = mockSupport.createMock(LoggingRequestHelperFactory.class); AmbariManagementController controllerMock = mockSupport.createMock(AmbariManagementController.class); LoggingRequestHelper helperMock = mockSupport.createMock(LoggingRequestHelper.class); Cache cacheMock = mockSupport.createMock(Cache.class); XXX currentRequestsMock = mockSupport.createMock(Set.class); expect(helperFactoryMock.getHelper(controllerMock, expectedClusterName)).andReturn(helperMock); expect(helperMock.sendGetLogFileNamesRequest(expectedComponentName, expectedHostName)).andReturn(Collections.singleton("/this/is/just/a/test/directory")); cacheMock.put(expectedComponentAndHostName, Collections.singleton("/this/is/just/a/test/directory")); expect(currentRequestsMock.remove(expectedComponentAndHostName)).andReturn(true).once(); mockSupport.replayAll(); LogSearchDataRetrievalService.LogSearchFileNameRequestRunnable loggingRunnable = new LogSearchDataRetrievalService.LogSearchFileNameRequestRunnable(expectedHostName, expectedComponentName, expectedClusterName, cacheMock, currentRequestsMock, helperFactoryMock, controllerMock); loggingRunnable.run(); mockSupport.verifyAll(); }', 'ground_truth': 'public void testRunnableWithSuccessfulCall() throws Exception { final String expectedHostName = "c6401.ambari.apache.org"; final String expectedComponentName = "DATANODE"; final String expectedClusterName = "clusterone"; final String expectedComponentAndHostName = expectedComponentName + "+" + expectedHostName; EasyMockSupport mockSupport = new EasyMockSupport(); LoggingRequestHelperFactory helperFactoryMock = mockSupport.createMock(LoggingRequestHelperFactory.class); AmbariManagementController controllerMock = mockSupport.createMock(AmbariManagementController.class); LoggingRequestHelper helperMock = mockSupport.createMock(LoggingRequestHelper.class); Cache<String, Set<String>> cacheMock = mockSupport.createMock(Cache.class); Set<String> currentRequestsMock = mockSupport.createMock(Set.class); expect(helperFactoryMock.getHelper(controllerMock, expectedClusterName)).andReturn(helperMock); expect(helperMock.sendGetLogFileNamesRequest(expectedComponentName, expectedHostName)).andReturn(Collections.singleton("/this/is/just/a/test/directory")); cacheMock.put(expectedComponentAndHostName, Collections.singleton("/this/is/just/a/test/directory")); expect(currentRequestsMock.remove(expectedComponentAndHostName)).andReturn(true).once(); mockSupport.replayAll(); LogSearchDataRetrievalService.LogSearchFileNameRequestRunnable loggingRunnable = new LogSearchDataRetrievalService.LogSearchFileNameRequestRunnable(expectedHostName, expectedComponentName, expectedClusterName, cacheMock, currentRequestsMock, helperFactoryMock, controllerMock); loggingRunnable.run(); mockSupport.verifyAll(); }', 'output': 'public void testRunnableWithSuccessfulCall() throws Exception { final String expectedHostName = "c6401.ambari.apache.org"; final String expectedComponentName = "DATANODE"; final String expectedClusterName = "clusterone"; final String expectedComponentAndHostName = expectedComponentName + "+" + expectedHostName; EasyMockSupport mockSupport = new EasyMockSupport(); LoggingRequestHelperFactory helperFactoryMock = mockSupport.createMock(LoggingRequestHelperFactory.class); AmbariManagementController controllerMock = mockSupport.createMock(AmbariManagementController.class); LoggingRequestHelper helperMock = mockSupport.createMock(LoggingRequestHelper.class); Cache cacheMock = mockSupport.createMock(Cache.class); Set<String> currentRequestsMock = mockSupport.createMock(Set.class); expect(helperFactoryMock.getHelper(controllerMock, expectedClusterName)).andReturn(helperMock); expect(helperMock.sendGetLogFileNamesRequest(expectedComponentName, expectedHostName)).andReturn(Collections.singleton("/this/is/just/a/test/directory")); cacheMock.put(expectedComponentAndHostName, Collections.singleton("/this/is/just/a/test/directory")); expect(currentRequestsMock.remove(expectedComponentAndHostName)).andReturn(true).once(); mockSupport.replayAll(); LogSearchDataRetrievalService.LogSearchFileNameRequestRunnable loggingRunnable = new LogSearchDataRetrievalService.LogSearchFileNameRequestRunnable(expectedHostName, expectedComponentName, expectedClusterName, cacheMock, currentRequestsMock, helperFactoryMock, controllerMock); loggingRunnable.run(); mockSupport.verifyAll(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testDeploy() throws Exception { Class<? extends ComputeTask<?, ?>> task = GridFileDeploymentTestTask.class; deploy(task); GridDeploymentResource t1 = getSpi().findResource(task.getName()); assert t1 != null; XXX t2 = getSpi().findResource(task.getName()); assert t1.equals(t2); assert t1.getResourceClass() == t2.getResourceClass(); getSpi().unregister(task.getName()); checkUndeployed(task); assert getSpi().findResource(task.getName()) == null; }', 'ground_truth': 'public void testDeploy() throws Exception { Class<? extends ComputeTask<?, ?>> task = GridFileDeploymentTestTask.class; deploy(task); DeploymentResource t1 = getSpi().findResource(task.getName()); assert t1 != null; DeploymentResource t2 = getSpi().findResource(task.getName()); assert t1.equals(t2); assert t1.getResourceClass() == t2.getResourceClass(); getSpi().unregister(task.getName()); checkUndeployed(task); assert getSpi().findResource(task.getName()) == null; }', 'output': 'public void testDeploy() throws Exception { Class<? extends ComputeTask<?, ?>> task = GridFileDeploymentTestTask.class; deploy(task); GridDeploymentResource t1 = getSpi().findResource(task.getName()); assert t1 != null; GridDeploymentResource t2 = getSpi().findResource(task.getName()); assert t1.equals(t2); assert t1.getResourceClass() == t2.getResourceClass(); getSpi().unregister(task.getName()); checkUndeployed(task); assert getSpi().findResource(task.getName()) == null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMapRun() throws Exception { int lineCnt = 10000; String fileName = "/testFile"; prepareFile(fileName, lineCnt); totalLineCnt.set(0); taskWorkDirs.clear(); Configuration cfg = new Configuration(); cfg.setStrings("fs.ggfs.impl", GridGgfsHadoopFileSystem.class.getName()); Job job = Job.getInstance(cfg); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setMapperClass(TestMapper.class); job.setNumReduceTasks(0); job.setInputFormatClass(TextInputFormat.class); FileInputFormat.setInputPaths(job, new Path("ggfs: FileOutputFormat.setOutputPath(job, new Path("ggfs: job.setJarByClass(getClass()); XXX fut = grid(0).hadoop().submit(new GridHadoopJobId(UUID.randomUUID(), 1), createJobInfo(job.getConfiguration())); fut.get(); assertEquals(lineCnt, totalLineCnt.get()); assertEquals(32, taskWorkDirs.size()); }', 'ground_truth': 'public void testMapRun() throws Exception { int lineCnt = 10000; String fileName = "/testFile"; prepareFile(fileName, lineCnt); totalLineCnt.set(0); taskWorkDirs.clear(); Configuration cfg = new Configuration(); cfg.setStrings("fs.ggfs.impl", GridGgfsHadoopFileSystem.class.getName()); Job job = Job.getInstance(cfg); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setMapperClass(TestMapper.class); job.setNumReduceTasks(0); job.setInputFormatClass(TextInputFormat.class); FileInputFormat.setInputPaths(job, new Path("ggfs: FileOutputFormat.setOutputPath(job, new Path("ggfs: job.setJarByClass(getClass()); IgniteInternalFuture<?> fut = grid(0).hadoop().submit(new GridHadoopJobId(UUID.randomUUID(), 1), createJobInfo(job.getConfiguration())); fut.get(); assertEquals(lineCnt, totalLineCnt.get()); assertEquals(32, taskWorkDirs.size()); }', 'output': 'public void testMapRun() throws Exception { int lineCnt = 10000; String fileName = "/testFile"; prepareFile(fileName, lineCnt); totalLineCnt.set(0); taskWorkDirs.clear(); Configuration cfg = new Configuration(); cfg.setStrings("fs.ggfs.impl", GridGgfsHadoopFileSystem.class.getName()); Job job = Job.getInstance(cfg); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setMapperClass(TestMapper.class); job.setNumReduceTasks(0); job.setInputFormatClass(TextInputFormat.class); FileInputFormat.setInputPaths(job, new Path("ggfs: FileOutputFormat.setOutputPath(job, new Path("ggfs: job.setJarByClass(getClass()); IgniteFuture<?> fut = grid(0).hadoop().submit(new GridHadoopJobId(UUID.randomUUID(), 1), createJobInfo(job.getConfiguration())); fut.get(); assertEquals(lineCnt, totalLineCnt.get()); assertEquals(32, taskWorkDirs.size()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean execute(GString gstring) throws SQLException { XXX params = getParameters(gstring); String sql = asSql(gstring, params); return execute(sql, params); } public List executeInsert(GString gstring) throws SQLException { XXX params = getParameters(gstring); String sql = asSql(gstring, params); return executeInsert(sql, params); } public int executeUpdate(GString gstring) throws SQLException { XXX params = getParameters(gstring); String sql = asSql(gstring, params); return executeUpdate(sql, params); }', 'ground_truth': 'public boolean execute(GString gstring) throws SQLException { List<Object> params = getParameters(gstring); String sql = asSql(gstring, params); return execute(sql, params); }', 'output': 'public boolean execute(GString gstring) throws SQLException { Object[] params = getParameters(gstring); String sql = asSql(gstring, params); return execute(sql, params); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void put(Object o, String cacheKey, String terminalId) throws IgniteCheckedException { GridCache<GridCacheAffinityKey<String>, Object> cache = ignite.cache(null); GridCacheAffinityKey<String> affinityKey = new GridCacheAffinityKey<>(cacheKey, terminalId); GridCacheEntry<GridCacheAffinityKey<String>, Object> entry = cache.entry(affinityKey); entry.setx(o); }', 'ground_truth': 'private void put(Object o, String cacheKey, String terminalId) throws IgniteCheckedException { Cache<CacheAffinityKey<String>, Object> cache = ignite.cache(null); CacheAffinityKey<String> affinityKey = new CacheAffinityKey<>(cacheKey, terminalId); CacheEntry<CacheAffinityKey<String>, Object> entry = cache.entry(affinityKey); entry.setx(o); }', 'output': 'private void put(Object o, String cacheKey, String terminalId, int version) throws IgniteCheckedException { GridCache<GridCacheAffinityKey<String>, Object> cache = ignite.cache(null); GridCacheAffinityKey<String> affinityKey = new GridCacheAffinityKey<>(cacheKey, terminalId); GridCacheEntry<GridCacheAffinityKey<String>, Object> entry = cache.entry(affinityKey); entry.setx(o, version); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testUserDefinedCompaction() throws Exception { Keyspace keyspace = Keyspace.open(KEYSPACE1); final String cfname = "Standard3"; ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(cfname); TableMetadata table = cfs.metadata(); cfs.disableAutoCompaction(); final int ROWS_PER_SSTABLE = 10; for (int i = 0; i < ROWS_PER_SSTABLE; i++) { DecoratedKey key = Util.dk(String.valueOf(i)); new RowUpdateBuilder(table, FBUtilities.timestampMicros(), key.getKey()) .clustering(ByteBufferUtil.bytes("cols")) .add("val", "val1") .build().applyUnsafe(); } cfs.forceBlockingFlush(); Collection<SSTableReader> sstables = cfs.getLiveSSTables(); assertEquals(1, sstables.size()); SSTableReader sstable = sstables.iterator().next(); XXX prevGeneration = sstable.descriptor.generation; String file = new File(sstable.descriptor.filenameFor(Component.DATA)).absolutePath(); CompactionManager.instance.forceUserDefinedCompaction(file); do { Thread.sleep(100); } while (CompactionManager.instance.getPendingTasks() > 0 || CompactionManager.instance.getActiveCompactions() > 0); sstables = cfs.getLiveSSTables(); assertEquals(1, sstables.size()); assertEquals( prevGeneration + 1, sstables.iterator().next().descriptor.generation); }', 'ground_truth': 'public void testUserDefinedCompaction() throws Exception { Keyspace keyspace = Keyspace.open(KEYSPACE1); final String cfname = "Standard3"; ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(cfname); TableMetadata table = cfs.metadata(); cfs.disableAutoCompaction(); final int ROWS_PER_SSTABLE = 10; for (int i = 0; i < ROWS_PER_SSTABLE; i++) { DecoratedKey key = Util.dk(String.valueOf(i)); new RowUpdateBuilder(table, FBUtilities.timestampMicros(), key.getKey()) .clustering(ByteBufferUtil.bytes("cols")) .add("val", "val1") .build().applyUnsafe(); } cfs.forceBlockingFlush(); Collection<SSTableReader> sstables = cfs.getLiveSSTables(); assertEquals(1, sstables.size()); SSTableReader sstable = sstables.iterator().next(); SSTableId prevGeneration = sstable.descriptor.id; String file = new File(sstable.descriptor.filenameFor(Component.DATA)).absolutePath(); CompactionManager.instance.forceUserDefinedCompaction(file); do { Thread.sleep(100); } while (CompactionManager.instance.getPendingTasks() > 0 || CompactionManager.instance.getActiveCompactions() > 0); sstables = cfs.getLiveSSTables(); assertEquals(1, sstables.size()); assertThat(SSTableIdFactory.COMPARATOR.compare(prevGeneration, sstables.iterator().next().descriptor.id)).isLessThan(0); }', 'output': 'public void testUserDefinedCompaction() throws Exception { Keyspace keyspace = Keyspace.open(KEYSPACE1); final String cfname = "Standard3"; ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(cfname); TableMetadata table = cfs.metadata(); cfs.disableAutoCompaction(); final int ROWS_PER_SSTABLE = 10; for (int i = 0; i < ROWS_PER_SSTABLE; i++) { DecoratedKey key = Util.dk(String.valueOf(i)); new RowUpdateBuilder(table, FBUtilities.timestampMicros(), key.getKey()) .clustering(ByteBufferUtil.bytes("cols")) .add("val", "val1") .build().applyUnsafe(); } cfs.forceBlockingFlush(); Collection<SSTableReader> sstables = cfs.getLiveSSTables(); assertEquals(1, sstables.size()); SSTableReader sstable = sstables.iterator().next(); long prevGeneration = sstable.descriptor.generation; String file = new File(sstable.descriptor.filenameFor(Component.DATA)).absolutePath(); CompactionManager.instance.forceUserDefinedCompaction(file); do { Thread.sleep(100); } while (CompactionManager.instance.getPendingTasks() > 0 || CompactionManager.instance.getActiveCompactions() > 0); sstables = cfs.getLiveSSTables(); assertEquals(1, sstables.size()); assertEquals( prevGeneration + 1, sstables.iterator().next().descriptor.generation); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void run() { try { resolver.resolve(); } catch (DigestMismatchException e) { assert resolver instanceof RowDigestResolver; if (traceState != null) traceState.trace("Digest mismatch: {}", e.toString()); if (logger.isDebugEnabled()) logger.debug("Digest mismatch:", e); ReadRepairMetrics.repairedBackground.mark(); ReadCommand readCommand = (ReadCommand) command; final XXX repairResolver = new RowDataResolver(readCommand.ksName, readCommand.key, readCommand.filter(), readCommand.timestamp, endpoints.size()); AsyncRepairCallback repairHandler = new AsyncRepairCallback(repairResolver, endpoints.size()); MessageOut<ReadCommand> message = ((ReadCommand) command).createMessage(); for (InetAddress endpoint : endpoints) MessagingService.instance().sendRR(message, endpoint, repairHandler); } }', 'ground_truth': 'public void run() { try { resolver.resolve(); } catch (DigestMismatchException e) { assert resolver instanceof DigestResolver; if (traceState != null) traceState.trace("Digest mismatch: {}", e.toString()); if (logger.isDebugEnabled()) logger.debug("Digest mismatch:", e); ReadRepairMetrics.repairedBackground.mark(); final DataResolver repairResolver = new DataResolver(keyspace, command, consistencyLevel, endpoints.size()); AsyncRepairCallback repairHandler = new AsyncRepairCallback(repairResolver, endpoints.size()); MessageOut<ReadCommand> message = command.createMessage(); for (InetAddress endpoint : endpoints) MessagingService.instance().sendRR(message, endpoint, repairHandler); } }', 'output': 'public void run() { try { resolver.resolve(); } catch (DigestMismatchException e) { assert resolver instanceof RowDigestResolver; if (traceState != null) traceState.trace("Digest mismatch: {}", e.toString()); if (logger.isDebugEnabled()) logger.debug("Digest mismatch:", e); ReadRepairMetrics.repairedBackground.mark(); ReadCommand readCommand = (ReadCommand) command; final RowDataResolver rowDataResolver = new RowDataResolver(readCommand.ksName, readCommand.key, readCommand.filter(), readCommand.timestamp, endpoints.size()); AsyncRepairCallback repairHandler = new AsyncRepairCallback(rowDataResolver, endpoints.size()); MessageOut<ReadCommand> message = ((ReadCommand) command).createMessage(); for (InetAddress endpoint : endpoints) MessagingService.instance().sendRR(message, endpoint, repairHandler); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void run1(AtomicInteger cnt) throws Exception { GridCompute comp = compute(prj).enableAsync(); comp.broadcast(runJob); XXX fut = comp.future(); waitForExecution(fut); cnt.set(0); compute(prj).broadcast(runJob); waitForValue(cnt, projectionSize()); }', 'ground_truth': 'private void run1(AtomicInteger cnt) throws Exception { GridCompute comp = compute(prj).enableAsync(); comp.broadcast(runJob); IgniteFuture fut = comp.future(); waitForExecution(fut); cnt.set(0); compute(prj).broadcast(runJob); waitForValue(cnt, projectionSize()); }', 'output': 'private void run1(AtomicInteger cnt) throws Exception { GridCompute comp = compute(prj).enableAsync(); comp.broadcast(runJob); IgniteFuture<?> fut = comp.future(); waitForExecution(fut); cnt.set(0); compute(prj).broadcast(runJob); waitForValue(cnt, projectionSize()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Project remove() { if (!removeFromProject) { return null; } boolean found = false; final List<ClassPathSupport.Item> resources = getClassPathItems(); for (XXX i = resources.iterator(); i.hasNext();) { ClassPathSupport.Item item = (ClassPathSupport.Item)i.next(); if (entryId.equals(CommonProjectUtils.getAntPropertyName(item.getReference()))) { lastRef.set(item.getReference()); i.remove(); ClassPathPackageAccessor.getInstance().removeUnusedReference(item, classPathId, helper, rh); found = true; } } if (found) { String[] itemRefs = cs.encodeToStrings(resources, webModuleElementName); final EditableProperties props = helper.getProperties (AntProjectHelper.PROJECT_PROPERTIES_PATH); props.setProperty (classPathId, itemRefs); helper.putProperties(AntProjectHelper.PROJECT_PROPERTIES_PATH, props); return FileOwnerQuery.getOwner(helper.getAntProjectHelper().getProjectDirectory()); } else { return null; } }', 'ground_truth': 'public Project remove() { if (!removeFromProject) { return null; } boolean found = false; final List<ClassPathSupport.Item> resources = getClassPathItems(); for (Iterator<ClassPathSupport.Item> i = resources.iterator(); i.hasNext();) { ClassPathSupport.Item item = i.next(); if (entryId.equals(CommonProjectUtils.getAntPropertyName(item.getReference()))) { lastRef.set(item.getReference()); i.remove(); ClassPathPackageAccessor.getInstance().removeUnusedReference(item, classPathId, helper, rh); found = true; } } if (found) { String[] itemRefs = cs.encodeToStrings(resources, webModuleElementName); final EditableProperties props = helper.getProperties (AntProjectHelper.PROJECT_PROPERTIES_PATH); props.setProperty (classPathId, itemRefs); helper.putProperties(AntProjectHelper.PROJECT_PROPERTIES_PATH, props); return FileOwnerQuery.getOwner(helper.getAntProjectHelper().getProjectDirectory()); } else { return null; } }', 'output': 'public Project remove() { if (!removeFromProject) { return null; } boolean found = false; final List<ClassPathSupport.Item> resources = getClassPathItems(); for (Iterator<ClassPathSupport.Item> i = resources.iterator(); i.hasNext();) { ClassPathSupport.Item item = (ClassPathSupport.Item)i.next(); if (entryId.equals(CommonProjectUtils.getAntPropertyName(item.getReference()))) { lastRef.set(item.getReference()); i.remove(); ClassPathPackageAccessor.getInstance().removeUnusedReference(item, classPathId, helper, rh); found = true; } } if (found) { String[] itemRefs = cs.encodeToStrings(resources, webModuleElementName); final EditableProperties props = helper.getProperties (AntProjectHelper.PROJECT_PROPERTIES_PATH); props.setProperty (classPathId, itemRefs); helper.putProperties(AntProjectHelper.PROJECT_PROPERTIES_PATH, props); return FileOwnerQuery.getOwner(helper.getAntProjectHelper().getProjectDirectory()); } else { return null; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean start() throws IOException { XXX spec = source.spec; ConnectionFactory connectionFactory = spec.getConnectionFactory(); try { Connection connection; if (spec.getUsername() != null) { connection = connectionFactory.createConnection(spec.getUsername(), spec.getPassword()); } else { connection = connectionFactory.createConnection(); } connection.start(); this.connection = connection; } catch (Exception e) { throw new IOException("Error connecting to JMS", e); } try { this.session = this.connection.createSession(false, Session.CLIENT_ACKNOWLEDGE); } catch (Exception e) { throw new IOException("Error creating JMS session", e); } try { if (spec.getTopic() != null) { this.consumer = this.session.createConsumer(this.session.createTopic(spec.getTopic())); } else { this.consumer = this.session.createConsumer(this.session.createQueue(spec.getQueue())); } } catch (Exception e) { throw new IOException("Error creating JMS consumer", e); } return advance(); }', 'ground_truth': 'public boolean start() throws IOException { Read<T> spec = source.spec; ConnectionFactory connectionFactory = spec.getConnectionFactory(); try { Connection connection; if (spec.getUsername() != null) { connection = connectionFactory.createConnection(spec.getUsername(), spec.getPassword()); } else { connection = connectionFactory.createConnection(); } connection.start(); this.connection = connection; } catch (Exception e) { throw new IOException("Error connecting to JMS", e); } try { this.session = this.connection.createSession(false, Session.CLIENT_ACKNOWLEDGE); } catch (Exception e) { throw new IOException("Error creating JMS session", e); } try { if (spec.getTopic() != null) { this.consumer = this.session.createConsumer(this.session.createTopic(spec.getTopic())); } else { this.consumer = this.session.createConsumer(this.session.createQueue(spec.getQueue())); } } catch (Exception e) { throw new IOException("Error creating JMS consumer", e); } return advance(); }', 'output': 'public boolean start() throws IOException { JmsSpec spec = source.spec; ConnectionFactory connectionFactory = spec.getConnectionFactory(); try { Connection connection; if (spec.getUsername() != null) { connection = connectionFactory.createConnection(spec.getUsername(), spec.getPassword()); } else { connection = connectionFactory.createConnection(); } connection.start(); this.connection = connection; } catch (Exception e) { throw new IOException("Error connecting to JMS", e); } try { this.session = this.connection.createSession(false, Session.CLIENT_ACKNOWLEDGE); } catch (Exception e) { throw new IOException("Error creating JMS session", e); } try { if (spec.getTopic() != null) { this.consumer = this.session.createConsumer(this.session.createTopic(spec.getTopic())); } else { this.consumer = this.session.createConsumer(this.session.createQueue(spec.getQueue())); } } catch (Exception e) { throw new IOException("Error creating JMS consumer", e); } return advance(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void realTimePopulate(final Ignite g) throws GridException { try (GridDataLoader<Integer, Long> ldr = g.dataLoader(null)) { ldr.perNodeParallelLoadOperations(1); ldr.updater(new IncrementingUpdater()); for (int i = 0; i < CNT; i++) ldr.addData(i % (CNT / 2), 1L); } }', 'ground_truth': 'private static void realTimePopulate(final Ignite g) throws GridException { try (IgniteDataLoader<Integer, Long> ldr = g.dataLoader(null)) { ldr.perNodeParallelLoadOperations(1); ldr.updater(new IncrementingUpdater()); for (int i = 0; i < CNT; i++) ldr.addData(i % (CNT / 2), 1L); } }', 'output': 'private static void realTimePopulate(final Ignite g) throws GridException { try (IgniteDataLoader<Integer, Long> ldr = g.dataLoader(null)) { ldr.perNodeParallelLoadOperations(1); ldr.updater(new IncrementingUpdater()); for (int i = 0; i < CNT; i++) ldr.addData(i % (CNT / 2), 1L); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected Expression dotExpression(AST node) { AST leftNode = node.getFirstChild(); if (leftNode != null) { AST identifierNode = leftNode.getNextSibling(); if (identifierNode != null) { Expression leftExpression = expression(leftNode); if (isType(SELECT_SLOT, identifierNode)) { String field = identifier(identifierNode.getFirstChild()); AttributeExpression attributeExpression = new AttributeExpression(leftExpression, field, node.getType() != DOT); if (node.getType() == SPREAD_DOT) { attributeExpression.setSpreadSafe(true); } configureAST(attributeExpression, node); return attributeExpression; } XXX property = identifier(identifierNode); PropertyExpression propertyExpression = new PropertyExpression(leftExpression, property, node.getType() != DOT); if (node.getType() == SPREAD_DOT) { propertyExpression.setSpreadSafe(true); } configureAST(propertyExpression, node); return propertyExpression; } } return methodCallExpression(node); }', 'ground_truth': 'protected Expression dotExpression(AST node) { AST leftNode = node.getFirstChild(); if (leftNode != null) { AST identifierNode = leftNode.getNextSibling(); if (identifierNode != null) { Expression leftExpression = expression(leftNode); if (isType(SELECT_SLOT, identifierNode)) { Expression field = expression(identifierNode.getFirstChild(),true); AttributeExpression attributeExpression = new AttributeExpression(leftExpression, field, node.getType() != DOT); if (node.getType() == SPREAD_DOT) { attributeExpression.setSpreadSafe(true); } configureAST(attributeExpression, node); return attributeExpression; } Expression property = expression(identifierNode,true); PropertyExpression propertyExpression = new PropertyExpression(leftExpression, property, node.getType() != DOT); if (node.getType() == SPREAD_DOT) { propertyExpression.setSpreadSafe(true); } configureAST(propertyExpression, node); return propertyExpression; } } return methodCallExpression(node); }', 'output': 'protected Expression dotExpression(AST node) { AST leftNode = node.getFirstChild(); if (leftNode != null) { AST identifierNode = leftNode.getNextSibling(); if (identifierNode != null) { Expression leftExpression = expression(leftNode); if (isType(SELECT_SLOT, identifierNode)) { String field = identifier(identifierNode.getFirstChild()); AttributeExpression attributeExpression = new AttributeExpression(leftExpression, field, node.getType() != DOT); if (node.getType() == SPREAD_DOT) { attributeExpression.setSpreadSafe(true); } configureAST(attributeExpression, node); return attributeExpression; } String property = identifier(identifierNode); PropertyExpression propertyExpression = new PropertyExpression(leftExpression, property, node.getType() != DOT); if (node.getType() == SPREAD_DOT) { propertyExpression.setSpreadSafe(true); } configureAST(propertyExpression, node); return propertyExpression; } } return methodCallExpression(node); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void _testNodeJoinWithP2P() throws Exception { final Collection<UUID> nodeIds = new HashSet<>(); final AtomicInteger cnt = new AtomicInteger(); final CountDownLatch latch = new CountDownLatch(GRID_CNT + 1); ClassLoader ldr = getExternalClassLoader(); IgnitePredicate<ClusterNode> prjPred = (IgnitePredicate<ClusterNode>)ldr.loadClass(PRJ_PRED_CLS_NAME).newInstance(); XXX filter = (IgnitePredicate<IgniteEvent>)ldr.loadClass(FILTER_CLS_NAME).newInstance(); UUID consumeId = events(grid(0).forPredicate(prjPred)).remoteListen(new P2<UUID, IgniteEvent>() { @Override public boolean apply(UUID nodeId, IgniteEvent evt) { info("Event from " + nodeId + " [" + evt.shortDisplay() + \']\'); assertEquals(EVT_JOB_STARTED, evt.type()); nodeIds.add(nodeId); cnt.incrementAndGet(); latch.countDown(); return true; } }, filter, EVT_JOB_STARTED); try { assertNotNull(consumeId); startGrid("anotherGrid"); grid(0).compute().broadcast(F.noop()); assert latch.await(2, SECONDS); assertEquals(GRID_CNT + 1, nodeIds.size()); assertEquals(GRID_CNT + 1, cnt.get()); } finally { stopGrid("anotherGrid1"); stopGrid("anotherGrid2"); grid(0).events().stopRemoteListen(consumeId); } }', 'ground_truth': 'public void _testNodeJoinWithP2P() throws Exception { final Collection<UUID> nodeIds = new HashSet<>(); final AtomicInteger cnt = new AtomicInteger(); final CountDownLatch latch = new CountDownLatch(GRID_CNT + 1); ClassLoader ldr = getExternalClassLoader(); IgnitePredicate<ClusterNode> prjPred = (IgnitePredicate<ClusterNode>)ldr.loadClass(PRJ_PRED_CLS_NAME).newInstance(); IgnitePredicate<Event> filter = (IgnitePredicate<Event>)ldr.loadClass(FILTER_CLS_NAME).newInstance(); UUID consumeId = events(grid(0).forPredicate(prjPred)).remoteListen(new P2<UUID, Event>() { @Override public boolean apply(UUID nodeId, Event evt) { info("Event from " + nodeId + " [" + evt.shortDisplay() + \']\'); assertEquals(EVT_JOB_STARTED, evt.type()); nodeIds.add(nodeId); cnt.incrementAndGet(); latch.countDown(); return true; } }, filter, EVT_JOB_STARTED); try { assertNotNull(consumeId); startGrid("anotherGrid"); grid(0).compute().broadcast(F.noop()); assert latch.await(2, SECONDS); assertEquals(GRID_CNT + 1, nodeIds.size()); assertEquals(GRID_CNT + 1, cnt.get()); } finally { stopGrid("anotherGrid1"); stopGrid("anotherGrid2"); grid(0).events().stopRemoteListen(consumeId); } }', 'output': 'public void _testNodeJoinWithP2P() throws Exception { final Collection<UUID> nodeIds = new HashSet<>(); final AtomicInteger cnt = new AtomicInteger(); final CountDownLatch latch = new CountDownLatch(GRID_CNT + 1); ClassLoader ldr = getExternalClassLoader(); IgnitePredicate<ClusterNode> prjPred = (IgnitePredicate<ClusterNode>)ldr.loadClass(PRJ_PRED_CLS_NAME).newInstance(); IgnitePredicate<IgniteEvent> filter = (IgnitePredicate<IgniteEvent>)ldr.loadClass(FILTER_CLS_NAME).newInstance(); UUID consumeId = events(grid(0).forPredicate(prjPred)).remoteListen(new P2<UUID, IgniteEvent>() { @Override public boolean apply(UUID nodeId, IgniteEvent evt) { info("Event from " + nodeId + " [" + evt.shortDisplay() + \']\'); assertEquals(EVT_JOB_STARTED, evt.type()); nodeIds.add(nodeId); cnt.incrementAndGet(); latch.countDown(); return true; } }, filter, EVT_JOB_STARTED); try { assertNotNull(consumeId); startGrid("anotherGrid"); grid(0).compute().broadcast(F.noop()); assert latch.await(2, SECONDS); assertEquals(GRID_CNT + 1, nodeIds.size()); assertEquals(GRID_CNT + 1, cnt.get()); } finally { stopGrid("anotherGrid1"); stopGrid("anotherGrid2"); grid(0).events().stopRemoteListen(consumeId); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkReaderRemove() throws Exception { Ignite ignite0 = grid(0); GridCache<Integer, Integer> cache0 = ignite0.cache(null); CacheAffinity<Integer> aff = cache0.affinity(); UUID id0 = ignite0.cluster().localNode().id(); Integer nearKey = key(ignite0, NOT_PRIMARY_AND_BACKUP); cache0.put(nearKey, 1); for (int i = 0; i < GRID_CNT; i++) { UUID[] expReaders = aff.isPrimary(grid(i).localNode(), nearKey) ? new UUID[]{id0} : new UUID[]{}; checkEntry(grid(i), nearKey, 1, i == 0, expReaders); } cache0.remove(nearKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), nearKey, null, i == 0); Ignite primaryNode = G.ignite((String) aff.mapKeyToNode(nearKey).attribute(ATTR_GRID_NAME)); delay(); GridCache<Integer, Integer> primaryCache = primaryNode.cache(null); primaryCache.put(nearKey, 2); checkEntry(primaryNode, nearKey, 2, false); }', 'ground_truth': 'private void checkReaderRemove() throws Exception { Ignite ignite0 = grid(0); IgniteCache<Integer, Integer> cache0 = ignite0.jcache(null); CacheAffinity<Integer> aff = ignite0.affinity(null); UUID id0 = ignite0.cluster().localNode().id(); Integer nearKey = key(ignite0, NOT_PRIMARY_AND_BACKUP); cache0.put(nearKey, 1); for (int i = 0; i < GRID_CNT; i++) { UUID[] expReaders = aff.isPrimary(grid(i).localNode(), nearKey) ? new UUID[]{id0} : new UUID[]{}; checkEntry(grid(i), nearKey, 1, i == 0, expReaders); } cache0.remove(nearKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), nearKey, null, i == 0); Ignite primaryNode = G.ignite((String) aff.mapKeyToNode(nearKey).attribute(ATTR_GRID_NAME)); delay(); IgniteCache<Integer, Integer> primaryCache = primaryNode.jcache(null); primaryCache.put(nearKey, 2); checkEntry(primaryNode, nearKey, 2, false); }', 'output': 'private void checkReaderRemove() throws Exception { Ignite ignite0 = grid(0); GridCache<Integer, Integer> cache0 = ignite0.cache(null); CacheAffinity<Integer> aff = cache0.affinity(); UUID id0 = ignite0.cluster().localNode().id(); Integer nearKey = key(ignite0, NOT_PRIMARY_AND_BACKUP); cache0.put(nearKey, 1); for (int i = 0; i < GRID_CNT; i++) { UUID[] expReaders = aff.isPrimary(grid(i).localNode(), nearKey) ? new UUID[]{id0} : new UUID[]{}; checkEntry(grid(i), nearKey, 1, i == 0, expReaders); } cache0.remove(nearKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), nearKey, null, i == 0); Ignite primaryNode = G.ignite((String) aff.mapKeyToNode(nearKey).attribute(ATTR_GRID_NAME)); delay(); GridCache<Integer, Integer> primaryCache = primaryNode.cache(null); primaryCache.put(nearKey, 2); checkEntry(primaryNode, nearKey, 2, false); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkDuplicateNameClashes(ListExpression list) { final XXX seen = new HashSet<String>(); @SuppressWarnings("unchecked") final List<ClassExpression> classes = (List)list.getExpressions(); for (ClassExpression ce : classes) { final String name = ce.getType().getNameWithoutPackage(); if (seen.contains(name)) { throw new RuntimeException("Duplicate name \'" + name + "\' found during @" + MY_NAME + " processing."); } seen.add(name); } }', 'ground_truth': 'private void checkDuplicateNameClashes(ListExpression list) { final Set<String> seen = new HashSet<String>(); @SuppressWarnings("unchecked") final List<ClassExpression> classes = (List)list.getExpressions(); for (ClassExpression ce : classes) { final String name = ce.getType().getNameWithoutPackage(); if (seen.contains(name)) { throw new RuntimeException("Duplicate name \'" + name + "\' found during @" + MY_NAME + " processing."); } seen.add(name); } }', 'output': 'private void checkDuplicateNameClashes(ListExpression list) { final Set<String> seen = new HashSet<>(); @SuppressWarnings("unchecked") final List<ClassExpression> classes = (List)list.getExpressions(); for (ClassExpression ce : classes) { final String name = ce.getType().getNameWithoutPackage(); if (seen.contains(name)) { throw new RuntimeException("Duplicate name \'" + name + "\' found during @" + MY_NAME + " processing."); } seen.add(name); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public HostRepositories retrieveHostRepositories(Cluster cluster, Host host) throws AmbariException { List<ServiceComponentHost> hostComponents = cluster.getServiceComponentHosts(host.getHostName()); Map<Long, CommandRepository> hostRepositories = new HashMap<>(); Map<String, Long> componentsRepos = new HashMap<>(); for (ServiceComponentHost serviceComponentHost : hostComponents) { CommandRepository commandRepository; try { commandRepository = repoVersionHelper.getCommandRepository(cluster, serviceComponentHost.getServiceComponent(), host); } catch (SystemException e) { throw new RuntimeException(e); } hostRepositories.put(commandRepository.getRepoVersionId(), commandRepository); componentsRepos.put(serviceComponentHost.getServiceComponentName(), commandRepository.getRepoVersionId()); } return new HostRepositories(hostRepositories, componentsRepos); }', 'ground_truth': 'public HostRepositories retrieveHostRepositories(Cluster cluster, Host host) throws AmbariException { List<ServiceComponentHost> hostComponents = cluster.getServiceComponentHosts(host.getHostName()); SortedMap<Long, CommandRepository> hostRepositories = new TreeMap<>(); SortedMap<String, Long> componentsRepos = new TreeMap<>(); for (ServiceComponentHost serviceComponentHost : hostComponents) { CommandRepository commandRepository; try { commandRepository = repoVersionHelper.getCommandRepository(cluster, serviceComponentHost.getServiceComponent(), host); } catch (SystemException e) { throw new RuntimeException(e); } hostRepositories.put(commandRepository.getRepoVersionId(), commandRepository); componentsRepos.put(serviceComponentHost.getServiceComponentName(), commandRepository.getRepoVersionId()); } return new HostRepositories(hostRepositories, componentsRepos); }', 'output': 'public HostRepositories retrieveHostRepositories(Cluster cluster, Host host) throws AmbariException { List<ServiceComponentHost> hostComponents = cluster.getServiceComponentHosts(host.getHostName()); Map<Long, CommandRepository> hostRepositories = new HashMap<>(); Map<String, Long> componentsRepos = new HashMap<>(); for (ServiceComponentHost serviceComponentHost : hostComponents) { CommandRepository commandRepository; try { commandRepository = repoVersionHelper.getCommandRepository(cluster, serviceComponentHost.getServiceComponent(), host); } catch (SystemException e) { throw new RuntimeException(e); } hostRepositories.put(commandRepository.getRepoVersionId(), commandRepository); componentsRepos.put(serviceComponentHost.getServiceComponentName(), commandRepository.getRepoVersionId()); } return new HostRepositories(hostRepositories, componentsRepos); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void storeThisInstanceField(FieldExpression expression) { FieldNode field = expression.getField(); boolean holder = field.isHolder() && !isInClosureConstructor(); XXX type = field.getType(); String ownerName =  (field.getOwner().equals(classNode.getType())) ? internalClassName : org.objectweb.asm.Type.getInternalName(loadClass(field.getOwner())); if (holder) { Variable tv = visitASTOREInTemp(field.getName()); int tempIndex = tv.getIndex(); cv.visitVarInsn(ALOAD, 0); cv.visitFieldInsn(GETFIELD, ownerName, expression.getFieldName(), BytecodeHelper.getTypeDescription(type)); cv.visitVarInsn(ALOAD, tempIndex); cv.visitMethodInsn(INVOKEVIRTUAL, "groovy/lang/Reference", "set", "(Ljava/lang/Object;)V"); removeVar(tv); } else { if (isInClosureConstructor()) { helper.doCast(type); } else { doConvertAndCast(type); } Variable tmpVar = defineVariable(createVariableName(field.getName()), field.getType(), false); helper.store(tmpVar, MARK_START); helper.loadThis(); helper.load(tmpVar); helper.putField(field, ownerName); removeVar(tmpVar); } }', 'ground_truth': 'public void storeThisInstanceField(FieldExpression expression) { FieldNode field = expression.getField(); boolean holder = field.isHolder() && !isInClosureConstructor(); ClassNode type = field.getType(); String ownerName =  (field.getOwner().equals(classNode)) ? internalClassName : BytecodeHelper.getClassInternalName(field.getOwner()); if (holder) { Variable tv = visitASTOREInTemp(field.getName()); int tempIndex = tv.getIndex(); cv.visitVarInsn(ALOAD, 0); cv.visitFieldInsn(GETFIELD, ownerName, expression.getFieldName(), BytecodeHelper.getTypeDescription(type)); cv.visitVarInsn(ALOAD, tempIndex); cv.visitMethodInsn(INVOKEVIRTUAL, "groovy/lang/Reference", "set", "(Ljava/lang/Object;)V"); removeVar(tv); } else { if (isInClosureConstructor()) { helper.doCast(type); } else { doConvertAndCast(type); } Variable tmpVar = defineVariable(createVariableName(field.getName()), field.getType(), false); helper.store(tmpVar, MARK_START); helper.loadThis(); helper.load(tmpVar); helper.putField(field, ownerName); removeVar(tmpVar); } }', 'output': 'public void storeThisInstanceField(FieldExpression expression) { FieldNode field = expression.getField(); boolean holder = field.isHolder() && !isInClosureConstructor(); Class<?> type = field.getType(); String ownerName =  (field.getOwner().equals(classNode.getType())) ? internalClassName : org.objectweb.asm.Type.getInternalName(loadClass(field.getOwner())); if (holder) { Variable tv = visitASTOREInTemp(field.getName()); int tempIndex = tv.getIndex(); cv.visitVarInsn(ALOAD, 0); cv.visitFieldInsn(GETFIELD, ownerName, expression.getFieldName(), BytecodeHelper.getTypeDescription(type)); cv.visitVarInsn(ALOAD, tempIndex); cv.visitMethodInsn(INVOKEVIRTUAL, "groovy/lang/Reference", "set", "(Ljava/lang/Object;)V"); removeVar(tv); } else { if (isInClosureConstructor()) { helper.doCast(type); } else { doConvertAndCast(type); } Variable tmpVar = defineVariable(createVariableName(field.getName()), field.getType(), false); helper.store(tmpVar, MARK_START); helper.loadThis(); helper.load(tmpVar); helper.putField(field, ownerName); removeVar(tmpVar); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void runBenchmark(IndexConfiguration idxCfg) throws Exception { int thrCnt = getIntProperty(THREADS_CNT, 1); int dur = getIntProperty(TEST_DUR_SEC, 60); int winSize = getIntProperty("GG_WIN_SIZE", 5000); dumpProperties(System.out); final GridStreamerBoundedSizeWindow<Integer> win = new GridStreamerBoundedSizeWindow<>(); win.setMaximumSize(winSize); win.setIndexes(idxCfg.indexProvider()); win.start(); final AtomicLong enqueueCntr = new AtomicLong(); GridFuture<Long> enqueueFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { Random rnd = new Random(); while (!Thread.currentThread().isInterrupted()) { win.enqueue(rnd.nextInt()); enqueueCntr.incrementAndGet(); } } }, thrCnt, "generator"); final AtomicLong evictCntr = new AtomicLong(); GridFuture<Long> evictFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { while (!Thread.currentThread().isInterrupted()) { win.pollEvicted(); evictCntr.incrementAndGet(); } } }, thrCnt, "evictor"); XXX collFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() { int nSec = 0; long prevEnqueue = enqueueCntr.get(); long prevEvict = evictCntr.get(); try { while (!Thread.currentThread().isInterrupted()) { U.sleep(1000); nSec++; long curEnqueue = enqueueCntr.get(); long curEvict = evictCntr.get(); X.println("Stats [enqueuePerSec=" + (curEnqueue - prevEnqueue) + ", evictPerSec=" + (curEvict - prevEvict) + \']\'); prevEnqueue = curEnqueue; prevEvict = curEvict; } } catch (GridInterruptedException ignored) { } X.println("Final results [enqueuePerSec=" + (enqueueCntr.get() / nSec) + ", evictPerSec=" + (evictCntr.get() / nSec) + \']\'); } }, 1, "collector"); U.sleep(dur * 1000); X.println("Finishing test."); collFut.cancel(); enqueueFut.cancel(); evictFut.cancel(); }', 'ground_truth': 'public static void runBenchmark(IndexConfiguration idxCfg) throws Exception { int thrCnt = getIntProperty(THREADS_CNT, 1); int dur = getIntProperty(TEST_DUR_SEC, 60); int winSize = getIntProperty("GG_WIN_SIZE", 5000); dumpProperties(System.out); final GridStreamerBoundedSizeWindow<Integer> win = new GridStreamerBoundedSizeWindow<>(); win.setMaximumSize(winSize); win.setIndexes(idxCfg.indexProvider()); win.start(); final AtomicLong enqueueCntr = new AtomicLong(); IgniteFuture<Long> enqueueFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { Random rnd = new Random(); while (!Thread.currentThread().isInterrupted()) { win.enqueue(rnd.nextInt()); enqueueCntr.incrementAndGet(); } } }, thrCnt, "generator"); final AtomicLong evictCntr = new AtomicLong(); IgniteFuture<Long> evictFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { while (!Thread.currentThread().isInterrupted()) { win.pollEvicted(); evictCntr.incrementAndGet(); } } }, thrCnt, "evictor"); IgniteFuture<Long> collFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() { int nSec = 0; long prevEnqueue = enqueueCntr.get(); long prevEvict = evictCntr.get(); try { while (!Thread.currentThread().isInterrupted()) { U.sleep(1000); nSec++; long curEnqueue = enqueueCntr.get(); long curEvict = evictCntr.get(); X.println("Stats [enqueuePerSec=" + (curEnqueue - prevEnqueue) + ", evictPerSec=" + (curEvict - prevEvict) + \']\'); prevEnqueue = curEnqueue; prevEvict = curEvict; } } catch (GridInterruptedException ignored) { } X.println("Final results [enqueuePerSec=" + (enqueueCntr.get() / nSec) + ", evictPerSec=" + (evictCntr.get() / nSec) + \']\'); } }, 1, "collector"); U.sleep(dur * 1000); X.println("Finishing test."); collFut.cancel(); enqueueFut.cancel(); evictFut.cancel(); }', 'output': 'public static void runBenchmark(IndexConfiguration idxCfg) throws Exception { int thrCnt = getIntProperty(THREADS_CNT, 1); int dur = getIntProperty(TEST_DUR_SEC, 60); int winSize = getIntProperty("GG_WIN_SIZE", 5000); dumpProperties(System.out); final GridStreamerBoundedSizeWindow<Integer> win = new GridStreamerBoundedSizeWindow<>(); win.setMaximumSize(winSize); win.setIndexes(idxCfg.indexProvider()); win.start(); final AtomicLong enqueueCntr = new AtomicLong(); GridFuture<Long> enqueueFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { Random rnd = new Random(); while (!Thread.currentThread().isInterrupted()) { win.enqueue(rnd.nextInt()); enqueueCntr.incrementAndGet(); } } }, thrCnt, "generator"); final AtomicLong evictCntr = new AtomicLong(); GridFuture<Long> evictFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { while (!Thread.currentThread().isInterrupted()) { win.pollEvicted(); evictCntr.incrementAndGet(); } } }, thrCnt, "evictor"); IgniteInternalFuture<?> collFut = runMultiThreadedAsync(new CAX() { @Override public void applyx() { int nSec = 0; long prevEnqueue = enqueueCntr.get(); long prevEvict = evictCntr.get(); try { while (!Thread.currentThread().isInterrupted()) { U.sleep(1000); nSec++; long curEnqueue = enqueueCntr.get(); long curEvict = evictCntr.get(); X.println("Stats [enqueuePerSec=" + (curEnqueue - prevEnqueue) + ", evictPerSec=" + (curEvict - prevEvict) + \']\'); prevEnqueue = curEnqueue; prevEvict = curEvict; } } catch (GridInterruptedException ignored) { } X.println("Final results'}
{'instruction': 'Please refactor the following code:', 'input': 'private ClientListenerResponse executeBatch(JdbcBatchExecuteRequest req) { String schemaName = req.schemaName(); if (F.isEmpty(schemaName)) schemaName = QueryUtils.DFLT_SCHEMA; int successQueries = 0; int updCnts[] = new int[req.queries().size()]; try { String sql = null; for (JdbcQuery q : req.queries()) { if (q.sql() != null) sql = q.sql(); XXX qry = new SqlFieldsQueryEx(sql, false); qry.setArgs(q.args()); qry.setDistributedJoins(distributedJoins); qry.setEnforceJoinOrder(enforceJoinOrder); qry.setCollocated(collocated); qry.setReplicatedOnly(replicatedOnly); qry.setLazy(lazy); qry.setSchema(schemaName); QueryCursorImpl<List<?>> qryCur = (QueryCursorImpl<List<?>>)ctx.query() .querySqlFieldsNoCache(qry, true, true).get(0); assert !qryCur.isQuery(); List<List<?>> items = qryCur.getAll(); updCnts[successQueries++] = ((Long)items.get(0).get(0)).intValue(); } return new JdbcResponse(new JdbcBatchExecuteResult(updCnts, ClientListenerResponse.STATUS_SUCCESS, null)); } catch (Exception e) { U.error(log, "Failed to execute batch query [reqId=" + req.requestId() + ", req=" + req + \']\', e); int code; String msg; if (e instanceof IgniteSQLException) { code = ((IgniteSQLException) e).statusCode(); msg = e.getMessage(); } else { code = IgniteQueryErrorCode.UNKNOWN; msg = e.getMessage(); } return new JdbcResponse(new JdbcBatchExecuteResult(Arrays.copyOf(updCnts, successQueries), code, msg)); } }', 'ground_truth': 'private ClientListenerResponse executeBatch(JdbcBatchExecuteRequest req) { String schemaName = req.schemaName(); if (F.isEmpty(schemaName)) schemaName = QueryUtils.DFLT_SCHEMA; int qryCnt = req.queries().size(); List<Integer> updCntsAcc = new ArrayList<>(qryCnt); IgniteBiTuple<Integer, String> firstErr = new IgniteBiTuple<>(); SqlFieldsQueryEx qry = null; for (JdbcQuery q : req.queries()) { if (q.sql() != null) { if (qry != null) executeBatchedQuery(qry, updCntsAcc, firstErr); qry = new SqlFieldsQueryEx(q.sql(), false); qry.setDistributedJoins(distributedJoins); qry.setEnforceJoinOrder(enforceJoinOrder); qry.setCollocated(collocated); qry.setReplicatedOnly(replicatedOnly); qry.setLazy(lazy); qry.setSchema(schemaName); } assert qry != null; qry.addBatchedArgs(q.args()); } if (qry != null) executeBatchedQuery(qry, updCntsAcc, firstErr); int updCnts[] = U.toIntArray(updCntsAcc); if (firstErr.isEmpty()) return new JdbcResponse(new JdbcBatchExecuteResult(updCnts, ClientListenerResponse.STATUS_SUCCESS, null)); else return new JdbcResponse(new JdbcBatchExecuteResult(updCnts, firstErr.getKey(), firstErr.getValue())); }', 'output': 'private ClientListenerResponse executeBatch(JdbcBatchExecuteRequest req) { String schemaName = req.schemaName(); if (F.isEmpty(schemaName)) schemaName = QueryUtils.DFLT_SCHEMA; int successQueries = 0; int updCnts[] = new int[req.queries().size()]; try { String sql = null; for (JdbcQuery q : req.queries()) { if (q.sql() != null) sql = q.sql(); SqlFieldsQueryEx qry = new SqlFieldsQueryEx(sql, false); qry.setArgs(q.args()); qry.setDistributedJoins(distributedJoins); qry.setEnforceJoinOrder(enforceJoinOrder); qry.setCollocated(collocated); qry.setReplicatedOnly(replicatedOnly); qry.setLazy(lazy); qry.setSchema(schemaName); QueryCursorImpl<List<?>> qryCur = (QueryCursorImpl<List<?>>)ctx.query() .querySqlFieldsNoCache(qry, true, true).get(0); assert !qryCur.isQuery(); List<List<?>> items = qryCur.getAll(); updCnts[successQueries++] = ((Long)items.get(0).get(0)).intValue(); } return new JdbcResponse(new JdbcBatchExecuteResult(updCnts, ClientListenerResponse.STATUS_SUCCESS, null)); } catch (Exception e) { U.error(log, "Failed to execute batch query [reqId=" + req.requestId() + ", req=" + req + \']\', e); int code; String msg; if (e instanceof IgniteSQLException) { code = ((IgniteSQLException) e).statusCode(); msg = e.getMessage(); } else { code = IgniteQueryErrorCode.UNKNOWN; msg = e.getMessage(); } return new JdbcResponse(new JdbcBatchExecuteResult(Arrays.copyOf(updCnts, successQueries), code, msg)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkUniqueHashIndexMultithreaded(int threadCnt, final int iters) throws Exception { GridStreamerIndexProvider<String, String, Integer> idxProvider = indexProvider(false, "idx", new IndexUpdater(), EVENT_TRACKING_ON_DEDUP, true); for (int i = 0; i < iters && !Thread.currentThread().isInterrupted(); i++) { final StreamerBoundedSizeWindow<String> win = new StreamerBoundedSizeWindow<>(); win.setMaximumSize(threadCnt * 2); win.setIndexes(idxProvider); win.start(); final String evt = "evt" + i; final AtomicInteger nIdxErrors = new AtomicInteger(); runMultiThreaded(new CAX() { @Override public void applyx() throws GridException { try { win.enqueue(evt); } catch (GridException e) { if (e.getMessage().contains("Index unique key violation")) nIdxErrors.incrementAndGet(); else throw e; } } }, threadCnt, "put"); assertEquals(threadCnt - 1, nIdxErrors.get()); GridStreamerIndex<String, String, Integer> idx = win.index("idx"); assertEquals(1, idx.entries(0).size()); assertEquals((Integer)1, idx.entry(evt).value()); } }', 'ground_truth': 'private void checkUniqueHashIndexMultithreaded(int threadCnt, final int iters) throws Exception { StreamerIndexProvider<String, String, Integer> idxProvider = indexProvider(false, "idx", new IndexUpdater(), EVENT_TRACKING_ON_DEDUP, true); for (int i = 0; i < iters && !Thread.currentThread().isInterrupted(); i++) { final StreamerBoundedSizeWindow<String> win = new StreamerBoundedSizeWindow<>(); win.setMaximumSize(threadCnt * 2); win.setIndexes(idxProvider); win.start(); final String evt = "evt" + i; final AtomicInteger nIdxErrors = new AtomicInteger(); runMultiThreaded(new CAX() { @Override public void applyx() throws GridException { try { win.enqueue(evt); } catch (GridException e) { if (e.getMessage().contains("Index unique key violation")) nIdxErrors.incrementAndGet(); else throw e; } } }, threadCnt, "put"); assertEquals(threadCnt - 1, nIdxErrors.get()); StreamerIndex<String, String, Integer> idx = win.index("idx"); assertEquals(1, idx.entries(0).size()); assertEquals((Integer)1, idx.entry(evt).value()); } }', 'output': 'private void checkUniqueHashIndexMultithreaded(int threadCnt, final int iters) throws Exception { GridStreamerIndexProvider<String, String, Integer> idxProvider = indexProvider(false, "idx", new IndexUpdater(), EVENT_TRACKING_ON_DEDUP, true); for (int i = 0; i < iters && !Thread.currentThread().isInterrupted(); i++) { final StreamerBoundedSizeWindow<String> win = new StreamerBoundedSizeWindow<>(); win.setMaximumSize(threadCnt * 2); win.setIndexes(idxProvider); win.start(); final String evt = "evt" + i; final AtomicInteger nIdxErrors = new AtomicInteger(); runMultiThreaded(new CAX() { @Override public void applyx() throws GridException { try { win.enqueue(evt); } catch (GridException e) { if (e.getMessage().contains("Index unique key violation")) nIdxErrors.incrementAndGet(); else throw e; } } }, threadCnt, "put"); assertEquals(threadCnt - 1, nIdxErrors.get()); GridStreamerIndex<String, String, Integer> idx = win.index("idx"); assertEquals(1, idx.entries(0).size()); assertEquals((Integer)1, idx.entry(evt).value()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static CodeTemplateSpiPackageAccessor get() { if (INSTANCE == null) { try { XXX clazz = Class.forName(CodeTemplateInsertRequest.class.getName()); } catch (ClassNotFoundException e) { } } assert INSTANCE != null : "There is no SPI package accessor available!"; return INSTANCE; } public abstract CodeTemplateParameterImpl getImpl(CodeTemplateParameter parameter);\n\n} } }', 'ground_truth': 'public static CodeTemplateSpiPackageAccessor get() { if (INSTANCE == null) { try { Class<?> clazz = Class.forName(CodeTemplateInsertRequest.class.getName()); } catch (ClassNotFoundException e) { } } assert INSTANCE != null : "There is no SPI package accessor available!"; return INSTANCE; }', 'output': 'public static CodeTemplateSpiPackageAccessor get() { if (INSTANCE == null) { try { Class<?> clazz = Class.forName(CodeTemplateInsertRequest.class.getName()); } catch (ClassNotFoundException e) { } } assert INSTANCE != null : "There is no SPI package accessor available!"; return INSTANCE; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public JPopupMenu getPopupMenu() { JPopupMenu popup = super.getPopupMenu(); JMenuItem mi; if (!initialized) { popup.removeAll(); boolean isSynthLAF = UIManager.getLookAndFeel() instanceof SynthLookAndFeel; String lafName = UIManager.getLookAndFeel().getClass().getName(); UIManager.LookAndFeelInfo[] lafs = UIManager.getInstalledLookAndFeels(); for (int i=0; i<lafs.length; i++) { String className = lafs[i].getClassName(); if (isSynthLAF) { try { Class lafClass = Class.forName(className); if (!lafName.equals(className) && SynthLookAndFeel.class.isAssignableFrom(lafClass)) { continue; } } catch (ClassNotFoundException cnfex) { ErrorManager.getDefault().notify(ErrorManager.INFORMATIONAL, cnfex); } } mi = new JMenuItem(lafs[i].getName()); mi.putClientProperty("lafInfo", new LookAndFeelItem(lafs[i].getClassName())); mi.addActionListener(this); popup.add(mi); } Node[] cats = PaletteUtils.getCategoryNodes(PaletteUtils.getPaletteNode(), false); for (int i=0; i<cats.length; i++) { if ("LookAndFeels".equals(cats[i].getName())) { final Node lafNode = cats[i]; Node[] items = PaletteUtils.getItemNodes(lafNode, true); if (items.length != 0) { popup.add(new JSeparator()); } for (int j=0; j<items.length; j++) { PaletteItem pitem = items[j].getLookup().lookup(PaletteItem.class); boolean supported = false; try { XXX clazz = pitem.getComponentClass(); if ((clazz != null) && (LookAndFeel.class.isAssignableFrom(clazz))) { LookAndFeel laf = (LookAndFeel)clazz.newInstance(); supported = laf.isSupportedLookAndFeel(); if (supported && isSynthLAF && !lafName.equals(pitem.getComponentClassName()) && SynthLookAndFeel.class.isAssignableFrom(clazz)) { supported = false; } } } catch (Exception ex) { ErrorManager.getDefault().notify(ErrorManager.INFORMATIONAL, ex); } catch (LinkageError ex) { ErrorManager.getDefault().notify(ErrorManager.INFORMATIONAL, ex); } if (supported) { mi = new JMenuItem(items[j].getDisplayName()); mi.putClientProperty("lafInfo", new LookAndFeelItem(pitem)); mi.addActionListener(this); popup.add(mi); } } } } initialized = true; } return popup; }', 'ground_truth': 'public JPopupMenu getPopupMenu() { JPopupMenu popup = super.getPopupMenu(); JMenuItem mi; if (!initialized) { popup.removeAll(); boolean isSynthLAF = UIManager.getLookAndFeel() instanceof SynthLookAndFeel; String lafName = UIManager.getLookAndFeel().getClass().getName(); UIManager.LookAndFeelInfo[] lafs = UIManager.getInstalledLookAndFeels(); for (int i=0; i<lafs.length; i++) { String className = lafs[i].getClassName(); if (isSynthLAF) { try { Class lafClass = Class.forName(className); if (!lafName.equals(className) && SynthLookAndFeel.class.isAssignableFrom(lafClass)) { continue; } } catch (ClassNotFoundException cnfex) { ErrorManager.getDefault().notify(ErrorManager.INFORMATIONAL, cnfex); } } mi = new JMenuItem(lafs[i].getName()); mi.putClientProperty("lafInfo", new LookAndFeelItem(lafs[i].getClassName())); mi.addActionListener(this); popup.add(mi); } Node[] cats = PaletteUtils.getCategoryNodes(PaletteUtils.getPaletteNode(), false); for (int i=0; i<cats.length; i++) { if ("LookAndFeels".equals(cats[i].getName())) { final Node lafNode = cats[i]; Node[] items = PaletteUtils.getItemNodes(lafNode, true); if (items.length != 0) { popup.add(new JSeparator()); } for (int j=0; j<items.length; j++) { PaletteItem pitem = items[j].getLookup().lookup(PaletteItem.class); boolean supported = false; try { Class<?> clazz = pitem.getComponentClass(); if ((clazz != null) && (LookAndFeel.class.isAssignableFrom(clazz))) { LookAndFeel laf = (LookAndFeel)clazz.newInstance(); supported = laf.isSupportedLookAndFeel(); if (supported && isSynthLAF && !lafName.equals(pitem.getComponentClassName()) && SynthLookAndFeel.class.isAssignableFrom(clazz)) { supported = false; } } } catch (Exception ex) { ErrorManager.getDefault().notify(ErrorManager.INFORMATIONAL, ex); } catch (LinkageError ex) { ErrorManager.getDefault().notify(ErrorManager.INFORMATIONAL, ex); } if (supported) { mi = new JMenuItem(items[j].getDisplayName()); mi.putClientProperty("lafInfo", new LookAndFeelItem(pitem)); mi.addActionListener(this); popup.add(mi); } } } } initialized = true; } return popup; }', 'output': 'public JPopupMenu getPopupMenu() { JPopupMenu popup = super.getPopupMenu(); JMenuItem mi; if (!initialized) { popup.removeAll(); boolean isSynthLAF = UIManager.getLookAndFeel() instanceof SynthLookAndFeel; String lafName = UIManager.getLookAndFeel().getClass().getName(); UIManager.LookAndFeelInfo[] lafs = UIManager.getInstalledLookAndFeels(); for (int i=0; i<lafs.length; i++) { String className = lafs[i].getClassName(); if (isSynthLAF) { try { Class lafClass = Class.forName(className); if (!lafName.equals(className) && SynthLookAndFeel.class.isAssignableFrom(lafClass)) { continue; } } catch (ClassNotFoundException cnfex) { ErrorManager.getDefault().notify(ErrorManager.INFORMATIONAL, cnfex); } } mi = new JMenuItem(lafs[i].getName()); mi.putClientProperty("lafInfo", new LookAndFeelItem(lafs[i].getClassName())); mi.addActionListener(this); popup.add(mi); } Node[] cats = PaletteUtils.getCategoryNodes(PaletteUtils.getPaletteNode(), false); for (int i=0; i<cats.length; i++) { if ("LookAndFeels".equals(cats[i].getName())) { final Node lafNode = cats[i]; Node[] items = PaletteUtils.getItemNodes(lafNode, true); if (items.length != 0) { popup.add(new JSeparator()); } for (int j=0; j<items.length; j++) { PaletteItem pitem = items[j].getLookup().lookup(PaletteItem.class); boolean supported = false; try { XXX clazz = pitem.getComponentClass(); if ((clazz != null) && (LookAndFeel.class.isAssignableFrom(clazz))) { LookAndFeel laf = (LookAndFeel)clazz.newInstance(); supported = laf.isSupportedLookAndFeel(); if (supported && isSynthLAF && !laf'}
{'instruction': 'Please refactor the following code:', 'input': 'private void processFilterTest(IgniteDeploymentMode depMode) throws Exception { this.depMode = depMode; try { Ignite ignite = startGrid(1); startGrid(2); ignite.compute().execute(GridP2PTestTask.class, 777); String path = GridTestProperties.getProperty("p2p.uri.cls"); GridTestExternalClassLoader ldr = new GridTestExternalClassLoader(new URL[] {new URL(path)}); Class filter1 = ldr.loadClass("org.apache.ignite.tests.p2p.GridP2PEventFilterExternalPath1"); Class filter2 = ldr.loadClass("org.apache.ignite.tests.p2p.GridP2PEventFilterExternalPath2"); ldr.setTimeout(100); ignite.events().remoteQuery((IgnitePredicate<IgniteEvent>) filter1.newInstance(), 0); ldr.setTimeout(2000); try { ignite.events().remoteQuery((IgnitePredicate<IgniteEvent>) filter2.newInstance(), 0); assert false; } catch (XXX ignored) { } } finally { stopAllGrids(); } }', 'ground_truth': 'private void processFilterTest(IgniteDeploymentMode depMode) throws Exception { this.depMode = depMode; try { Ignite ignite = startGrid(1); startGrid(2); ignite.compute().execute(GridP2PTestTask.class, 777); String path = GridTestProperties.getProperty("p2p.uri.cls"); GridTestExternalClassLoader ldr = new GridTestExternalClassLoader(new URL[] {new URL(path)}); Class filter1 = ldr.loadClass("org.apache.ignite.tests.p2p.GridP2PEventFilterExternalPath1"); Class filter2 = ldr.loadClass("org.apache.ignite.tests.p2p.GridP2PEventFilterExternalPath2"); ldr.setTimeout(100); ignite.events().remoteQuery((IgnitePredicate<IgniteEvent>) filter1.newInstance(), 0); ldr.setTimeout(2000); try { ignite.events().remoteQuery((IgnitePredicate<IgniteEvent>) filter2.newInstance(), 0); assert false; } catch (IgniteException ignored) { } } finally { stopAllGrids(); } }', 'output': 'private void processFilterTest(IgniteDeploymentMode depMode) throws Exception { this.depMode = depMode; try { Ignite ignite = startGrid(1); startGrid(2); ignite.compute().execute(GridP2PTestTask.class, 777); String path = GridTestProperties.getProperty("p2p.uri.cls"); GridTestExternalClassLoader ldr = new GridTestExternalClassLoader(new URL[] {new URL(path)}); Class filter1 = ldr.loadClass("org.apache.ignite.tests.p2p.GridP2PEventFilterExternalPath1"); Class filter2 = ldr.loadClass("org.apache.ignite.tests.p2p.GridP2PEventFilterExternalPath2"); ldr.setTimeout(100); ignite.events().remoteQuery((IgnitePredicate<IgniteEvent>) filter1.newInstance(), 0); ldr.setTimeout(2000); try { ignite.events().remoteQuery((IgnitePredicate<IgniteEvent>) filter2.newInstance(), 0); assert false; } catch (Exception ignored) { } } finally { stopAllGrids(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMetricsServiceStart() { MetricsService metricsService = new MetricsServiceImpl(); metricsService.start(); Assert.assertTrue(metricsService.getSources().size() == 2); for (XXX source : metricsService.getSources()) { Assert.assertTrue ( source instanceof JvmMetricsSource || source instanceof TestMetricsSource); } }', 'ground_truth': 'public void testMetricsServiceStart() { MetricsServiceImpl metricsService = new MetricsServiceImpl(); metricsService.start(); MetricsSource source = MetricsServiceImpl.getSource("jvm"); Assert.assertNotNull(source); Assert.assertTrue(source instanceof JvmMetricsSource); source = MetricsServiceImpl.getSource("testsource"); Assert.assertNotNull(source); Assert.assertTrue(source instanceof TestMetricsSource); }', 'output': 'public void testMetricsServiceStart() { MetricsService metricsService = new MetricsServiceImpl(); metricsService.start(); Assert.assertTrue(metricsService.getSources().size() == 2); for (MetricsSource source : metricsService.getSources()) { Assert.assertTrue ( source instanceof JvmMetricsSource || source instanceof TestMetricsSource); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void recomposeConjunction() { SargBinding currBinding, nextBinding; RexInputRef currRef, nextRef; SargExpr currSargExpr, nextSargExpr; RexNode currAndNode; boolean recomp; XXX iter; for (int i = 0; i < sargBindingList.size(); i++) { currBinding = sargBindingList.get(i); currRef = currBinding.getInputRef(); currSargExpr = currBinding.getExpr(); currAndNode = sarg2RexMap.get(currSargExpr); sarg2RexMap.remove(currSargExpr); recomp = false; iter = sargBindingList.listIterator(i + 1); while (iter.hasNext()) { nextBinding = (SargBinding) iter.next(); nextRef = nextBinding.getInputRef(); nextSargExpr = nextBinding.getExpr(); if (nextRef.getIndex() == currRef.getIndex()) { SargSetExpr expr = factory.newSetExpr( currSargExpr.getDataType(), SargSetOperator.INTERSECTION); expr.addChild(currSargExpr); expr.addChild(nextSargExpr); currAndNode = factory.getRexBuilder().makeCall( SqlStdOperatorTable.andOperator, currAndNode, sarg2RexMap.get(nextSargExpr)); currSargExpr = expr; sarg2RexMap.remove(nextSargExpr); iter.remove(); recomp = true; } } if (recomp) { assert (!simpleMode); if (!testDynamicParamSupport(currSargExpr)) { nonSargFilterList.add(currAndNode); sargBindingList.remove(i); continue; } } if (recomp) { SargBinding newBinding = new SargBinding(currSargExpr, currRef); sargBindingList.remove(i); sargBindingList.add(i, newBinding); } sarg2RexMap.put(currSargExpr, currAndNode); } }', 'ground_truth': 'private void recomposeConjunction() { for (int i = 0; i < sargBindingList.size(); i++) { final SargBinding currBinding = sargBindingList.get(i); final RexInputRef currRef = currBinding.getInputRef(); SargExpr currSargExpr = currBinding.getExpr(); RexNode currAndNode = sarg2RexMap.get(currSargExpr); sarg2RexMap.remove(currSargExpr); boolean recomp = false; ListIterator<SargBinding> iter = sargBindingList.listIterator(i + 1); while (iter.hasNext()) { final SargBinding nextBinding = iter.next(); final RexInputRef nextRef = nextBinding.getInputRef(); final SargExpr nextSargExpr = nextBinding.getExpr(); if (nextRef.getIndex() == currRef.getIndex()) { SargSetExpr expr = factory.newSetExpr( currSargExpr.getDataType(), SargSetOperator.INTERSECTION); expr.addChild(currSargExpr); expr.addChild(nextSargExpr); currAndNode = factory.getRexBuilder().makeCall( SqlStdOperatorTable.andOperator, currAndNode, sarg2RexMap.get(nextSargExpr)); currSargExpr = expr; sarg2RexMap.remove(nextSargExpr); iter.remove(); recomp = true; } } if (recomp) { assert !simpleMode; if (!testDynamicParamSupport(currSargExpr)) { nonSargFilterList.add(currAndNode); sargBindingList.remove(i); continue; } } if (recomp) { SargBinding newBinding = new SargBinding(currSargExpr, currRef); sargBindingList.remove(i); sargBindingList.add(i, newBinding); } sarg2RexMap.put(currSargExpr, currAndNode); } }', 'output': 'private void recomposeConjunction() { SargBinding currBinding, nextBinding; RexInputRef currRef, nextRef; SargExpr currSargExpr, nextSargExpr; RexNode currAndNode; boolean recomp; Iterator<SargBinding> iter; for (int i = 0; i < sargBindingList.size(); i++) { currBinding = sargBindingList.get(i); currRef = currBinding.getInputRef(); currSargExpr = currBinding.getExpr(); currAndNode = sarg2RexMap.get(currSargExpr); sarg2RexMap.remove(currSargExpr); recomp = false; iter = sargBindingList.listIterator(i + 1); while (iter.hasNext()) { nextBinding = (SargBinding) iter.next(); nextRef = nextBinding.getInputRef(); nextSargExpr = nextBinding.getExpr(); if (nextRef.getIndex() == currRef.getIndex()) { SargSetExpr expr = factory.newSetExpr( currSargExpr.getDataType(), SargSetOperator.INTERSECTION); expr.addChild(currSargExpr); expr.addChild(nextSargExpr); currAndNode = factory.getRexBuilder().makeCall( SqlStdOperatorTable.andOperator, currAndNode, sarg2RexMap.get(nextSargExpr)); currSargExpr = expr; sarg2RexMap.remove(nextSargExpr); iter.remove(); recomp = true; } } if (recomp) { assert (!simpleMode); if (!testDynamicParamSupport(currSargExpr)) { nonSargFilterList.add(currAndNode); sargBindingList.remove(i); continue; } } if (recomp) { SargBinding newBinding = new SargBinding(currSargExpr, currRef); sargBindingList.remove(i); sargBindingList.add(i, newBinding); } sarg2RexMap.put(currSargExpr, currAndNode); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String getCanonicalForm() { String directiveName = isFunction ? "function" : "macro"; XXX buf = new StringBuffer("<#"); buf.append(directiveName); buf.append(\' \'); buf.append(name); buf.append(\'(\'); int size = argumentNames.length; for (int i = 0; i<size; i++) { buf.append(argumentNames[i]); if (i != (size-1)) { buf.append(","); } } buf.append(")>"); if (nestedBlock != null) { buf.append(nestedBlock.getCanonicalForm()); } buf.append("</#"); buf.append(directiveName); buf.append(\'>\'); return buf.toString(); }', 'ground_truth': 'public String getCanonicalForm() { String directiveName = isFunction ? "function" : "macro"; StringBuilder buf = new StringBuilder("<#"); buf.append(directiveName); buf.append(\' \'); buf.append(name); buf.append(\'(\'); int size = argumentNames.length; for (int i = 0; i<size; i++) { buf.append(argumentNames[i]); if (i != (size-1)) { buf.append(","); } } buf.append(")>"); if (nestedBlock != null) { buf.append(nestedBlock.getCanonicalForm()); } buf.append("</#"); buf.append(directiveName); buf.append(\'>\'); return buf.toString(); }', 'output': 'public String getCanonicalForm() { String directiveName = isFunction ? "function" : "macro"; StringBuilder buf = new StringBuilder("<#"); buf.append(directiveName); buf.append(\' \'); buf.append(name); buf.append(\'(\'); int size = argumentNames.length; for (int i = 0; i<size; i++) { buf.append(argumentNames[i]); if (i != (size-1)) { buf.append(","); } } buf.append(")>"); if (nestedBlock != null) { buf.append(nestedBlock.getCanonicalForm()); } buf.append("</#"); buf.append(directiveName); buf.append(\'>\'); return buf.toString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testConvergenceCheckingWithAnomaliesInData() { data.put(new double[]{10, 11}, 100000.0); LocalDatasetBuilder<double[], Double> datasetBuilder = new LocalDatasetBuilder<>(data, 1); ConvergenceChecker<double[], Double> checker = createChecker( new MeanAbsValueConvergenceCheckerFactory(0.1), datasetBuilder); try(LocalDataset<EmptyContext, FeatureMatrixWithLabelsOnHeapData> dataset = datasetBuilder.build( TestUtils.testEnvBuilder(), new EmptyContextBuilder<>(), new FeatureMatrixWithLabelsOnHeapDataBuilder<>(fExtr, lbExtr))) { double onDSError = checker.computeMeanErrorOnDataset(dataset, notConvergedMdl); Assert.assertEquals(9090.41, onDSError, 0.01); } catch (Exception e) { throw new RuntimeException(e); } }', 'ground_truth': 'public void testConvergenceCheckingWithAnomaliesInData() { data.put(666, VectorUtils.of(10, 11).labeled(100000.0)); LocalDatasetBuilder<Integer, LabeledVector<Double>> datasetBuilder = new LocalDatasetBuilder<>(data, 1); ConvergenceChecker<Integer, LabeledVector<Double>, Integer> checker = createChecker( new MeanAbsValueConvergenceCheckerFactory(0.1), datasetBuilder); try(LocalDataset<EmptyContext, FeatureMatrixWithLabelsOnHeapData> dataset = datasetBuilder.build( TestUtils.testEnvBuilder(), new EmptyContextBuilder<>(), new FeatureMatrixWithLabelsOnHeapDataBuilder<>(vectorizer))) { double onDSError = checker.computeMeanErrorOnDataset(dataset, notConvergedMdl); Assert.assertEquals(9090.41, onDSError, 0.01); } catch (Exception e) { throw new RuntimeException(e); } }', 'output': 'public void testConvergenceCheckingWithAnomaliesInData() { data.put(new double[]{10, 11}, 100000.0); LocalDatasetBuilder<double[], Double> datasetBuilder = new LocalDatasetBuilder<>(data, 1); ConvergenceChecker<double[], Double> checker = createChecker( new MeanAbsValueConvergenceCheckerFactory(0.1), datasetBuilder); try(LocalDataset<EmptyContext, FeatureMatrixWithLabelsOnHeapData> dataset = datasetBuilder.build( TestUtils.testEnvBuilder(), new EmptyContextBuilder<>(), new FeatureMatrixWithLabelsOnHeapDataBuilder<>(fExtr, lbExtr))) { double onDSError = checker.computeMeanErrorOnDataset(dataset, notConvergedMdl); Assert.assertEquals(9090.41, onDSError, 0.01); } catch (Exception e) { throw new RuntimeException(e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkNodeRemoved(int backups) throws Exception { int parts = 256; GridCacheAffinityFunction aff = new GridCachePartitionFairAffinity(parts); int nodesCnt = 50; List<ClusterNode> nodes = new ArrayList<>(nodesCnt); List<List<ClusterNode>> prev = null; for (int i = 0; i < nodesCnt; i++) { info("======================================"); info("Assigning partitions: " + i); info("======================================"); ClusterNode node = new GridTestNode(UUID.randomUUID()); nodes.add(node); XXX discoEvt = new GridDiscoveryEvent(node, "", IgniteEventType.EVT_NODE_JOINED, node); List<List<ClusterNode>> assignment = aff.assignPartitions( new GridCacheAffinityFunctionContextImpl(nodes, prev, discoEvt, i, backups)); info("Assigned."); verifyAssignment(assignment, backups, parts, nodes.size()); prev = assignment; } info("======================================"); info("Will remove nodes."); info("======================================"); for (int i = 0; i < nodesCnt - 1; i++) { info("======================================"); info("Assigning partitions: " + i); info("======================================"); ClusterNode rmv = nodes.remove(nodes.size() - 1); XXX discoEvt = new GridDiscoveryEvent(rmv, "", IgniteEventType.EVT_NODE_LEFT, rmv); List<List<ClusterNode>> assignment = aff.assignPartitions( new GridCacheAffinityFunctionContextImpl(nodes, prev, discoEvt, i, backups)); info("Assigned."); verifyAssignment(assignment, backups, parts, nodes.size()); prev = assignment; } }', 'ground_truth': 'private void checkNodeRemoved(int backups) throws Exception { int parts = 256; GridCacheAffinityFunction aff = new GridCachePartitionFairAffinity(parts); int nodesCnt = 50; List<ClusterNode> nodes = new ArrayList<>(nodesCnt); List<List<ClusterNode>> prev = null; for (int i = 0; i < nodesCnt; i++) { info("======================================"); info("Assigning partitions: " + i); info("======================================"); ClusterNode node = new GridTestNode(UUID.randomUUID()); nodes.add(node); IgniteDiscoveryEvent discoEvt = new IgniteDiscoveryEvent(node, "", IgniteEventType.EVT_NODE_JOINED, node); List<List<ClusterNode>> assignment = aff.assignPartitions( new GridCacheAffinityFunctionContextImpl(nodes, prev, discoEvt, i, backups)); info("Assigned."); verifyAssignment(assignment, backups, parts, nodes.size()); prev = assignment; } info("======================================"); info("Will remove nodes."); info("======================================"); for (int i = 0; i < nodesCnt - 1; i++) { info("======================================"); info("Assigning partitions: " + i); info("======================================"); ClusterNode rmv = nodes.remove(nodes.size() - 1); IgniteDiscoveryEvent discoEvt = new IgniteDiscoveryEvent(rmv, "", IgniteEventType.EVT_NODE_LEFT, rmv); List<List<ClusterNode>> assignment = aff.assignPartitions( new GridCacheAffinityFunctionContextImpl(nodes, prev, discoEvt, i, backups)); info("Assigned."); verifyAssignment(assignment, backups, parts, nodes.size()); prev = assignment; } }', 'output': 'private void checkNodeRemoved(int backups) throws Exception { int parts = 256; GridCacheAffinityFunction aff = new GridCachePartitionFairAffinity(parts); int nodesCnt = 50; List<ClusterNode> nodes = new ArrayList<>(nodesCnt); List<List<ClusterNode>> prev = null; for (int i = 0; i < nodesCnt; i++) { info("======================================"); info("Assigning partitions: " + i); info("======================================"); ClusterNode node = new GridTestNode(UUID.randomUUID()); nodes.add(node); GridDiscoveryEvent discoEvt = new GridDiscoveryEvent(node, "", IgniteEventType.EVT_NODE_JOINED, node); List<List<ClusterNode>> assignment = aff.assignPartitions( new GridCacheAffinityFunctionContextImpl(nodes, prev, discoEvt, i, backups)); info("Assigned."); verifyAssignment(assignment, backups, parts, nodes.size()); prev = assignment; } info("======================================"); info("Will remove nodes."); info("======================================"); for (int i = 0; i < nodesCnt - 1; i++) { info("======================================"); info("Assigning partitions: " + i); info("======================================"); ClusterNode rmv = nodes.remove(nodes.size() - 1); GridDiscoveryEvent discoEvt = new GridDiscoveryEvent(rmv, "", IgniteEventType.EVT_NODE_LEFT, rmv); List<List<ClusterNode>> assignment = aff.assignPartitions( new GridCacheAffinityFunctionContextImpl(nodes, prev, discoEvt, i, backups)); info("Assigned."); verifyAssignment(assignment, backups, parts, nodes.size()); prev = assignment; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void main(String[] args) throws InterruptedException { try (Ignite ignite = Ignition.start("examples/config/example-ignite.xml")) { IgniteThread igniteThread = new IgniteThread(ignite.configuration().getIgniteInstanceName(), Step_2_Imputing.class.getSimpleName(), () -> { try { IgniteCache<Integer, Object[]> dataCache = TitanicUtils.readPassengers(ignite); IgniteBiFunction<Integer, Object[], double[]> featureExtractor = (k, v) -> new double[]{(double) v[0], (double) v[5], (double) v[6]}; IgniteBiFunction<Integer, Object[], Double> lbExtractor = (k, v) -> (double) v[1]; IgniteBiFunction<Integer, Object[], double[]> imputingPreprocessor = new ImputerTrainer<Integer, Object[]>() .fit(ignite, dataCache, featureExtractor ); DecisionTreeClassificationTrainer trainer = new DecisionTreeClassificationTrainer(5, 0); DecisionTreeNode mdl = trainer.fit( ignite, dataCache, imputingPreprocessor, lbExtractor ); double accuracy = Evaluator.evaluate( dataCache, mdl, imputingPreprocessor, lbExtractor, new Accuracy<>() ); System.out.println("\\n>>> Accuracy " + accuracy); System.out.println("\\n>>> Test Error " + (1 - accuracy)); } catch (FileNotFoundException e) { e.printStackTrace(); } }); igniteThread.start(); igniteThread.join(); } }', 'ground_truth': 'public static void main(String[] args) throws InterruptedException { try (Ignite ignite = Ignition.start("examples/config/example-ignite.xml")) { IgniteThread igniteThread = new IgniteThread(ignite.configuration().getIgniteInstanceName(), Step_2_Imputing.class.getSimpleName(), () -> { try { IgniteCache<Integer, Object[]> dataCache = TitanicUtils.readPassengers(ignite); IgniteBiFunction<Integer, Object[], Vector> featureExtractor = (k, v) -> VectorUtils.of((double) v[0], (double) v[5], (double) v[6]); IgniteBiFunction<Integer, Object[], Double> lbExtractor = (k, v) -> (double) v[1]; IgniteBiFunction<Integer, Object[], Vector> imputingPreprocessor = new ImputerTrainer<Integer, Object[]>() .fit(ignite, dataCache, featureExtractor ); DecisionTreeClassificationTrainer trainer = new DecisionTreeClassificationTrainer(5, 0); DecisionTreeNode mdl = trainer.fit( ignite, dataCache, imputingPreprocessor, lbExtractor ); double accuracy = Evaluator.evaluate( dataCache, mdl, imputingPreprocessor, lbExtractor, new Accuracy<>() ); System.out.println("\\n>>> Accuracy " + accuracy); System.out.println("\\n>>> Test Error " + (1 - accuracy)); } catch (FileNotFoundException e) { e.printStackTrace(); } }); igniteThread.start(); igniteThread.join(); } }', 'output': 'public static void main(String[] args) throws InterruptedException { try (Ignite ignite = Ignition.start("examples/config/example-ignite.xml")) { IgniteThread igniteThread = new IgniteThread(ignite.configuration().getIgniteInstanceName(), Step_2_Imputing.class.getSimpleName(), () -> { try { IgniteCache<Integer, Object[]> dataCache = TitanicUtils.readPassengers(ignite); IgniteBiFunction<Integer, Object[], double[]> featureExtractor = (k, v) -> new double[]{(double) v[0], (double) v[5], (double) v[6]}; IgniteBiFunction<Integer, Object[], Double> lbExtractor = (k, v) -> (double) v[1]; IgniteBiFunction<Integer, Object[], double[]> imputingPreprocessor = new ImputerTrainer<Integer, Object[]>() .fit(ignite, dataCache, featureExtractor ); DecisionTreeClassificationTrainer trainer = new DecisionTreeClassificationTrainer(5, 0); DecisionTreeNode mdl = trainer.fit( ignite, dataCache, imputingPreprocessor, lbExtractor ); double accuracy = Evaluator.evaluate( dataCache, mdl, imputingPreprocessor, lbExtractor, new Accuracy<>() ); System.out.println("\\n>>> Accuracy " + accuracy); System.out.println("\\n>>> Test Error " + (1 - accuracy)); } catch (FileNotFoundException e) { e.printStackTrace(); } }); igniteThread.start(); igniteThread.join(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testEviction() throws Exception { String locSeqName = UUID.randomUUID().toString(); XXX locSeq = grid().cache(null).dataStructures().atomicSequence(locSeqName, 0, true); locSeq.addAndGet(153); grid().cache(null).evictAll(); assert null != grid().cache(null).get(new GridCacheInternalKeyImpl(locSeqName)); }', 'ground_truth': 'public void testEviction() throws Exception { String locSeqName = UUID.randomUUID().toString(); IgniteAtomicSequence locSeq = grid().cache(null).dataStructures().atomicSequence(locSeqName, 0, true); locSeq.addAndGet(153); grid().cache(null).evictAll(); assert null != grid().cache(null).get(new GridCacheInternalKeyImpl(locSeqName)); }', 'output': 'public void testEviction() throws Exception { String locSeqName = UUID.randomUUID().toString(); AtomicSequence<Integer> locSeq = grid().cache(null).dataStructures().atomicSequence(locSeqName, 0, true); locSeq.addAndGet(153); grid().cache(null).evictAll(); assert null != grid().cache(null).get(new GridCacheInternalKeyImpl(locSeqName)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void readMutation(CommitLogReadHandler handler, byte[] inputBuffer, int size, CommitLogPosition minPosition, final int entryLocation, final CommitLogDescriptor desc) throws IOException { boolean shouldReplay = entryLocation > minPosition.position; final Mutation mutation; try (RebufferingInputStream bufIn = new DataInputBuffer(inputBuffer, 0, size)) { mutation = Mutation.serializer.deserialize(bufIn, desc.getMessagingVersion(), SerializationHelper.Flag.LOCAL); for (PartitionUpdate upd : mutation.getPartitionUpdates()) upd.validate(); } catch (XXX ex) { if (ex.cfId == null) return; AtomicInteger i = invalidMutations.get(ex.cfId); if (i == null) { i = new AtomicInteger(1); invalidMutations.put(ex.cfId, i); } else i.incrementAndGet(); return; } catch (Throwable t) { JVMStabilityInspector.inspectThrowable(t); File f = File.createTempFile("mutation", "dat"); try (DataOutputStream out = new DataOutputStream(new FileOutputStream(f))) { out.write(inputBuffer, 0, size); } handler.handleUnrecoverableError(new CommitLogReadException( String.format( "Unexpected error deserializing mutation; saved to %s.  " + "This may be caused by replaying a mutation against a table with the same name but incompatible schema.  " + "Exception follows: %s", f.getAbsolutePath(), t), CommitLogReadErrorReason.MUTATION_ERROR, false)); return; } if (logger.isTraceEnabled()) logger.trace("Read mutation for {}.{}: {}", mutation.getKeyspaceName(), mutation.key(), "{" + StringUtils.join(mutation.getPartitionUpdates().iterator(), ", ") + "}"); if (shouldReplay) handler.handleMutation(mutation, size, entryLocation, desc); }', 'ground_truth': 'protected void readMutation(CommitLogReadHandler handler, byte[] inputBuffer, int size, CommitLogPosition minPosition, final int entryLocation, final CommitLogDescriptor desc) throws IOException { boolean shouldReplay = entryLocation > minPosition.position; final Mutation mutation; try (RebufferingInputStream bufIn = new DataInputBuffer(inputBuffer, 0, size)) { mutation = Mutation.serializer.deserialize(bufIn, desc.getMessagingVersion(), SerializationHelper.Flag.LOCAL); for (PartitionUpdate upd : mutation.getPartitionUpdates()) upd.validate(); } catch (UnknownTableException ex) { if (ex.id == null) return; AtomicInteger i = invalidMutations.get(ex.id); if (i == null) { i = new AtomicInteger(1); invalidMutations.put(ex.id, i); } else i.incrementAndGet(); return; } catch (Throwable t) { JVMStabilityInspector.inspectThrowable(t); File f = File.createTempFile("mutation", "dat"); try (DataOutputStream out = new DataOutputStream(new FileOutputStream(f))) { out.write(inputBuffer, 0, size); } handler.handleUnrecoverableError(new CommitLogReadException( String.format( "Unexpected error deserializing mutation; saved to %s.  " + "This may be caused by replaying a mutation against a table with the same name but incompatible schema.  " + "Exception follows: %s", f.getAbsolutePath(), t), CommitLogReadErrorReason.MUTATION_ERROR, false)); return; } if (logger.isTraceEnabled()) logger.trace("Read mutation for {}.{}: {}", mutation.getKeyspaceName(), mutation.key(), "{" + StringUtils.join(mutation.getPartitionUpdates().iterator(), ", ") + "}"); if (shouldReplay) handler.handleMutation(mutation, size, entryLocation, desc); }', 'output': 'protected void readMutation(CommitLogReadHandler handler, byte[] inputBuffer, int size, CommitLogPosition minPosition, final int entryLocation, final CommitLogDescriptor desc) throws IOException { boolean shouldReplay = entryLocation > minPosition.position; final Mutation mutation; try (RebufferingInputStream bufIn = new DataInputBuffer(inputBuffer, 0, size)) { mutation = Mutation.serializer.deserialize(bufIn, desc.getMessagingVersion(), SerializationHelper.Flag.LOCAL); for (PartitionUpdate upd : mutation.getPartitionUpdates()) upd.validate(); } catch (Throwable t) { JVMStabilityInspector.inspectThrowable(t); File f = File.createTempFile("mutation", "dat"); try (DataOutputStream out = new DataOutputStream(new FileOutputStream(f))) { out.write(inputBuffer, 0, size); } handler.handleUnrecoverableError(new CommitLogReadException( String.format( "Unexpected error deserializing mutation; saved to %s.  " + "This may be caused by replaying a mutation against a table with the same name but incompatible schema.  " + "Exception follows: %s", f.getAbsolutePath(), t), CommitLogReadErrorReason.MUTATION_ERROR, false)); return; } if (logger.isTraceEnabled()) logger.trace("Read mutation for {}.{}: {}", mutation.getKeyspaceName(), mutation.key(), "{" + StringUtils.join(mutation.getPartitionUpdates().iterator(), ", ") + "}"); if (shouldReplay) handler.handleMutation(mutation, size, entryLocation, desc); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testExtendsClauseWithSuperClassInTree() throws Exception { XXX srcList = new ArrayList(); srcList.add("groovy/xml/DOMBuilder.java"); srcList.add("groovy/util/BuilderSupport.java"); xmlTool.add(srcList); MockOutputTool output = new MockOutputTool(); xmlTool.renderToOutput(output, MOCK_DIR); String domBuilderDoc = output.getText(MOCK_DIR + "/groovy/xml/DOMBuilder.html"); assertTrue(domBuilderDoc.indexOf("<extends>BuilderSupport</extends>") > 0); }', 'ground_truth': 'public void testExtendsClauseWithSuperClassInTree() throws Exception { List<String> srcList = new ArrayList<String>(); srcList.add("groovy/xml/DOMBuilder.java"); srcList.add("groovy/util/BuilderSupport.java"); xmlTool.add(srcList); MockOutputTool output = new MockOutputTool(); xmlTool.renderToOutput(output, MOCK_DIR); String domBuilderDoc = output.getText(MOCK_DIR + "/groovy/xml/DOMBuilder.html"); assertTrue(domBuilderDoc.indexOf("<extends>BuilderSupport</extends>") > 0); }', 'output': 'public void testExtendsClauseWithSuperClassInTree() throws Exception { List<String> srcList = new ArrayList<>(); srcList.add("groovy/xml/DOMBuilder.java"); srcList.add("groovy/util/BuilderSupport.java"); xmlTool.add(srcList); MockOutputTool output = new MockOutputTool(); xmlTool.renderToOutput(output, MOCK_DIR); String domBuilderDoc = output.getText(MOCK_DIR + "/groovy/xml/DOMBuilder.html"); assertTrue(domBuilderDoc.indexOf("<extends>BuilderSupport</extends>") > 0); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String recv_system_update_column_family() throws InvalidRequestException, SchemaDisagreementException, TException { TMessage msg = iprot_.readMessageBegin(); if (msg.type == TMessageType.EXCEPTION) { XXX x = TApplicationException.read(iprot_); iprot_.readMessageEnd(); throw x; } if (msg.seqid != seqid_) { throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "system_update_column_family failed: out of sequence response"); } system_update_column_family_result result = new system_update_column_family_result(); result.read(iprot_); iprot_.readMessageEnd(); if (result.isSetSuccess()) { return result.success; } if (result.ire != null) { throw result.ire; } if (result.sde != null) { throw result.sde; } throw new TApplicationException(TApplicationException.MISSING_RESULT, "system_update_column_family failed: unknown result"); }', 'ground_truth': 'public String recv_system_update_column_family() throws InvalidRequestException, SchemaDisagreementException, org.apache.thrift.TException { org.apache.thrift.protocol.TMessage msg = iprot_.readMessageBegin(); if (msg.type == org.apache.thrift.protocol.TMessageType.EXCEPTION) { org.apache.thrift.TApplicationException x = org.apache.thrift.TApplicationException.read(iprot_); iprot_.readMessageEnd(); throw x; } if (msg.seqid != seqid_) { throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.BAD_SEQUENCE_ID, "system_update_column_family failed: out of sequence response"); } system_update_column_family_result result = new system_update_column_family_result(); result.read(iprot_); iprot_.readMessageEnd(); if (result.isSetSuccess()) { return result.success; } if (result.ire != null) { throw result.ire; } if (result.sde != null) { throw result.sde; } throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.MISSING_RESULT, "system_update_column_family failed: unknown result"); }', 'output': 'public String recv_system_update_column_family() throws InvalidRequestException, SchemaDisagreementException, TException { TMessage msg = iprot_.readMessageBegin(); if (msg.type == TMessageType.EXCEPTION) { TApplicationException x = TApplicationException.read(iprot_); iprot_.readMessageEnd(); throw x; } if (msg.seqid != seqid_) { throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "system_update_column_family failed: out of sequence response"); } system_update_column_family_result result = new system_update_column_family_result(); result.read(iprot_); iprot_.readMessageEnd(); if (result.isSetSuccess()) { return result.success; } if (result.ire != null) { throw result.ire; } if (result.sde != null) { throw result.sde; } throw new TApplicationException(TApplicationException.MISSING_RESULT, "system_update_column_family failed: unknown result"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testNodeAttributes() { for (DiscoverySpi spi : spis) { assert !spi.getRemoteNodes().isEmpty() : "No remote nodes found in Spi."; Collection<UUID> nodeIds = new HashSet<>(); for (XXX rsrc : spiRsrcs) { nodeIds.add(rsrc.getNodeId()); } for (ClusterNode node : spi.getRemoteNodes()) { if (nodeIds.contains(node.id())) { Serializable attr = node.attribute(TEST_ATTRIBUTE_NAME); if (attr == null || !(attr instanceof String)) { fail("Node does not contains attribute [attr=" + TEST_ATTRIBUTE_NAME + ", nodeId=" + node.id() + ", spiIdx=" + spis.indexOf(spi) + \']\'); } else if (!"true".equals(attr)) { fail("Attribute value is wrong [attr=" + TEST_ATTRIBUTE_NAME + ", value=" + attr + ", nodeId=" + node.id() + ", spiIdx=" + spis.indexOf(spi) + \']\'); } else { info("Node contains attribute [attr=" + TEST_ATTRIBUTE_NAME + ", value=" + attr + ", nodeId=" + node.id() + ", spiIdx=" + spis.indexOf(spi) + \']\'); } } else error("Discovered unknown node [node=" + node + ", spiIdx=" + spis.indexOf(spi) + \']\'); } } }', 'ground_truth': 'public void testNodeAttributes() { for (DiscoverySpi spi : spis) { assert !spi.getRemoteNodes().isEmpty() : "No remote nodes found in Spi."; Collection<UUID> nodeIds = new HashSet<>(); for (IgniteTestResources rsrc : spiRsrcs) { nodeIds.add(rsrc.getNodeId()); } for (ClusterNode node : spi.getRemoteNodes()) { if (nodeIds.contains(node.id())) { Serializable attr = node.attribute(TEST_ATTRIBUTE_NAME); if (attr == null || !(attr instanceof String)) { fail("Node does not contains attribute [attr=" + TEST_ATTRIBUTE_NAME + ", nodeId=" + node.id() + ", spiIdx=" + spis.indexOf(spi) + \']\'); } else if (!"true".equals(attr)) { fail("Attribute value is wrong [attr=" + TEST_ATTRIBUTE_NAME + ", value=" + attr + ", nodeId=" + node.id() + ", spiIdx=" + spis.indexOf(spi) + \']\'); } else { info("Node contains attribute [attr=" + TEST_ATTRIBUTE_NAME + ", value=" + attr + ", nodeId=" + node.id() + ", spiIdx=" + spis.indexOf(spi) + \']\'); } } else error("Discovered unknown node [node=" + node + ", spiIdx=" + spis.indexOf(spi) + \']\'); } } }', 'output': 'public void testNodeAttributes() { for (DiscoverySpi spi : spis) { assert !spi.getRemoteNodes().isEmpty() : "No remote nodes found in Spi."; Collection<UUID> nodeIds = new HashSet<>(); for (String rsrc : spiRsrcs) { nodeIds.add(rsrc); } for (ClusterNode node : spi.getRemoteNodes()) { if (nodeIds.contains(node.id())) { Serializable attr = node.attribute(TEST_ATTRIBUTE_NAME); if (attr == null || !(attr instanceof String)) { fail("Node does not contains attribute [attr=" + TEST_ATTRIBUTE_NAME + ", nodeId=" + node.id() + ", spiIdx=" + spis.indexOf(spi) + \']\'); } else if (!"true".equals(attr)) { fail("Attribute value is wrong [attr=" + TEST_ATTRIBUTE_NAME + ", value=" + attr + ", nodeId=" + node.id() + ", spiIdx=" + spis.indexOf(spi) + \']\'); } else { info("Node contains attribute [attr=" + TEST_ATTRIBUTE_NAME + ", value=" + attr + ", nodeId=" + node.id() + ", spiIdx=" + spis.indexOf(spi) + \']\'); } } else error("Discovered unknown node [node=" + node + ", spiIdx=" + spis.indexOf(spi) + \']\'); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testForEmptyGraph() throws Exception { Multimap<Range<Token>, InetAddressAndPort> rangesWithSources = HashMultimap.create(); addNonTrivialRangeAndSources(rangesWithSources, 1, 10, "127.0.0.1"); addNonTrivialRangeAndSources(rangesWithSources, 11, 20, "127.0.0.1"); addNonTrivialRangeAndSources(rangesWithSources, 21, 30, "127.0.0.1"); addNonTrivialRangeAndSources(rangesWithSources, 31, 40, "127.0.0.1"); addNonTrivialRangeAndSources(rangesWithSources, 41, 50, "127.0.0.1"); RangeFetchMapCalculator calculator = new RangeFetchMapCalculator(rangesWithSources, new ArrayList<RangeStreamer.ISourceFilter>(), "Test"); Multimap<InetAddressAndPort, Range<Token>> map = calculator.getRangeFetchMap(); assertTrue(map.isEmpty()); }', 'ground_truth': 'public void testForEmptyGraph() throws Exception { EndpointsByRange.Mutable rangesWithSources = new EndpointsByRange.Mutable(); addNonTrivialRangeAndSources(rangesWithSources, 1, 10, "127.0.0.1"); addNonTrivialRangeAndSources(rangesWithSources, 11, 20, "127.0.0.1"); addNonTrivialRangeAndSources(rangesWithSources, 21, 30, "127.0.0.1"); addNonTrivialRangeAndSources(rangesWithSources, 31, 40, "127.0.0.1"); addNonTrivialRangeAndSources(rangesWithSources, 41, 50, "127.0.0.1"); RangeFetchMapCalculator calculator = new RangeFetchMapCalculator(rangesWithSources.asImmutableView(), Collections.emptyList(), "Test"); Multimap<InetAddressAndPort, Range<Token>> map = calculator.getRangeFetchMap(); assertTrue(map.isEmpty()); }', 'output': 'public void testForEmptyGraph() throws Exception { Multimap<Range<Token>, InetAddressAndPort> rangesWithSources = HashMultimap.create(); addNonTrivialRangeAndSources(rangesWithSources, 1, 10, "127.0.0.1"); addNonTrivialRangeAndSources(rangesWithSources, 11, 20, "127.0.0.1"); addNonTrivialRangeAndSources(rangesWithSources, 21, 30, "127.0.0.1"); addNonTrivialRangeAndSources(rangesWithSources, 31, 40, "127.0.0.1"); addNonTrivialRangeAndSources(rangesWithSources, 41, 50, "127.0.0.1"); RangeFetchMapCalculator calculator = new RangeFetchMapCalculator(rangesWithSources, new ArrayList<RangeStreamer.ISourceFilter>(), "Test"); Multimap<InetAddressAndPort, Range<Token>> map = calculator.getRangeFetchMap(); assertTrue(map.isEmpty()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void gridGainGroupLock(Ignite ignite, final long max, int threads) throws Exception { X.println(">>>"); X.println(">>> Testing group lock"); X.println(">>>"); final GridCache<GridCacheAffinityKey<Long>, Long> cache = ignite.cache(CACHE); assert cache != null; final AtomicLong opCnt = new AtomicLong(); cntr.set(0); final AtomicInteger range = new AtomicInteger(); final long start = System.currentTimeMillis(); GridTestUtils.runMultiThreaded(new Callable<Object>() { @Nullable @Override public Object call() throws Exception { int affIdx = range.getAndIncrement(); String affKey = Thread.currentThread().getName(); long rangeCnt = OBJECT_CNT / THREADS; long base = affIdx * rangeCnt; X.println("Going to put vals in range [" + base + ", " + (base + rangeCnt - 1) + \']\'); long key = 0; while (true) { long total = cntr.getAndAdd(BATCH_SIZE); if (total >= max) break; try (IgniteTx tx = cache.txStartAffinity(affKey, PESSIMISTIC, REPEATABLE_READ, 0, BATCH_SIZE)) { for (long i = 0; i < BATCH_SIZE; i++) { cache.put(new GridCacheAffinityKey<>((key % rangeCnt) + base, affKey), i); key++; } tx.commit(); } long ops = opCnt.addAndGet(BATCH_SIZE); if (ops % LOG_MOD == 0) X.println(">>> Performed " + ops + " operations."); } return null; } }, threads, "load-worker"); long dur = System.currentTimeMillis() - start; X.println(">>>"); X.println(">>> Cache size: " + cache.size()); X.println(">>> Group lock timed results [dur=" + dur + " ms, tx/sec=" + (opCnt.get() * 1000 / dur) + ", total=" + opCnt.get() + ", duration=" + (dur + 500) / 1000 + "s]"); X.println(">>>"); }', 'ground_truth': 'private static void gridGainGroupLock(Ignite ignite, final long max, int threads) throws Exception { X.println(">>>"); X.println(">>> Testing group lock"); X.println(">>>"); final Cache<CacheAffinityKey<Long>, Long> cache = ignite.cache(CACHE); assert cache != null; final AtomicLong opCnt = new AtomicLong(); cntr.set(0); final AtomicInteger range = new AtomicInteger(); final long start = System.currentTimeMillis(); GridTestUtils.runMultiThreaded(new Callable<Object>() { @Nullable @Override public Object call() throws Exception { int affIdx = range.getAndIncrement(); String affKey = Thread.currentThread().getName(); long rangeCnt = OBJECT_CNT / THREADS; long base = affIdx * rangeCnt; X.println("Going to put vals in range [" + base + ", " + (base + rangeCnt - 1) + \']\'); long key = 0; while (true) { long total = cntr.getAndAdd(BATCH_SIZE); if (total >= max) break; try (IgniteTx tx = cache.txStartAffinity(affKey, PESSIMISTIC, REPEATABLE_READ, 0, BATCH_SIZE)) { for (long i = 0; i < BATCH_SIZE; i++) { cache.put(new CacheAffinityKey<>((key % rangeCnt) + base, affKey), i); key++; } tx.commit(); } long ops = opCnt.addAndGet(BATCH_SIZE); if (ops % LOG_MOD == 0) X.println(">>> Performed " + ops + " operations."); } return null; } }, threads, "load-worker"); long dur = System.currentTimeMillis() - start; X.println(">>>"); X.println(">>> Cache size: " + cache.size()); X.println(">>> Group lock timed results [dur=" + dur + " ms, tx/sec=" + (opCnt.get() * 1000 / dur) + ", total=" + opCnt.get() + ", duration=" + (dur + 500) / 1000 + "s]"); X.println(">>>"); }', 'output': 'private static void gridGainGroupLock(Ignite ignite, final long max, int threads) throws Exception { X.println(">>>"); X.println(">>> Testing group lock"); X.println(">>>"); final GridCache<GridCacheAffinityKey<Long>, Long> cache = ignite.cache(CACHE); assert cache != null; final AtomicLong opCnt = new AtomicLong(); cntr.set(0); final AtomicInteger range = new AtomicInteger(); final long start = System.currentTimeMillis(); GridTestUtils.runMultiThreaded(new Callable<Object>() { @Nullable @Override public Object call() throws Exception { int affIdx = range.getAndIncrement(); String affKey = Thread.currentThread().getName(); long rangeCnt = OBJECT_CNT / THREADS; long base = affIdx * rangeCnt; X.println("Going to put vals in range [" + base + ", " + (base + rangeCnt - 1) + \']\'); long key = 0; while (true) { long total = cntr.getAndAdd(BATCH_SIZE); if (total >= max) break; try (IgniteTx tx = cache.txStartAffinity(affKey, PESSIMISTIC, REPEATABLE_READ, 0, BATCH_SIZE)) { for (long i = 0; i < BATCH_SIZE; i++) { cache.put(new GridCacheAffinityKey<>((key % rangeCnt) + base, affKey), i); key++; } tx.commit(); } long ops = opCnt.addAndGet(BATCH_SIZE); if (ops % LOG_MOD == 0) X.println(">>> Performed " + ops + " operations."); } return null; } }, threads, "load-worker"); long dur = System.currentTimeMillis() - start; X.println(">>>"); X.println(">>> Cache size: " + cache.size()); X.println(">>> Group lock timed results [dur=" + dur + " ms, tx/sec=" + (opCnt.get() * 1000 / dur) + ", total=" + opCnt.get() + ", duration=" +'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAtomicReference() throws Exception { final String refName = UUID.randomUUID().toString(); final String val = UUID.randomUUID().toString(); final String newVal = UUID.randomUUID().toString(); XXX ref = grid(0).cache(null).dataStructures().atomicReference(refName, val, true); final Ignite ignite = grid(0); ignite.compute().call(new IgniteCallable<Object>() { @Override public String call() throws IgniteCheckedException { XXX ref = ignite.cache(null).dataStructures().atomicReference(refName, val, true); assertEquals(val, ref.get()); return ref.get(); } }); ref.compareAndSet("WRONG EXPECTED VALUE", newVal); ignite.compute().call(new IgniteCallable<String>() { @Override public String call() throws IgniteCheckedException { XXX ref = ignite.cache(null).dataStructures().atomicReference(refName, val, true); assertEquals(val, ref.get()); return ref.get(); } }); ref.compareAndSet(val, newVal); ignite.compute().call(new IgniteCallable<String>() { @Override public String call() throws IgniteCheckedException { XXX ref = ignite.cache(null).dataStructures().atomicReference(refName, val, true); assertEquals(newVal, ref.get()); return ref.get(); } }); }', 'ground_truth': 'public void testAtomicReference() throws Exception { final String refName = UUID.randomUUID().toString(); final String val = UUID.randomUUID().toString(); final String newVal = UUID.randomUUID().toString(); IgniteAtomicReference<String> ref = grid(0).cache(null).dataStructures().atomicReference(refName, val, true); final Ignite ignite = grid(0); ignite.compute().call(new IgniteCallable<Object>() { @Override public String call() throws IgniteCheckedException { IgniteAtomicReference<String> ref = ignite.cache(null).dataStructures().atomicReference(refName, val, true); assertEquals(val, ref.get()); return ref.get(); } }); ref.compareAndSet("WRONG EXPECTED VALUE", newVal); ignite.compute().call(new IgniteCallable<String>() { @Override public String call() throws IgniteCheckedException { IgniteAtomicReference<String> ref = ignite.cache(null).dataStructures().atomicReference(refName, val, true); assertEquals(val, ref.get()); return ref.get(); } }); ref.compareAndSet(val, newVal); ignite.compute().call(new IgniteCallable<String>() { @Override public String call() throws IgniteCheckedException { IgniteAtomicReference<String> ref = ignite.cache(null).dataStructures().atomicReference(refName, val, true); assertEquals(newVal, ref.get()); return ref.get(); } }); }', 'output': 'public void testAtomicReference() throws Exception { final String refName = UUID.randomUUID().toString(); final String val = UUID.randomUUID().toString(); final String newVal = UUID.randomUUID().toString(); AtomicReference<String> ref = grid(0).cache(null).dataStructures().atomicReference(refName, val, true); final Ignite ignite = grid(0); ignite.compute().call(new IgniteCallable<Object>() { @Override public String call() throws IgniteCheckedException { AtomicReference<String> ref = ignite.cache(null).dataStructures().atomicReference(refName, val, true); assertEquals(val, ref.get()); return ref.get(); } }); ref.compareAndSet("WRONG EXPECTED VALUE", newVal); ignite.compute().call(new IgniteCallable<String>() { @Override public String call() throws IgniteCheckedException { AtomicReference<String> ref = ignite.cache(null).dataStructures().atomicReference(refName, val, true); assertEquals(val, ref.get()); return ref.get(); } }); ref.compareAndSet(val, newVal); ignite.compute().call(new IgniteCallable<String>() { @Override public String call() throws IgniteCheckedException { AtomicReference<String> ref = ignite.cache(null).dataStructures().atomicReference(refName, val, true); assertEquals(newVal, ref.get()); return ref.get(); } }); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void finalize(Iterable<FileResult> writerResults, PipelineOptions options) throws Exception { Map<String, String> outputFilenames = buildOutputFilenames(writerResults); copyToOutputFiles(outputFilenames, options); removeTemporaryFiles(outputFilenames.keySet(), !windowedWrites, options); }', 'ground_truth': 'public void finalize(Iterable<FileResult> writerResults) throws Exception { Map<ResourceId, ResourceId> outputFilenames = buildOutputFilenames(writerResults); copyToOutputFiles(outputFilenames); removeTemporaryFiles(outputFilenames.keySet(), !windowedWrites); }', 'output': 'public void finalize(List<FileResult> writerResults, PipelineOptions options) throws Exception { Map<String, String> outputFilenames = buildOutputFilenames(writerResults); copyToOutputFiles(outputFilenames, options); removeTemporaryFiles(outputFilenames.keySet(), !windowedWrites, options); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static synchronized Marshaller getMarshaller() throws IgniteCheckedException { String marshallerName = System.getProperty(MARSH_CLASS_NAME, GridTestProperties.getProperty(GridTestProperties.MARSH_CLASS_NAME)); Marshaller marsh; if (marshallerName == null) marsh = new OptimizedMarshaller(); else { try { Class<? extends Marshaller> cls = (Class<? extends Marshaller>)Class.forName(marshallerName); marsh = cls.newInstance(); } catch (ClassNotFoundException | IllegalAccessException | InstantiationException e) { throw new IgniteCheckedException("Failed to create test marshaller [marshaller=" + marshallerName + \']\', e); } } if (marsh instanceof OptimizedMarshaller) ((OptimizedMarshaller)marsh).setRequireSerializable(false); marsh.setContext(new MarshallerContextTestImpl()); if (marsh instanceof BinaryMarshaller) { XXX ctx = new PortableContext(BinaryCachingMetadataHandler.create(), new IgniteConfiguration()); IgniteUtils.invoke(BinaryMarshaller.class, marsh, "setPortableContext", ctx, new IgniteConfiguration()); } return marsh; }', 'ground_truth': 'public static synchronized Marshaller getMarshaller() throws IgniteCheckedException { String marshallerName = System.getProperty(MARSH_CLASS_NAME, GridTestProperties.getProperty(GridTestProperties.MARSH_CLASS_NAME)); Marshaller marsh; if (marshallerName == null) marsh = new OptimizedMarshaller(); else { try { Class<? extends Marshaller> cls = (Class<? extends Marshaller>)Class.forName(marshallerName); marsh = cls.newInstance(); } catch (ClassNotFoundException | IllegalAccessException | InstantiationException e) { throw new IgniteCheckedException("Failed to create test marshaller [marshaller=" + marshallerName + \']\', e); } } if (marsh instanceof OptimizedMarshaller) ((OptimizedMarshaller)marsh).setRequireSerializable(false); marsh.setContext(new MarshallerContextTestImpl()); if (marsh instanceof BinaryMarshaller) { BinaryContext ctx = new BinaryContext(BinaryCachingMetadataHandler.create(), new IgniteConfiguration()); IgniteUtils.invoke(BinaryMarshaller.class, marsh, "setPortableContext", ctx, new IgniteConfiguration()); } return marsh; }', 'output': 'public static Marshaller getMarshaller() throws IgniteCheckedException { String marshallerName = System.getProperty(MARSH_CLASS_NAME, GridTestProperties.getProperty(GridTestProperties.MARSH_CLASS_NAME)); Marshaller marsh; if (marshallerName == null) marsh = new OptimizedMarshaller(); else { try { Class<? extends Marshaller> cls = (Class<? extends Marshaller>)Class.forName(marshallerName); marsh = cls.newInstance(); } catch (ClassNotFoundException | IllegalAccessException | InstantiationException e) { throw new IgniteCheckedException("Failed to create test marshaller [marshaller=" + marshallerName + \']\', e); } } if (marsh instanceof OptimizedMarshaller) ((OptimizedMarshaller)marsh).setRequireSerializable(false); marsh.setContext(new MarshallerContextTestImpl()); return marsh; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testInternalTaskDefaultBalancing() throws Exception { customLoadBalancer = false; ignite = startGrids(GRID_CNT); assertEquals(TASK_RESULT, ignite.compute().execute(GridInternalTestTask.class.getName(), null)); UUID nid = ignite.cluster().localNode().id(); GridTuple3<Boolean, Long, Long> ping = ignite.compute() .execute(VisorNodePingTask.class.getName(), new VisorTaskArgument<>(nid, nid, false)); assertTrue(ping.get1()); assertEquals(TASK_RESULT, ignite.compute().execute(CustomTestTask.class.getName(), null)); }', 'ground_truth': 'public void testInternalTaskDefaultBalancing() throws Exception { customLoadBalancer = false; ignite = startGrids(GRID_CNT); assertEquals(TASK_RESULT, ignite.compute().execute(GridInternalTestTask.class.getName(), null)); UUID nid = ignite.cluster().localNode().id(); VisorNodePingTaskResult ping = ignite.compute() .execute(VisorNodePingTask.class.getName(), new VisorTaskArgument<>(nid, nid, false)); assertTrue(ping.isAlive()); assertEquals(TASK_RESULT, ignite.compute().execute(CustomTestTask.class.getName(), null)); }', 'output': 'public void testInternalTaskDefaultBalancing() throws Exception { customLoadBalancer = false; IgniteEx ignite = startGrids(GRID_CNT); assertEquals(TASK_RESULT, ignite.compute().execute(GridInternalTestTask.class.getName(), null)); UUID nid = ignite.cluster().localNode().id(); GridTuple3<Boolean, Long, Long> ping = ignite.compute() .execute(VisorNodePingTask.class.getName(), new VisorTaskArgument<>(nid, nid, false)); assertTrue(ping.get1()); assertEquals(TASK_RESULT, ignite.compute().execute(CustomTestTask.class.getName(), null)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static List<CompactionInfo.Holder> getCompactions() { return new ArrayList<CompactionInfo.Holder>(compactions); } public List<CompactionInfo> getCompactions()\n    { { XXX out = new ArrayList<CompactionInfo>(); for (CompactionInfo.Holder ci : CompactionExecutor.getCompactions()) out.add(ci.getCompactionInfo()); return out; }', 'ground_truth': 'public List<Map<String, String>> getCompactions() { List<Map<String, String>> out = new ArrayList<Map<String, String>>(); for (CompactionInfo.Holder ci : CompactionExecutor.getCompactions()) out.add(ci.getCompactionInfo().asMap()); return out; }', 'output': 'public static List<CompactionInfo> getCompactions() { return new ArrayList<CompactionInfo>(compactions); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void testCommits(IgniteTxConcurrency concurrency, IgniteTxIsolation isolation, boolean put) throws Exception { GridCache<Integer, Integer> cache = grid(0).cache(null); for (int i = 0; i < TX_CNT; i++) { IgniteTx tx = cache.txStart(concurrency, isolation); if (put) for (int j = 0; j < keyCount(); j++) cache.put(j, j); tx.commit(); } for (int i = 0; i < gridCount(); i++) { IgniteTxMetrics metrics = grid(i).transactions().metrics(); XXX cacheMetrics = grid(i).cache(null).metrics(); if (i == 0) { assertEquals(TX_CNT, metrics.txCommits()); if (put) assertEquals(TX_CNT, cacheMetrics.txCommits()); } else { assertEquals(0, metrics.txCommits()); assertEquals(0, cacheMetrics.txCommits()); } assertEquals(0, metrics.txRollbacks()); assertEquals(0, cacheMetrics.txRollbacks()); } }', 'ground_truth': 'private void testCommits(IgniteTxConcurrency concurrency, IgniteTxIsolation isolation, boolean put) throws Exception { GridCache<Integer, Integer> cache = grid(0).cache(null); for (int i = 0; i < TX_CNT; i++) { IgniteTx tx = cache.txStart(concurrency, isolation); if (put) for (int j = 0; j < keyCount(); j++) cache.put(j, j); tx.commit(); } for (int i = 0; i < gridCount(); i++) { IgniteTxMetrics metrics = grid(i).transactions().metrics(); CacheMetricsMxBean cacheMetrics = grid(i).cache(null).metrics(); if (i == 0) { assertEquals(TX_CNT, metrics.txCommits()); if (put) assertEquals(TX_CNT, cacheMetrics.getCacheTxCommits()); } else { assertEquals(0, metrics.txCommits()); assertEquals(0, cacheMetrics.getCacheTxCommits()); } assertEquals(0, metrics.txRollbacks()); assertEquals(0, cacheMetrics.getCacheTxRollbacks()); } }', 'output': 'private void testCommits(IgniteTxConcurrency concurrency, IgniteTxIsolation isolation, boolean put) throws Exception { Cache<Integer, Integer> cache = grid(0).cache(null); for (int i = 0; i < TX_CNT; i++) { IgniteTx tx = cache.txStart(concurrency, isolation); if (put) for (int j = 0; j < keyCount(); j++) cache.put(j, j); tx.commit(); } for (int i = 0; i < gridCount(); i++) { IgniteTxMetrics metrics = grid(i).transactions().metrics(); CacheMetrics cacheMetrics = grid(i).cache(null).metrics(); if (i == 0) { assertEquals(TX_CNT, metrics.txCommits()); if (put) assertEquals(TX_CNT, cacheMetrics.txCommits()); } else { assertEquals(0, metrics.txCommits()); assertEquals(0, cacheMetrics.txCommits()); } assertEquals(0, metrics.txRollbacks()); assertEquals(0, cacheMetrics.txRollbacks()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Map generateBytecodes(String name) throws ClassNotFoundException { XXX map = super.generateBytecodes(name); if (map == null) { return map; } for (Object obj : map.values()) { byte [] bytes = (byte []) obj; nBytes += bytes.length; } return map; }', 'ground_truth': 'public Map generateBytecodes(String name) throws ClassNotFoundException { Map<String, byte[]> map = super.generateBytecodes(name); if (map == null) { return map; } if (destDir != null) { try { for (Map.Entry<String, byte[]> entry : map.entrySet()) { File file = new File(destDir, entry.getKey() + ".class"); FileOutputStream fos = new FileOutputStream(file); fos.write(entry.getValue()); fos.close(); } } catch (IOException e) { throw new RuntimeException(e); } } for (Object obj : map.values()) { byte [] bytes = (byte []) obj; nBytes += bytes.length; } return map; }', 'output': 'public Map<String, byte[]> generateBytecodes(String name) throws ClassNotFoundException { Map<String, byte[]> map = super.generateBytecodes(name); if (map == null) { return map; } for (Object obj : map.values()) { byte [] bytes = (byte []) obj; nBytes += bytes.length; } return map; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGetProperties() { XXX propertyId = new PropertyIdImpl("foo", "category1", false); GreaterPredicate predicate = new GreaterPredicate(propertyId, 10); Set<PropertyId> ids = predicate.getPropertyIds(); Assert.assertEquals(1, ids.size()); Assert.assertTrue(ids.contains(propertyId)); }', 'ground_truth': 'public void testGetProperties() { String propertyId = PropertyHelper.getPropertyId("category1", "foo"); GreaterPredicate predicate = new GreaterPredicate<Integer>(propertyId, 10); Set<String> ids = predicate.getPropertyIds(); Assert.assertEquals(1, ids.size()); Assert.assertTrue(ids.contains(propertyId)); }', 'output': 'public void testGetProperties() { PropertyIdImpl propertyId = new PropertyIdImpl("foo", "category1", false); GreaterPredicate predicate = new GreaterPredicate(propertyId, 10); Set<PropertyId> ids = predicate.getPropertyIds(); Assert.assertEquals(1, ids.size()); Assert.assertTrue(ids.contains(propertyId)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public boolean equals(Object obj) { if (this == obj) return true; if (obj == null || getClass() != obj.getClass()) return false; XXX other = (ClusterDiscoveryMetricsAdapter)obj; return availProcs == other.availProcs && totalPhysicalMemory == other.totalPhysicalMemory && curActiveJobs == other.curActiveJobs && curCancelledJobs == other.curCancelledJobs && curIdleTime == other.curIdleTime && curJobExecTime == other.curJobExecTime && curJobWaitTime == other.curJobWaitTime && curRejectedJobs == other.curRejectedJobs && curWaitingJobs == other.curWaitingJobs && daemonThreadCnt == other.daemonThreadCnt && heapCommitted == other.heapCommitted && heapInit == other.heapInit && heapMax == other.heapMax && heapUsed == other.heapUsed && maxActiveJobs == other.maxActiveJobs && maxCancelledJobs == other.maxCancelledJobs && maxJobExecTime == other.maxJobExecTime && maxJobWaitTime == other.maxJobWaitTime && maxRejectedJobs == other.maxRejectedJobs && maxWaitingJobs == other.maxWaitingJobs && nonHeapCommitted == other.nonHeapCommitted && nonHeapInit == other.nonHeapInit && nonHeapMax == other.nonHeapMax && nonHeapUsed == other.nonHeapUsed && peakThreadCnt == other.peakThreadCnt && rcvdBytesCnt == other.rcvdBytesCnt && outMesQueueSize == other.outMesQueueSize && rcvdMsgsCnt == other.rcvdMsgsCnt && sentBytesCnt == other.sentBytesCnt && sentMsgsCnt == other.sentMsgsCnt && startTime == other.startTime && nodeStartTime == other.nodeStartTime && startedThreadCnt == other.startedThreadCnt && threadCnt == other.threadCnt && totalCancelledJobs == other.totalCancelledJobs && totalExecutedJobs == other.totalExecutedJobs && totalExecTasks == other.totalExecTasks && totalIdleTime == other.totalIdleTime && totalRejectedJobs == other.totalRejectedJobs && upTime == other.upTime; } /** {@inheritDoc} */ @Override public String toString() { return S.toString(ClusterDiscoveryMetricsAdapter.class, this); } } }', 'ground_truth': '@Override public boolean equals(Object obj) { if (this == obj) return true; if (obj == null || getClass() != obj.getClass()) return false; DiscoveryNodeMetricsAdapter other = (DiscoveryNodeMetricsAdapter)obj; return availProcs == other.availProcs && totalPhysicalMemory == other.totalPhysicalMemory && curActiveJobs == other.curActiveJobs && curCancelledJobs == other.curCancelledJobs && curIdleTime == other.curIdleTime && curJobExecTime == other.curJobExecTime && curJobWaitTime == other.curJobWaitTime && curRejectedJobs == other.curRejectedJobs && curWaitingJobs == other.curWaitingJobs && daemonThreadCnt == other.daemonThreadCnt && heapCommitted == other.heapCommitted && heapInit == other.heapInit && heapMax == other.heapMax && heapUsed == other.heapUsed && maxActiveJobs == other.maxActiveJobs && maxCancelledJobs == other.maxCancelledJobs && maxJobExecTime == other.maxJobExecTime && maxJobWaitTime == other.maxJobWaitTime && maxRejectedJobs == other.maxRejectedJobs && maxWaitingJobs == other.maxWaitingJobs && nonHeapCommitted == other.nonHeapCommitted && nonHeapInit == other.nonHeapInit && nonHeapMax == other.nonHeapMax && nonHeapUsed == other.nonHeapUsed && peakThreadCnt == other.peakThreadCnt && rcvdBytesCnt == other.rcvdBytesCnt && outMesQueueSize == other.outMesQueueSize && rcvdMsgsCnt == other.rcvdMsgsCnt && sentBytesCnt == other.sentBytesCnt && sentMsgsCnt == other.sentMsgsCnt && startTime == other.startTime && nodeStartTime == other.nodeStartTime && startedThreadCnt == other.startedThreadCnt && threadCnt == other.threadCnt && totalCancelledJobs == other.totalCancelledJobs && totalExecutedJobs == other.totalExecutedJobs && totalExecTasks == other.totalExecTasks && totalIdleTime == other.totalIdleTime && totalRejectedJobs == other.totalRejectedJobs && upTime == other.upTime; }', 'output': '@Override public boolean equals(Object obj) { if (this == obj) return true; if (obj == null || getClass() != obj.getClass()) return false; ClusterDiscoveryMetricsAdapter other = (ClusterDiscoveryMetricsAdapter)obj; return availProcs == other.availProcs && totalPhysicalMemory == other.totalPhysicalMemory && curActiveJobs == other.curActiveJobs && curCancelledJobs == other.curCancelledJobs && curIdleTime == other.curIdleTime && curJobExecTime == other.curJobExecTime && curJobWaitTime == other.curJobWaitTime && curRejectedJobs == other.curRejectedJobs && curWaitingJobs == other.curWaitingJobs && daemonThreadCnt == other.daemonThreadCnt && heapCommitted == other.heapCommitted && heapInit == other.heapInit && heapMax == other.heapMax && heapUsed == other.heapUsed && maxActiveJobs == other.maxActiveJobs && maxCancelledJobs == other.maxCancelledJobs && maxJobExecTime == other.maxJobExecTime && maxJobWaitTime == other.maxJobWaitTime && maxRejectedJobs == other.maxRejectedJobs && maxWaitingJobs == other.maxWaitingJobs && nonHeapCommitted == other.nonHeapCommitted && nonHeapInit == other.nonHeapInit && nonHeapMax == other.nonHeapMax && nonHeapUsed == other.nonHeapUsed && peakThreadCnt == other.peakThreadCnt && rcvdBytesCnt == other.rcvdBytesCnt && outMesQueueSize == other.outMesQueueSize && rcvdMsgsCnt == other.rcvdMsgsCnt && sentBytesCnt == other.sentBytesCnt && sentMsgsCnt == other.sentMsgsCnt && startTime == other.startTime && nodeStartTime == other.nodeStartTime && startedThreadCnt == other.startedThreadCnt && threadCnt == other.threadCnt && totalCancelledJobs == other.totalCancelledJobs && totalExecutedJobs == other'}
{'instruction': 'Please refactor the following code:', 'input': 'public CompressionMetadata open(long dataLength, long compressedLength, OpenType type) { XXX offsets = this.offsets; int count = this.count; switch (type) { case FINAL: case SHARED_FINAL: if (this.offsets.size() != count * 8L) { offsets = this.offsets.copy(count * 8L); this.offsets.unreference(); } this.offsets = null; if (type == OpenType.SHARED_FINAL) { this.offsets = offsets; this.offsets.reference(); } break; case SHARED: assert dataLength % parameters.chunkLength() == 0; count = (int) (dataLength / parameters.chunkLength()); if (count < this.count) compressedLength = offsets.getLong(count * 8); break; default: throw new AssertionError(); } return new CompressionMetadata(filePath, parameters, offsets, count * 8L, dataLength, compressedLength, Descriptor.Version.CURRENT.hasPostCompressionAdlerChecksums); }', 'ground_truth': 'public CompressionMetadata open(long dataLength, long compressedLength, OpenType type) { SafeMemory offsets = this.offsets; int count = this.count; switch (type) { case FINAL: case SHARED_FINAL: if (this.offsets.size() != count * 8L) { offsets = this.offsets.copy(count * 8L); this.offsets.free(); } this.offsets = null; if (type == OpenType.SHARED_FINAL) this.offsets = offsets.sharedCopy(); break; case SHARED: assert dataLength % parameters.chunkLength() == 0; count = (int) (dataLength / parameters.chunkLength()); if (count < this.count) compressedLength = offsets.getLong(count * 8); break; default: throw new AssertionError(); } return new CompressionMetadata(filePath, parameters, offsets, count * 8L, dataLength, compressedLength, Descriptor.Version.CURRENT.hasPostCompressionAdlerChecksums); }', 'output': 'public CompressionMetadata open(long dataLength, long compressedLength, OpenType type) { LongOffsets offsets = this.offsets; int count = this.count; switch (type) { case FINAL: case SHARED_FINAL: if (this.offsets.size() != count * 8L) { offsets = this.offsets.copy(count * 8L); this.offsets.unreference(); } this.offsets = null; if (type == OpenType.SHARED_FINAL) { this.offsets = offsets; this.offsets.reference(); } break; case SHARED: assert dataLength % parameters.chunkLength() == 0; count = (int) (dataLength / parameters.chunkLength()); if (count < this.count) compressedLength = offsets.getLong(count * 8); break; default: throw new AssertionError(); } return new CompressionMetadata(filePath, parameters, offsets, count * 8L, dataLength, compressedLength, Descriptor.Version.CURRENT.hasPostCompressionAdlerChecksums); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void addValidatorToEnsureUniqueLogicalTypeNames(ProgrammingModel pm) { final _Multimaps.ListMultimap<String, ObjectSpecification> collidingSpecsByLogicalTypeName = _Multimaps.newConcurrentListMultimap(); final XXX ensureUniqueObjectIds = new MetaModelValidatorVisiting.SummarizingVisitor(){ @Override public boolean visit(ObjectSpecification objSpec) { collidingSpecsByLogicalTypeName.putElement(objSpec.getLogicalTypeName() , objSpec); return true; } @Override public void summarize() { for (val logicalTypeName : collidingSpecsByLogicalTypeName.keySet()) { val collidingSpecs = collidingSpecsByLogicalTypeName.get(logicalTypeName); val isCollision = collidingSpecs.size()>1; if(isCollision) { val csv = asCsv(collidingSpecs); collidingSpecs.forEach(spec->{ DeficiencyFacet.appendToWithFormat( spec, "Logical-type-name (aka. object-type) \'%s\' mapped to multiple classes: %s", logicalTypeName, csv); }); } } collidingSpecsByLogicalTypeName.clear(); } private String asCsv(final List<ObjectSpecification> specList) { return stream(specList) .map(ObjectSpecification::getFullIdentifier) .collect(Collectors.joining(",")); } }; pm.addValidatorSkipManagedBeans(ensureUniqueObjectIds); }', 'ground_truth': 'private void addValidatorToEnsureUniqueLogicalTypeNames(ProgrammingModel pm) { final _Multimaps.ListMultimap<String, ObjectSpecification> collidingSpecsByLogicalTypeName = _Multimaps.newConcurrentListMultimap(); final MetaModelVisitingValidatorAbstract ensureUniqueObjectIds = new MetaModelVisitingValidatorAbstract(){ @Override public void validate(ObjectSpecification objSpec) { if(objSpec.isManagedBean()) { return; } collidingSpecsByLogicalTypeName.putElement(objSpec.getLogicalTypeName() , objSpec); } @Override public void summarize() { for (val logicalTypeName : collidingSpecsByLogicalTypeName.keySet()) { val collidingSpecs = collidingSpecsByLogicalTypeName.get(logicalTypeName); val isCollision = collidingSpecs.size()>1; if(isCollision) { val csv = asCsv(collidingSpecs); collidingSpecs.forEach(spec->{ ValidationFailure.raiseFormatted( spec, "Logical-type-name (aka. object-type) \'%s\' mapped to multiple classes: %s", logicalTypeName, csv); }); } } collidingSpecsByLogicalTypeName.clear(); } private String asCsv(final List<ObjectSpecification> specList) { return stream(specList) .map(ObjectSpecification::getFullIdentifier) .collect(Collectors.joining(",")); } }; pm.addValidator(ensureUniqueObjectIds); }', 'output': 'private void addValidatorToEnsureUniqueLogicalTypeNames(ProgrammingModel pm) { final _Multimaps.ListMultimap<String, ObjectSpecification> collidingSpecsByLogicalTypeName = _Multimaps.newConcurrentListMultimap(); final Validator ensureUniqueObjectIds = new MetaModelValidatorVisiting.SummarizingVisitor(){ @Override public boolean visit(ObjectSpecification objSpec) { collidingSpecsByLogicalTypeName.putElement(objSpec.getLogicalTypeName() , objSpec); return true; } @Override public void summarize() { for (val logicalTypeName : collidingSpecsByLogicalTypeName.keySet()) { val collidingSpecs = collidingSpecsByLogicalTypeName.get(logicalTypeName); val isCollision = collidingSpecs.size()>1; if(isCollision) { val csv = asCsv(collidingSpecs); collidingSpecs.forEach(spec->{ DeficiencyFacet.appendToWithFormat( spec, "Logical-type-name (aka. object-type) \'%s\' mapped to multiple classes: %s", logicalTypeName, csv); }); } } collidingSpecsByLogicalTypeName.clear(); } private String asCsv(final List<ObjectSpecification> specList) { return stream(specList) .map(ObjectSpecification::getFullIdentifier) .collect(Collectors.joining(",")); } }; pm.addValidatorSkipManagedBeans(ensureUniqueObjectIds); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void validateKeyRange(CFMetaData metadata, ByteBuffer superColumn, KeyRange range) throws org.apache.cassandra.exceptions.InvalidRequestException { if ((range.start_key == null) == (range.start_token == null) || (range.end_key == null) == (range.end_token == null)) { throw new org.apache.cassandra.exceptions.InvalidRequestException("exactly one each of {start key, start token} and {end key, end token} must be specified"); } if (range.start_token != null && range.end_key != null) throw new org.apache.cassandra.exceptions.InvalidRequestException("start token + end key is not a supported key range"); IPartitioner p = StorageService.getPartitioner(); if (range.start_key != null && range.end_key != null) { Token startToken = p.getToken(range.start_key); Token endToken = p.getToken(range.end_key); if (startToken.compareTo(endToken) > 0 && !endToken.isMinimum()) { if (p.preservesOrder()) throw new org.apache.cassandra.exceptions.InvalidRequestException("start key must sort before (or equal to) finish key in your partitioner!"); else throw new org.apache.cassandra.exceptions.InvalidRequestException("start key\'s token sorts after end key\'s token.  this is not allowed; you probably should not specify end key at all except with an ordered partitioner"); } } else if (range.start_key != null && range.end_token != null) { XXX stop = p.getTokenFactory().fromString(range.end_token).maxKeyBound(); if (RowPosition.ForKey.get(range.start_key, p).compareTo(stop) > 0 && !stop.isMinimum()) throw new org.apache.cassandra.exceptions.InvalidRequestException("Start key\'s token sorts after end token"); } validateFilterClauses(metadata, range.row_filter); if (!isEmpty(range.row_filter) && superColumn != null) { throw new org.apache.cassandra.exceptions.InvalidRequestException("super columns are not supported for indexing"); } if (range.count <= 0) { throw new org.apache.cassandra.exceptions.InvalidRequestException("maxRows must be positive"); } }', 'ground_truth': 'public static void validateKeyRange(CFMetaData metadata, ByteBuffer superColumn, KeyRange range) throws org.apache.cassandra.exceptions.InvalidRequestException { if ((range.start_key == null) == (range.start_token == null) || (range.end_key == null) == (range.end_token == null)) { throw new org.apache.cassandra.exceptions.InvalidRequestException("exactly one each of {start key, start token} and {end key, end token} must be specified"); } if (range.start_token != null && range.end_key != null) throw new org.apache.cassandra.exceptions.InvalidRequestException("start token + end key is not a supported key range"); IPartitioner p = StorageService.getPartitioner(); if (range.start_key != null && range.end_key != null) { Token startToken = p.getToken(range.start_key); Token endToken = p.getToken(range.end_key); if (startToken.compareTo(endToken) > 0 && !endToken.isMinimum()) { if (p.preservesOrder()) throw new org.apache.cassandra.exceptions.InvalidRequestException("start key must sort before (or equal to) finish key in your partitioner!"); else throw new org.apache.cassandra.exceptions.InvalidRequestException("start key\'s token sorts after end key\'s token.  this is not allowed; you probably should not specify end key at all except with an ordered partitioner"); } } else if (range.start_key != null && range.end_token != null) { PartitionPosition stop = p.getTokenFactory().fromString(range.end_token).maxKeyBound(); if (PartitionPosition.ForKey.get(range.start_key, p).compareTo(stop) > 0 && !stop.isMinimum()) throw new org.apache.cassandra.exceptions.InvalidRequestException("Start key\'s token sorts after end token"); } validateFilterClauses(metadata, range.row_filter); if (!isEmpty(range.row_filter) && superColumn != null) { throw new org.apache.cassandra.exceptions.InvalidRequestException("super columns are not supported for indexing"); } if (range.count <= 0) { throw new org.apache.cassandra.exceptions.InvalidRequestException("maxRows must be positive"); } }', 'output': 'public static void validateKeyRange(CFMetaData metadata, ByteBuffer superColumn, KeyRange range) throws org.apache.cassandra.exceptions.InvalidRequestException { if ((range.start_key == null) == (range.start_token == null) || (range.end_key == null) == (range.end_token == null)) { throw new org.apache.cassandra.exceptions.InvalidRequestException("exactly one each of {start key, start token} and {end key, end token} must be specified"); } if (range.start_token != null && range.end_key != null) throw new org.apache.cassandra.exceptions.InvalidRequestException("start token + end key is not a supported key range"); IPartitioner p = StorageService.getPartitioner(); if (range.start_key != null && range.end_key != null) { Token startToken = p.getToken(range.start_key); Token endToken = p.getToken(range.end_key); if (startToken.compareTo(endToken) > 0 && !endToken.isMinimum()) { if (p.preservesOrder()) throw new org.apache.cassandra.exceptions.InvalidRequestException("start key must sort before (or equal to) finish key in your partitioner!"); else throw new org.apache.cassandra.exceptions.InvalidRequestException("start key\'s token sorts after end key\'s token.  this is not allowed; you probably should not specify end key at all except with an ordered partitioner"); } } else if (range.start_key != null && range.end_token != null) { Token stop = p.getTokenFactory().fromString(range.end_token).maxKeyBound(); if (RowPosition.ForKey.get(range.start_key, p).compareTo(stop) > 0 && !stop.isMinimum()) throw new org.apache.cassandra.exceptions.InvalidRequestException("Start key\'s token sorts after end token"); } validateFilterClauses(metadata, range.row_filter); if (!isEmpty(range.row_filter) && superColumn != null) { throw new org.apache.cassandra.exceptions.InvalidRequestException("super columns are not supported for indexing"); } if ('}
{'instruction': 'Please refactor the following code:', 'input': 'public void run() { Map<K,V> expungedValues = new HashMap<K,V>(); synchronized(cache_) { Enumeration<K> e = cache_.keys(); while( e.hasMoreElements() ) { K key = e.nextElement(); XXX co = cache_.get(key); if ( co != null && co.isReadyToDie(expiration_) ) { V v = co.getValue(); if(null != v) expungedValues.put(key, v); cache_.remove(key); } } } /* Calling the hooks on the keys that have been expunged */ Set<K> keys = expungedValues.keySet(); for ( K key : keys ) { V value = expungedValues.get(key); ICacheExpungeHook<K,V> hook = hooks_.remove(key); if (hook != null) { hook.callMe(key, value); } else if (globalHook_ != null) { globalHook_.callMe(key, value); } } expungedValues.clear(); }', 'ground_truth': 'public void run() { Map<K, V> expungedValues = new HashMap<K, V>(); synchronized (cache_) { Enumeration<K> e = cache_.keys(); while (e.hasMoreElements()) { K key = e.nextElement(); CacheableObject co = cache_.get(key); if (co != null && co.isReadyToDie(expiration_)) { V v = co.getValue(); if (null != v) { expungedValues.put(key, v); } cache_.remove(key); } } } /* Calling the hooks on the keys that have been expunged */ Set<K> keys = expungedValues.keySet(); for (K key : keys) { V value = expungedValues.get(key); ICacheExpungeHook<K, V> hook = hooks_.remove(key); if (hook != null) { hook.callMe(key, value); } else if (globalHook_ != null) { globalHook_.callMe(key, value); } } expungedValues.clear(); }', 'output': 'public void run() { Map<K,V> expungedValues = new HashMap<K,V>(); synchronized(cache_) { Enumeration<K> e = cache_.keys(); while( e.hasMoreElements() ) { K key = e.nextElement(); CacheEntry<K,V> co = cache_.get(key); if ( co != null && co.isReadyToDie(expiration_) ) { V v = co.getValue(); if(null != v) expungedValues.put(key, v); cache_.remove(key); } } } /* Calling the hooks on the keys that have been expunged */ Set<K> keys = expungedValues.keySet(); for ( K key : keys ) { V value = expungedValues.get(key); ICacheExpungeHook<K,V> hook = hooks_.remove(key); if (hook != null) { hook.callMe(key, value); } else if (globalHook_ != null) { globalHook_.callMe(key, value); } } expungedValues.clear(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMoveFile() throws Exception { final List<Event> evtList = new ArrayList<>(); final int evtsCnt = 5 + 1; final CountDownLatch latch = new CountDownLatch(evtsCnt); grid(1).events().localListen(lsnr = new IgnitePredicate<Event>() { @Override public boolean apply(Event evt) { log.info("Received event [evt=" + evt + \']\'); evtList.add(evt); latch.countDown(); return true; } }, EVTS_GGFS); IgfsPath dir = new IgfsPath("/dir1/dir2"); IgfsPath file = new IgfsPath(dir, "file1"); ggfs.create(file, true).close(); ggfs.rename(file, dir.parent()); assertTrue(ggfs.exists(new IgfsPath(dir.parent(), file.name()))); assertTrue(latch.await(10, TimeUnit.SECONDS)); assertEquals(evtsCnt, evtList.size()); IgniteFsEvent evt = (IgniteFsEvent)evtList.get(0); assertEquals(EVT_GGFS_DIR_CREATED, evt.type()); assertEquals(new IgfsPath("/dir1"), evt.path()); evt = (IgniteFsEvent)evtList.get(1); assertEquals(EVT_GGFS_DIR_CREATED, evt.type()); assertEquals(new IgfsPath("/dir1/dir2"), evt.path()); evt = (IgniteFsEvent)evtList.get(2); assertEquals(EVT_GGFS_FILE_CREATED, evt.type()); assertEquals(new IgfsPath("/dir1/dir2/file1"), evt.path()); evt = (IgniteFsEvent)evtList.get(3); assertEquals(EVT_GGFS_FILE_OPENED_WRITE, evt.type()); assertEquals(new IgfsPath("/dir1/dir2/file1"), evt.path()); evt = (IgniteFsEvent)evtList.get(4); assertEquals(EVT_GGFS_FILE_CLOSED_WRITE, evt.type()); assertEquals(new IgfsPath("/dir1/dir2/file1"), evt.path()); assertEquals(0, evt.dataSize()); XXX evt4 = (IgniteFsEvent)evtList.get(5); assertEquals(EVT_GGFS_FILE_RENAMED, evt4.type()); assertEquals(new IgfsPath("/dir1/dir2/file1"), evt4.path()); assertEquals(new IgfsPath("/dir1/file1"), evt4.newPath()); }', 'ground_truth': 'public void testMoveFile() throws Exception { final List<Event> evtList = new ArrayList<>(); final int evtsCnt = 5 + 1; final CountDownLatch latch = new CountDownLatch(evtsCnt); grid(1).events().localListen(lsnr = new IgnitePredicate<Event>() { @Override public boolean apply(Event evt) { log.info("Received event [evt=" + evt + \']\'); evtList.add(evt); latch.countDown(); return true; } }, EVTS_GGFS); IgfsPath dir = new IgfsPath("/dir1/dir2"); IgfsPath file = new IgfsPath(dir, "file1"); ggfs.create(file, true).close(); ggfs.rename(file, dir.parent()); assertTrue(ggfs.exists(new IgfsPath(dir.parent(), file.name()))); assertTrue(latch.await(10, TimeUnit.SECONDS)); assertEquals(evtsCnt, evtList.size()); IgfsEvent evt = (IgfsEvent)evtList.get(0); assertEquals(EVT_GGFS_DIR_CREATED, evt.type()); assertEquals(new IgfsPath("/dir1"), evt.path()); evt = (IgfsEvent)evtList.get(1); assertEquals(EVT_GGFS_DIR_CREATED, evt.type()); assertEquals(new IgfsPath("/dir1/dir2"), evt.path()); evt = (IgfsEvent)evtList.get(2); assertEquals(EVT_GGFS_FILE_CREATED, evt.type()); assertEquals(new IgfsPath("/dir1/dir2/file1"), evt.path()); evt = (IgfsEvent)evtList.get(3); assertEquals(EVT_GGFS_FILE_OPENED_WRITE, evt.type()); assertEquals(new IgfsPath("/dir1/dir2/file1"), evt.path()); evt = (IgfsEvent)evtList.get(4); assertEquals(EVT_GGFS_FILE_CLOSED_WRITE, evt.type()); assertEquals(new IgfsPath("/dir1/dir2/file1"), evt.path()); assertEquals(0, evt.dataSize()); IgfsEvent evt4 = (IgfsEvent)evtList.get(5); assertEquals(EVT_GGFS_FILE_RENAMED, evt4.type()); assertEquals(new IgfsPath("/dir1/dir2/file1"), evt4.path()); assertEquals(new IgfsPath("/dir1/file1"), evt4.newPath()); }', 'output': 'public void testMoveFile() throws Exception { final List<Event> evtList = new ArrayList<>(); final int evtsCnt = 5 + 1; final CountDownLatch latch = new CountDownLatch(evtsCnt); grid(1).events().localListen(lsnr = new IgfsPredicate<Event>() { @Override public boolean apply(Event evt) { log.info("Received event [evt=" + evt + \']\'); evtList.add(evt); latch.countDown(); return true; } }, EVTS_GGFS); IgfsPath dir = new IgfsPath("/dir1/dir2"); IgfsPath file = new IgfsPath(dir, "file1"); ggfs.create(file, true).close(); ggfs.rename(file, dir.parent()); assertTrue(ggfs.exists(new IgfsPath(dir.parent(), file.name()))); assertTrue(latch.await(10, TimeUnit.SECONDS)); assertEquals(evtsCnt, evtList.size()); IgniteFsEvent evt = (IgniteFsEvent)evtList.get(0); assertEquals(EVT_GGFS_DIR_CREATED, evt.type()); assertEquals(new IgfsPath("/dir1"), evt.path()); evt = (IgniteFsEvent)evtList.get(1); assertEquals(EVT_GGFS_DIR_CREATED, evt.type()); assertEquals(new IgfsPath("/dir1/dir2"), evt.path()); evt = (IgniteFsEvent)evtList.get(2); assertEquals(EVT_GGFS_FILE_CREATED, evt.type()); assertEquals(new IgfsPath("/dir1/dir2/file1"), evt.path()); evt = (IgniteFsEvent)evtList.get(3); assertEquals(EVT_GGFS_FILE_OPENED_WRITE, evt.type()); assertEquals(new IgfsPath("/dir1/dir2/file1"), evt.path()); evt = (IgniteFsEvent)evtList.get(4); assertEquals(EVT_GGFS_FILE_CLOSED_WRITE, evt'}
{'instruction': 'Please refactor the following code:', 'input': 'public RelTraitSet plus(RelTrait trait) { if (contains(trait)) { return this; } int i = findIndex(trait.getTraitDef()); if (i >= 0) { return replace(i, trait); } final RelTrait canonizedTrait = canonize(trait); XXX newTraits = Arrays.copyOf(traits, traits.length + 1); newTraits[newTraits.length - 1] = canonizedTrait; return cache.getOrAdd(new RelTraitSet(cache, newTraits)); } public RelTraitSet plusAll(RelTrait[] traits) { RelTraitSet t = this; for (RelTrait trait : traits) { t = t.plus(trait); } return t; }', 'ground_truth': 'public RelTraitSet plus(RelTrait trait) { if (contains(trait)) { return this; } int i = findIndex(trait.getTraitDef()); if (i >= 0) { return replace(i, trait); } final RelTrait canonizedTrait = canonize(trait); List<RelTrait> newTraits; switch (traits.length) { case 0: newTraits = ImmutableList.of(canonizedTrait); break; case 1: newTraits = FlatLists.of(traits[0], canonizedTrait); break; case 2: newTraits = FlatLists.of(traits[0], traits[1], canonizedTrait); break; default: newTraits = ImmutableList.<RelTrait>builder().add(traits) .add(canonizedTrait).build(); } return cache.getOrAdd(newTraits); }', 'output': 'public RelTraitSet plus(RelTrait trait) { if (contains(trait)) { return this; } int i = findIndex(trait.getTraitDef()); if (i >= 0) { return replace(i, trait); } final RelTrait canonizedTrait = canonize(trait); final List<RelTrait> newTraits = new ArrayList<>(traits); newTraits.add(canonizedTrait); return cache.getOrAdd(new RelTraitSet(cache, newTraits)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void removeQueryResult(@Nullable UUID sndId, long reqId) { if (sndId == null) return; Map<Long, GridFutureAdapter<QueryResult<K, V>>> futs = qryIters.get(sndId); if (futs != null) { IgniteFuture<QueryResult<K, V>> fut; synchronized (futs) { fut = futs.remove(reqId); } if (fut != null) { try { fut.get().closeIfNotShared(recipient(sndId, reqId)); } catch (IgniteCheckedException e) { U.error(log, "Failed to close iterator.", e); } } } }', 'ground_truth': 'protected void removeQueryResult(@Nullable UUID sndId, long reqId) { if (sndId == null) return; Map<Long, GridFutureAdapter<QueryResult<K, V>>> futs = qryIters.get(sndId); if (futs != null) { IgniteInternalFuture<QueryResult<K, V>> fut; synchronized (futs) { fut = futs.remove(reqId); } if (fut != null) { try { fut.get().closeIfNotShared(recipient(sndId, reqId)); } catch (IgniteCheckedException e) { U.error(log, "Failed to close iterator.", e); } } } }', 'output': 'protected void removeQueryResult(@Nullable UUID sndId, long reqId) { if (sndId == null) return; Map<Long, GridFutureAdapter<QueryResult<K, V>>> futs = qryIters.get(sndId); if (futs != null) { IgniteFuture<QueryResult<K, V>> fut; synchronized (futs) { fut = futs.remove(reqId); } if (fut != null) { try { fut.get().closeIfNotShared(recipient(sndId, reqId)); } catch (IgniteCheckedException e) { U.error(log, "Failed to close iterator.", e); } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void onLifecycleEvent(LifecycleEventType type) throws IgniteCheckedException { if (type == LifecycleEventType.AFTER_GRID_START) { GridCache<GridTestKey, Long> cache = g.cache("partitioned"); assert cache != null; cache.loadCache(null, 0, GridTestConstants.LOAD_THREADS, GridTestConstants.ENTRY_COUNT); } } } }', 'ground_truth': '@Override public void onLifecycleEvent(LifecycleEventType type) { if (type == LifecycleEventType.AFTER_GRID_START) { IgniteCache<GridTestKey, Long> cache = g.jcache("partitioned"); assert cache != null; cache.loadCache(null, 0, GridTestConstants.LOAD_THREADS, GridTestConstants.ENTRY_COUNT); } }', 'output': '@Override public void onLifecycleEvent(LifecycleEventType type) throws IgniteCheckedException { if (type == LifecycleEventType.AFTER_GRID_START) { Cache<GridTestKey, Long> cache = g.cache("partitioned"); assert cache != null; cache.loadCache(null, 0, GridTestConstants.LOAD_THREADS, GridTestConstants.ENTRY_COUNT); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public boolean contains(Object o) { Entry<K, V> e = map.get(o); return e != null && F.isAll(e, filter); } } }', 'ground_truth': '@Override public boolean contains(Object o) { Cache.Entry<K, V> e = map.get(o); return e != null && F.isAll(e, filter); }', 'output': '@Override public boolean containsKey(Object key) { Entry<K, V> e = map.get(key); return e != null && F.isAll(e, filter); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkTransactions(final GridCacheTxConcurrency concur, final GridCacheTxIsolation isolation, final int jobCnt) throws Exception { info("Grid 0: " + grid(0).localNode().id()); info("Grid 1: " + grid(1).localNode().id()); info("Grid 2: " + grid(2).localNode().id()); info("Grid 3: " + grid(3).localNode().id()); Ignite ignite = grid(0); Collection<GridFuture<?>> futs = new LinkedList<>(); GridCompute comp = ignite.compute().enableAsync(); for (int i = 0; i < jobCnt; i++) { comp.apply(new CX1<Integer, Void>() { @GridInstanceResource private Ignite ignite; @Override public Void applyx(final Integer i) throws GridException { GridCache<String, int[]> cache = this.ignite.cache(null); try (GridCacheTx tx = cache.txStart(concur, isolation)) { int[] arr = cache.get("TestKey"); if (arr == null) arr = new int[jobCnt]; arr[i] = 1; cache.put("TestKey", arr); int c = cntr.getAndIncrement(); if (c % 50 == 0) X.println("Executing transaction [i=" + i + ", c=" + c + \']\'); tx.commit(); } return null; } }, i); futs.add(comp.future()); } for (XXX fut : futs) fut.get(); for (int i = 0; i < GRID_CNT; i++) { GridCacheProjection<String, int[]> c = grid(i).cache(null).projection(String.class, int[].class); try (GridCacheTx tx = c.txStart(concur, isolation)) { int[] arr = c.get("TestKey"); assertNotNull(arr); assertEquals(jobCnt, arr.length); for (int j : arr) assertEquals(1, j); tx.commit(); } } }', 'ground_truth': 'private void checkTransactions(final GridCacheTxConcurrency concur, final GridCacheTxIsolation isolation, final int jobCnt) throws Exception { info("Grid 0: " + grid(0).localNode().id()); info("Grid 1: " + grid(1).localNode().id()); info("Grid 2: " + grid(2).localNode().id()); info("Grid 3: " + grid(3).localNode().id()); Ignite ignite = grid(0); Collection<IgniteFuture<?>> futs = new LinkedList<>(); GridCompute comp = ignite.compute().enableAsync(); for (int i = 0; i < jobCnt; i++) { comp.apply(new CX1<Integer, Void>() { @GridInstanceResource private Ignite ignite; @Override public Void applyx(final Integer i) throws GridException { GridCache<String, int[]> cache = this.ignite.cache(null); try (GridCacheTx tx = cache.txStart(concur, isolation)) { int[] arr = cache.get("TestKey"); if (arr == null) arr = new int[jobCnt]; arr[i] = 1; cache.put("TestKey", arr); int c = cntr.getAndIncrement(); if (c % 50 == 0) X.println("Executing transaction [i=" + i + ", c=" + c + \']\'); tx.commit(); } return null; } }, i); futs.add(comp.future()); } for (IgniteFuture<?> fut : futs) fut.get(); for (int i = 0; i < GRID_CNT; i++) { GridCacheProjection<String, int[]> c = grid(i).cache(null).projection(String.class, int[].class); try (GridCacheTx tx = c.txStart(concur, isolation)) { int[] arr = c.get("TestKey"); assertNotNull(arr); assertEquals(jobCnt, arr.length); for (int j : arr) assertEquals(1, j); tx.commit(); } } }', 'output': 'private void checkTransactions(final GridCacheTxConcurrency concur, final GridCacheTxIsolation isolation, final int jobCnt) throws Exception { info("Grid 0: " + grid(0).localNode().id()); info("Grid 1: " + grid(1).localNode().id()); info("Grid 2: " + grid(2).localNode().id()); info("Grid 3: " + grid(3).localNode().id()); Ignite ignite = grid(0); Collection<GridFuture<?>> futs = new LinkedList<>(); GridCompute comp = ignite.compute().enableAsync(); for (int i = 0; i < jobCnt; i++) { comp.apply(new CX1<Integer, Void>() { @GridInstanceResource private Ignite ignite; @Override public Void applyx(final Integer i) throws GridException { GridCache<String, int[]> cache = this.ignite.cache(null); try (GridCacheTx tx = cache.txStart(concur, isolation)) { int[] arr = cache.get("TestKey"); if (arr == null) arr = new int[jobCnt]; arr[i] = 1; cache.put("TestKey", arr); int c = cntr.getAndIncrement(); if (c % 50 == 0) X.println("Executing transaction [i=" + i + ", c=" + c + \']\'); tx.commit(); } return null; } }, i); futs.add(comp.future()); } for (XXX fut : futs) fut.get(); for (int i = 0; i < GRID_CNT; i++) { GridCacheProjection<String, int[]> c = grid(i).cache(null).projection(String.class, int[].class); try (GridCacheTx tx = c.txStart(concur, isolation)) { int[] arr = c.get("TestKey"); assertNotNull(arr); assertEquals(jobCnt, arr.length); for (int j : arr) assertEquals(1, j); tx.commit(); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private <K> Map<GridNode, Collection<K>> affinityMap(AffinityInfo aff, Collection<? extends K> keys) throws GridException { assert aff != null; assert !F.isEmpty(keys); try { if (keys.size() == 1) return Collections.singletonMap(primary(aff, F.first(keys)), (Collection<K>)keys); Map<GridNode, Collection<K>> map = new GridLeanMap<>(); for (K k : keys) { XXX n = primary(aff, k); Collection<K> mapped = map.get(n); if (mapped == null) map.put(n, mapped = new LinkedList<>()); mapped.add(k); } return map; } catch (GridRuntimeException e) { throw new GridException("Failed to get affinity map for keys: " + keys, e); } }', 'ground_truth': 'private <K> Map<ClusterNode, Collection<K>> affinityMap(AffinityInfo aff, Collection<? extends K> keys) throws GridException { assert aff != null; assert !F.isEmpty(keys); try { if (keys.size() == 1) return Collections.singletonMap(primary(aff, F.first(keys)), (Collection<K>)keys); Map<ClusterNode, Collection<K>> map = new GridLeanMap<>(); for (K k : keys) { ClusterNode n = primary(aff, k); Collection<K> mapped = map.get(n); if (mapped == null) map.put(n, mapped = new LinkedList<>()); mapped.add(k); } return map; } catch (GridRuntimeException e) { throw new GridException("Failed to get affinity map for keys: " + keys, e); } }', 'output': 'private <K> Map<ClusterNode, Collection<K>> affinityMap(AffinityInfo aff, Collection<? extends K> keys) throws GridException { assert aff != null; assert !F.isEmpty(keys); try { if (keys.size() == 1) return Collections.singletonMap(primary(aff, F.first(keys)), (Collection<K>)keys); Map<ClusterNode, Collection<K>> map = new GridLeanMap<>(); for (K k : keys) { ClusterNode n = primary(aff, k); Collection<K> mapped = map.get(n); if (mapped == null) map.put(n, mapped = new LinkedList<>()); mapped.add(k); } return map; } catch (GridRuntimeException e) { throw new GridException("Failed to get affinity map for keys: " + keys, e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Object fromCache(MutableCallSite callSite, Class<?> sender, String methodName, int callID, Boolean safeNavigation, Boolean thisCall, Boolean spreadCall, Object dummyReceiver, Object[] arguments) throws Throwable { XXX fallbackSupplier = () -> fallback(callSite, sender, methodName, callID, safeNavigation, thisCall, spreadCall, dummyReceiver, arguments); MethodHandleWrapper mhw = doWithCallSite( callSite, arguments, (cs, receiver) -> cs.getAndPut( receiver.getClass().getName(), c -> { MethodHandleWrapper fbMhw = fallbackSupplier.get(); return fbMhw.isCanSetTarget() ? fbMhw : NULL_METHOD_HANDLE_WRAPPER; } ) ); if (NULL_METHOD_HANDLE_WRAPPER == mhw) { final MethodHandleWrapper fbMhw = fallbackSupplier.get(); if (fbMhw.isCanSetTarget()) { doWithCallSite(callSite, arguments, (cs, receiver) -> cs.put(receiver.getClass().getName(), fbMhw)); } mhw = fbMhw; } if (mhw.isCanSetTarget() && (callSite.getTarget() != mhw.getTargetMethodHandle()) && (mhw.getLatestHitCount() > INDY_OPTIMIZE_THRESHOLD)) { callSite.setTarget(mhw.getTargetMethodHandle()); if (LOG_ENABLED) LOG.info("call site target set, preparing outside invocation"); mhw.resetLatestHitCount(); } return mhw.getCachedMethodHandle().invokeExact(arguments); }', 'ground_truth': 'public static Object fromCache(MutableCallSite callSite, Class<?> sender, String methodName, int callID, Boolean safeNavigation, Boolean thisCall, Boolean spreadCall, Object dummyReceiver, Object[] arguments) throws Throwable { FallbackSupplier fallbackSupplier = new FallbackSupplier(callSite, sender, methodName, callID, safeNavigation, thisCall, spreadCall, dummyReceiver, arguments); MethodHandleWrapper mhw = doWithCallSite( callSite, arguments, (cs, receiver) -> cs.getAndPut( receiver.getClass().getName(), c -> { MethodHandleWrapper fbMhw = fallbackSupplier.get(); return fbMhw.isCanSetTarget() ? fbMhw : NULL_METHOD_HANDLE_WRAPPER; } ) ); if (NULL_METHOD_HANDLE_WRAPPER == mhw) { final MethodHandleWrapper fbMhw = fallbackSupplier.get(); if (fbMhw.isCanSetTarget()) { doWithCallSite(callSite, arguments, (cs, receiver) -> cs.put(receiver.getClass().getName(), fbMhw)); } mhw = fbMhw; } if (mhw.isCanSetTarget() && (callSite.getTarget() != mhw.getTargetMethodHandle()) && (mhw.getLatestHitCount() > INDY_OPTIMIZE_THRESHOLD)) { callSite.setTarget(mhw.getTargetMethodHandle()); if (LOG_ENABLED) LOG.info("call site target set, preparing outside invocation"); mhw.resetLatestHitCount(); } return mhw.getCachedMethodHandle().invokeExact(arguments); }', 'output': 'public static Object fromCache(MutableCallSite callSite, Class<?> sender, String methodName, int callID, Boolean safeNavigation, Boolean thisCall, Boolean spreadCall, Object dummyReceiver, Object[] arguments) throws Throwable { MethodHandleWrapper mhw = doWithCallSite( callSite, arguments, (cs, receiver) -> cs.getAndPut( receiver.getClass().getName(), c -> { MethodHandleWrapper fbMhw = fallbackSupplier.get(); return fbMhw.isCanSetTarget() ? fbMhw : NULL_METHOD_HANDLE_WRAPPER; } ) ); if (NULL_METHOD_HANDLE_WRAPPER == mhw) { final MethodHandleWrapper fbMhw = fallbackSupplier.get(); if (fbMhw.isCanSetTarget()) { doWithCallSite(callSite, arguments, (cs, receiver) -> cs.put(receiver.getClass().getName(), fbMhw)); } mhw = fbMhw; } if (mhw.isCanSetTarget() && (callSite.getTarget() != mhw.getTargetMethodHandle()) && (mhw.getLatestHitCount() > INDY_OPTIMIZE_THRESHOLD)) { callSite.setTarget(mhw.getTargetMethodHandle()); if (LOG_ENABLED) LOG.info("call site target set, preparing outside invocation"); mhw.resetLatestHitCount(); } return mhw.getCachedMethodHandle().invokeExact(arguments); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testNearRead() throws Exception { Ignite g0 = grid(0); GridCache<Integer, Integer> cache0 = g0.cache(null); int key; for (int i = 0; ; i++) { if (!cache0.affinity().isPrimaryOrBackup(g0.cluster().localNode(), i)) { cache0.get(i); cache0.get(i); key = i; info("Writes: " + cache0.metrics().getCachePuts()); info("Reads: " + cache0.metrics().getCacheGets()); info("Hits: " + cache0.metrics().getCacheHits()); info("Misses: " + cache0.metrics().getCacheMisses()); info("Affinity nodes: " + U.nodes2names(cache0.affinity().mapKeyToPrimaryAndBackups(i))); break; } } for (int j = 0; j < gridCount(); j++) { Ignite g = grid(j); assertEquals(0, g.cache(null).metrics().getCachePuts()); if (g.cache(null).affinity().isPrimary(g.cluster().localNode(), key)) { assertEquals(2, g.cache(null).metrics().getCacheGets()); assertEquals(0, g.cache(null).metrics().getCacheHits()); assertEquals(2, g.cache(null).metrics().getCacheMisses()); } else if (g.cache(null).affinity().isBackup(g.cluster().localNode(), key)){ assertEquals(0, g.cache(null).metrics().getCacheGets()); assertEquals(0, g.cache(null).metrics().getCacheHits()); assertEquals(0, g.cache(null).metrics().getCacheMisses()); } else { assertEquals(2, g.cache(null).metrics().getCacheGets()); assertEquals(0, g.cache(null).metrics().getCacheHits()); assertEquals(2, g.cache(null).metrics().getCacheMisses()); } } }', 'ground_truth': 'public void testNearRead() throws Exception { Ignite g0 = grid(0); IgniteCache<Integer, Integer> cache0 = g0.jcache(null); int key; for (int i = 0; ; i++) { if (!affinity(cache0).isPrimaryOrBackup(g0.cluster().localNode(), i)) { cache0.get(i); cache0.get(i); key = i; info("Writes: " + cache0.metrics().getCachePuts()); info("Reads: " + cache0.metrics().getCacheGets()); info("Hits: " + cache0.metrics().getCacheHits()); info("Misses: " + cache0.metrics().getCacheMisses()); info("Affinity nodes: " + U.nodes2names(affinity(cache0).mapKeyToPrimaryAndBackups(i))); break; } } for (int j = 0; j < gridCount(); j++) { Ignite g = grid(j); IgniteCache<Object, Object> jcache = g.jcache(null); assertEquals(0, jcache.metrics().getCachePuts()); if (affinity(jcache).isPrimary(g.cluster().localNode(), key)) { assertEquals(2, jcache.metrics().getCacheGets()); assertEquals(0, jcache.metrics().getCacheHits()); assertEquals(2, jcache.metrics().getCacheMisses()); } else if (affinity(jcache).isBackup(g.cluster().localNode(), key)){ assertEquals(0, jcache.metrics().getCacheGets()); assertEquals(0, jcache.metrics().getCacheHits()); assertEquals(0, jcache.metrics().getCacheMisses()); } else { assertEquals(2, jcache.metrics().getCacheGets()); assertEquals(0, jcache.metrics().getCacheHits()); assertEquals(2, jcache.metrics().getCacheMisses()); } } }', 'output': 'public void testNearRead() throws Exception { Ignite g0 = grid(0); Cache<Integer, Integer> cache0 = g0.cache(null); int key; for (int i = 0; ; i++) { if (!cache0.affinity().isPrimaryOrBackup(g0.cluster().localNode(), i)) { cache0.get(i); cache0.get(i); key = i; info("Writes: " + cache0.metrics().getCachePuts()); info("Reads: " + cache0.metrics().getCacheGets()); info("Hits: " + cache0.metrics().getCacheHits()); info("Misses: " + cache0.metrics().getCacheMisses()); info("Affinity nodes: " + U.nodes2names(cache0.affinity().mapKeyToPrimaryAndBackups(i))); break; } } for (int j = 0; j < gridCount(); j++) { Ignite g = grid(j); assertEquals(0, g.cache(null).metrics().getCachePuts()); if (g.cache(null).affinity().isPrimary(g.cluster().localNode(), key)) { assertEquals(2, g.cache(null).metrics().getCacheGets()); assertEquals(0, g.cache(null).metrics().getCacheHits()); assertEquals(2, g.cache(null).metrics().getCacheMisses()); } else if (g.cache(null).affinity().isBackup(g.cluster().localNode(), key)){ assertEquals(0, g.cache(null).metrics().getCacheGets()); assertEquals(0, g.cache(null).metrics().getCacheHits()); assertEquals(0, g.cache(null).metrics().getCacheMisses()); } else { assertEquals(2, g.cache(null).metrics().getCacheGets()); assertEquals(0, g.cache(null).metrics().getCacheHits()); assertEquals(2, g.cache(null).metrics().getCacheMisses()); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static VisorGgfsConfiguration from(IgniteFsConfiguration ggfs) { VisorGgfsConfiguration cfg = new VisorGgfsConfiguration(); cfg.name(ggfs.getName()); cfg.metaCacheName(ggfs.getMetaCacheName()); cfg.dataCacheName(ggfs.getDataCacheName()); cfg.blockSize(ggfs.getBlockSize()); cfg.prefetchBlocks(ggfs.getPrefetchBlocks()); cfg.streamBufferSize(ggfs.getStreamBufferSize()); cfg.perNodeBatchSize(ggfs.getPerNodeBatchSize()); cfg.perNodeParallelBatchCount(ggfs.getPerNodeParallelBatchCount()); XXX secFs = ggfs.getSecondaryFileSystem(); if (secFs != null) { Map<String, String> props = secFs.properties(); cfg.secondaryHadoopFileSystemUri(props.get(SECONDARY_FS_URI)); cfg.secondaryHadoopFileSystemConfigPath(props.get(SECONDARY_FS_CONFIG_PATH)); } cfg.defaultMode(ggfs.getDefaultMode()); cfg.pathModes(ggfs.getPathModes()); cfg.dualModePutExecutorService(compactClass(ggfs.getDualModePutExecutorService())); cfg.dualModePutExecutorServiceShutdown(ggfs.getDualModePutExecutorServiceShutdown()); cfg.dualModeMaxPendingPutsSize(ggfs.getDualModeMaxPendingPutsSize()); cfg.maxTaskRangeLength(ggfs.getMaximumTaskRangeLength()); cfg.fragmentizerConcurrentFiles(ggfs.getFragmentizerConcurrentFiles()); cfg.fragmentizerLocalWritesRatio(ggfs.getFragmentizerLocalWritesRatio()); cfg.fragmentizerEnabled(ggfs.isFragmentizerEnabled()); cfg.fragmentizerThrottlingBlockLength(ggfs.getFragmentizerThrottlingBlockLength()); cfg.fragmentizerThrottlingDelay(ggfs.getFragmentizerThrottlingDelay()); Map<String, String> endpointCfg = ggfs.getIpcEndpointConfiguration(); cfg.ipcEndpointConfiguration(endpointCfg != null ? endpointCfg.toString() : null); cfg.ipcEndpointEnabled(ggfs.isIpcEndpointEnabled()); cfg.maxSpace(ggfs.getMaxSpaceSize()); cfg.managementPort(ggfs.getManagementPort()); cfg.sequenceReadsBeforePrefetch(ggfs.getSequentialReadsBeforePrefetch()); cfg.trashPurgeTimeout(ggfs.getTrashPurgeTimeout()); return cfg; }', 'ground_truth': 'public static VisorGgfsConfiguration from(IgniteFsConfiguration ggfs) { VisorGgfsConfiguration cfg = new VisorGgfsConfiguration(); cfg.name(ggfs.getName()); cfg.metaCacheName(ggfs.getMetaCacheName()); cfg.dataCacheName(ggfs.getDataCacheName()); cfg.blockSize(ggfs.getBlockSize()); cfg.prefetchBlocks(ggfs.getPrefetchBlocks()); cfg.streamBufferSize(ggfs.getStreamBufferSize()); cfg.perNodeBatchSize(ggfs.getPerNodeBatchSize()); cfg.perNodeParallelBatchCount(ggfs.getPerNodeParallelBatchCount()); IgniteFsFileSystem secFs = ggfs.getSecondaryFileSystem(); if (secFs != null) { Map<String, String> props = secFs.properties(); cfg.secondaryHadoopFileSystemUri(props.get(SECONDARY_FS_URI)); cfg.secondaryHadoopFileSystemConfigPath(props.get(SECONDARY_FS_CONFIG_PATH)); } cfg.defaultMode(ggfs.getDefaultMode()); cfg.pathModes(ggfs.getPathModes()); cfg.dualModePutExecutorService(compactClass(ggfs.getDualModePutExecutorService())); cfg.dualModePutExecutorServiceShutdown(ggfs.getDualModePutExecutorServiceShutdown()); cfg.dualModeMaxPendingPutsSize(ggfs.getDualModeMaxPendingPutsSize()); cfg.maxTaskRangeLength(ggfs.getMaximumTaskRangeLength()); cfg.fragmentizerConcurrentFiles(ggfs.getFragmentizerConcurrentFiles()); cfg.fragmentizerLocalWritesRatio(ggfs.getFragmentizerLocalWritesRatio()); cfg.fragmentizerEnabled(ggfs.isFragmentizerEnabled()); cfg.fragmentizerThrottlingBlockLength(ggfs.getFragmentizerThrottlingBlockLength()); cfg.fragmentizerThrottlingDelay(ggfs.getFragmentizerThrottlingDelay()); Map<String, String> endpointCfg = ggfs.getIpcEndpointConfiguration(); cfg.ipcEndpointConfiguration(endpointCfg != null ? endpointCfg.toString() : null); cfg.ipcEndpointEnabled(ggfs.isIpcEndpointEnabled()); cfg.maxSpace(ggfs.getMaxSpaceSize()); cfg.managementPort(ggfs.getManagementPort()); cfg.sequenceReadsBeforePrefetch(ggfs.getSequentialReadsBeforePrefetch()); cfg.trashPurgeTimeout(ggfs.getTrashPurgeTimeout()); return cfg; }', 'output': 'public static VisorGgfsConfiguration from(IgniteFsConfiguration ggfs) { VisorGgfsConfiguration cfg = new VisorGgfsConfiguration(); cfg.name(ggfs.getName()); cfg.metaCacheName(ggfs.getMetaCacheName()); cfg.dataCacheName(ggfs.getDataCacheName()); cfg.blockSize(ggfs.getBlockSize()); cfg.prefetchBlocks(ggfs.getPrefetchBlocks()); cfg.streamBufferSize(ggfs.getStreamBufferSize()); cfg.perNodeBatchSize(ggfs.getPerNodeBatchSize()); cfg.perNodeParallelBatchCount(ggfs.getPerNodeParallelBatchCount()); IgniteFsSecondaryFileSystem secFs = ggfs.getSecondaryFileSystem(); if (secFs != null) { Map<String, String> props = secFs.properties(); cfg.secondaryHadoopFileSystemUri(props.get(SECONDARY_FS_URI)); cfg.secondaryHadoopFileSystemConfigPath(props.get(SECONDARY_FS_CONFIG_PATH)); } cfg.defaultMode(ggfs.getDefaultMode()); cfg.pathModes(ggfs.getPathModes()); cfg.dualModePutExecutorService(compactClass(ggfs.getDualModePutExecutorService())); cfg.dualModePutExecutorServiceShutdown(ggfs.getDualModePutExecutorServiceShutdown()); cfg.dualModeMaxPendingPutsSize(ggfs.getDualModeMaxPendingPutsSize()); cfg.maxTaskRangeLength(ggfs.getMaximumTaskRangeLength()); cfg.fragmentizerConcurrentFiles(ggfs.getFragmentizerConcurrentFiles()); cfg.fragmentizerLocalWritesRatio(ggfs.getFragmentizerLocalWritesRatio()); cfg.fragmentizerEnabled(ggfs.isFragmentizerEnabled()); cfg.fragmentizerThrottlingBlockLength(ggfs.getFragmentizerThrottlingBlockLength()); cfg.fragmentizerThrottlingDelay(ggfs.getFragmentizerThrottlingDelay()); Map<String, String> endpointCfg = ggfs.getIpcEndpointConfiguration(); cfg.ipcEndpointConfiguration('}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCancelledJobs() throws Exception { final Ignite ignite = G.grid(getTestGridName()); Collection<GridComputeTaskFuture<?>> futs = new ArrayList<>(); IgniteCompute comp = ignite.compute().enableAsync(); for (int i = 1; i <= 10; i++) { comp.execute(CancelledTask.class, null); futs.add(comp.future()); } GridTestUtils.waitForCondition(new GridAbsPredicate() { @Override public boolean apply() { return ignite.cluster().localNode().metrics().getTotalCancelledJobs() > 0; } }, 5000); colSpi.externalCollision(); for (XXX fut : futs) { try { fut.get(); assert false : "Job was not interrupted."; } catch (GridException e) { if (e.hasCause(InterruptedException.class)) throw new GridException("Test run has been interrupted.", e); info("Caught expected exception: " + e.getMessage()); } } int totalCancelledJobs = ignite.cluster().localNode().metrics().getTotalCancelledJobs(); assert totalCancelledJobs == 10 : "Metrics were not updated. Expected 10 got " + totalCancelledJobs; }', 'ground_truth': 'public void testCancelledJobs() throws Exception { final Ignite ignite = G.grid(getTestGridName()); Collection<ComputeTaskFuture<?>> futs = new ArrayList<>(); IgniteCompute comp = ignite.compute().enableAsync(); for (int i = 1; i <= 10; i++) { comp.execute(CancelledTask.class, null); futs.add(comp.future()); } GridTestUtils.waitForCondition(new GridAbsPredicate() { @Override public boolean apply() { return ignite.cluster().localNode().metrics().getTotalCancelledJobs() > 0; } }, 5000); colSpi.externalCollision(); for (ComputeTaskFuture<?> fut : futs) { try { fut.get(); assert false : "Job was not interrupted."; } catch (GridException e) { if (e.hasCause(InterruptedException.class)) throw new GridException("Test run has been interrupted.", e); info("Caught expected exception: " + e.getMessage()); } } int totalCancelledJobs = ignite.cluster().localNode().metrics().getTotalCancelledJobs(); assert totalCancelledJobs == 10 : "Metrics were not updated. Expected 10 got " + totalCancelledJobs; }', 'output': 'public void testCancelledJobs() throws Exception { final Ignite ignite = G.grid(getTestGridName()); Collection<ComputeTaskFuture<?>> futs = new ArrayList<>(); IgniteCompute comp = ignite.compute().enableAsync(); for (int i = 1; i <= 10; i++) { comp.execute(CancelledTask.class, null); futs.add(comp.future()); } GridTestUtils.waitForCondition(new GridAbsPredicate() { @Override public boolean apply() { return ignite.cluster().localNode().metrics().getTotalCancelledJobs() > 0; } }, 5000); colSpi.externalCollision(); for (ComputeTaskFuture<?> fut : futs) { try { fut.get(); assert false : "Job was not interrupted."; } catch (GridException e) { if (e.hasCause(InterruptedException.class)) throw new GridException("Test run has been interrupted.", e); info("Caught expected exception: " + e.getMessage()); } } int totalCancelledJobs = ignite.cluster().localNode().metrics().getTotalCancelledJobs(); assert totalCancelledJobs == 10 : "Metrics were not updated. Expected 10 got " + totalCancelledJobs; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPutAndRollbackCheckDht() throws Exception { doPutAndRollback(); GridCache<Integer, Integer> c = ((GridNearCacheAdapter<Integer, Integer>)cache().<Integer, Integer>cache()).dht(); assert c.entrySet().isEmpty() : "Actual size: " + c.entrySet().size(); }', 'ground_truth': 'public void testPutAndRollbackCheckDht() throws Exception { doPutAndRollback(); Cache<Integer, Integer> c = ((GridNearCacheAdapter<Integer, Integer>)cache().<Integer, Integer>cache()).dht(); assert c.entrySet().isEmpty() : "Actual size: " + c.entrySet().size(); }', 'output': 'public void testPutAndRollbackCheckDht() throws Exception { doPutAndRollback(); IgniteCache<Integer, Integer> c = (IgniteCache<Integer, Integer>)cache(); assert c.entrySet().isEmpty() : "Actual size: " + c.entrySet().size(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void refreshEndpointMap() { for (String seed : seeds_) { try { TSocket socket = new TSocket(seed, port_); TBinaryProtocol binaryProtocol = new TBinaryProtocol(new TFramedTransport(socket)); Cassandra.Client client = new Cassandra.Client(binaryProtocol); socket.open(); List<TokenRange> ring = client.describe_ring(keyspace); BiMap<Token, InetAddress> tokenEndpointMap = HashBiMap.create(); for (TokenRange range : ring) { Token<?> token = StorageService.getPartitioner().getTokenFactory().fromString(range.start_token); String host = range.endpoints.get(0); try { tokenEndpointMap.put(token, InetAddress.getByName(host)); } catch (UnknownHostException e) { throw new AssertionError(e); } } tokenMetadata = new TokenMetadata(tokenEndpointMap); break; } catch (InvalidRequestException e) { throw new RuntimeException(e); } catch (TException e) { /* let the Exception go and try another seed. log this though */ logger_.debug("Error contacting seed " + seed + " " + e.getMessage()); } } }', 'ground_truth': 'public void refreshEndpointMap() { for (String seed : seeds_) { try { TSocket socket = new TSocket(seed, port_); TBinaryProtocol binaryProtocol = new TBinaryProtocol(new TFramedTransport(socket)); Cassandra.Client client = new Cassandra.Client(binaryProtocol); socket.open(); List<TokenRange> ring = client.describe_ring(keyspace); rangeMap = HashMultimap.create(); for (TokenRange range : ring) { Token<?> left = partitioner_.getTokenFactory().fromString(range.start_token); Token<?> right = partitioner_.getTokenFactory().fromString(range.end_token); String host = range.endpoints.get(0); try { rangeMap.put(new Range(left, right, partitioner_), InetAddress.getByName(host)); } catch (UnknownHostException e) { throw new AssertionError(e); } } rangeSet = new HashSet(rangeMap.keySet()); break; } catch (InvalidRequestException e) { throw new RuntimeException(e); } catch (TException e) { /* let the Exception go and try another seed. log this though */ logger_.debug("Error contacting seed " + seed + " " + e.getMessage()); } } }', 'output': 'public void refreshEndpointMap() { for (String seed : seeds_) { try { TSocket socket = new TSocket(seed, port_); TBinaryProtocol binaryProtocol = new TBinaryProtocol(new TFramedTransport(socket)); Cassandra.Client client = new Cassandra.Client(binaryProtocol); socket.open(); List<TokenRange> ring = client.describe_ring(keyspace); BiMap<Token, InetAddress> tokenEndpointMap = HashBiMap.create(); for (TokenRange range : ring) { Token<?> token = StorageService.getPartitioner().getTokenFactory().fromString(range.start_token); String host = range.endpoints.get(0); try { tokenEndpointMap.put(token, InetAddress.getByName(host)); } catch (UnknownHostException e) { throw new AssertionError(e); } } tokenMetadata = new TokenMetadata(tokenEndpointMap); break; } catch (InvalidRequestException e) { throw new RuntimeException(e); } catch (TException e) { /* let the Exception go and try another seed. log this though */ logger_.debug("Error contacting seed " + seed + " " + e.getMessage()); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMergeCombiningValueIntoNewNamespace() throws Exception { CombiningValueStateInternal<Integer, int[], Integer> value1 = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR); CombiningValueStateInternal<Integer, int[], Integer> value2 = underTest.state(NAMESPACE_2, SUM_INTEGER_ADDR); CombiningValueStateInternal<Integer, int[], Integer> value3 = underTest.state(NAMESPACE_3, SUM_INTEGER_ADDR); value1.add(5); value2.add(10); value1.add(6); StateMerging.mergeCombiningValues(Arrays.asList(value1, value2), value3); assertThat(value1.get().read(), Matchers.equalTo(0)); assertThat(value2.get().read(), Matchers.equalTo(0)); assertThat(value3.get().read(), Matchers.equalTo(21)); }', 'ground_truth': 'public void testMergeCombiningValueIntoNewNamespace() throws Exception { AccumulatorCombiningState<Integer, int[], Integer> value1 = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR); AccumulatorCombiningState<Integer, int[], Integer> value2 = underTest.state(NAMESPACE_2, SUM_INTEGER_ADDR); AccumulatorCombiningState<Integer, int[], Integer> value3 = underTest.state(NAMESPACE_3, SUM_INTEGER_ADDR); value1.add(5); value2.add(10); value1.add(6); StateMerging.mergeCombiningValues(Arrays.asList(value1, value2), value3); assertThat(value1.read(), Matchers.equalTo(0)); assertThat(value2.read(), Matchers.equalTo(0)); assertThat(value3.read(), Matchers.equalTo(21)); }', 'output': 'public void testMergeCombiningValueIntoNewNamespace() throws Exception { CombiningValueStateInternal<Integer, int[], Integer> value1 = state(NAMESPACE_1, SUM_INTEGER_ADDR); CombiningValueStateInternal<Integer, int[], Integer> value2 = state(NAMESPACE_2, SUM_INTEGER_ADDR); CombiningValueStateInternal<Integer, int[], Integer> value3 = state(NAMESPACE_3, SUM_INTEGER_ADDR); value1.add(5); value2.add(10); value1.add(6); StateMerging.mergeCombiningValues(Arrays.asList(value1, value2), value3); assertThat(value1.get().read(), Matchers.equalTo(0)); assertThat(value2.get().read(), Matchers.equalTo(0)); assertThat(value3.get().read(), Matchers.equalTo(21)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void simpleLeftJoinTest() { final Flow flow = Flow.create(); ListDataSource<Pair<Integer, String>> left = getLeftDataSource(); ListDataSource<Pair<Integer, Integer>> right = getRightDataSource(); checkLeftJoin(flow, flow.createInput(left), flow.createInput(right)); }', 'ground_truth': 'public void simpleLeftJoinTest() { final Flow flow = Flow.create(); ListDataSource<KV<Integer, String>> left = getLeftDataSource(); ListDataSource<KV<Integer, Integer>> right = getRightDataSource(); checkLeftJoin(flow, flow.createInput(left), flow.createInput(right)); }', 'output': 'public void simpleLeftJoinTest() { final Flow flow = Flow.create(); ListDataSource<Tuple2<Integer, String>> left = getLeftDataSource(); ListDataSource<Tuple2<Integer, Integer>> right = getRightDataSource(); checkLeftJoin(flow, flow.createInput(left), flow.createInput(right)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAtomicReferenceConstantTopologyChange() throws Exception { try { XXX s = cache().dataStructures().atomicReference(STRUCTURE_NAME, 1, true); IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new CA() { @Override public void apply() { try { for (int i = 0; i < TOP_CHANGE_CNT; i++) { String name = UUID.randomUUID().toString(); try { Ignite g = startGrid(name); assert g.cache(null).dataStructures().<Integer>atomicReference(STRUCTURE_NAME, 1, true) .get() > 0; } finally { if (i != TOP_CHANGE_CNT - 1) stopGrid(name); } } } catch (Exception e) { throw F.wrap(e); } } }, TOP_CHANGE_THREAD_CNT, "topology-change-thread"); int val = s.get(); while (!fut.isDone()) { assert s.get() == val; s.set(++val); } fut.get(); for (Ignite g : G.allGrids()) assert g.cache(null).dataStructures().<Integer>atomicReference(STRUCTURE_NAME, 1, true).get() == val; } finally { cache().dataStructures().removeAtomicReference(STRUCTURE_NAME); } }', 'ground_truth': 'public void testAtomicReferenceConstantTopologyChange() throws Exception { try { IgniteAtomicReference<Integer> s = cache().dataStructures().atomicReference(STRUCTURE_NAME, 1, true); IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new CA() { @Override public void apply() { try { for (int i = 0; i < TOP_CHANGE_CNT; i++) { String name = UUID.randomUUID().toString(); try { Ignite g = startGrid(name); assert g.cache(null).dataStructures().<Integer>atomicReference(STRUCTURE_NAME, 1, true) .get() > 0; } finally { if (i != TOP_CHANGE_CNT - 1) stopGrid(name); } } } catch (Exception e) { throw F.wrap(e); } } }, TOP_CHANGE_THREAD_CNT, "topology-change-thread"); int val = s.get(); while (!fut.isDone()) { assert s.get() == val; s.set(++val); } fut.get(); for (Ignite g : G.allGrids()) assert g.cache(null).dataStructures().<Integer>atomicReference(STRUCTURE_NAME, 1, true).get() == val; } finally { cache().dataStructures().removeAtomicReference(STRUCTURE_NAME); } }', 'output': 'public void testAtomicReferenceConstantTopologyChange() throws Exception { try { AtomicReference<Integer> s = cache().dataStructures().atomicReference(STRUCTURE_NAME, 1, true); IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new CA() { @Override public void apply() { try { for (int i = 0; i < TOP_CHANGE_CNT; i++) { String name = UUID.randomUUID().toString(); try { Ignite g = startGrid(name); assert g.cache(null).dataStructures().<Integer>atomicReference(STRUCTURE_NAME, 1, true) .get() > 0; } finally { if (i != TOP_CHANGE_CNT - 1) stopGrid(name); } } } catch (Exception e) { throw F.wrap(e); } } }, TOP_CHANGE_THREAD_CNT, "topology-change-thread"); int val = s.get(); while (!fut.isDone()) { assert s.get() == val; s.set(++val); } fut.get(); for (Ignite g : G.allGrids()) assert g.cache(null).dataStructures().<Integer>atomicReference(STRUCTURE_NAME, 1, true).get() == val; } finally { cache().dataStructures().removeAtomicReference(STRUCTURE_NAME); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testOptimisticTxWriteThrough() throws Exception { GridCache<Integer, String> near = cache(); GridCacheAdapter<Integer, String> dht = dht(); try (IgniteTx tx = cache().txStart(OPTIMISTIC, REPEATABLE_READ) ) { near.putx(2, "2"); near.put(3, "3"); assert "2".equals(near.get(2)); assert "3".equals(near.get(3)); GridCacheEntryEx<Integer, String> entry = dht.peekEx(2); assert entry == null || entry.rawGetOrUnmarshal(false) == null : "Invalid entry: " + entry; assert dht.peek(3) != null; tx.commit(); } assert "2".equals(near.get(2)); assert "3".equals(near.get(3)); assert "2".equals(dht.get(2)); assert "3".equals(dht.get(3)); assertEquals(2, near.size()); assertEquals(2, near.size()); assertEquals(2, dht.size()); assertEquals(2, dht.size()); }', 'ground_truth': 'public void testOptimisticTxWriteThrough() throws Exception { Cache<Integer, String> near = cache(); GridCacheAdapter<Integer, String> dht = dht(); try (IgniteTx tx = cache().txStart(OPTIMISTIC, REPEATABLE_READ) ) { near.putx(2, "2"); near.put(3, "3"); assert "2".equals(near.get(2)); assert "3".equals(near.get(3)); GridCacheEntryEx<Integer, String> entry = dht.peekEx(2); assert entry == null || entry.rawGetOrUnmarshal(false) == null : "Invalid entry: " + entry; assert dht.peek(3) != null; tx.commit(); } assert "2".equals(near.get(2)); assert "3".equals(near.get(3)); assert "2".equals(dht.get(2)); assert "3".equals(dht.get(3)); assertEquals(2, near.size()); assertEquals(2, near.size()); assertEquals(2, dht.size()); assertEquals(2, dht.size()); }', 'output': 'public void testOptimisticTxWriteThrough() throws Exception { Cache<Integer, String> near = cache(); CacheAdapter<Integer, String> dht = dht(); try (IgniteTx tx = cache().txStart(OPTIMISTIC, REPEATABLE_READ) ) { near.putx(2, "2"); near.put(3, "3"); assert "2".equals(near.get(2)); assert "3".equals(near.get(3)); CacheEntryEx<Integer, String> entry = dht.peekEx(2); assert entry == null || entry.rawGetOrUnmarshal(false) == null : "Invalid entry: " + entry; assert dht.peek(3) != null; tx.commit(); } assert "2".equals(near.get(2)); assert "3".equals(near.get(3)); assert "2".equals(dht.get(2)); assert "3".equals(dht.get(3)); assertEquals(2, near.size()); assertEquals(2, near.size()); assertEquals(2, dht.size()); assertEquals(2, dht.size()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkRemove() throws Exception { log.info("Check remove."); Ignite ignite0 = grid(0); GridCache<Integer, Integer> cache0 = ignite0.cache(null); Integer primaryKey = key(ignite0, PRIMARY); log.info("Put from primary."); cache0.put(primaryKey, primaryKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, primaryKey, false); log.info("Remove from primary."); cache0.remove(primaryKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, null, false); if (backups > 0) { Integer backupKey = key(ignite0, BACKUP); log.info("Put from backup."); cache0.put(backupKey, backupKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), backupKey, backupKey, false); log.info("Remove from backup."); cache0.remove(backupKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), backupKey, null, false); } }', 'ground_truth': 'private void checkRemove() throws Exception { log.info("Check remove."); Ignite ignite0 = grid(0); Cache<Integer, Integer> cache0 = ignite0.cache(null); Integer primaryKey = key(ignite0, PRIMARY); log.info("Put from primary."); cache0.put(primaryKey, primaryKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, primaryKey, false); log.info("Remove from primary."); cache0.remove(primaryKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, null, false); if (backups > 0) { Integer backupKey = key(ignite0, BACKUP); log.info("Put from backup."); cache0.put(backupKey, backupKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), backupKey, backupKey, false); log.info("Remove from backup."); cache0.remove(backupKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), backupKey, null, false); } }', 'output': 'private void checkRemove() throws Exception { log.info("Check remove."); Ignite ignite0 = grid(0); Cache<Integer, Integer> cache0 = ignite0.cache(null); Integer primaryKey = key(ignite0, PRIMARY); log.info("Put from primary."); cache0.put(primaryKey, primaryKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, primaryKey, false); log.info("Remove from primary."); cache0.remove(primaryKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, null, false); if (backups > 0) { Integer backupKey = key(ignite0, BACKUP); log.info("Put from backup."); cache0.put(backupKey, backupKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), backupKey, backupKey, false); log.info("Remove from backup."); cache0.remove(backupKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), backupKey, null, false); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void getUncommittedElementsEqualInput() { CommittedBundle<Integer> bundle = bundleFactory.createBundle(created) .add(WindowedValue.valueInGlobalWindow(2)) .commit(Instant.now()); XXX result = CommittedResult.create( StepTransformResult.withoutHold(transform).build(), Optional.of(bundle), Collections.emptyList(), EnumSet.noneOf(OutputType.class)); assertThat(result.getUnprocessedInputs().get(), Matchers.equalTo(bundle)); }', 'ground_truth': 'public void getUncommittedElementsEqualInput() { CommittedBundle<Integer> bundle = bundleFactory.createBundle(created) .add(WindowedValue.valueInGlobalWindow(2)) .commit(Instant.now()); CommittedResult<AppliedPTransform<?, ?, ?>> result = CommittedResult.create( StepTransformResult.withoutHold(transform).build(), Optional.of(bundle), Collections.emptyList(), EnumSet.noneOf(OutputType.class)); assertThat(result.getUnprocessedInputs().get(), Matchers.equalTo(bundle)); }', 'output': 'public void getUncommittedElementsEqualInput() { CommittedBundle<Integer> bundle = bundleFactory.createBundle(created) .add(WindowedValue.valueInGlobalWindow(2)) .commit(Instant.now()); CommittedResult result = CommittedResult.create( StepTransformResult.withoutHold(transform).build(), Optional.of(bundle), Collections.emptyList(), EnumSet.noneOf(OutputType.class)); assertThat(result.getUnprocessedInputs().get(), Matchers.equalTo(bundle)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RequestStatus createResources(Request request) throws AmbariException { Set<ActionRequest> requests = new HashSet<ActionRequest>(); for (Map<PropertyId, Object> propertyMap : request.getProperties()) { requests.add(getRequest(propertyMap)); } return getRequestStatus(getManagementController().createActions(requests)); }', 'ground_truth': 'public RequestStatus createResources(Request request) throws AmbariException, UnsupportedPropertyException { checkRequestProperties(Resource.Type.Action, request); Set<ActionRequest> requests = new HashSet<ActionRequest>(); for (Map<String, Object> propertyMap : request.getProperties()) { requests.add(getRequest(propertyMap)); } return getRequestStatus(getManagementController().createActions(requests)); }', 'output': 'public RequestStatus createResources(Request request) throws AmbariException { Set<ActionRequest> requests = new HashSet<ActionRequest>(); for (Map<PropertyId, Object> propertyMap : request.getProperties()) { requests.add(getRequest(propertyMap)); } return getRequestStatus(getManagementController().createActions(requests)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkLaunchCoordinator(GridDiscoveryEvent discoEvt) { rw.readLock(); try { if (stopping) return; if (timeCoord == null) { long minNodeOrder = Long.MAX_VALUE; Collection<GridNode> nodes = discoEvt.topologyNodes(); for (XXX node : nodes) { if (node.order() < minNodeOrder) minNodeOrder = node.order(); } GridNode locNode = ctx.grid().localNode(); if (locNode.order() == minNodeOrder) { if (log.isDebugEnabled()) log.debug("Detected local node to be the eldest node in topology, starting time " + "coordinator thread [discoEvt=" + discoEvt + ", locNode=" + locNode + \']\'); synchronized (this) { if (timeCoord == null && !stopping) { timeCoord = new TimeCoordinator(discoEvt); GridThread th = new GridThread(timeCoord); th.setPriority(Thread.MAX_PRIORITY); th.start(); } } } } } finally { rw.readUnlock(); } }', 'ground_truth': 'private void checkLaunchCoordinator(GridDiscoveryEvent discoEvt) { rw.readLock(); try { if (stopping) return; if (timeCoord == null) { long minNodeOrder = Long.MAX_VALUE; Collection<ClusterNode> nodes = discoEvt.topologyNodes(); for (ClusterNode node : nodes) { if (node.order() < minNodeOrder) minNodeOrder = node.order(); } ClusterNode locNode = ctx.grid().localNode(); if (locNode.order() == minNodeOrder) { if (log.isDebugEnabled()) log.debug("Detected local node to be the eldest node in topology, starting time " + "coordinator thread [discoEvt=" + discoEvt + ", locNode=" + locNode + \']\'); synchronized (this) { if (timeCoord == null && !stopping) { timeCoord = new TimeCoordinator(discoEvt); GridThread th = new GridThread(timeCoord); th.setPriority(Thread.MAX_PRIORITY); th.start(); } } } } } finally { rw.readUnlock(); } }', 'output': 'private void checkLaunchCoordinator(GridDiscoveryEvent discoEvt) { rw.readLock(); try { if (stopping) return; if (timeCoord == null) { long minNodeOrder = Long.MAX_VALUE; Collection<ClusterNode> nodes = discoEvt.topologyNodes(); for (ClusterNode node : nodes) { if (node.order() < minNodeOrder) minNodeOrder = node.order(); } ClusterNode locNode = ctx.grid().localNode(); if (locNode.order() == minNodeOrder) { if (log.isDebugEnabled()) log.debug("Detected local node to be the eldest node in topology, starting time " + "coordinator thread [discoEvt=" + discoEvt + ", locNode=" + locNode + \']\'); synchronized (this) { if (timeCoord == null && !stopping) { timeCoord = new TimeCoordinator(discoEvt); GridThread th = new GridThread(timeCoord); th.setPriority(Thread.MAX_PRIORITY); th.start(); } } } } } finally { rw.readUnlock(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean isIndexPrimitiveValue(String spaceName) { XXX cfg = cacheQueryConfiguration(spaceName); return cfg != null && cfg.isIndexPrimitiveValue(); }', 'ground_truth': 'public boolean isIndexPrimitiveValue(String spaceName) { CacheQueryConfiguration cfg = cacheQueryConfiguration(spaceName); return cfg != null && cfg.isIndexPrimitiveValue(); }', 'output': 'public boolean isIndexPrimitiveValue(String spaceName, String indexName) { CacheQueryConfiguration cfg = cacheQueryConfiguration(spaceName, indexName); return cfg != null && cfg.isIndexPrimitiveValue(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void commit() { Instant earliestHold = getEarliestWatermarkHold(); if (underlying.isPresent()) { XXX readThroughBinder = new ReadThroughBinderFactory<>(underlying.get()); binderFactory = readThroughBinder; Instant earliestUnderlyingHold = readThroughBinder.readThroughAndGetEarliestHold(this); if (earliestUnderlyingHold.isBefore(earliestHold)) { earliestHold = earliestUnderlyingHold; } } earliestWatermarkHold = Optional.of(earliestHold); clearEmpty(); binderFactory = new InMemoryStateBinderFactory<>(key); underlying = Optional.absent(); }', 'ground_truth': 'private void commit() { Instant earliestHold = getEarliestWatermarkHold(); if (underlying.isPresent()) { ReadThroughBinderFactory readThroughBinder = new ReadThroughBinderFactory<>(underlying.get()); binderFactory = readThroughBinder; Instant earliestUnderlyingHold = readThroughBinder.readThroughAndGetEarliestHold(this); if (earliestUnderlyingHold.isBefore(earliestHold)) { earliestHold = earliestUnderlyingHold; } } earliestWatermarkHold = Optional.of(earliestHold); clearEmpty(); binderFactory = new InMemoryStateBinderFactory(); underlying = Optional.absent(); }', 'output': 'private void commit() { Instant earliestHold = getEarliestWatermarkHold(); if (underlying.isPresent()) { Binder<?> readThroughBinder = new ReadThroughBinderFactory<>(underlying.get()); binderFactory = readThroughBinder; Instant earliestUnderlyingHold = readThroughBinder.readThroughAndGetEarliestHold(this); if (earliestUnderlyingHold.isBefore(earliestHold)) { earliestHold = earliestUnderlyingHold; } } earliestWatermarkHold = Optional.of(earliestHold); clearEmpty(); binderFactory = new InMemoryStateBinderFactory<>(key); underlying = Optional.absent(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static GridCacheProjectionEx<Object, Object> cache(Ignite ignite, String cacheName) throws IgniteCheckedException { GridCache<Object, Object> cache = ignite.cache(cacheName); if (cache == null) throw new IgniteCheckedException( "Failed to find cache for given cache name (null for default cache): " + cacheName); return (GridCacheProjectionEx<Object, Object>)cache; }', 'ground_truth': 'private static GridCacheProjectionEx<Object, Object> cache(Ignite ignite, String cacheName) throws IgniteCheckedException { Cache<Object, Object> cache = ignite.cache(cacheName); if (cache == null) throw new IgniteCheckedException( "Failed to find cache for given cache name (null for default cache): " + cacheName); return (GridCacheProjectionEx<Object, Object>)cache; }', 'output': 'private static <K, V> GridCacheProjectionEx<K, V> cache(Ignite ignite, String cacheName) throws IgniteCheckedException { GridCache<K, V> cache = ignite.cache(cacheName); if (cache == null) throw new IgniteCheckedException( "Failed to find cache for given cache name (null for default cache): " + cacheName); return (GridCacheProjectionEx<K, V>)cache; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void addTableToSchemaMutation(CFMetaData table, long timestamp, boolean withColumnsAndTriggers, Mutation mutation) { ColumnFamily cells = mutation.addOrGet(Columnfamilies); Composite prefix = Columnfamilies.comparator.make(table.cfName); XXX adder = new CFRowAdder(cells, prefix, timestamp); adder.add("cf_id", table.cfId); adder.add("type", table.cfType.toString()); if (table.isSuper()) { adder.add("comparator", table.comparator.subtype(0).toString()); adder.add("subcomparator", table.comparator.subtype(1).toString()); } else { adder.add("comparator", table.comparator.toString()); } adder.add("bloom_filter_fp_chance", table.getBloomFilterFpChance()); adder.add("caching", table.getCaching().toString()); adder.add("comment", table.getComment()); adder.add("compaction_strategy_class", table.compactionStrategyClass.getName()); adder.add("compaction_strategy_options", json(table.compactionStrategyOptions)); adder.add("compression_parameters", json(table.compressionParameters.asThriftOptions())); adder.add("default_time_to_live", table.getDefaultTimeToLive()); adder.add("default_validator", table.getDefaultValidator().toString()); adder.add("gc_grace_seconds", table.getGcGraceSeconds()); adder.add("key_validator", table.getKeyValidator().toString()); adder.add("local_read_repair_chance", table.getDcLocalReadRepairChance()); adder.add("max_compaction_threshold", table.getMaxCompactionThreshold()); adder.add("max_index_interval", table.getMaxIndexInterval()); adder.add("memtable_flush_period_in_ms", table.getMemtableFlushPeriod()); adder.add("min_compaction_threshold", table.getMinCompactionThreshold()); adder.add("min_index_interval", table.getMinIndexInterval()); adder.add("read_repair_chance", table.getReadRepairChance()); adder.add("speculative_retry", table.getSpeculativeRetry().toString()); for (Map.Entry<ColumnIdentifier, Long> entry : table.getDroppedColumns().entrySet()) adder.addMapEntry("dropped_columns", entry.getKey().toString(), entry.getValue()); adder.add("is_dense", table.getIsDense()); if (withColumnsAndTriggers) { for (ColumnDefinition column : table.allColumns()) addColumnToSchemaMutation(table, column, timestamp, mutation); for (TriggerDefinition trigger : table.getTriggers().values()) addTriggerToSchemaMutation(table, trigger, timestamp, mutation); } }', 'ground_truth': 'private static void addTableToSchemaMutation(CFMetaData table, long timestamp, boolean withColumnsAndTriggers, Mutation mutation) { RowUpdateBuilder adder = new RowUpdateBuilder(Columnfamilies, timestamp, mutation) .clustering(table.cfName); adder.add("cf_id", table.cfId); adder.add("type", table.isSuper() ? "Super" : "Standard"); if (table.isSuper()) { adder.add("comparator", table.comparator.subtype(0).toString()); adder.add("subcomparator", ((MapType)table.compactValueColumn().type).getKeysType().toString()); } else { adder.add("comparator", LegacyLayout.makeLegacyComparator(table).toString()); } adder.add("bloom_filter_fp_chance", table.getBloomFilterFpChance()); adder.add("caching", table.getCaching().toString()); adder.add("comment", table.getComment()); adder.add("compaction_strategy_class", table.compactionStrategyClass.getName()); adder.add("compaction_strategy_options", json(table.compactionStrategyOptions)); adder.add("compression_parameters", json(table.compressionParameters.asThriftOptions())); adder.add("default_time_to_live", table.getDefaultTimeToLive()); adder.add("gc_grace_seconds", table.getGcGraceSeconds()); adder.add("key_validator", table.getKeyValidator().toString()); adder.add("local_read_repair_chance", table.getDcLocalReadRepairChance()); adder.add("max_compaction_threshold", table.getMaxCompactionThreshold()); adder.add("max_index_interval", table.getMaxIndexInterval()); adder.add("memtable_flush_period_in_ms", table.getMemtableFlushPeriod()); adder.add("min_compaction_threshold", table.getMinCompactionThreshold()); adder.add("min_index_interval", table.getMinIndexInterval()); adder.add("read_repair_chance", table.getReadRepairChance()); adder.add("speculative_retry", table.getSpeculativeRetry().toString()); for (Map.Entry<ColumnIdentifier, CFMetaData.DroppedColumn> entry : table.getDroppedColumns().entrySet()) { String name = entry.getKey().toString(); CFMetaData.DroppedColumn column = entry.getValue(); adder.addMapEntry("dropped_columns", name, column.droppedTime); if (column.type != null) adder.addMapEntry("dropped_columns_types", name, column.type.toString()); } adder.add("is_dense", table.isDense()); adder.add("default_validator", table.makeLegacyDefaultValidator().toString()); if (withColumnsAndTriggers) { for (ColumnDefinition column : table.allColumns()) addColumnToSchemaMutation(table, column, timestamp, mutation); for (TriggerDefinition trigger : table.getTriggers().values()) addTriggerToSchemaMutation(table, trigger, timestamp, mutation); } adder.build(); }', 'output': 'private static void addTableToSchemaMutation(CFMetaData table, long timestamp, boolean withColumnsAndTriggers, Mutation mutation) { ColumnFamily cells = mutation.addOrGet(Columnfamilies); Composite prefix = Columnfamilies.comparator.make(table.cfName); CFRowAdder adder = new CFRowAdder(cells, prefix, timestamp); adder.add("cf_id", table.cfId); adder.add("type", table.cfType.toString()); if (table.isSuper()) { adder.add("comparator", table.comparator.subtype(0).toString()); adder.add("subcomparator", table.comparator.subtype(1).toString()); } else { adder.add("comparator", table.comparator.toString()); } adder.add("bloom_filter_fp_chance", table.getBloomFilterFpChance()); adder.add("caching", table.getCaching().toString()); adder.add("comment", table.getComment()); adder.add("compaction_strategy_class", table.compactionStrategyClass.getName()); adder.add("compaction_strategy_options", json(table.compactionStrategyOptions)); adder.add("compression_parameters", json(table.compressionParameters.asThriftOptions())); adder.add("default_time_to_live", table.getDefaultTimeToLive()); adder.add("default_validator", table.getDefaultValidator().toString()); adder.add("gc_grace_seconds", table.getGcGraceSeconds()); adder.add("key_validator", table.getKeyValidator().toString()); adder.add("local_read_repair_chance", table.getDcLocalReadRepairChance()); adder.add("max_compaction_threshold", table.getMaxCompactionThreshold()); adder.add("max_index_interval", table.getMaxIndexInterval()); adder.add("memtable_flush_period_in_ms", table.getMemtableFlushPeriod()); adder.add("min_compaction_threshold", table.getMinCompactionThreshold()); adder.add("min_index_interval", table.getMin'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void testCompaction(int sstableCount, int rowsPerSSTable, int colsPerRow) throws Exception { CompactionManager.instance.disableAutoCompaction(); Table table = Table.open(TABLE1); ColumnFamilyStore store = table.getColumnFamilyStore("Standard1"); ArrayList<SSTableReader> sstables = new ArrayList<SSTableReader>(); for (int k = 0; k < sstableCount; k++) { SortedMap<String,ColumnFamily> rows = new TreeMap<String,ColumnFamily>(); for (int j = 0; j < rowsPerSSTable; j++) { String key = String.valueOf(j); XXX cols = new IColumn[colsPerRow]; for (int i = 0; i < colsPerRow; i++) { cols[i] = Util.column(String.valueOf(i), String.valueOf(i), k); } rows.put(key, SSTableUtils.createCF(Long.MIN_VALUE, Integer.MIN_VALUE, cols)); } SSTableReader sstable = SSTableUtils.prepare().write(rows); sstables.add(sstable); store.addSSTable(sstable); } Thread.sleep(1000); long start = System.currentTimeMillis(); final int gcBefore = (int) (System.currentTimeMillis() / 1000) - Schema.instance.getCFMetaData(TABLE1, "Standard1").getGcGraceSeconds(); new CompactionTask(store, sstables, gcBefore).execute(null); System.out.println(String.format("%s: sstables=%d rowsper=%d colsper=%d: %d ms", this.getClass().getName(), sstableCount, rowsPerSSTable, colsPerRow, System.currentTimeMillis() - start)); }', 'ground_truth': 'protected void testCompaction(int sstableCount, int rowsPerSSTable, int colsPerRow) throws Exception { CompactionManager.instance.disableAutoCompaction(); Table table = Table.open(TABLE1); ColumnFamilyStore store = table.getColumnFamilyStore("Standard1"); ArrayList<SSTableReader> sstables = new ArrayList<SSTableReader>(); for (int k = 0; k < sstableCount; k++) { SortedMap<String,ColumnFamily> rows = new TreeMap<String,ColumnFamily>(); for (int j = 0; j < rowsPerSSTable; j++) { String key = String.valueOf(j); Column[] cols = new Column[colsPerRow]; for (int i = 0; i < colsPerRow; i++) { cols[i] = Util.column(String.valueOf(i), String.valueOf(i), k); } rows.put(key, SSTableUtils.createCF(Long.MIN_VALUE, Integer.MIN_VALUE, cols)); } SSTableReader sstable = SSTableUtils.prepare().write(rows); sstables.add(sstable); store.addSSTable(sstable); } Thread.sleep(1000); long start = System.currentTimeMillis(); final int gcBefore = (int) (System.currentTimeMillis() / 1000) - Schema.instance.getCFMetaData(TABLE1, "Standard1").getGcGraceSeconds(); new CompactionTask(store, sstables, gcBefore).execute(null); System.out.println(String.format("%s: sstables=%d rowsper=%d colsper=%d: %d ms", this.getClass().getName(), sstableCount, rowsPerSSTable, colsPerRow, System.currentTimeMillis() - start)); }', 'output': 'protected void testCompaction(int sstableCount, int rowsPerSSTable, int colsPerRow) throws Exception { CompactionManager.instance.disableAutoCompaction(); Table table = Table.open(TABLE1); ColumnFamilyStore store = table.getColumnFamilyStore("Standard1"); ArrayList<SSTableReader> sstables = new ArrayList<SSTableReader>(); for (int k = 0; k < sstableCount; k++) { SortedMap<String,ColumnFamily> rows = new TreeMap<String,ColumnFamily>(); for (int j = 0; j < rowsPerSSTable; j++) { String key = String.valueOf(j); IColumn[] cols = new IColumn[colsPerRow]; for (int i = 0; i < colsPerRow; i++) { cols[i] = Util.column(String.valueOf(i), String.valueOf(i), k); } rows.put(key, SSTableUtils.createCF(Long.MIN_VALUE, Integer.MIN_VALUE, cols)); } SSTableReader sstable = SSTableUtils.prepare().write(rows); sstables.add(sstable); store.addSSTable(sstable); } Thread.sleep(1000); long start = System.currentTimeMillis(); final int gcBefore = (int) (System.currentTimeMillis() / 1000) - Schema.instance.getCFMetaData(TABLE1, "Standard1").getGcGraceSeconds(); new CompactionTask(store, sstables, gcBefore).execute(null); System.out.println(String.format("%s: sstables=%d rowsper=%d colsper=%d: %d ms", this.getClass().getName(), sstableCount, rowsPerSSTable, colsPerRow, System.currentTimeMillis() - start)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public XXX configFor(final PdfJsViewerAdvisor.InstanceKey instanceKey) { final XXX config = super.configFor(instanceKey); if(advisors != null) { for (PdfJsViewerAdvisor advisor : advisors) { final PdfJsViewerAdvisor.Advice advice = advisor.advise(instanceKey); if(advice != null) { final Integer pageNum = advice.getPageNum(); if(pageNum != null) { config.withInitialPage(pageNum); } final Scale scale = advice.getScale(); if(scale != null) { config.withInitialScale(scale); } final Integer height = advice.getHeight(); if(height != null) { config.withInitialHeight(height); } break; } } } return config; }', 'ground_truth': 'public PdfJsConfig configFor(final PdfJsViewerAdvisor.InstanceKey instanceKey) { var config = super.configFor(instanceKey); if(advisors != null) { for (PdfJsViewerAdvisor advisor : advisors) { final PdfJsViewerAdvisor.Advice advice = advisor.advise(instanceKey); if(advice != null) { final Integer pageNum = advice.getPageNum(); if(pageNum != null) { config = config.withInitialPage(pageNum); } final Scale scale = advice.getScale(); if(scale != null) { config = config.withInitialScale(scale); } final Integer height = advice.getHeight(); if(height != null) { config = config.withInitialHeight(height); } break; } } } return config; }', 'output': 'public PdfJsViewerConfig configFor(final PdfJsViewerAdvisor.InstanceKey instanceKey) { final PdfJsViewerConfig config = super.configFor(instanceKey); if(advisors != null) { for (PdfJsViewerAdvisor advisor : advisors) { final PdfJsViewerAdvisor.Advice advice = advisor.advise(instanceKey); if(advice != null) { final Integer pageNum = advice.getPageNum(); if(pageNum != null) { config.withInitialPage(pageNum); } final Scale scale = advice.getScale(); if(scale != null) { config.withInitialScale(scale); } final Integer height = advice.getHeight(); if(height != null) { config.withInitialHeight(height); } break; } } } return config; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static synchronized Collection<KSMetaData> loadFromStorage(UUID version) throws IOException { DecoratedKey vkey = StorageService.getPartitioner().decorateKey(Migration.toUTF8Bytes(version)); Table defs = Table.open(Table.SYSTEM_TABLE); ColumnFamilyStore cfStore = defs.getColumnFamilyStore(Migration.SCHEMA_CF); QueryFilter filter = QueryFilter.getIdentityFilter(vkey, new QueryPath(Migration.SCHEMA_CF)); ColumnFamily cf = cfStore.getColumnFamily(filter); IColumn avroschema = cf.getColumn(DEFINITION_SCHEMA_COLUMN_NAME); if (avroschema == null) throw new RuntimeException("Cannot read system table! Are you upgrading a pre-release version?"); Schema schema = Schema.parse(new String(avroschema.value())); Collection<KSMetaData> keyspaces = new ArrayList<KSMetaData>(); try { for (IColumn column : cf.getSortedColumns()) { if (Arrays.equals(column.name(), DEFINITION_SCHEMA_COLUMN_NAME)) continue; XXX ks = SerDeUtils.<org.apache.cassandra.avro.KsDef>deserialize(schema, column.value()); keyspaces.add(KSMetaData.inflate(ks)); } } catch (ConfigurationException e) { throw new IOException(e); } return keyspaces; }', 'ground_truth': 'public static synchronized Collection<KSMetaData> loadFromStorage(UUID version) throws IOException { DecoratedKey vkey = StorageService.getPartitioner().decorateKey(Migration.toUTF8Bytes(version)); Table defs = Table.open(Table.SYSTEM_TABLE); ColumnFamilyStore cfStore = defs.getColumnFamilyStore(Migration.SCHEMA_CF); QueryFilter filter = QueryFilter.getIdentityFilter(vkey, new QueryPath(Migration.SCHEMA_CF)); ColumnFamily cf = cfStore.getColumnFamily(filter); IColumn avroschema = cf.getColumn(DEFINITION_SCHEMA_COLUMN_NAME); if (avroschema == null) throw new RuntimeException("Cannot read system table! Are you upgrading a pre-release version?"); Schema schema = Schema.parse(new String(avroschema.value())); Collection<KSMetaData> keyspaces = new ArrayList<KSMetaData>(); try { for (IColumn column : cf.getSortedColumns()) { if (Arrays.equals(column.name(), DEFINITION_SCHEMA_COLUMN_NAME)) continue; org.apache.cassandra.config.avro.KsDef ks = SerDeUtils.<org.apache.cassandra.config.avro.KsDef>deserialize(schema, column.value()); keyspaces.add(KSMetaData.inflate(ks)); } } catch (ConfigurationException e) { throw new IOException(e); } return keyspaces; }', 'output': 'public static List<KSMetaData> loadFromStorage(UUID version) throws IOException { DecoratedKey vkey = StorageService.getPartitioner().decorateKey(Migration.toUTF8Bytes(version)); Table defs = Table.open(Table.SYSTEM_TABLE); ColumnFamilyStore cfStore = defs.getColumnFamilyStore(Migration.SCHEMA_CF); QueryFilter filter = QueryFilter.getIdentityFilter(vkey, new QueryPath(Migration.SCHEMA_CF)); ColumnFamily cf = cfStore.getColumnFamily(filter); IColumn avroschema = cf.getColumn(DEFINITION_SCHEMA_COLUMN_NAME); if (avroschema == null) throw new RuntimeException("Cannot read system table! Are you upgrading a pre-release version?"); Schema schema = Schema.parse(new String(avroschema.value())); List<KSMetaData> keyspaces = new ArrayList<KSMetaData>(); try { for (IColumn column : cf.getSortedColumns()) { if (Arrays.equals(column.name(), DEFINITION_SCHEMA_COLUMN_NAME)) continue; org.apache.cassandra.avro.KsDef ks = SerDeUtils.<org.apache.cassandra.avro.KsDef>deserialize(schema, column.value()); keyspaces.add(KSMetaData.inflate(ks)); } } catch (ConfigurationException e) { throw new IOException(e); } return keyspaces; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String recv_describe_version() throws TException { TMessage msg = iprot_.readMessageBegin(); if (msg.type == TMessageType.EXCEPTION) { XXX x = TApplicationException.read(iprot_); iprot_.readMessageEnd(); throw x; } if (msg.seqid != seqid_) { throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "describe_version failed: out of sequence response"); } describe_version_result result = new describe_version_result(); result.read(iprot_); iprot_.readMessageEnd(); if (result.isSetSuccess()) { return result.success; } throw new TApplicationException(TApplicationException.MISSING_RESULT, "describe_version failed: unknown result"); }', 'ground_truth': 'public String recv_describe_version() throws org.apache.thrift.TException { org.apache.thrift.protocol.TMessage msg = iprot_.readMessageBegin(); if (msg.type == org.apache.thrift.protocol.TMessageType.EXCEPTION) { org.apache.thrift.TApplicationException x = org.apache.thrift.TApplicationException.read(iprot_); iprot_.readMessageEnd(); throw x; } if (msg.seqid != seqid_) { throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.BAD_SEQUENCE_ID, "describe_version failed: out of sequence response"); } describe_version_result result = new describe_version_result(); result.read(iprot_); iprot_.readMessageEnd(); if (result.isSetSuccess()) { return result.success; } throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.MISSING_RESULT, "describe_version failed: unknown result"); }', 'output': 'public String recv_describe_version() throws TException { TMessage msg = iprot_.readMessageBegin(); if (msg.type == TMessageType.EXCEPTION) { TApplicationException x = TApplicationException.read(iprot_); iprot_.readMessageEnd(); throw x; } if (msg.seqid != seqid_) { throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "describe_version failed: out of sequence response"); } describe_version_result result = new describe_version_result(); result.read(iprot_); iprot_.readMessageEnd(); if (result.isSetSuccess()) { return result.success; } throw new TApplicationException(TApplicationException.MISSING_RESULT, "describe_version failed: unknown result"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testExpirationOnClient() throws Exception { Ignite ignite = grid(NODES - 1); assertTrue(ignite.configuration().isClientMode()); IgniteCache<Object, Object> cache = ignite.cache(DEFAULT_CACHE_NAME); assertTrue(((IgniteCacheProxy)cache).context().isNear()); for (int i = 0 ; i < KEYS_COUNT; i++) cache.put(i, i); CreatedExpiryPolicy plc = new CreatedExpiryPolicy(new Duration(TimeUnit.MILLISECONDS, 500)); IgniteCache<Object, Object> cacheWithExpiry = cache.withExpiryPolicy(plc); for (int i = KEYS_COUNT ; i < KEYS_COUNT * 2; i++) { cacheWithExpiry.put(i, i); assertEquals(i, cacheWithExpiry.localPeek(i)); } U.sleep(1000); IgniteEx igniteEx = (IgniteEx)ignite; XXX map = GridTestUtils.getFieldValue( ((GridCacheProxyImpl)igniteEx.cachex(DEFAULT_CACHE_NAME)).delegate(), GridCacheAdapter.class, "map"); assertEquals(KEYS_COUNT, map.publicSize()); assertEquals(KEYS_COUNT, cache.size()); for (int i = 0 ; i < KEYS_COUNT; i++) assertEquals(i, cacheWithExpiry.localPeek(i)); for (int i = KEYS_COUNT ; i < KEYS_COUNT * 2; i++) assertNull(cache.localPeek(i)); }', 'ground_truth': 'public void testExpirationOnClient() throws Exception { Ignite ignite = grid(NODES - 1); IgniteEx igniteEx = (IgniteEx)ignite; GridCacheAdapter internalCache = igniteEx.context().cache().internalCache(DEFAULT_CACHE_NAME); GridCacheLocalConcurrentMap map = GridTestUtils.getFieldValue(internalCache, GridCacheAdapter.class, "map"); assertTrue(ignite.configuration().isClientMode()); IgniteCache<Object, Object> cache = ignite.cache(DEFAULT_CACHE_NAME); assertTrue(((IgniteCacheProxy)cache).context().isNear()); for (int i = 0 ; i < KEYS_COUNT; i++) cache.put(i, i); CreatedExpiryPolicy plc = new CreatedExpiryPolicy(new Duration(TimeUnit.MILLISECONDS, 500)); IgniteCache<Object, Object> cacheWithExpiry = cache.withExpiryPolicy(plc); for (int i = KEYS_COUNT ; i < KEYS_COUNT * 2; i++) { cacheWithExpiry.put(i, i); assertEquals(i, cacheWithExpiry.localPeek(i)); } assertEquals(KEYS_COUNT * 2, map.publicSize(internalCache.context().cacheId())); U.sleep(1000); assertEquals(KEYS_COUNT, map.publicSize(internalCache.context().cacheId())); assertEquals(KEYS_COUNT, cache.size()); for (int i = 0 ; i < KEYS_COUNT; i++) assertEquals(i, cacheWithExpiry.localPeek(i)); for (int i = KEYS_COUNT ; i < KEYS_COUNT * 2; i++) assertNull(cache.localPeek(i)); }', 'output': 'public void testExpirationOnClient() throws Exception { Ignite ignite = grid(NODES - 1); assertTrue(ignite.configuration().isClientMode()); IgniteCache<Object, Object> cache = ignite.cache(DEFAULT_CACHE_NAME); assertTrue(((IgniteCacheProxy)cache).context().isNear()); for (int i = 0 ; i < KEYS_COUNT; i++) cache.put(i, i); CreatedExpiryPolicy plc = new CreatedExpiryPolicy(new Duration(TimeUnit.MILLISECONDS, 500)); IgniteCache<Object, Object> cacheWithExpiry = cache.withExpiryPolicy(plc); for (int i = KEYS_COUNT ; i < KEYS_COUNT * 2; i++) { cacheWithExpiry.put(i, i); assertEquals(i, cacheWithExpiry.localPeek(i)); } U.sleep(1000); IgniteEx igniteEx = (IgniteEx)ignite; Map<Object, Object> map = GridTestUtils.getFieldValue( ((GridCacheProxyImpl)igniteEx.cachex(DEFAULT_CACHE_NAME)).delegate(), GridCacheAdapter.class, "map"); assertEquals(KEYS_COUNT, map.size()); for (int i = 0 ; i < KEYS_COUNT; i++) assertEquals(i, cacheWithExpiry.localPeek(i)); for (int i = KEYS_COUNT ; i < KEYS_COUNT * 2; i++) assertNull(cacheWithExpiry.localPeek(i)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testTwoAnonymousClasses() throws Exception { GridCache<Integer, Object> cache = ignite.cache(null); Object val1 = new Object() { @Override public String toString() { return "Test anonymous object1."; } }; Object val2 = new Object() { @Override public String toString() { return "Test anonymous object2."; } }; assertTrue(cache.putx(1, val1)); assertTrue(cache.putx(2, val2)); CacheQuery<Map.Entry<Integer, Object>> q = cache.queries().createSqlQuery(val1.getClass(), "_key >= 0"); q.enableDedup(true); Collection<Map.Entry<Integer, Object>> res = q.execute().get(); assertEquals(1, res.size()); }', 'ground_truth': 'public void testTwoAnonymousClasses() throws Exception { IgniteCache<Integer, Object> cache = ignite.jcache(null); Object val1 = new Object() { @Override public String toString() { return "Test anonymous object1."; } }; Object val2 = new Object() { @Override public String toString() { return "Test anonymous object2."; } }; cache.put(1, val1); cache.put(2, val2); QueryCursor<Cache.Entry<Integer, Object>> q = cache.query(new QuerySqlPredicate<Integer, Object>("_key >= 0")); Collection<Cache.Entry<Integer, Object>> res = q.getAll(); assertEquals(1, res.size()); }', 'output': 'public void testTwoAnonymousClasses() throws Exception { Cache<Integer, Object> cache = ignite.cache(null); Object val1 = new Object() { @Override public String toString() { return "Test anonymous object1."; } }; Object val2 = new Object() { @Override public String toString() { return "Test anonymous object2."; } }; assertTrue(cache.putx(1, val1)); assertTrue(cache.putx(2, val2)); CacheQuery<Map.Entry<Integer, Object>> q = cache.queries().createSqlQuery(val1.getClass(), "_key >= 0"); q.enableDedup(true); Collection<Map.Entry<Integer, Object>> res = q.execute().get(); assertEquals(1, res.size()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void doFilteredPut() throws Exception { GridCache<Integer, Integer> c = cache(); try (IgniteTx tx = c.txStart()) { assert !c.putx(1, 1, F.<Integer, Integer>cacheHasPeekValue()); tx.commit(); } assert c.isEmpty(); assert c.peek(1) == null; assert c.get(1) == null; }', 'ground_truth': 'private void doFilteredPut() throws Exception { Cache<Integer, Integer> c = cache(); try (IgniteTx tx = c.txStart()) { assert !c.putx(1, 1, F.<Integer, Integer>cacheHasPeekValue()); tx.commit(); } assert c.isEmpty(); assert c.peek(1) == null; assert c.get(1) == null; }', 'output': 'private void doFilteredPut() throws Exception { Cache<Integer, Integer> c = cache(); try (IgniteTx tx = c.txStart()) { assert !c.putx(1, 1, F.<Integer, Integer>cacheHasPeekValue()); tx.commit(); } assert c.isEmpty(); assert c.peek(1) == null; assert c.get(1) == null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void execute(NodeProbe probe) { String[] percentiles = {"50%", "75%", "95%", "98%", "99%", "Min", "Max"}; double[] readLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("Read")); double[] writeLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("Write")); double[] rangeLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("RangeSlice")); double[] casReadLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("CASRead")); double[] casWriteLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("CASWrite")); XXX viewWriteLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("ViewWrite")); System.out.println("proxy histograms"); System.out.println(format("%-10s%19s%19s%19s%19s%19s%19s", "Percentile", "Read Latency", "Write Latency", "Range Latency", "CAS Read Latency", "CAS Write Latency", "View Write Latency")); System.out.println(format("%-10s%19s%19s%19s%19s%19s%19s", "", "(micros)", "(micros)", "(micros)", "(micros)", "(micros)", "(micros)")); for (int i = 0; i < percentiles.length; i++) { System.out.println(format("%-10s%19.2f%19.2f%19.2f%19.2f%19.2f%19.2f", percentiles[i], readLatency[i], writeLatency[i], rangeLatency[i], casReadLatency[i], casWriteLatency[i], viewWriteLatency[i])); } System.out.println(); }', 'ground_truth': 'public void execute(NodeProbe probe) { String[] percentiles = {"50%", "75%", "95%", "98%", "99%", "Min", "Max"}; Double[] readLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("Read")); Double[] writeLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("Write")); Double[] rangeLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("RangeSlice")); Double[] casReadLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("CASRead")); Double[] casWriteLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("CASWrite")); Double[] viewWriteLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("ViewWrite")); System.out.println("proxy histograms"); System.out.println(format("%-10s%19s%19s%19s%19s%19s%19s", "Percentile", "Read Latency", "Write Latency", "Range Latency", "CAS Read Latency", "CAS Write Latency", "View Write Latency")); System.out.println(format("%-10s%19s%19s%19s%19s%19s%19s", "", "(micros)", "(micros)", "(micros)", "(micros)", "(micros)", "(micros)")); for (int i = 0; i < percentiles.length; i++) { System.out.println(format("%-10s%19.2f%19.2f%19.2f%19.2f%19.2f%19.2f", percentiles[i], readLatency[i], writeLatency[i], rangeLatency[i], casReadLatency[i], casWriteLatency[i], viewWriteLatency[i])); } System.out.println(); }', 'output': 'public void execute(NodeProbe probe) { String[] percentiles = {"50%", "75%", "95%", "98%", "99%", "Min", "Max"}; double[] readLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("Read")); double[] writeLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("Write")); double[] rangeLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("RangeSlice")); double[] casReadLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("CASRead")); double[] casWriteLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("CASWrite")); double[] viewWriteLatency = probe.metricPercentilesAsArray(probe.getProxyMetric("ViewWrite")); System.out.println("proxy histograms"); System.out.println(format("%-10s%19s%19s%19s%19s%19s%19s", "Percentile", "Read Latency", "Write Latency", "Range Latency", "CAS Read Latency", "CAS Write Latency", "View Write Latency")); System.out.println(format("%-10s%19s%19s%19s%19s%19s%19s", "", "(micros)", "(micros)", "(micros)", "(micros)", "(micros)", "(micros)", "(micros)")); for (int i = 0; i < percentiles.length; i++) { System.out.println(format("%-10s%19.2f%19.2f%19.2f%19.2f%19.2f%19.2f", percentiles[i], readLatency[i], writeLatency[i], rangeLatency[i], casReadLatency[i], casWriteLatency[i], viewWriteLatency[i])); } System.out.println(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void annotate(ErrorInfo[] errors){ ArrayList added, removed, unchanged; Collection newAnnotations; DataObject doJsp; try { doJsp = DataObject.find(jspFo); } catch (DataObjectNotFoundException e){ return; } EditorCookie editor = (EditorCookie)doJsp.getCookie(EditorCookie.class); if (editor == null) return; StyledDocument document = editor.getDocument(); if (document == null) return; if(editor.getOpenedPanes()==null) return; JTextComponent component = editor.getOpenedPanes()[0]; if (component != null){ if (errors != null && errors.length > 0){ org.netbeans.editor.Utilities.setStatusBoldText(component , " " + errors[0].getDescription()); } else{ org.netbeans.editor.Utilities.clearStatusText(component); } } newAnnotations = getAnnotations(errors, document); added=new ArrayList(newAnnotations); added.removeAll(annotations); unchanged=new ArrayList(annotations); unchanged.retainAll(newAnnotations); removed = annotations; removed.removeAll(newAnnotations); detachAnnotations(removed); if (!added.isEmpty()) { final XXX finalAdded = added; final DataObject doJsp2 = doJsp; Runnable docRenderer = new Runnable() { @Override public void run() { LineCookie cookie = (LineCookie)doJsp2.getCookie(LineCookie.class); Line.Set lines = cookie.getLineSet(); for (Iterator i=finalAdded.iterator();i.hasNext();) { LineSetAnnotation ann=(LineSetAnnotation)i.next(); ann.attachToLineSet(lines); } } }; document.render(docRenderer); } annotations=unchanged; annotations.addAll(added); }', 'ground_truth': 'public void annotate(ErrorInfo[] errors){ List added, removed, unchanged; Collection newAnnotations; DataObject doJsp; try { doJsp = DataObject.find(jspFo); } catch (DataObjectNotFoundException e){ return; } EditorCookie editor = (EditorCookie)doJsp.getCookie(EditorCookie.class); if (editor == null) return; StyledDocument document = editor.getDocument(); if (document == null) return; if(editor.getOpenedPanes()==null) return; JTextComponent component = editor.getOpenedPanes()[0]; if (component != null){ if (errors != null && errors.length > 0){ org.netbeans.editor.Utilities.setStatusBoldText(component , " " + errors[0].getDescription()); } else{ org.netbeans.editor.Utilities.clearStatusText(component); } } newAnnotations = getAnnotations(errors, document); added=new ArrayList<>(newAnnotations); added.removeAll(annotations); unchanged=new ArrayList<>(annotations); unchanged.retainAll(newAnnotations); removed = annotations; removed.removeAll(newAnnotations); detachAnnotations(removed); if (!added.isEmpty()) { final List finalAdded = added; final DataObject doJsp2 = doJsp; Runnable docRenderer = new Runnable() { @Override public void run() { LineCookie cookie = (LineCookie)doJsp2.getCookie(LineCookie.class); Line.Set lines = cookie.getLineSet(); for (Iterator i=finalAdded.iterator();i.hasNext();) { LineSetAnnotation ann=(LineSetAnnotation)i.next(); ann.attachToLineSet(lines); } } }; document.render(docRenderer); } annotations=unchanged; annotations.addAll(added); }', 'output': 'public void annotate(ErrorInfo[] errors, AnnotationManager annotationManager) { ArrayList added, removed, unchanged; Collection newAnnotations; DataObject doJsp; try { doJsp = DataObject.find(jspFo); } catch (DataObjectNotFoundException e){ return; } EditorCookie editor = (EditorCookie)doJsp.getCookie(EditorCookie.class); if (editor == null) return; StyledDocument document = editor.getDocument(); if (document == null) return; if(editor.getOpenedPanes()==null) return; JTextComponent component = editor.getOpenedPanes()[0]; if (component != null){ if (errors != null && errors.length > 0){ org.netbeans.editor.Utilities.setStatusBoldText(component , " " + errors[0].getDescription()); } else{ org.netbeans.editor.Utilities.clearStatusText(component); } } newAnnotations = getAnnotations(errors, document, annotationManager); added=new ArrayList(newAnnotations); added.removeAll(annotations); unchanged=new ArrayList(annotations); unchanged.retainAll(newAnnotations); removed = annotations; removed.removeAll(newAnnotations); detachAnnotations(removed); if (!added.isEmpty()) { final XXX finalAdded = added; final DataObject doJsp2 = doJsp; Runnable docRenderer = new Runnable() { @Override public void run() { LineCookie cookie = (LineCookie)doJsp2.getCookie(LineCookie.class); Line.Set lines = cookie.getLineSet(); for (Iterator i=finalAdded.iterator();i.hasNext();) { LineSetAnnotation ann=(LineSetAnnotation)i.next(); ann.attachToLineSet(lines); } } }; document.render(docRenderer); } annotations=unchanged; annotations.addAll(added); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void getSslContext_ParamChanges() throws IOException { XXX options = addKeystoreOptions(encryptionOptions) .withEnabled(true) .withCipherSuites("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"); SslContext ctx1 = SSLFactory.getOrCreateSslContext(options, true, ISslContextFactory.SocketType.SERVER); Assert.assertTrue(ctx1.isServer()); Assert.assertEquals(ctx1.cipherSuites(), options.cipher_suites); options = options.withCipherSuites("TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256"); SslContext ctx2 = SSLFactory.getOrCreateSslContext(options, true, ISslContextFactory.SocketType.CLIENT); Assert.assertTrue(ctx2.isClient()); Assert.assertEquals(ctx2.cipherSuites(), options.cipher_suites); }', 'ground_truth': 'public void getSslContext_ParamChanges() throws IOException { ServerEncryptionOptions options = addKeystoreOptions(encryptionOptions) .withCipherSuites("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"); SslContext ctx1 = SSLFactory.getOrCreateSslContext(options, true, ISslContextFactory.SocketType.SERVER); Assert.assertTrue(ctx1.isServer()); Assert.assertEquals(ctx1.cipherSuites(), options.cipher_suites); options = options.withCipherSuites("TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256"); SslContext ctx2 = SSLFactory.getOrCreateSslContext(options, true, ISslContextFactory.SocketType.CLIENT); Assert.assertTrue(ctx2.isClient()); Assert.assertEquals(ctx2.cipherSuites(), options.cipher_suites); }', 'output': 'public void getSslContext_ParamChanges() throws IOException { Options options = addKeystoreOptions(encryptionOptions) .withEnabled(true) .withCipherSuites("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"); SslContext ctx1 = SSLFactory.getOrCreateSslContext(options, true, ISslContextFactory.SocketType.SERVER); Assert.assertTrue(ctx1.isServer()); Assert.assertEquals(ctx1.cipherSuites(), options.cipher_suites); options = options.withCipherSuites("TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256"); SslContext ctx2 = SSLFactory.getOrCreateSslContext(options, true, ISslContextFactory.SocketType.CLIENT); Assert.assertTrue(ctx2.isClient()); Assert.assertEquals(ctx2.cipherSuites(), options.cipher_suites); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void defineSchema() throws ConfigurationException { DatabaseDescriptor.daemonInitialization(); CFMetaData metadata1 = SchemaLoader.standardCFMD(KEYSPACE, CF1); CFMetaData metadata2 = CFMetaData.Builder.create(KEYSPACE, CF2) .addPartitionKey("key", BytesType.instance) .addClusteringColumn("col", AsciiType.instance) .addRegularColumn("a", AsciiType.instance) .addRegularColumn("b", AsciiType.instance).build(); XXX metadata3 = CFMetaData.Builder.create(KEYSPACE, CF3) .addPartitionKey("key", BytesType.instance) .addClusteringColumn("col", AsciiType.instance) .addRegularColumn("a", AsciiType.instance) .addRegularColumn("b", AsciiType.instance) .addRegularColumn("c", AsciiType.instance) .addRegularColumn("d", AsciiType.instance) .addRegularColumn("e", AsciiType.instance) .addRegularColumn("f", AsciiType.instance).build(); SchemaLoader.prepareServer(); SchemaLoader.createKeyspace(KEYSPACE, KeyspaceParams.simple(1), metadata1, metadata2, metadata3); }', 'ground_truth': 'public static void defineSchema() throws ConfigurationException { DatabaseDescriptor.daemonInitialization(); TableMetadata.Builder metadata1 = SchemaLoader.standardCFMD(KEYSPACE, CF1); TableMetadata.Builder metadata2 = TableMetadata.builder(KEYSPACE, CF2) .addPartitionKeyColumn("key", BytesType.instance) .addClusteringColumn("col", AsciiType.instance) .addRegularColumn("a", AsciiType.instance) .addRegularColumn("b", AsciiType.instance); TableMetadata.Builder metadata3 = TableMetadata.builder(KEYSPACE, CF3) .addPartitionKeyColumn("key", BytesType.instance) .addClusteringColumn("col", AsciiType.instance) .addRegularColumn("a", AsciiType.instance) .addRegularColumn("b", AsciiType.instance) .addRegularColumn("c", AsciiType.instance) .addRegularColumn("d", AsciiType.instance) .addRegularColumn("e", AsciiType.instance) .addRegularColumn("f", AsciiType.instance); SchemaLoader.prepareServer(); SchemaLoader.createKeyspace(KEYSPACE, KeyspaceParams.simple(1), metadata1, metadata2, metadata3); }', 'output': 'public static void defineSchema() throws ConfigurationException { DatabaseDescriptor.daemonInitialization(); CFMetaData metadata1 = SchemaLoader.standardCFMD(KEYSPACE, CF1); CFMetaData metadata2 = CFMetaData.Builder.create(KEYSPACE, CF2) .addPartitionKey("key", BytesType.instance) .addClusteringColumn("col", AsciiType.instance) .addRegularColumn("a", AsciiType.instance) .addRegularColumn("b", AsciiType.instance).build(); CFMetaData metadata3 = CFMetaData.Builder.create(KEYSPACE, CF3) .addPartitionKey("key", BytesType.instance) .addClusteringColumn("col", AsciiType.instance) .addRegularColumn("a", AsciiType.instance) .addRegularColumn("b", AsciiType.instance) .addRegularColumn("c", AsciiType.instance) .addRegularColumn("d", AsciiType.instance) .addRegularColumn("e", AsciiType.instance) .addRegularColumn("f", AsciiType.instance).build(); SchemaLoader.prepareServer(); SchemaLoader.createKeyspace(KEYSPACE, KeyspaceParams.simple(1), metadata1, metadata2, metadata3); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void stopLifecycleAware(IgniteLogger log, Iterable<?> objs) { for (Object obj : objs) { if (obj instanceof LifecycleAware) { try { ((LifecycleAware)obj).stop(); } catch (XXX e) { U.error(log, "Failed to stop component (ignoring): " + obj, e); } } } }', 'ground_truth': 'public static void stopLifecycleAware(IgniteLogger log, Iterable<?> objs) { for (Object obj : objs) { if (obj instanceof LifecycleAware) { try { ((LifecycleAware)obj).stop(); } catch (Exception e) { U.error(log, "Failed to stop component (ignoring): " + obj, e); } } } }', 'output': 'public static void stopLifecycleAware(IgniteLogger log, Iterable<?> objs) { for (Object obj : objs) { if (obj instanceof LifecycleAware) { try { ((LifecycleAware)obj).stop(); } catch (Exception e) { U.error(log, "Failed to stop component (ignoring): " + obj, e); } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testDefaultCharsetLiteral() { String sql = "SELECT * FROM TABLE_A WHERE f_string = \'\'"; XXX result = PCollectionTuple.of(new TupleTag<>("TABLE_A"), boundedInput1) .apply("testCompositeFilter", BeamSql.queryMulti(sql)); PAssert.that(result).containsInAnyOrder(recordsInTableA.get(3)); pipeline.run().waitUntilFinish(); }', 'ground_truth': 'public void testDefaultCharsetLiteral() { String sql = "SELECT * FROM TABLE_A WHERE f_string = \'\'"; PCollection<Row> result = PCollectionTuple.of(new TupleTag<>("TABLE_A"), boundedInput1) .apply("testCompositeFilter", BeamSql.queryMulti(sql)); PAssert.that(result).containsInAnyOrder(rowsInTableA.get(3)); pipeline.run().waitUntilFinish(); }', 'output': 'public void testDefaultCharsetLiteral() { String sql = "SELECT * FROM TABLE_A WHERE f_string = \'\'"; PCollection<Row> result = PCollectionTuple.of(new TupleTag<>("TABLE_A"), boundedInput1) .apply("testCompositeFilter", BeamSql.queryMulti(sql)); PAssert.that(result).containsInAnyOrder(recordsInTableA.get(3)); pipeline.run().waitUntilFinish(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void givenRootPackage() throws Exception { final ApplicationFeatureId applicationFeatureId = ApplicationFeatureId.newNamespace(""); final XXX parentPackageId = applicationFeatureId.getParentPackageId(); assertThat(parentPackageId, is(nullValue())); }', 'ground_truth': 'public void givenRootPackage() throws Exception { val applicationFeatureId = ApplicationFeatureId.newNamespace(""); val parentPackageId = applicationFeatureId.getParentPackageId(); assertThat(parentPackageId, is(nullValue())); }', 'output': 'public void givenRootPackage() throws Exception { final ApplicationFeatureId applicationFeatureId = ApplicationFeatureId.newNamespace(""); final PackageId parentPackageId = applicationFeatureId.getParentPackageId(); assertThat(parentPackageId, is(nullValue())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void run() { while (true) { try { Buffer b = writeQueue.take(); if (b == SENTINEL) return; try (SSTableTxnWriter writer = createWriter()) { for (Map.Entry<DecoratedKey, PartitionUpdate> entry : b.entrySet()) writer.append(entry.getValue().unfilteredIterator()); writer.finish(false); } } catch (Throwable e) { JVMStabilityInspector.inspectThrowable(e); if (exception == null) exception = e; } } }', 'ground_truth': 'public void run() { while (true) { try { Buffer b = writeQueue.take(); if (b == SENTINEL) return; try (SSTableTxnWriter writer = createWriter()) { for (Map.Entry<DecoratedKey, PartitionUpdate.Builder> entry : b.entrySet()) writer.append(entry.getValue().build().unfilteredIterator()); writer.finish(false); } } catch (Throwable e) { JVMStabilityInspector.inspectThrowable(e); if (exception == null) exception = e; } } }', 'output': 'public void run() { while (true) { try { Buffer b = writeQueue.take(); if (b == SENTINEL) return; try (SSTableTxnWriter writer = createWriter()) { for (Map.Entry<DecoratedKey, PartitionUpdate> entry : b.entrySet()) writer.append(entry.getValue().unfilteredIterator()); writer.finish(false); } } catch (Throwable e) { JVMStabilityInspector.inspectThrowable(e); if (exception == null) exception = e; } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSerializationDeserialization() { UnionCoder newCoder = UnionCoder.of(Arrays.<Coder<?>>asList(StringUtf8Coder.of(), DoubleCoder.of())); CloudObject encoding = newCoder.asCloudObject(); XXX decodedCoder = Serializer.deserialize(encoding, Coder.class); assertEquals(newCoder, decodedCoder); }', 'ground_truth': 'public void testSerializationDeserialization() { UnionCoder newCoder = UnionCoder.of(Arrays.<Coder<?>>asList(StringUtf8Coder.of(), DoubleCoder.of())); CloudObject encoding = newCoder.asCloudObject(); Coder<?> decodedCoder = Serializer.deserialize(encoding, Coder.class); assertEquals(newCoder, decodedCoder); }', 'output': 'public void testSerializationDeserialization() { UnionCoder newCoder = UnionCoder.of(Arrays.<Coder<?>>asList(StringUtf8Coder.of(), DoubleCoder.of())); CloudObject encoding = newCoder.asCloudObject(); Coder<?> decodedCoder = Serializer.deserialize(encoding, Coder.class); assertEquals(newCoder, decodedCoder); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAddRemoveGlobalListener() throws Exception { XXX lsnr = new GridPredicate<GridEvent>() { @Override public boolean apply(GridEvent evt) { info("Received local event: " + evt); return true; } }; ignite1.events().localListen(lsnr, EVTS_ALL_MINUS_METRIC_UPDATE); assert ignite1.events().stopLocalListen(lsnr); }', 'ground_truth': 'public void testAddRemoveGlobalListener() throws Exception { IgnitePredicate<GridEvent> lsnr = new IgnitePredicate<GridEvent>() { @Override public boolean apply(GridEvent evt) { info("Received local event: " + evt); return true; } }; ignite1.events().localListen(lsnr, EVTS_ALL_MINUS_METRIC_UPDATE); assert ignite1.events().stopLocalListen(lsnr); }', 'output': 'public void testAddRemoveGlobalListener() throws Exception { IgnitePredicate<GridEvent> lsnr = new IgnitePredicate<GridEvent>() { @Override public boolean apply(GridEvent evt) { info("Received local event: " + evt); return true; } }; ignite1.events().localListen(lsnr, EVTS_ALL_MINUS_METRIC_UPDATE); assert ignite1.events().stopLocalListen(lsnr); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void customEventType() throws Exception { final SomeDomainObject sdo = new SomeDomainObject(); final XXX identifier = FeatureIdentifier.actionIdentifier(TypeIdentifier.fqcn(SomeDomainObject.class), "foo", new Class[]{int.class, String.class}); Utils.domainEventHelper(); final ActionDomainEvent<SomeDomainObject> ev = DomainEventHelper.newActionDomainEvent( SomeDomainObjectFooInvokedDomainEvent.class, identifier, sdo, new Object[]{1, "bar"}); assertSame(ev.getSource(), sdo); assertThat(ev.getIdentifier(), is(identifier)); assertThat(ev.getArguments(), is(not(nullValue()))); assertEquals(ev.getArguments().get(0), Integer.valueOf(1)); assertEquals(ev.getArguments().get(1), "bar"); }', 'ground_truth': 'public void customEventType() throws Exception { final SomeDomainObject sdo = new SomeDomainObject(); final Identifier identifier = Identifier.actionIdentifier(TypeIdentifier.fqcn(SomeDomainObject.class), "foo", new Class[]{int.class, String.class}); Utils.domainEventHelper(); final ActionDomainEvent<SomeDomainObject> ev = DomainEventHelper.newActionDomainEvent( SomeDomainObjectFooInvokedDomainEvent.class, identifier, sdo, new Object[]{1, "bar"}); assertSame(ev.getSource(), sdo); assertThat(ev.getIdentifier(), is(identifier)); assertThat(ev.getArguments(), is(not(nullValue()))); assertEquals(ev.getArguments().get(0), Integer.valueOf(1)); assertEquals(ev.getArguments().get(1), "bar"); }', 'output': 'public void customEventType() throws Exception { final SomeDomainObject sdo = new SomeDomainObject(); final Identifier identifier = FeatureIdentifier.actionIdentifier(TypeIdentifier.fqcn(SomeDomainObject.class), "foo", new Class[]{int.class, String.class}); Utils.domainEventHelper(); final ActionDomainEvent<SomeDomainObject> ev = DomainEventHelper.newActionDomainEvent( SomeDomainObjectFooInvokedDomainEvent.class, identifier, sdo, new Object[]{1, "bar"}); assertSame(ev.getSource(), sdo); assertThat(ev.getIdentifier(), is(identifier)); assertThat(ev.getArguments(), is(not(nullValue()))); assertEquals(ev.getArguments().get(0), Integer.valueOf(1)); assertEquals(ev.getArguments().get(1), "bar"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void visit(JsonCustomSchema jsonSchema) { try { final XXX parentSchema = currentMutableSchema("sub-schema"); final Class clazz = Class.forName(jsonSchema.factory); final SchemaFactory schemaFactory = (SchemaFactory) clazz.newInstance(); final Schema schema = schemaFactory.create( parentSchema, jsonSchema.name, jsonSchema.operand); parentSchema.addSchema(jsonSchema.name, schema); if (schema instanceof MapSchema) { ((MapSchema) schema).initialize(); } populateSchema(jsonSchema, schema); } catch (Exception e) { throw new RuntimeException("Error instantiating " + jsonSchema, e); } }', 'ground_truth': 'public void visit(JsonMapSchema jsonSchema) { final SchemaPlus parentSchema = currentMutableSchema("schema"); final AbstractSchema mapSchema = new AbstractSchema(parentSchema, jsonSchema.name); final SchemaPlus schema = parentSchema.add(mapSchema); populateSchema(jsonSchema, schema); if (schema.getName().equals("mat")) { final List<Table> tables = new ArrayList<Table>(); final String[] tableNames = { "sales_fact_1997", "time_by_day", "product", "product_class" }; final SchemaPlus schema2 = parentSchema.getSubSchema("foodmart"); for (String tableName : tableNames) { tables.add(schema2.getTable(tableName)); } final String tableName = "star"; schema.add(tableName, StarTable.of(tables)); } }', 'output': 'public void visit(JsonCustomSchema jsonSchema) { try { final Schema parentSchema = currentMutableSchema("sub-schema"); final Class clazz = Class.forName(jsonSchema.factory); final SchemaFactory schemaFactory = (SchemaFactory) clazz.newInstance(); final Schema schema = schemaFactory.create( parentSchema, jsonSchema.name, jsonSchema.operand); parentSchema.addSchema(jsonSchema.name, schema); if (schema instanceof MapSchema) { ((MapSchema) schema).initialize(); } populateSchema(jsonSchema, schema); } catch (Exception e) { throw new RuntimeException("Error instantiating " + jsonSchema, e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Cell createCounterCellFromContext(ColumnFamilyStore cfs, ByteBuffer colName, ContextState context, long ts) { XXX cDef = cfs.metadata.getColumnDefinition(colName); return BufferCell.live(cDef, ts, context.context); }', 'ground_truth': 'private Cell createCounterCellFromContext(ColumnFamilyStore cfs, ByteBuffer colName, ContextState context, long ts) { ColumnMetadata cDef = cfs.metadata().getColumn(colName); return BufferCell.live(cDef, ts, context.context); }', 'output': 'private Cell createCounterCellFromContext(ColumnFamilyStore cfs, ByteBuffer colName, ContextState context, long ts, int version) { ColumnMetadata cDef = cfs.metadata.getColumnMetadata(colName); return BufferCell.live(cDef, ts, version, context.context); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testServices() throws Exception { IgniteConfiguration cfg = optimize(getConfiguration("g1")); try (Ignite g1 = G.start(cfg)) { XXX services = grid().managed(grid().forNode(g1.cluster().localNode())); services.deployNodeSingleton("test", new DummyService()); GridMarshallerTestBean inBean = newTestBean(services); byte[] buf = marshal(inBean); GridMarshallerTestBean outBean = unmarshal(buf); assert inBean.getObjectField() != null; assert outBean.getObjectField() != null; assert inBean.getObjectField().getClass().equals(IgniteManagedImpl.class); assert outBean.getObjectField().getClass().equals(IgniteManagedImpl.class); assert inBean != outBean; assert inBean.equals(outBean); ClusterGroup inPrj = services.clusterGroup(); ClusterGroup outPrj = ((IgniteManaged)outBean.getObjectField()).clusterGroup(); assert inPrj.getClass().equals(outPrj.getClass()); assert F.eqNotOrdered(inPrj.nodes(), outPrj.nodes()); outBean.checkNullResources(); } }', 'ground_truth': 'public void testServices() throws Exception { IgniteConfiguration cfg = optimize(getConfiguration("g1")); try (Ignite g1 = G.start(cfg)) { IgniteServices services = grid().services(grid().forNode(g1.cluster().localNode())); services.deployNodeSingleton("test", new DummyService()); GridMarshallerTestBean inBean = newTestBean(services); byte[] buf = marshal(inBean); GridMarshallerTestBean outBean = unmarshal(buf); assert inBean.getObjectField() != null; assert outBean.getObjectField() != null; assert inBean.getObjectField().getClass().equals(IgniteServicesImpl.class); assert outBean.getObjectField().getClass().equals(IgniteServicesImpl.class); assert inBean != outBean; assert inBean.equals(outBean); ClusterGroup inPrj = services.clusterGroup(); ClusterGroup outPrj = ((IgniteServices)outBean.getObjectField()).clusterGroup(); assert inPrj.getClass().equals(outPrj.getClass()); assert F.eqNotOrdered(inPrj.nodes(), outPrj.nodes()); outBean.checkNullResources(); } }', 'output': 'public void testServices() throws Exception { IgniteConfiguration cfg = optimize(getConfiguration("g1")); try (Ignite g1 = G.start(cfg)) { IgniteServices services = grid().managed(grid().forNode(g1.cluster().localNode())); services.deployNodeSingleton("test", new DummyService()); GridMarshallerTestBean inBean = newTestBean(services); byte[] buf = marshal(inBean); GridMarshallerTestBean outBean = unmarshal(buf); assert inBean.getObjectField() != null; assert outBean.getObjectField() != null; assert inBean.getObjectField().getClass().equals(IgniteManagedImpl.class); assert outBean.getObjectField().getClass().equals(IgniteManagedImpl.class); assert inBean != outBean; assert inBean.equals(outBean); ClusterGroup inPrj = services.clusterGroup(); ClusterGroup outPrj = ((IgniteManaged)outBean.getObjectField()).clusterGroup(); assert inPrj.getClass().equals(outPrj.getClass()); assert F.eqNotOrdered(inPrj.nodes(), outPrj.nodes()); outBean.checkNullResources(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Columns without(ColumnDefinition column) { int idx = column.isComplex() ? complexIdx(column, 0) : simpleIdx(column, 0); if (idx < 0) return this; int realIdx = column.isComplex() ? complexIdx + idx : idx; XXX newColumns = new ColumnDefinition[columns.length - 1]; System.arraycopy(columns, 0, newColumns, 0, realIdx); System.arraycopy(columns, realIdx + 1, newColumns, realIdx, newColumns.length - realIdx); return new Columns(newColumns); }', 'ground_truth': 'public Columns without(ColumnDefinition column) { if (!contains(column)) return this; Object[] newColumns = BTree.<ColumnDefinition>transformAndFilter(columns, (c) -> c.equals(column) ? null : c); return new Columns(newColumns); }', 'output': 'public Columns without(ColumnDefinition column) { int idx = column.isComplex() ? complexIdx(column, 0) : simpleIdx(column, 0); if (idx < 0) return this; int realIdx = column.isComplex() ? complexIdx + idx : idx; ColumnDefinition[] newColumns = new ColumnDefinition[columns.length - 1]; System.arraycopy(columns, 0, newColumns, 0, realIdx); System.arraycopy(columns, realIdx + 1, newColumns, realIdx, newColumns.length - realIdx); return new Columns(newColumns); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GroupingState deserialize(DataInputPlus in, int version, ClusteringComparator comparator) throws IOException { if (!in.readBoolean()) return GroupingState.EMPTY_STATE; ByteBuffer partitionKey = ByteBufferUtil.readWithVIntLength(in); XXX clustering = null; if (in.readBoolean()) clustering = Clustering.serializer.deserialize(in, version, comparator.subtypes()); return new GroupingState(partitionKey, clustering); }', 'ground_truth': 'public GroupingState deserialize(DataInputPlus in, int version, ClusteringComparator comparator) throws IOException { if (!in.readBoolean()) return GroupingState.EMPTY_STATE; ByteBuffer partitionKey = ByteBufferUtil.readWithVIntLength(in); Clustering<byte[]> clustering = null; if (in.readBoolean()) clustering = Clustering.serializer.deserialize(in, version, comparator.subtypes()); return new GroupingState(partitionKey, clustering); }', 'output': 'public GroupingState deserialize(DataInputPlus in, int version, ClusteringComparator comparator) throws IOException { if (!in.readBoolean()) return GroupingState.EMPTY_STATE; ByteBuffer partitionKey = ByteBufferUtil.readWithVIntLength(in); Clustering clustering = null; if (in.readBoolean()) clustering = Clustering.serializer.deserialize(in, version, comparator.subtypes()); return new GroupingState(partitionKey, clustering); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean close(boolean rollOver) throws IgniteCheckedException, StorageException { if (stop.compareAndSet(false, true)) { lock.lock(); try { flushOrWait(null, true); assert stopped() : "Segment is not closed after close flush: " + head.get(); try { int switchSegmentRecSize = RecordV1Serializer.REC_TYPE_SIZE + RecordV1Serializer.FILE_WAL_POINTER_SIZE + RecordV1Serializer.CRC_SIZE; if (rollOver && written < (maxSegmentSize - switchSegmentRecSize)) { XXX backwardSerializer = new RecordV1Serializer(new RecordDataV1Serializer(cctx), true); final ByteBuffer buf = ByteBuffer.allocate(switchSegmentRecSize); SwitchSegmentRecord segmentRecord = new SwitchSegmentRecord(); segmentRecord.position( new FileWALPointer(idx, (int)written, -1)); backwardSerializer.writeRecord(segmentRecord,buf); buf.rewind(); int rem = buf.remaining(); while (rem > 0) { int written0 = fileIO.write(buf, written); written += written0; rem -= written0; } } if (mode == WALMode.DEFAULT) { fileIO.force(); lastFsyncPos = written; } fileIO.close(); } catch (IOException e) { throw new IgniteCheckedException(e); } if (log.isDebugEnabled()) log.debug("Closed WAL write handle [idx=" + idx + "]"); return true; } finally { lock.unlock(); } } else return false; }', 'ground_truth': 'private boolean close(boolean rollOver) throws IgniteCheckedException, StorageException { if (stop.compareAndSet(false, true)) { lock.lock(); try { flushOrWait(null, true); assert stopped() : "Segment is not closed after close flush: " + head.get(); try { RecordSerializer backwardSerializer = new RecordSerializerFactoryImpl(cctx) .createSerializer(serializerVersion); SwitchSegmentRecord segmentRecord = new SwitchSegmentRecord(); int switchSegmentRecSize = backwardSerializer.size(segmentRecord); if (rollOver && written < (maxSegmentSize - switchSegmentRecSize)) { final ByteBuffer buf = ByteBuffer.allocate(switchSegmentRecSize); segmentRecord.position(new FileWALPointer(idx, (int)written, switchSegmentRecSize)); backwardSerializer.writeRecord(segmentRecord, buf); buf.rewind(); int rem = buf.remaining(); while (rem > 0) { int written0 = fileIO.write(buf, written); written += written0; rem -= written0; } } if (mode == WALMode.DEFAULT) { fileIO.force(); lastFsyncPos = written; } fileIO.close(); } catch (IOException e) { throw new IgniteCheckedException(e); } if (log.isDebugEnabled()) log.debug("Closed WAL write handle [idx=" + idx + "]"); return true; } finally { lock.unlock(); } } else return false; }', 'output': 'private boolean close(boolean rollOver) throws IgniteCheckedException, StorageException { if (stop.compareAndSet(false, true)) { lock.lock(); try { flushOrWait(null, true); assert stopped() : "Segment is not closed after close flush: " + head.get(); try { int switchSegmentRecSize = RecordV1Serializer.REC_TYPE_SIZE + RecordV1Serializer.FILE_WAL_POINTER_SIZE + RecordV1Serializer.CRC_SIZE; if (rollOver && written < (maxSegmentSize - switchSegmentRecSize)) { BackwardSerializer backwardSerializer = new RecordV1Serializer(new RecordDataV1Serializer(cctx), true); final ByteBuffer buf = ByteBuffer.allocate(switchSegmentRecSize); SwitchSegmentRecord segmentRecord = new SwitchSegmentRecord(); segmentRecord.position( new FileWALPointer(idx, (int)written, -1)); backwardSerializer.writeRecord(segmentRecord,buf); buf.rewind(); int rem = buf.remaining(); while (rem > 0) { int written0 = fileIO.write(buf, written); written += written0; rem -= written0; } } if (mode == WALMode.DEFAULT) { fileIO.force(); lastFsyncPos = written; } fileIO.close(); } catch (IOException e) { throw new IgniteCheckedException(e); } if (log.isDebugEnabled()) log.debug("Closed WAL write handle [idx=" + idx + "]"); return true; } finally { lock.unlock(); } } else return false; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testOpenPrefetch() throws Exception { create(ggfsSecondary, paths(DIR, SUBDIR), paths(FILE)); final int blockSize = ggfs.info(FILE).blockSize(); IgniteFsOutputStream out = ggfsSecondary.append(FILE, false); int totalWritten = 0; while (totalWritten < blockSize * 2 + chunk.length) { out.write(chunk); totalWritten += chunk.length; } out.close(); awaitFileClose(ggfsSecondary, FILE); int totalRead = 0; IgniteFsInputStream in = ggfs.open(FILE, blockSize); final byte[] readBuf = new byte[1024]; while (totalRead + readBuf.length <= blockSize * 2) { in.read(readBuf); totalRead += readBuf.length; } GridGgfsMetaManager meta = ggfs.context().meta(); GridGgfsFileInfo info = meta.info(meta.fileId(FILE)); GridGgfsBlockKey key = new GridGgfsBlockKey(info.id(), info.affinityKey(), info.evictExclude(), 2); GridCache<GridGgfsBlockKey, byte[]> dataCache = ggfs.context().kernalContext().cache().cache( ggfs.configuration().getDataCacheName()); for (int i = 0; i < 10; i++) { if (dataCache.containsKey(key)) break; else U.sleep(100); } ggfsSecondary.delete(FILE, false); U.sleep(300); totalRead = 0; in.seek(blockSize * 2); while (totalRead + readBuf.length <= blockSize) { in.read(readBuf); totalRead += readBuf.length; } in.close(); }', 'ground_truth': 'public void testOpenPrefetch() throws Exception { create(ggfsSecondary, paths(DIR, SUBDIR), paths(FILE)); final int blockSize = ggfs.info(FILE).blockSize(); IgniteFsOutputStream out = ggfsSecondary.append(FILE, false); int totalWritten = 0; while (totalWritten < blockSize * 2 + chunk.length) { out.write(chunk); totalWritten += chunk.length; } out.close(); awaitFileClose(ggfsSecondary, FILE); int totalRead = 0; IgniteFsInputStream in = ggfs.open(FILE, blockSize); final byte[] readBuf = new byte[1024]; while (totalRead + readBuf.length <= blockSize * 2) { in.read(readBuf); totalRead += readBuf.length; } IgfsMetaManager meta = ggfs.context().meta(); IgfsFileInfo info = meta.info(meta.fileId(FILE)); IgfsBlockKey key = new IgfsBlockKey(info.id(), info.affinityKey(), info.evictExclude(), 2); GridCache<IgfsBlockKey, byte[]> dataCache = ggfs.context().kernalContext().cache().cache( ggfs.configuration().getDataCacheName()); for (int i = 0; i < 10; i++) { if (dataCache.containsKey(key)) break; else U.sleep(100); } ggfsSecondary.delete(FILE, false); U.sleep(300); totalRead = 0; in.seek(blockSize * 2); while (totalRead + readBuf.length <= blockSize) { in.read(readBuf); totalRead += readBuf.length; } in.close(); }', 'output': 'public void testOpenPrefetch() throws Exception { create(ggfsSecondary, paths(DIR, SUBDIR), paths(FILE)); final int blockSize = ggfs.info(FILE).blockSize(); IgniteFsOutputStream out = ggfsSecondary.append(FILE, false); int totalWritten = 0; while (totalWritten < blockSize * 2 + chunk.length) { out.write(chunk); totalWritten += chunk.length; } out.close(); awaitFileClose(ggfsSecondary, FILE); int totalRead = 0; IgniteFsInputStream in = ggfs.open(FILE, blockSize); final byte[] readBuf = new byte[1024]; while (totalRead + readBuf.length <= blockSize * 2) { in.read(readBuf); totalRead += readBuf.length; } GridGgfsMetaManager meta = ggfs.context().meta(); GridGgfsFileInfo info = meta.info(meta.fileId(FILE)); GridGgfsBlockKey key = new GridGgfsBlockKey(info.id(), info.affinityKey(), info.evictExclude(), 2); GridCache<GridGgfsBlockKey, byte[]> dataCache = ggfs.context().kernalContext().cache().cache( ggfs.configuration().getDataCacheName()); for (int i = 0; i < 10; i++) { if (dataCache.containsKey(key)) break; else U.sleep(100); } ggfsSecondary.delete(FILE, false); U.sleep(300); totalRead = 0; in.seek(blockSize * 2); while (totalRead + readBuf.length <= blockSize) { in.read(readBuf); totalRead += readBuf.length; } in.close(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private HostsType filterSameVersion(HostsType hostsType, String service, String component) { try { org.apache.ambari.server.state.Service svc = m_cluster.getService(service); ServiceComponent sc = svc.getServiceComponent(component); List<ServiceComponentHost> unhealthy = new ArrayList<ServiceComponentHost>(); XXX toUpgrade = new LinkedHashSet<String>(); for (String host : hostsType.hosts) { ServiceComponentHost sch = sc.getServiceComponentHost(host); if (HostState.HEALTHY != sch.getHostState() && !sc.isMasterComponent()) { unhealthy.add(sch); } else if (null == m_version || null == sch.getVersion() || !sch.getVersion().equals(m_version)) { toUpgrade.add(host); } } hostsType.unhealthy = unhealthy; hostsType.hosts = toUpgrade; return hostsType; } catch (AmbariException e) { LOG.warn("Could not determine host components to upgrade. Defaulting to saved hosts.", e); return hostsType; } }', 'ground_truth': 'private HostsType filterSameVersion(HostsType hostsType, String service, String component) { try { org.apache.ambari.server.state.Service svc = m_cluster.getService(service); ServiceComponent sc = svc.getServiceComponent(component); List<ServiceComponentHost> unhealthy = new ArrayList<ServiceComponentHost>(); LinkedHashSet<String> toUpgrade = new LinkedHashSet<String>(); for (String host : hostsType.hosts) { ServiceComponentHost sch = sc.getServiceComponentHost(host); if (HostState.HEALTHY != sch.getHostState() && !sc.isMasterComponent()) { unhealthy.add(sch); } else if (null == m_version || null == sch.getVersion() || !sch.getVersion().equals(m_version)) { toUpgrade.add(host); } } hostsType.unhealthy = unhealthy; hostsType.hosts = toUpgrade; return hostsType; } catch (AmbariException e) { LOG.warn("Could not determine host components to upgrade. Defaulting to saved hosts.", e); return hostsType; } }', 'output': 'private HostsType filterSameVersion(HostsType hostsType, String service, String component) { try { org.apache.ambari.server.state.Service svc = m_cluster.getService(service); ServiceComponent sc = svc.getServiceComponent(component); List<ServiceComponentHost> unhealthy = new ArrayList<ServiceComponentHost>(); Set<String> toUpgrade = new LinkedHashSet<String>(); for (String host : hostsType.hosts) { ServiceComponentHost sch = sc.getServiceComponentHost(host); if (HostState.HEALTHY != sch.getHostState() && !sc.isMasterComponent()) { unhealthy.add(sch); } else if (null == m_version || null == sch.getVersion() || !sch.getVersion().equals(m_version)) { toUpgrade.add(host); } } hostsType.unhealthy = unhealthy; hostsType.hosts = toUpgrade; return hostsType; } catch (AmbariException e) { LOG.warn("Could not determine host components to upgrade. Defaulting to saved hosts.", e); return hostsType; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testQueryIntegersWithJoin() throws Exception { GridCacheQuery<List<?>> qry = ((GridCacheQueriesEx<?, ?>)grid(0).cache(null).queries()).createSqlFieldsQuery( "select i._KEY, i._VAL, j._KEY, j._VAL from Integer i join Integer j where i._VAL >= 100", true) .projection(grid(0)); GridCacheQueryFuture<List<?>> fut = qry.execute(); List<GridIndexingFieldMetadata> meta = metadata(fut); assert meta != null; assert meta.size() == 4; Iterator<GridIndexingFieldMetadata> metaIt = meta.iterator(); assert metaIt.hasNext(); XXX field = metaIt.next(); assert field != null; assert "INTEGER".equals(field.typeName()); assert "_KEY".equals(field.fieldName()); assert metaIt.hasNext(); field = metaIt.next(); assert field != null; assert "INTEGER".equals(field.typeName()); assert "_VAL".equals(field.fieldName()); assert metaIt.hasNext(); field = metaIt.next(); assert field != null; assert "INTEGER".equals(field.typeName()); assert "_KEY".equals(field.fieldName()); assert metaIt.hasNext(); field = metaIt.next(); assert field != null; assert "INTEGER".equals(field.typeName()); assert "_VAL".equals(field.fieldName()); assert !metaIt.hasNext(); Collection<List<?>> res = fut.get(); assert res != null; if (cacheMode() == LOCAL) assert res.size() == 20000; else if (cacheMode() == REPLICATED) assert res.size() == 60000; else assert res.size() <= 20000; for (List<?> row : res) { assert (Integer)row.get(0) >= 100; assert (Integer)row.get(1) >= 100; assert (Integer)row.get(2) >= 0; assert (Integer)row.get(3) >= 0; } }', 'ground_truth': 'public void testQueryIntegersWithJoin() throws Exception { GridCacheQuery<List<?>> qry = ((GridCacheQueriesEx<?, ?>)grid(0).cache(null).queries()).createSqlFieldsQuery( "select i._KEY, i._VAL, j._KEY, j._VAL from Integer i join Integer j where i._VAL >= 100", true) .projection(grid(0)); GridCacheQueryFuture<List<?>> fut = qry.execute(); List<IndexingFieldMetadata> meta = metadata(fut); assert meta != null; assert meta.size() == 4; Iterator<IndexingFieldMetadata> metaIt = meta.iterator(); assert metaIt.hasNext(); IndexingFieldMetadata field = metaIt.next(); assert field != null; assert "INTEGER".equals(field.typeName()); assert "_KEY".equals(field.fieldName()); assert metaIt.hasNext(); field = metaIt.next(); assert field != null; assert "INTEGER".equals(field.typeName()); assert "_VAL".equals(field.fieldName()); assert metaIt.hasNext(); field = metaIt.next(); assert field != null; assert "INTEGER".equals(field.typeName()); assert "_KEY".equals(field.fieldName()); assert metaIt.hasNext(); field = metaIt.next(); assert field != null; assert "INTEGER".equals(field.typeName()); assert "_VAL".equals(field.fieldName()); assert !metaIt.hasNext(); Collection<List<?>> res = fut.get(); assert res != null; if (cacheMode() == LOCAL) assert res.size() == 20000; else if (cacheMode() == REPLICATED) assert res.size() == 60000; else assert res.size() <= 20000; for (List<?> row : res) { assert (Integer)row.get(0) >= 100; assert (Integer)row.get(1) >= 100; assert (Integer)row.get(2) >= 0; assert (Integer)row.get(3) >= 0; } }', 'output': 'public void testQueryIntegersWithJoin() throws Exception { GridCacheQuery<List<?>> qry = ((GridCacheQueriesEx<?, ?>)grid(0).cache(null).queries()).createSqlFieldsQuery( "select i._KEY, i._VAL, j._KEY, j._VAL from Integer i join Integer j where i._VAL >= 100", true) .projection(grid(0)); GridCacheQueryFuture<List<?>> fut = qry.execute(); List<GridIndexingFieldMetadata> meta = metadata(fut); assert meta != null; assert meta.size() == 4; Iterator<GridIndexingFieldMetadata> metaIt = meta.iterator(); assert metaIt.hasNext(); GridIndexingFieldMetadata field = metaIt.next(); assert field != null; assert "INTEGER".equals(field.typeName()); assert "_KEY".equals(field.fieldName()); assert metaIt.hasNext(); field = metaIt.next(); assert field != null; assert "INTEGER".equals(field.typeName()); assert "_VAL".equals(field.fieldName()); assert metaIt.hasNext(); field = metaIt.next(); assert field != null; assert "INTEGER".equals(field.typeName()); assert "_KEY".equals(field.fieldName()); assert metaIt.hasNext(); field = metaIt.next(); assert field != null; assert "INTEGER".equals(field.typeName()); assert "_VAL".equals(field.fieldName()); assert !metaIt.hasNext(); Collection<List<?>> res = fut.get(); assert res != null; if (cacheMode() == LOCAL) assert res.size() == 20000; else if (cacheMode() == REPLICATED) assert res.size() == 60000; else assert res.size() <= 20000; for (List<?> row : res) { assert (Integer)row.get(0) >= 100; assert (Integer)row.get(1) >= 100; assert (Integer)row.get(2) >= 0; assert (Integer)row.get(3) >= 0; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testReconnectInDeploying() throws Exception { Ignite client = grid(serverCount()); assertTrue(client.cluster().localNode().isClient()); final IgniteServices services = client.services(); Ignite srv = clientRouter(client); XXX commSpi = commSpi(srv); commSpi.blockMessage(GridNearTxPrepareResponse.class); final IgniteInternalFuture<Object> fut = GridTestUtils.runAsync(new Callable<Object>() { @Override public Object call() throws Exception { try { services.deployClusterSingleton("testReconnectInDeploying", new TestServiceImpl()); } catch (IgniteClientDisconnectedException e) { checkAndWait(e); return true; } return false; } }); GridTestUtils.assertThrows(log, new Callable<Object>() { @Override public Object call() throws Exception { return fut.get(200); } }, IgniteFutureTimeoutCheckedException.class, null); assertNotDone(fut); commSpi.unblockMessage(); reconnectClientNode(client, srv, null); assertTrue((Boolean)fut.get(2, TimeUnit.SECONDS)); }', 'ground_truth': 'public void testReconnectInDeploying() throws Exception { Ignite client = grid(serverCount()); assertTrue(client.cluster().localNode().isClient()); final IgniteServices services = client.services(); Ignite srv = clientRouter(client); BlockTcpCommunicationSpi commSpi = commSpi(srv); commSpi.blockMessage(GridNearTxPrepareResponse.class); final IgniteInternalFuture<Object> fut = GridTestUtils.runAsync(new Callable<Object>() { @Override public Object call() throws Exception { try { services.deployClusterSingleton("testReconnectInDeploying", new TestServiceImpl()); } catch (IgniteClientDisconnectedException e) { checkAndWait(e); return true; } return false; } }); GridTestUtils.assertThrows(log, new Callable<Object>() { @Override public Object call() throws Exception { return fut.get(200); } }, IgniteFutureTimeoutCheckedException.class, null); assertNotDone(fut); commSpi.unblockMessage(); reconnectClientNode(client, srv, null); assertTrue((Boolean)fut.get(2, TimeUnit.SECONDS)); }', 'output': 'public void testReconnectInDeploying() throws Exception { Ignite client = grid(serverCount()); assertTrue(client.cluster().localNode().isClient()); final IgniteServices services = client.services(); Ignite srv = clientRouter(client); IgniteCommunicationSpi commSpi = commSpi(srv); commSpi.blockMessage(GridNearTxPrepareResponse.class); final IgniteInternalFuture<Object> fut = GridTestUtils.runAsync(new Callable<Object>() { @Override public Object call() throws Exception { try { services.deployClusterSingleton("testReconnectInDeploying", new TestServiceImpl()); } catch (IgniteClientDisconnectedException e) { checkAndWait(e); return true; } return false; } }); GridTestUtils.assertThrows(log, new Callable<Object>() { @Override public Object call() throws Exception { return fut.get(200); } }, IgniteFutureTimeoutCheckedException.class, null); assertNotDone(fut); commSpi.unblockMessage(); reconnectClientNode(client, srv, null); assertTrue((Boolean)fut.get(2, TimeUnit.SECONDS)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testBoundsAsClusteringWithNoRestrictions() { CFMetaData cfMetaData = newCFMetaData(Sort.ASC); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(cfMetaData); XXX bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEmptyStart(get(bounds, 0)); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEmptyEnd(get(bounds, 0)); }', 'ground_truth': 'public void testBoundsAsClusteringWithNoRestrictions() { CFMetaData cfMetaData = newCFMetaData(Sort.ASC); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(cfMetaData); SortedSet<ClusteringBound> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEmptyStart(get(bounds, 0)); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEmptyEnd(get(bounds, 0)); }', 'output': 'public void testBoundsAsClusteringWithNoRestrictions() { CFMetaData cfMetaData = newCFMetaData(Sort.ASC); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(cfMetaData); Bounds bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEmptyStart(get(bounds, 0)); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEmptyEnd(get(bounds, 0)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private File initFolder() throws IgniteSpiException { if (initGuard.compareAndSet(false, true)) { if (path == null) throw new IgniteSpiException("Shared file system path is null " + "(it should be configured via setPath(..) configuration property)."); if (path.equals(DFLT_PATH) && warnGuard.compareAndSet(false, true)) U.warn(log, "Default local computer-only share is used by IP finder."); try { File tmp; if (new File(path).exists()) tmp = new File(path); else { try { tmp = U.resolveWorkDirectory(path, false); } catch (IgniteCheckedException e) { throw new IgniteSpiException("Failed to resolve directory [path=" + path + ", exception=" + e.getMessage() + \']\'); } } if (!tmp.isDirectory()) throw new IgniteSpiException("Failed to initialize shared file system path " + "(path must point to folder): " + path); if (!tmp.canRead() || !tmp.canWrite()) throw new IgniteSpiException("Failed to initialize shared file system path " + "(path must be readable and writable): " + path); folder = tmp; } finally { initLatch.countDown(); } } else { try { U.await(initLatch); } catch (XXX e) { throw new IgniteSpiException("Thread has been interrupted.", e); } if (folder == null) throw new IgniteSpiException("Failed to initialize shared file system folder (check logs for errors)."); } return folder; }', 'ground_truth': 'private File initFolder() throws IgniteSpiException { if (initGuard.compareAndSet(false, true)) { if (path == null) throw new IgniteSpiException("Shared file system path is null " + "(it should be configured via setPath(..) configuration property)."); if (path.equals(DFLT_PATH) && warnGuard.compareAndSet(false, true)) U.warn(log, "Default local computer-only share is used by IP finder."); try { File tmp; if (new File(path).exists()) tmp = new File(path); else { try { tmp = U.resolveWorkDirectory(path, false); } catch (IgniteCheckedException e) { throw new IgniteSpiException("Failed to resolve directory [path=" + path + ", exception=" + e.getMessage() + \']\'); } } if (!tmp.isDirectory()) throw new IgniteSpiException("Failed to initialize shared file system path " + "(path must point to folder): " + path); if (!tmp.canRead() || !tmp.canWrite()) throw new IgniteSpiException("Failed to initialize shared file system path " + "(path must be readable and writable): " + path); folder = tmp; } finally { initLatch.countDown(); } } else { try { U.await(initLatch); } catch (IgniteInterruptedException e) { throw new IgniteSpiException("Thread has been interrupted.", e); } if (folder == null) throw new IgniteSpiException("Failed to initialize shared file system folder (check logs for errors)."); } return folder; }', 'output': 'private File initFolder() throws IgniteSpiException { if (initGuard.compareAndSet(false, true)) { if (path == null) throw new IgniteSpiException("Shared file system path is null " + "(it should be configured via setPath(..) configuration property)."); if (path.equals(DFLT_PATH) && warnGuard.compareAndSet(false, true)) U.warn(log, "Default local computer-only share is used by IP finder."); try { File tmp; if (new File(path).exists()) tmp = new File(path); else { try { tmp = U.resolveWorkDirectory(path, false); } catch (IgniteCheckedException e) { throw new IgniteSpiException("Failed to resolve directory [path=" + path + ", exception=" + e.getMessage() + \']\'); } } if (!tmp.isDirectory()) throw new IgniteSpiException("Failed to initialize shared file system path " + "(path must point to folder): " + path); if (!tmp.canRead() || !tmp.canWrite()) throw new IgniteSpiException("Failed to initialize shared file system path " + "(path must be readable and writable): " + path); folder = tmp; } finally { initLatch.countDown(); } } else { try { U.await(initLatch); } catch (InterruptedException e) { throw new IgniteSpiException("Thread has been interrupted.", e); } if (folder == null) throw new IgniteSpiException("Failed to initialize shared file system folder (check logs for errors)."); } return folder; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private ColumnFilter makeColumnFilter(CFMetaData metadata, ColumnParent parent, SlicePredicate predicate) throws org.apache.cassandra.exceptions.InvalidRequestException { try { if (predicate.column_names != null) { if (metadata.isSuper()) { if (parent.isSetSuper_column()) { ColumnFilter.Builder builder = ColumnFilter.selectionBuilder(); ColumnDefinition dynamicDef = metadata.compactValueColumn(); for (ByteBuffer bb : predicate.column_names) { ColumnDefinition staticDef = metadata.getColumnDefinition(bb); if (staticDef == null) builder.select(dynamicDef, CellPath.create(bb)); else builder.add(staticDef); } return builder.build(); } else { return ColumnFilter.all(metadata); } } else { PartitionColumns.Builder builder = PartitionColumns.builder(); for (ByteBuffer bb : predicate.column_names) { LegacyLayout.LegacyCellName name = LegacyLayout.decodeCellName(metadata, parent.bufferForSuper_column(), bb); builder.add(name.column); } if (metadata.isStaticCompactTable()) builder.add(metadata.compactValueColumn()); return ColumnFilter.selection(builder.build()); } } else { return makeColumnFilter(metadata, parent, predicate.slice_range); } } catch (UnknownColumnException | IllegalLegacyColumnException e) { throw new org.apache.cassandra.exceptions.InvalidRequestException(e.getMessage()); } }', 'ground_truth': 'private ColumnFilter makeColumnFilter(CFMetaData metadata, ColumnParent parent, SlicePredicate predicate) throws org.apache.cassandra.exceptions.InvalidRequestException { try { if (predicate.column_names != null) { if (metadata.isSuper()) { if (parent.isSetSuper_column()) { ColumnFilter.Builder builder = ColumnFilter.selectionBuilder(); ColumnDefinition dynamicDef = metadata.compactValueColumn(); for (ByteBuffer bb : predicate.column_names) { ColumnDefinition staticDef = metadata.getColumnDefinition(bb); if (staticDef == null) builder.select(dynamicDef, CellPath.create(bb)); else builder.add(staticDef); } return builder.build(); } else { return ColumnFilter.all(metadata); } } else { PartitionColumns.Builder builder = PartitionColumns.builder(); for (ByteBuffer bb : predicate.column_names) { LegacyLayout.LegacyCellName name = LegacyLayout.decodeCellName(metadata, parent.bufferForSuper_column(), bb); builder.add(name.column); } if (metadata.isStaticCompactTable()) builder.add(metadata.compactValueColumn()); return ColumnFilter.selection(builder.build()); } } else { return makeColumnFilter(metadata, parent, predicate.slice_range); } } catch (UnknownColumnException e) { throw new org.apache.cassandra.exceptions.InvalidRequestException(e.getMessage()); } }', 'output': 'private ColumnFilter makeColumnFilter(CFMetaData metadata, ColumnParent parent, SlicePredicate predicate) throws org.apache.cassandra.exceptions.InvalidRequestException { try { if (predicate.column_names != null) { if (metadata.isSuper()) { if (parent.isSetSuper_column()) { ColumnFilter.Builder builder = ColumnFilter.selectionBuilder(); ColumnDefinition dynamicDef = metadata.compactValueColumn(); for (ByteBuffer bb : predicate.column_names) { ColumnDefinition staticDef = metadata.getColumnDefinition(bb); if (staticDef == null) builder.select(dynamicDef, CellPath.create(bb)); else builder.add(staticDef); } return builder.build(); } else { return ColumnFilter.all(metadata); } } else { PartitionColumns.Builder builder = PartitionColumns.builder(); for (ByteBuffer bb : predicate.column_names) { LegacyLayout.LegacyCellName name = LegacyLayout.decodeCellName(metadata, parent.bufferForSuper_column(), bb); builder.add(name.column); } if (metadata.isStaticCompactTable()) builder.add(metadata.compactValueColumn()); return ColumnFilter.selection(builder.build()); } } else { return makeColumnFilter(metadata, parent, predicate.slice_range); } } catch (UnknownColumnException | IllegalLegacyColumnException e) { throw new org.apache.cassandra.exceptions.InvalidRequestException(e.getMessage()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testShouldGroupAttributeConfigurationCheckSucceedWhenGroupForUserDnIsFound() throws Exception { Map<String, Object> configMap = groupConfigObjectMap(); SearchRequest sr = new SearchRequestImpl(); AmbariLdapConfiguration ambariLdapConfiguration = new TestAmbariLdapConfigurationFactory().createLdapConfiguration(configMap); EasyMock.expect(ldapConnectionTemplateFactory.create(ambariLdapConfiguration)).andReturn(ldapConnectionTemplateMock); EasyMock.expect(ldapConnectionTemplateMock.newSearchRequest(EasyMock.anyObject(Dn.class), EasyMock.anyString(), EasyMock.anyObject(SearchScope.class))).andReturn(sr); EasyMock.expect(ldapConnectionTemplateMock.search(EasyMock.anyObject(SearchRequest.class), EasyMock.anyObject(EntryMapper.class))) .andReturn(Lists.newArrayList("userGroup")); replayAll(); Set<String> userGroups = ldapConfigurationService.checkGroupAttributes("userDn", ambariLdapConfiguration); Assert.assertNotNull("No groups found", userGroups); }', 'ground_truth': 'public void testShouldGroupAttributeConfigurationCheckSucceedWhenGroupForUserDnIsFound() throws Exception { Map<String, String> configMap = groupConfigObjectMap(); SearchRequest sr = new SearchRequestImpl(); AmbariLdapConfiguration ambariLdapConfiguration = new TestAmbariLdapConfigurationFactory().createLdapConfiguration(configMap); EasyMock.expect(ldapConnectionTemplateFactory.create(ambariLdapConfiguration)).andReturn(ldapConnectionTemplateMock); EasyMock.expect(ldapConnectionTemplateMock.newSearchRequest(EasyMock.anyObject(Dn.class), EasyMock.anyString(), EasyMock.anyObject(SearchScope.class))).andReturn(sr); EasyMock.expect(ldapConnectionTemplateMock.search(EasyMock.anyObject(SearchRequest.class), EasyMock.anyObject(EntryMapper.class))) .andReturn(Lists.newArrayList("userGroup")); replayAll(); Set<String> userGroups = ldapConfigurationService.checkGroupAttributes("userDn", ambariLdapConfiguration); Assert.assertNotNull("No groups found", userGroups); }', 'output': 'public void testShouldGroupAttributeConfigurationCheckSucceedWhenGroupForUserDnIsFound() throws Exception { Map<String, Object> configMap = groupConfigObjectMap(); SearchRequest sr = new SearchRequestImpl(); AmbariLdapConfiguration ambariLdapConfiguration = new TestAmbariLdapConfigurationFactory().createLdapConfiguration(configMap); EasyMock.expect(ldapConnectionTemplateFactory.create(ambariLdapConfiguration)).andReturn(ldapConnectionTemplateMock); EasyMock.expect(ldapConnectionTemplateMock.newSearchRequest(EasyMock.anyObject(Dn.class), EasyMock.anyString(), EasyMock.anyObject(SearchScope.class))).andReturn(sr); EasyMock.expect(ldapConnectionTemplateMock.search(EasyMock.anyObject(SearchRequest.class), EasyMock.anyObject(EntryMapper.class))) .andReturn(Lists.newArrayList("userGroup")); replayAll(); Set<String> userGroups = ldapConfigurationService.checkGroupAttributes("userDn", ambariLdapConfiguration); Assert.assertNotNull("No groups found", userGroups); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static Breakpoint[] getBreakpoints () { Breakpoint[] bs = DebuggerManager.getDebuggerManager (). getBreakpoints (); int i, k = bs.length; XXX bb = new ArrayList<>(); for (i = 0; i < k; i++) if (bs[i] instanceof AntBreakpoint) bb.add (bs [i]); bs = new Breakpoint [bb.size ()]; return (Breakpoint[]) bb.toArray (bs); }', 'ground_truth': 'private static Breakpoint[] getBreakpoints () { Breakpoint[] bs = DebuggerManager.getDebuggerManager (). getBreakpoints (); int i, k = bs.length; ArrayList bb = new ArrayList (); for (i = 0; i < k; i++) if (bs[i] instanceof AntBreakpoint) bb.add (bs [i]); bs = new Breakpoint [bb.size ()]; return (Breakpoint[]) bb.toArray (bs); }', 'output': 'private static Breakpoint[] getBreakpoints () { Breakpoint[] bs = DebuggerManager.getDebuggerManager (). getBreakpoints (); int i, k = bs.length; List<Breakpoint> bb = new ArrayList<>(); for (i = 0; i < k; i++) if (bs[i] instanceof AntBreakpoint) bb.add (bs [i]); bs = new Breakpoint [bb.size ()]; return (Breakpoint[]) bb.toArray (bs); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testParDoWithUnexpectedKeyedState() { Pipeline p = TestPipeline.create(); List<Integer> inputs = Arrays.asList(3, -42, 666); XXX input = createInts(p, inputs); input .apply(ParDo.of(new TestUnexpectedKeyedStateDoFn())); try { p.run(); fail("should have failed"); } catch (RuntimeException exn) { assertThat(exn.toString(), containsString("Keyed state is only available")); } }', 'ground_truth': 'public void testParDoWithUnexpectedKeyedState() { Pipeline p = TestPipeline.create(); List<KV<String, Integer>> inputs = Arrays.asList( KV.of("a", 1)); PCollection<KV<String, Integer>> input = p.apply(Create.of(inputs)); input .apply(ParDo.of(new TestUnexpectedKeyedStateDoFn())); try { p.run(); fail("should have failed"); } catch (RuntimeException exn) { assertThat(exn.toString(), containsString("Keyed state is only available")); } }', 'output': 'public void testParDoWithUnexpectedKeyedState() { Pipeline p = TestPipeline.create(); List<Integer> inputs = Arrays.asList(3, -42, 666); DoFn<Integer, Integer> input = createInts(p, inputs); input .apply(ParDo.of(new TestUnexpectedKeyedStateDoFn())); try { p.run(); fail("should have failed"); } catch (RuntimeException exn) { assertThat(exn.toString(), containsString("Keyed state is only available")); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void visit(GroovyCodeVisitor visitor) { AnnotationNode node = (AnnotationNode) getValue(); XXX attrs = node.getMembers(); for(Iterator it = attrs.values().iterator(); it.hasNext(); ) { ((Expression) it.next()).visit(visitor); } }', 'ground_truth': 'public void visit(GroovyCodeVisitor visitor) { AnnotationNode node = (AnnotationNode) getValue(); Map<String, Expression> attrs = node.getMembers(); for(Expression expr : attrs.values() ) { expr.visit(visitor); } }', 'output': 'public void visit(GroovyCodeVisitor visitor) { AnnotationNode node = (AnnotationNode) getValue(); Map<String, Expression> attrs = node.getMembers(); for(Iterator it = attrs.values().iterator(); it.hasNext(); ) { ((Expression) it.next()).visit(visitor); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected Boolean executeJob(Object arg) throws GridException { Collection args = (Collection)arg; Iterator<Object> it = args.iterator(); assert args.size() == 3 : args.size(); XXX obj = (GridPortableObject)it.next(); String cacheName = (String)it.next(); String expAffKey = (String)it.next(); Object affKey = ignite.cache(cacheName).affinity().affinityKey(obj); if (!expAffKey.equals(affKey)) throw new GridException("Unexpected affinity key: " + affKey); if (!ignite.cache(cacheName).affinity().mapKeyToNode(obj).isLocal()) throw new GridException("Job is not run on primary node."); return true; } } }', 'ground_truth': 'protected Boolean executeJob(Object arg) throws GridException { Collection args = (Collection)arg; Iterator<Object> it = args.iterator(); assert args.size() == 3 : args.size(); PortableObject obj = (PortableObject)it.next(); String cacheName = (String)it.next(); String expAffKey = (String)it.next(); Object affKey = ignite.cache(cacheName).affinity().affinityKey(obj); if (!expAffKey.equals(affKey)) throw new GridException("Unexpected affinity key: " + affKey); if (!ignite.cache(cacheName).affinity().mapKeyToNode(obj).isLocal()) throw new GridException("Job is not run on primary node."); return true; }', 'output': 'protected Boolean executeJob(Object arg) throws GridException { Collection args = (Collection)arg; Iterator<Object> it = args.iterator(); assert args.size() == 3 : args.size(); IgnitePortableObject obj = (IgnitePortableObject)it.next(); String cacheName = (String)it.next(); String expAffKey = (String)it.next(); Object affKey = ignite.cache(cacheName).affinity().affinityKey(obj); if (!expAffKey.equals(affKey)) throw new GridException("Unexpected affinity key: " + affKey); if (!ignite.cache(cacheName).affinity().mapKeyToNode(obj).isLocal()) throw new GridException("Job is not run on primary node."); return true; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMultipleOperations() throws Exception { GridCache<Integer, String> cache = cache(0); try (IgniteTx tx = cache.txStart(OPTIMISTIC, REPEATABLE_READ)) { cache.put(1, "val"); cache.put(2, "val"); cache.put(3, "val"); cache.get(4); cache.putAll(F.asMap(5, "val", 6, "val")); tx.commit(); } checkStoreUsage(4, 0, 1, 1); }', 'ground_truth': 'public void testMultipleOperations() throws Exception { Cache<Integer, String> cache = cache(0); try (IgniteTx tx = cache.txStart(OPTIMISTIC, REPEATABLE_READ)) { cache.put(1, "val"); cache.put(2, "val"); cache.put(3, "val"); cache.get(4); cache.putAll(F.asMap(5, "val", 6, "val")); tx.commit(); } checkStoreUsage(4, 0, 1, 1); }', 'output': 'public void testMultipleOperations() throws Exception { Cache<Integer, String> cache = cache(0); try (IgniteTx tx = cache.txStart(OPTIMISTIC, REPEATABLE_READ)) { cache.put(1, "val"); cache.put(2, "val"); cache.put(3, "val"); cache.get(4); cache.putAll(F.asMap(5, "val", 6, "val")); tx.commit(); } checkStoreUsage(4, 0, 1, 1); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public GridSelect toGridSelect(Select select) { XXX res = (GridSelect)h2ObjToGridObj.get(select); if (res != null) return res; res = new GridSelect(); h2ObjToGridObj.put(select, res); res.distinct(select.isDistinct()); Expression where = CONDITION.get(select); res.where(toGridExpression(where)); Set<TableFilter> allFilers = new HashSet<>(select.getTopFilters()); GridSqlElement from = null; TableFilter filter = select.getTopTableFilter(); do { assert0(filter != null, select); assert0(!filter.isJoinOuter(), select); assert0(filter.getNestedJoin() == null, select); assert0(filter.getJoinCondition() == null, select); assert0(filter.getFilterCondition() == null, select); GridSqlElement gridFilter = toGridTableFilter(filter); from = from == null ? gridFilter : new GridJoin(from, gridFilter); allFilers.remove(filter); filter = filter.getJoin(); } while (filter != null); res.from(from); assert allFilers.isEmpty(); ArrayList<Expression> expressions = select.getExpressions(); int[] grpIdx = GROUP_INDEXES.get(select); if (grpIdx != null) { for (int idx : grpIdx) res.addGroupExpression(toGridExpression(expressions.get(idx))); } assert0(select.getHaving() == null, select); int havingIdx = HAVING_INDEX.get(select); if (havingIdx >= 0) res.having(toGridExpression(expressions.get(havingIdx))); for (int i = 0; i < select.getColumnCount(); i++) res.addSelectExpression(toGridExpression(expressions.get(i))); SortOrder sortOrder = select.getSortOrder(); if (sortOrder != null) { int[] indexes = sortOrder.getQueryColumnIndexes(); int[] sortTypes = sortOrder.getSortTypes(); for (int i = 0; i < indexes.length; i++) res.addSort(toGridExpression(expressions.get(indexes[i])), sortTypes[i]); } return res; }', 'ground_truth': 'public GridSqlSelect toGridSelect(Select select) { GridSqlSelect res = (GridSqlSelect)h2ObjToGridObj.get(select); if (res != null) return res; res = new GridSqlSelect(); h2ObjToGridObj.put(select, res); res.distinct(select.isDistinct()); Expression where = CONDITION.get(select); res.where(toGridExpression(where)); Set<TableFilter> allFilers = new HashSet<>(select.getTopFilters()); GridSqlElement from = null; TableFilter filter = select.getTopTableFilter(); do { assert0(filter != null, select); assert0(!filter.isJoinOuter(), select); assert0(filter.getNestedJoin() == null, select); assert0(filter.getJoinCondition() == null, select); assert0(filter.getFilterCondition() == null, select); GridSqlElement gridFilter = toGridTableFilter(filter); from = from == null ? gridFilter : new GridSqlJoin(from, gridFilter); allFilers.remove(filter); filter = filter.getJoin(); } while (filter != null); res.from(from); assert allFilers.isEmpty(); ArrayList<Expression> expressions = select.getExpressions(); int[] grpIdx = GROUP_INDEXES.get(select); if (grpIdx != null) { for (int idx : grpIdx) res.addGroupExpression(toGridExpression(expressions.get(idx))); } assert0(select.getHaving() == null, select); int havingIdx = HAVING_INDEX.get(select); if (havingIdx >= 0) res.having(toGridExpression(expressions.get(havingIdx))); for (int i = 0; i < select.getColumnCount(); i++) res.addSelectExpression(toGridExpression(expressions.get(i))); SortOrder sortOrder = select.getSortOrder(); if (sortOrder != null) { int[] indexes = sortOrder.getQueryColumnIndexes(); int[] sortTypes = sortOrder.getSortTypes(); for (int i = 0; i < indexes.length; i++) res.addSort(toGridExpression(expressions.get(indexes[i])), sortTypes[i]); } return res; }', 'output': 'public GridSelect toGridSelect(Select select) { GridSelect res = (GridSelect)h2ObjToGridObj.get(select); if (res != null) return res; res = new GridSelect(); h2ObjToGridObj.put(select, res); res.distinct(select.isDistinct()); Expression where = CONDITION.get(select); res.where(toGridExpression(where)); Set<TableFilter> allFilers = new HashSet<>(select.getTopFilters()); GridSqlElement from = null; TableFilter filter = select.getTopTableFilter(); do { assert0(filter != null, select); assert0(!filter.isJoinOuter(), select); assert0(filter.getNestedJoin() == null, select); assert0(filter.getJoinCondition() == null, select); assert0(filter.getFilterCondition() == null, select); GridSqlElement gridFilter = toGridTableFilter(filter); from = from == null ? gridFilter : new GridJoin(from, gridFilter); allFilers.remove(filter); filter = filter.getJoin(); } while (filter != null); res.from(from); assert allFilers.isEmpty(); ArrayList<Expression> expressions = select.getExpressions(); int[] grpIdx = GROUP_INDEXES.get(select); if (grpIdx != null) { for (int idx : grpIdx) res.addGroupExpression(toGridExpression(expressions.get(idx))); } assert0(select.getHaving() == null, select); int havingIdx = HAVING_INDEX.get(select); if (havingIdx >= 0) res.having(toGridExpression(expressions.get(havingIdx))); for (int i = 0; i < select.getColumnCount(); i++) res.addSelectExpression(toGridExpression(expressions.get(i))); SortOrder sortOrder = select.getSortOrder(); if (sortOrder != null) { int[] indexes = sortOrder.getQueryColumnIndexes(); int[] sortTypes = sortOrder.getSortTypes(); for (int i = 0; i < indexes.length; i++) res.addSort(toGridExpression(expressions.get(indexes[i])), sortTypes[i]); } return res; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testDeadlock() throws Exception { try { Ignite g = startGrid(1); final CountDownLatch latch = new CountDownLatch(1); g.events().localListen(new IgnitePredicate<GridEvent>() { @Override public boolean apply(GridEvent evt) { assert evt.type() == EVT_NODE_JOINED; latch.countDown(); return true; } }, EVT_NODE_JOINED); XXX f = multithreadedAsync(new Callable<Object>() { @Override public Object call() throws Exception { startGrid(2); return null; } }, 1); assert latch.await(5, SECONDS); info(">>> Starting task."); executeAsync(compute(g.cluster().forPredicate(F.equalTo(F.first(g.cluster().forRemotes().nodes())))), "GridGarHelloWorldTask", "HELLOWORLD.MSG").get(60000); f.get(); } catch (Exception e) { error("Test failed.", e); } finally { stopAllGrids(); } }', 'ground_truth': 'public void testDeadlock() throws Exception { try { Ignite g = startGrid(1); final CountDownLatch latch = new CountDownLatch(1); g.events().localListen(new IgnitePredicate<GridEvent>() { @Override public boolean apply(GridEvent evt) { assert evt.type() == EVT_NODE_JOINED; latch.countDown(); return true; } }, EVT_NODE_JOINED); IgniteFuture<?> f = multithreadedAsync(new Callable<Object>() { @Override public Object call() throws Exception { startGrid(2); return null; } }, 1); assert latch.await(5, SECONDS); info(">>> Starting task."); executeAsync(compute(g.cluster().forPredicate(F.equalTo(F.first(g.cluster().forRemotes().nodes())))), "GridGarHelloWorldTask", "HELLOWORLD.MSG").get(60000); f.get(); } catch (Exception e) { error("Test failed.", e); } finally { stopAllGrids(); } }', 'output': 'public void testDeadlock() throws Exception { try { Ignite g = startGrid(1); final CountDownLatch latch = new CountDownLatch(1); g.events().localListen(new IgnitePredicate<GridEvent>() { @Override public boolean apply(GridEvent evt) { assert evt.type() == EVT_NODE_JOINED; latch.countDown(); return true; } }, EVT_NODE_JOINED); IgniteFuture f = multithreadedAsync(new Callable<Object>() { @Override public Object call() throws Exception { startGrid(2); return null; } }, 1); assert latch.await(5, SECONDS); info(">>> Starting task."); executeAsync(compute(g.cluster().forPredicate(F.equalTo(F.first(g.cluster().forRemotes().nodes())))), "GridGarHelloWorldTask", "HELLOWORLD.MSG").get(60000); f.get(); } catch (Exception e) { error("Test failed.", e); } finally { stopAllGrids(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected Can<ManagedObject> loadAdapters(final List<?> pojos) { val adapters = _Lists.<ManagedObject>newArrayList(); for (val pojo : pojos) { XXX adapter; if(pojo instanceof Persistable) { adapter = persistenceSession.initializeEntity((Persistable) pojo); _Assert.assertNotNull(adapter); } else { adapter = persistenceSession.adapterFor(pojo); _Assert.assertNotNull(adapter); } adapters.add(adapter); } return Can.ofCollection(adapters); }', 'ground_truth': 'protected Can<ManagedObject> loadAdapters(final List<?> pojos) { val adapters = _Lists.<ManagedObject>newArrayList(); for (val pojo : pojos) { ManagedObject adapter; if(pojo instanceof Persistable) { adapter = persistenceSession.initializeEntity((Persistable) pojo); _Assert.assertNotNull(adapter); } else { adapter = persistenceSession.adapterFor(pojo); _Assert.assertNotNull(adapter); } adapters.add(adapter); } return Can.ofCollection(adapters); }', 'output': 'protected Can<ManagedObject> loadAdapters(final List<?> pojos) { val adapters = _Lists.<ManagedObject>newArrayList(); for (val pojo : pojos) { ManagedObject adapter; if(pojo instanceof Persistable) { adapter = persistenceSession.initializeEntity((Persistable) pojo); _Assert.assertNotNull(adapter); } else { adapter = persistenceSession.adapterFor(pojo); _Assert.assertNotNull(adapter); } adapters.add(adapter); } return Can.ofCollection(adapters); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testLoadCacheAsyncFiltered() throws Exception { GridCache<Integer, ValueObject> cache = cache(); cache.loadCacheAsync(new P2<Integer, ValueObject>() { @Override public boolean apply(Integer key, ValueObject val) { return key >= 5; } }, 0).get(); assert cache.size() == PUT_CNT - 5; Collection<Map.Entry<Integer, ValueObject>> res = cache.queries().createSqlQuery(ValueObject.class, "val >= 0").execute().get(); assert res != null; assert res.size() == PUT_CNT - 5; assert size(ValueObject.class) == PUT_CNT - 5; }', 'ground_truth': 'public void testLoadCacheAsyncFiltered() throws Exception { IgniteCache<Integer, ValueObject> cache = grid(0).jcache(null); cache.withAsync().loadCache(new P2<Integer, ValueObject>() { @Override public boolean apply(Integer key, ValueObject val) { return key >= 5; } }, 0); cache.future().get(); assert cache.size() == PUT_CNT - 5; Collection<Cache.Entry<Integer, ValueObject>> res = cache.query(new QuerySqlPredicate<Integer, ValueObject>("val >= 0")).getAll(); assert res != null; assert res.size() == PUT_CNT - 5; assert size(ValueObject.class) == PUT_CNT - 5; }', 'output': 'public void testLoadCacheAsyncFiltered() throws Exception { Cache<Integer, ValueObject> cache = cache(); cache.loadCacheAsync(new P2<Integer, ValueObject>() { @Override public boolean apply(Integer key, ValueObject val) { return key >= 5; } }, 0).get(); assert cache.size() == PUT_CNT - 5; Collection<Map.Entry<Integer, ValueObject>> res = cache.queries().createSqlQuery(ValueObject.class, "val >= 0").execute().get(); assert res != null; assert res.size() == PUT_CNT - 5; assert size(ValueObject.class) == PUT_CNT - 5; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private AutoSavingCache<RowCacheKey, ColumnFamily> initRowCache() { logger.info("Initializing row cache with capacity of {} MBs and provider {}", DatabaseDescriptor.getRowCacheSizeInMB(), DatabaseDescriptor.getRowCacheProvider().getClass().getName()); int rowCacheInMemoryCapacity = DatabaseDescriptor.getRowCacheSizeInMB() * 1024 * 1024; ICache<RowCacheKey, ColumnFamily> rc = DatabaseDescriptor.getRowCacheProvider().create(rowCacheInMemoryCapacity, true); AutoSavingCache<RowCacheKey, ColumnFamily> rowCache = new AutoSavingCache<RowCacheKey, ColumnFamily>(rc, CacheType.ROW_CACHE); int rowCacheKeysToSave = DatabaseDescriptor.getRowCacheKeysToSave(); logger.info("Scheduling row cache save to each {} seconds (going to save {} keys).", rowCacheSavePeriod, rowCacheKeysToSave == Integer.MAX_VALUE ? "all" : rowCacheKeysToSave); rowCache.scheduleSaving(rowCacheSavePeriod, rowCacheKeysToSave); return rowCache; }', 'ground_truth': 'private AutoSavingCache<RowCacheKey, IRowCacheEntry> initRowCache() { logger.info("Initializing row cache with capacity of {} MBs and provider {}", DatabaseDescriptor.getRowCacheSizeInMB(), DatabaseDescriptor.getRowCacheProvider().getClass().getName()); int rowCacheInMemoryCapacity = DatabaseDescriptor.getRowCacheSizeInMB() * 1024 * 1024; ICache<RowCacheKey, IRowCacheEntry> rc = DatabaseDescriptor.getRowCacheProvider().create(rowCacheInMemoryCapacity, true); AutoSavingCache<RowCacheKey, IRowCacheEntry> rowCache = new AutoSavingCache<RowCacheKey, IRowCacheEntry>(rc, CacheType.ROW_CACHE); int rowCacheKeysToSave = DatabaseDescriptor.getRowCacheKeysToSave(); logger.info("Scheduling row cache save to each {} seconds (going to save {} keys).", rowCacheSavePeriod, rowCacheKeysToSave == Integer.MAX_VALUE ? "all" : rowCacheKeysToSave); rowCache.scheduleSaving(rowCacheSavePeriod, rowCacheKeysToSave); return rowCache; }', 'output': 'private AutoSavingCache<RowCacheKey, ColumnFamily> initRowCache() { logger.info("Initializing row cache with capacity of {} MBs and provider {}", DatabaseDescriptor.getRowCacheSizeInMB(), DatabaseDescriptor.getRowCacheProvider().getClass().getName()); int rowCacheInMemoryCapacity = DatabaseDescriptor.getRowCacheSizeInMB() * 1024 * 1024; ICache<RowCacheKey, ColumnFamily> rc = DatabaseDescriptor.getRowCacheProvider().create(rowCacheInMemoryCapacity, true); AutoSavingCache<RowCacheKey, ColumnFamily> rowCache = new AutoSavingCache<RowCacheKey, ColumnFamily>(rc, CacheType.ROW_CACHE); int rowCacheKeysToSave = DatabaseDescriptor.getRowCacheKeysToSave(); logger.info("Scheduling row cache save to each {} seconds (going to save {} keys).", rowCacheSavePeriod, rowCacheKeysToSave == Integer.MAX_VALUE ? "all" : rowCacheKeysToSave); rowCache.scheduleSaving(rowCacheSavePeriod, rowCacheKeysToSave); return rowCache; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void testPutGetRemove(final boolean clear) throws Exception { info(">>> Test grid concurrent linked hash map iterator..."); final ConcurrentLinkedHashMap<Integer, String> linkedMap = new ConcurrentLinkedHashMap<>(); Collection<Integer> original = new HashSet<>(); final int keyCnt = 10000; if (!clear) for (int i = 0; i < keyCnt; i++) { linkedMap.put(i, "value" + i); original.add(i); } long start = System.currentTimeMillis(); XXX fut = multithreadedAsync( new Callable<Object>() { @Nullable @Override public Object call() throws Exception { Random rnd = new Random(); int iterCnt = 100000; for (int i = 0; i < iterCnt; i++) { int key = rnd.nextInt(keyCnt); if (clear) { linkedMap.put(key, "value" + key); linkedMap.get(key); linkedMap.remove(key); } else { linkedMap.get(key); linkedMap.remove(key); linkedMap.put(key, "value" + key); } } return null; } }, 10, "updater" ); fut.get(); Set<Integer> keys = linkedMap.keySet(); if (clear) assertTrue("Keys must not be in map " + keys, keys.isEmpty()); else { original.removeAll(keys); assertTrue("Keys must be in map: " + original, original.isEmpty()); } info(">>> put get remove test complete [duration = " + (System.currentTimeMillis() - start) + \']\'); }', 'ground_truth': 'private void testPutGetRemove(final boolean clear) throws Exception { info(">>> Test grid concurrent linked hash map iterator..."); final ConcurrentLinkedHashMap<Integer, String> linkedMap = new ConcurrentLinkedHashMap<>(); Collection<Integer> original = new HashSet<>(); final int keyCnt = 10000; if (!clear) for (int i = 0; i < keyCnt; i++) { linkedMap.put(i, "value" + i); original.add(i); } long start = System.currentTimeMillis(); IgniteInternalFuture<?> fut = multithreadedAsync( new Callable<Object>() { @Nullable @Override public Object call() throws Exception { Random rnd = new Random(); int iterCnt = 100000; for (int i = 0; i < iterCnt; i++) { int key = rnd.nextInt(keyCnt); if (clear) { linkedMap.put(key, "value" + key); linkedMap.get(key); linkedMap.remove(key); } else { linkedMap.get(key); linkedMap.remove(key); linkedMap.put(key, "value" + key); } } return null; } }, 10, "updater" ); fut.get(); Set<Integer> keys = linkedMap.keySet(); if (clear) assertTrue("Keys must not be in map " + keys, keys.isEmpty()); else { original.removeAll(keys); assertTrue("Keys must be in map: " + original, original.isEmpty()); } info(">>> put get remove test complete [duration = " + (System.currentTimeMillis() - start) + \']\'); }', 'output': 'private void testPutGetRemove(final boolean clear) throws Exception { info(">>> Test grid concurrent linked hash map iterator..."); final ConcurrentLinkedHashMap<Integer, String> linkedMap = new ConcurrentLinkedHashMap<>(); Collection<Integer> original = new HashSet<>(); final int keyCnt = 10000; if (!clear) for (int i = 0; i < keyCnt; i++) { linkedMap.put(i, "value" + i); original.add(i); } long start = System.currentTimeMillis(); IgniteFuture<?> fut = multithreadedAsync( new Callable<Object>() { @Nullable @Override public Object call() throws Exception { Random rnd = new Random(); int iterCnt = 100000; for (int i = 0; i < iterCnt; i++) { int key = rnd.nextInt(keyCnt); if (clear) { linkedMap.put(key, "value" + key); linkedMap.get(key); linkedMap.remove(key); } else { linkedMap.get(key); linkedMap.remove(key); linkedMap.put(key, "value" + key); } } return null; } }, 10, "updater" ); fut.get(); Set<Integer> keys = linkedMap.keySet(); if (clear) assertTrue("Keys must not be in map " + keys, keys.isEmpty()); else { original.removeAll(keys); assertTrue("Keys must be in map: " + original, original.isEmpty()); } info(">>> put get remove test complete [duration = " + (System.currentTimeMillis() - start) + \']\'); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAtomicReferenceConstantMultipleTopologyChange() throws Exception { try { XXX s = cache().dataStructures().atomicReference(STRUCTURE_NAME, 1, true); IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new CA() { @Override public void apply() { try { for (int i = 0; i < TOP_CHANGE_CNT; i++) { Collection<String> names = new GridLeanSet<>(3); try { for (int j = 0; j < 3; j++) { String name = UUID.randomUUID().toString(); names.add(name); Ignite g = startGrid(name); assert g.cache(null).dataStructures().<Integer>atomicReference(STRUCTURE_NAME, 1, true) .get() > 0; } } finally { if (i != TOP_CHANGE_CNT - 1) for (String name : names) stopGrid(name); } } } catch (Exception e) { throw F.wrap(e); } } }, TOP_CHANGE_THREAD_CNT, "topology-change-thread"); int val = s.get(); while (!fut.isDone()) { assert s.get() == val; s.set(++val); } fut.get(); for (Ignite g : G.allGrids()) assert g.cache(null).dataStructures().<Integer>atomicReference(STRUCTURE_NAME, 1, true).get() == val; } finally { cache().dataStructures().removeAtomicReference(STRUCTURE_NAME); } }', 'ground_truth': 'public void testAtomicReferenceConstantMultipleTopologyChange() throws Exception { try { CacheAtomicReference<Integer> s = cache().dataStructures().atomicReference(STRUCTURE_NAME, 1, true); IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new CA() { @Override public void apply() { try { for (int i = 0; i < TOP_CHANGE_CNT; i++) { Collection<String> names = new GridLeanSet<>(3); try { for (int j = 0; j < 3; j++) { String name = UUID.randomUUID().toString(); names.add(name); Ignite g = startGrid(name); assert g.cache(null).dataStructures().<Integer>atomicReference(STRUCTURE_NAME, 1, true) .get() > 0; } } finally { if (i != TOP_CHANGE_CNT - 1) for (String name : names) stopGrid(name); } } } catch (Exception e) { throw F.wrap(e); } } }, TOP_CHANGE_THREAD_CNT, "topology-change-thread"); int val = s.get(); while (!fut.isDone()) { assert s.get() == val; s.set(++val); } fut.get(); for (Ignite g : G.allGrids()) assert g.cache(null).dataStructures().<Integer>atomicReference(STRUCTURE_NAME, 1, true).get() == val; } finally { cache().dataStructures().removeAtomicReference(STRUCTURE_NAME); } }', 'output': 'public void testAtomicReferenceConstantMultipleTopologyChange() throws Exception { try { AtomicReference<Integer> s = cache().dataStructures().atomicReference(STRUCTURE_NAME, 1, true); IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new CA() { @Override public void apply() { try { for (int i = 0; i < TOP_CHANGE_CNT; i++) { Collection<String> names = new GridLeanSet<>(3); try { for (int j = 0; j < 3; j++) { String name = UUID.randomUUID().toString(); names.add(name); Ignite g = startGrid(name); assert g.cache(null).dataStructures().<Integer>atomicReference(STRUCTURE_NAME, 1, true) .get() > 0; } } finally { if (i != TOP_CHANGE_CNT - 1) for (String name : names) stopGrid(name); } } } catch (Exception e) { throw F.wrap(e); } } }, TOP_CHANGE_THREAD_CNT, "topology-change-thread"); int val = s.get(); while (!fut.isDone()) { assert s.get() == val; s.set(++val); } fut.get(); for (Ignite g : G.allGrids()) assert g.cache(null).dataStructures().<Integer>atomicReference(STRUCTURE_NAME, 1, true).get() == val; } finally { cache().dataStructures().removeAtomicReference(STRUCTURE_NAME); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testRemoveAfterClear() throws Exception { IgniteEx ignite = grid(0); CacheDistributionMode distroMode = ignite.jcache(null).getConfiguration(CacheConfiguration.class).getDistributionMode(); if (distroMode == CacheDistributionMode.NEAR_ONLY || distroMode == CacheDistributionMode.CLIENT_ONLY) { if (gridCount() < 2) return; ignite = grid(1); } IgniteCache<Integer, Integer> cache = ignite.jcache(null); int key = 0; XXX keys = new ArrayList<>(); for (int k = 0; k < 2; k++) { while (!ignite.affinity(null).isPrimary(ignite.localNode(), key)) key++; keys.add(key); key++; } System.out.println(keys); for (Integer k : keys) cache.put(k, k); cache.clear(); for (int g = 0; g < gridCount(); g++) { Ignite grid0 = grid(g); grid0.jcache(null).removeAll(); assertTrue(grid0.jcache(null).localSize() == 0); } }', 'ground_truth': 'public void testRemoveAfterClear() throws Exception { IgniteEx ignite = grid(0); CacheDistributionMode distroMode = ignite.jcache(null).getConfiguration(CacheConfiguration.class).getDistributionMode(); if (distroMode == CacheDistributionMode.NEAR_ONLY || distroMode == CacheDistributionMode.CLIENT_ONLY) { if (gridCount() < 2) return; ignite = grid(1); } IgniteCache<Integer, Integer> cache = ignite.jcache(null); int key = 0; Collection<Integer> keys = new ArrayList<>(); for (int k = 0; k < 2; k++) { while (!ignite.affinity(null).isPrimary(ignite.localNode(), key)) key++; keys.add(key); key++; } info("Keys: " + keys); for (Integer k : keys) cache.put(k, k); cache.clear(); for (int g = 0; g < gridCount(); g++) { Ignite grid0 = grid(g); grid0.jcache(null).removeAll(); assertTrue(grid0.jcache(null).localSize() == 0); } }', 'output': 'public void testRemoveAfterClear() throws Exception { IgniteEx ignite = grid(0); CacheDistributionMode distroMode = ignite.jcache(null).getConfiguration(CacheConfiguration.class).getDistributionMode(); if (distroMode == CacheDistributionMode.NEAR_ONLY || distroMode == CacheDistributionMode.CLIENT_ONLY) { if (gridCount() < 2) return; ignite = grid(1); } IgniteCache<Integer, Integer> cache = ignite.jcache(null); int key = 0; List<Integer> keys = new ArrayList<>(); for (int k = 0; k < 2; k++) { while (!ignite.affinity(null).isPrimary(ignite.localNode(), key)) key++; keys.add(key); key++; } System.out.println(keys); for (Integer k : keys) cache.put(k, k); cache.clear(); for (int g = 0; g < gridCount(); g++) { Ignite grid0 = grid(g); grid0.jcache(null).removeAll(); assertTrue(grid0.jcache(null).localSize() == 0); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void run(CQLQueryExecutor executor) throws IOException { if (session.getColumnFamilyType() == ColumnFamilyType.Super) throw new RuntimeException("Super columns are not implemented for CQL"); if (cqlQuery == null) { StringBuilder query = new StringBuilder("SELECT "); if (session.cqlVersion.startsWith("2")) query.append("FIRST ").append(session.getColumnsPerKey()).append(" \'\'..\'\'"); else query.append("*"); String counterCF = session.cqlVersion.startsWith("2") ? "Counter1" : "Counter3"; query.append(" FROM ").append(wrapInQuotesIfRequired(counterCF)); if (session.cqlVersion.startsWith("2")) query.append(" USING CONSISTENCY ").append(session.getConsistencyLevel().toString()); cqlQuery = query.append(" WHERE KEY=?").toString(); } byte[] key = generateKey(); List<String> queryParams = Collections.singletonList(getUnQuotedCqlBlob(key, session.cqlVersion.startsWith("3"))); XXX context = session.latency.time(); boolean success = false; String exceptionMessage = null; for (int t = 0; t < session.getRetryTimes(); t++) { if (success) break; try { success = executor.execute(cqlQuery, queryParams); } catch (Exception e) { exceptionMessage = getExceptionMessage(e); success = false; } } if (!success) { error(String.format("Operation [%d] retried %d times - error reading counter key %s %s%n", index, session.getRetryTimes(), new String(key), (exceptionMessage == null) ? "" : "(" + exceptionMessage + ")")); } session.operations.getAndIncrement(); session.keys.getAndIncrement(); context.stop(); }', 'ground_truth': 'protected void run(CQLQueryExecutor executor) throws IOException { if (session.getColumnFamilyType() == ColumnFamilyType.Super) throw new RuntimeException("Super columns are not implemented for CQL"); if (cqlQuery == null) { StringBuilder query = new StringBuilder("SELECT "); if (session.cqlVersion.startsWith("2")) query.append("FIRST ").append(session.getColumnsPerKey()).append(" \'\'..\'\'"); else query.append("*"); String counterCF = session.cqlVersion.startsWith("2") ? "Counter1" : "Counter3"; query.append(" FROM ").append(wrapInQuotesIfRequired(counterCF)); if (session.cqlVersion.startsWith("2")) query.append(" USING CONSISTENCY ").append(session.getConsistencyLevel().toString()); cqlQuery = query.append(" WHERE KEY=?").toString(); } byte[] key = generateKey(); List<String> queryParams = Collections.singletonList(getUnQuotedCqlBlob(key, session.cqlVersion.startsWith("3"))); Context context = session.latency.time(); boolean success = false; String exceptionMessage = null; for (int t = 0; t < session.getRetryTimes(); t++) { if (success) break; try { success = executor.execute(cqlQuery, queryParams); } catch (Exception e) { exceptionMessage = getExceptionMessage(e); success = false; } } if (!success) { error(String.format("Operation [%d] retried %d times - error reading counter key %s %s%n", index, session.getRetryTimes(), new String(key), (exceptionMessage == null) ? "" : "(" + exceptionMessage + ")")); } session.operations.getAndIncrement(); session.keys.getAndIncrement(); context.stop(); }', 'output': 'protected void run(CQLQueryExecutor executor) throws IOException { if (session.getColumnFamilyType() == ColumnFamilyType.Super) throw new RuntimeException("Super columns are not implemented for CQL"); if (cqlQuery == null) { StringBuilder query = new StringBuilder("SELECT "); if (session.cqlVersion.startsWith("2")) query.append("FIRST ").append(session.getColumnsPerKey()).append(" \'\'..\'\'"); else query.append("*"); String counterCF = session.cqlVersion.startsWith("2") ? "Counter1" : "Counter3"; query.append(" FROM ").append(wrapInQuotesIfRequired(counterCF)); if (session.cqlVersion.startsWith("2")) query.append(" USING CONSISTENCY ").append(session.getConsistencyLevel().toString()); cqlQuery = query.append(" WHERE KEY=?").toString(); } byte[] key = generateKey(); List<String> queryParams = Collections.singletonList(getUnQuotedCqlBlob(key, session.cqlVersion.startsWith("3"))); ExecutionContext context = session.latency.time(); boolean success = false; String exceptionMessage = null; for (int t = 0; t < session.getRetryTimes(); t++) { if (success) break; try { success = executor.execute(cqlQuery, queryParams); } catch (Exception e) { exceptionMessage = getExceptionMessage(e); success = false; } } if (!success) { error(String.format("Operation [%d] retried %d times - error reading counter key %s %s%n", index, session.getRetryTimes(), new String(key), (exceptionMessage == null) ? "" : "(" + exceptionMessage + ")")); } session.operations.getAndIncrement(); session.keys.getAndIncrement(); context.stop(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testNodeLeft() throws Exception { hasCache = true; try { final Map<UUID, Map<Long, GridFutureAdapter<GridIndexingFieldsResult>>> map = U.field(((GridKernal)grid(0)).internalCache().context().queries(), "fieldsQryRes"); map.clear(); Ignite g = startGrid(); GridCache<Integer, Integer> cache = g.cache(null); GridCacheQuery<List<?>> q = cache.queries().createSqlFieldsQuery("select _key from Integer where _key >= " + "0 order by _key"); q.pageSize(50); ClusterGroup prj = g.cluster().forNodes(Arrays.asList(g.cluster().localNode(), grid(0).localNode())); q = q.projection(prj); GridCacheQueryFuture<List<?>> fut = q.execute(); assertEquals(0, fut.next().get(0)); assertTrue(GridTestUtils.waitForCondition(new PA() { @Override public boolean apply() { return map.size() == 1; } }, getTestTimeout())); Map<Long, GridFutureAdapter<GridIndexingFieldsResult>> futs = map.get(g.cluster().localNode().id()); assertEquals(1, futs.size()); final UUID nodeId = g.cluster().localNode().id(); final CountDownLatch latch = new CountDownLatch(1); grid(0).events().localListen(new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent evt) { if (((IgniteDiscoveryEvent) evt).eventNode().id().equals(nodeId)) latch.countDown(); return true; } }, EVT_NODE_LEFT); stopGrid(); latch.await(); assertEquals(0, map.size()); } finally { stopGrid(); } }', 'ground_truth': 'public void testNodeLeft() throws Exception { hasCache = true; try { final Map<UUID, Map<Long, GridFutureAdapter<IndexingFieldsResult>>> map = U.field(((GridKernal)grid(0)).internalCache().context().queries(), "fieldsQryRes"); map.clear(); Ignite g = startGrid(); GridCache<Integer, Integer> cache = g.cache(null); GridCacheQuery<List<?>> q = cache.queries().createSqlFieldsQuery("select _key from Integer where _key >= " + "0 order by _key"); q.pageSize(50); ClusterGroup prj = g.cluster().forNodes(Arrays.asList(g.cluster().localNode(), grid(0).localNode())); q = q.projection(prj); GridCacheQueryFuture<List<?>> fut = q.execute(); assertEquals(0, fut.next().get(0)); assertTrue(GridTestUtils.waitForCondition(new PA() { @Override public boolean apply() { return map.size() == 1; } }, getTestTimeout())); Map<Long, GridFutureAdapter<IndexingFieldsResult>> futs = map.get(g.cluster().localNode().id()); assertEquals(1, futs.size()); final UUID nodeId = g.cluster().localNode().id(); final CountDownLatch latch = new CountDownLatch(1); grid(0).events().localListen(new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent evt) { if (((IgniteDiscoveryEvent) evt).eventNode().id().equals(nodeId)) latch.countDown(); return true; } }, EVT_NODE_LEFT); stopGrid(); latch.await(); assertEquals(0, map.size()); } finally { stopGrid(); } }', 'output': 'public void testNodeLeft() throws Exception { hasCache = true; try { final Map<UUID, Map<Long, GridFutureAdapter<GridIndexingFieldsResult>>> map = U.field(((GridKernal)grid(0)).internalCache().context().queries(), "fieldsQryRes"); map.clear(); Ignite g = startGrid(); GridCache<Integer, Integer> cache = g.cache(null); GridCacheQuery<List<?>> q = cache.queries().createSqlFieldsQuery("select _key from Integer where _key >= " + "0 order by _key"); q.pageSize(50); ClusterGroup prj = g.cluster().forNodes(Arrays.asList(g.cluster().localNode(), grid(0).localNode())); q = q.projection(prj); GridCacheQueryFuture<List<?>> fut = q.execute(); assertEquals(0, fut.next().get(0)); assertTrue(GridTestUtils.waitForCondition(new PA() { @Override public boolean apply() { return map.size() == 1; } }, getTestTimeout())); Map<Long, GridFutureAdapter<GridIndexingFieldsResult>> futs = map.get(g.cluster().localNode().id()); assertEquals(1, futs.size()); final UUID nodeId = g.cluster().localNode().id(); final CountDownLatch latch = new CountDownLatch(1); grid(0).events().localListen(new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent evt) { if (((IgniteDiscoveryEvent) evt).eventNode().id().equals(nodeId)) latch.countDown(); return true; } }, EVT_NODE_LEFT); stopGrid(); latch.await(); assertEquals(0, map.size()); } finally { stopGrid(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void delete() { Response created = doCreateSavedQuery(); XXX createdId = ((SavedQuery) ((JSONObject) created.getEntity()).get("savedQuery")).getId(); Response response = savedQueryService.delete(String.valueOf(createdId)); Assert.assertEquals(204, response.getStatus()); thrown.expect(NotFoundFormattedException.class); savedQueryService.getOne(String.valueOf(createdId)); }', 'ground_truth': 'public void delete() { Response created = doCreateSavedQuery(); Object createdId = ((SavedQuery) ((JSONObject) created.getEntity()).get("savedQuery")).getId(); Response response = savedQueryService.delete(String.valueOf(createdId)); Assert.assertEquals(204, response.getStatus()); thrown.expect(NotFoundFormattedException.class); savedQueryService.getOne(String.valueOf(createdId)); }', 'output': 'public void delete() { Response created = doCreateSavedQuery(); String createdId = ((SavedQuery) ((JSONObject) created.getEntity()).get("savedQuery")).getId(); Response response = savedQueryService.delete(createdId); Assert.assertEquals(204, response.getStatus()); thrown.expect(NotFoundFormattedException.class); savedQueryService.getOne(createdId); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testHistorySupported() throws Exception { try { final Ignite g = startGrid(); GridDiscoverySpi spi = g.configuration().getDiscoverySpi(); XXX ann = U.getAnnotation(spi.getClass(), GridDiscoverySpiHistorySupport.class); assertNotNull("Spi does not have annotation for history support", ann); assertTrue("History support is disabled for current spi", ann.value()); } finally { stopGrid(); } }', 'ground_truth': 'public void testHistorySupported() throws Exception { try { final Ignite g = startGrid(); DiscoverySpi spi = g.configuration().getDiscoverySpi(); DiscoverySpiHistorySupport ann = U.getAnnotation(spi.getClass(), DiscoverySpiHistorySupport.class); assertNotNull("Spi does not have annotation for history support", ann); assertTrue("History support is disabled for current spi", ann.value()); } finally { stopGrid(); } }', 'output': 'public void testHistorySupported() throws Exception { try { final Ignite g = startGrid(); GridDiscoverySpi spi = g.configuration().getDiscoverySpi(); Annotation ann = U.getAnnotation(spi.getClass(), GridDiscoverySpiHistorySupport.class); assertNotNull("Spi does not have annotation for history support", ann); assertTrue("History support is disabled for current spi", ann.value()); } finally { stopGrid(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public IgniteInternalFuture<?> cancel(String name) { while (true) { try { GridFutureAdapter<?> fut = new GridFutureAdapter<>(ctx); GridFutureAdapter<?> old; if ((old = undepFuts.putIfAbsent(name, fut)) != null) fut = old; else { GridServiceDeploymentKey key = new GridServiceDeploymentKey(name); if (cache.remove(key) == null) { undepFuts.remove(name); fut.onDone(); } } return fut; } catch (XXX e) { if (log.isDebugEnabled()) log.debug("Topology changed while deploying service (will retry): " + e.getMessage()); } catch (IgniteCheckedException e) { log.error("Failed to undeploy service: " + name, e); return new GridFinishedFuture<>(ctx, e); } } }', 'ground_truth': 'public IgniteInternalFuture<?> cancel(String name) { while (true) { try { GridFutureAdapter<?> fut = new GridFutureAdapter<>(ctx); GridFutureAdapter<?> old; if ((old = undepFuts.putIfAbsent(name, fut)) != null) fut = old; else { GridServiceDeploymentKey key = new GridServiceDeploymentKey(name); if (cache.remove(key) == null) { undepFuts.remove(name); fut.onDone(); } } return fut; } catch (ClusterTopologyCheckedException e) { if (log.isDebugEnabled()) log.debug("Topology changed while deploying service (will retry): " + e.getMessage()); } catch (IgniteCheckedException e) { log.error("Failed to undeploy service: " + name, e); return new GridFinishedFuture<>(ctx, e); } } }', 'output': 'public IgniteInternalFuture<?> cancel(String name) { while (true) { try { GridFutureAdapter<?> fut = new GridFutureAdapter<>(ctx); GridFutureAdapter<?> old; if ((old = undepFuts.putIfAbsent(name, fut)) != null) fut = old; else { GridServiceDeploymentKey key = new GridServiceDeploymentKey(name); if (cache.remove(key) == null) { undepFuts.remove(name); fut.onDone(); } } return fut; } catch (IgniteCheckedException e) { log.error("Failed to undeploy service: " + name, e); return new GridFinishedFuture<>(ctx, e); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testRightOuterJoin() throws Exception { String sql = "SELECT * FROM " + "(select order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL \'1\' HOUR)) o1 " + " RIGHT OUTER JOIN " + "(select site_id as order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY site_id, TUMBLE(order_time, INTERVAL \'1\' HOUR)) o2 " + " on " + " o1.order_id=o2.order_id" ; XXX rows = BeamSqlCli.compilePipeline(sql, pipeline, beamSqlEnv); PAssert.that(rows.apply(ParDo.of(new TestUtils.BeamSqlRow2StringDoFn()))) .containsInAnyOrder( TestUtils.RowsBuilder.of( Types.INTEGER, "order_id", Types.INTEGER, "sum_site_id", Types.INTEGER, "order_id0", Types.INTEGER, "sum_site_id0" ).addRows( 1, 3, 1, 1, null, null, 2, 2, 2, 5, 2, 2, null, null, 3, 3 ).getStringRows() ); pipeline.run(); }', 'ground_truth': 'public void testRightOuterJoin() throws Exception { String sql = "SELECT * FROM " + "(select order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL \'1\' HOUR)) o1 " + " RIGHT OUTER JOIN " + "(select site_id as order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY site_id, TUMBLE(order_time, INTERVAL \'1\' HOUR)) o2 " + " on " + " o1.order_id=o2.order_id" ; PCollection<BeamRecord> rows = BeamSqlCli.compilePipeline(sql, pipeline, beamSqlEnv); PAssert.that(rows.apply(ParDo.of(new TestUtils.BeamSqlRow2StringDoFn()))) .containsInAnyOrder( TestUtils.RowsBuilder.of( Types.INTEGER, "order_id", Types.INTEGER, "sum_site_id", Types.INTEGER, "order_id0", Types.INTEGER, "sum_site_id0" ).addRows( 1, 3, 1, 1, null, null, 2, 2, 2, 5, 2, 2, null, null, 3, 3 ).getStringRows() ); pipeline.run(); }', 'output': 'public void testRightOuterJoin() throws Exception { String sql = "SELECT * FROM " + "(select order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL \'1\' HOUR)) o1 " + " RIGHT OUTER JOIN " + "(select site_id as order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY site_id, TUMBLE(order_time, INTERVAL \'1\' HOUR)) o2 " + " on " + " o1.order_id=o2.order_id" ; BeamSqlCli.compilePipeline(sql, pipeline, beamSqlEnv).run(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testDoFnWithContextDisplayData() { DoFn<String, String> fn = new DoFn<String, String>() { @ProcessElement public void proccessElement(ProcessContext c) {} @Override public void populateDisplayData(Builder builder) { builder.add(DisplayData.item("fnMetadata", "foobar")); } }; Bound<String, String> parDo = ParDo.of(fn); DisplayData displayData = DisplayData.from(parDo); assertThat(displayData, includesDisplayDataFor("fn", fn)); assertThat(displayData, hasDisplayItem("fn", fn.getClass())); }', 'ground_truth': 'public void testDoFnWithContextDisplayData() { DoFn<String, String> fn = new DoFn<String, String>() { @ProcessElement public void proccessElement(ProcessContext c) {} @Override public void populateDisplayData(Builder builder) { builder.add(DisplayData.item("fnMetadata", "foobar")); } }; SingleOutput<String, String> parDo = ParDo.of(fn); DisplayData displayData = DisplayData.from(parDo); assertThat(displayData, includesDisplayDataFor("fn", fn)); assertThat(displayData, hasDisplayItem("fn", fn.getClass())); }', 'output': 'public void testDoFnWithContextDisplayData() { DoFn<String, String> fn = new DoFn<String, String>() { @ProcessElement public void proccessElement(ProcessContext c) {} @Override public void populateDisplayData(Builder builder) { builder.add(DisplayData.item("fnMetadata", "foobar")); } }; Bound<String, String> parDo = ParDo.of(fn); DisplayData displayData = DisplayData.from(parDo); assertThat(displayData, includesDisplayDataFor("fn", fn)); assertThat(displayData, hasDisplayItem("fn", fn.getClass())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testJobFailedOver() throws Exception { failed.set(false); routed.set(false); try { Ignite ignite1 = startGrid(NODE1); Ignite ignite2 = startGrid(NODE2); Ignite ignite3 = startGrid(NODE3); assert ignite1 != null; assert ignite2 != null; assert ignite3 != null; Integer res = (Integer)compute(ignite1.cluster().forPredicate(p)).withTimeout(10000). execute(JobFailTask.class.getName(), "1"); assert res == 1; } catch (XXX ignored) { failed.set(true); } finally { assertFalse(failed.get()); assertTrue(routed.get()); stopGrid(NODE1); stopGrid(NODE2); stopGrid(NODE3); } }', 'ground_truth': 'public void testJobFailedOver() throws Exception { failed.set(false); routed.set(false); try { Ignite ignite1 = startGrid(NODE1); Ignite ignite2 = startGrid(NODE2); Ignite ignite3 = startGrid(NODE3); assert ignite1 != null; assert ignite2 != null; assert ignite3 != null; Integer res = (Integer)compute(ignite1.cluster().forPredicate(p)).withTimeout(10000). execute(JobFailTask.class.getName(), "1"); assert res == 1; } catch (ClusterTopologyException ignored) { failed.set(true); } finally { assertFalse(failed.get()); assertTrue(routed.get()); stopGrid(NODE1); stopGrid(NODE2); stopGrid(NODE3); } }', 'output': 'public void testJobFailedOver() throws Exception { failed.set(false); routed.set(false); try { Ignite ignite1 = startGrid(NODE1); Ignite ignite2 = startGrid(NODE2); Ignite ignite3 = startGrid(NODE3); assert ignite1 != null; assert ignite2 != null; assert ignite3 != null; Integer res = (Integer)compute(ignite1.cluster().forPredicate(p)).withTimeout(10000). execute(JobFailTask.class.getName(), "1"); assert res == 1; } catch (IgniteException ignored) { failed.set(true); } finally { assertFalse(failed.get()); assertTrue(routed.get()); stopGrid(NODE1); stopGrid(NODE2); stopGrid(NODE3); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testStackRegisterConditionalDependenciesNoOozieClient() throws Exception { EasyMockSupport mockSupport = new EasyMockSupport(); AmbariManagementController mockMgmtController = mockSupport.createMock(AmbariManagementController.class); expect(mockMgmtController.getStackServices(isA(Set.class))).andReturn(Collections.<StackServiceResponse>emptySet()); final DependencyInfo hCatDependency = new TestDependencyInfo("HIVE/HCAT"); final DependencyInfo yarnClientDependency = new TestDependencyInfo("YARN/YARN_CLIENT"); final DependencyInfo tezClientDependency = new TestDependencyInfo("TEZ/TEZ_CLIENT"); final DependencyInfo mapReduceTwoClientDependency = new TestDependencyInfo("YARN/MAPREDUCE2_CLIENT"); mockSupport.replayAll(); XXX testStack = new BaseBlueprintProcessor.Stack("HDP", "2.1", mockMgmtController) { @Override public Collection<DependencyInfo> getDependenciesForComponent(String component) { if (component.equals("NAGIOS_SERVER")) { Set<DependencyInfo> setOfDependencies = new HashSet<DependencyInfo>(); setOfDependencies.add(hCatDependency); setOfDependencies.add(yarnClientDependency); setOfDependencies.add(tezClientDependency); setOfDependencies.add(mapReduceTwoClientDependency); return setOfDependencies; } return Collections.emptySet(); } }; assertEquals("Initial conditional dependency map should be empty", 0, testStack.getDependencyConditionalServiceMap().size()); testStack.registerConditionalDependencies(); assertEquals("Set of conditional service mappings is an incorrect size", 4, testStack.getDependencyConditionalServiceMap().size()); assertEquals("Incorrect service dependency for HCAT", "HIVE", testStack.getDependencyConditionalServiceMap().get(hCatDependency)); assertEquals("Incorrect service dependency for YARN_CLIENT", "YARN", testStack.getDependencyConditionalServiceMap().get(yarnClientDependency)); assertEquals("Incorrect service dependency for TEZ_CLIENT", "TEZ", testStack.getDependencyConditionalServiceMap().get(tezClientDependency)); assertEquals("Incorrect service dependency for MAPREDUCE2_CLIENT", "MAPREDUCE2", testStack.getDependencyConditionalServiceMap().get(mapReduceTwoClientDependency)); mockSupport.verifyAll(); }', 'ground_truth': 'public void testStackRegisterConditionalDependenciesNoOozieClient() throws Exception { EasyMockSupport mockSupport = new EasyMockSupport(); AmbariManagementController mockMgmtController = mockSupport.createMock(AmbariManagementController.class); expect(mockMgmtController.getStackServices(isA(Set.class))).andReturn(Collections.<StackServiceResponse>emptySet()); final DependencyInfo hCatDependency = new TestDependencyInfo("HIVE/HCAT"); final DependencyInfo yarnClientDependency = new TestDependencyInfo("YARN/YARN_CLIENT"); final DependencyInfo tezClientDependency = new TestDependencyInfo("TEZ/TEZ_CLIENT"); final DependencyInfo mapReduceTwoClientDependency = new TestDependencyInfo("YARN/MAPREDUCE2_CLIENT"); mockSupport.replayAll(); Stack testStack = new Stack("HDP", "2.1", mockMgmtController) { @Override public Collection<DependencyInfo> getDependenciesForComponent(String component) { if (component.equals("NAGIOS_SERVER")) { Set<DependencyInfo> setOfDependencies = new HashSet<DependencyInfo>(); setOfDependencies.add(hCatDependency); setOfDependencies.add(yarnClientDependency); setOfDependencies.add(tezClientDependency); setOfDependencies.add(mapReduceTwoClientDependency); return setOfDependencies; } return Collections.emptySet(); } }; assertEquals("Initial conditional dependency map should be empty", 0, testStack.getDependencyConditionalServiceMap().size()); testStack.registerConditionalDependencies(); assertEquals("Set of conditional service mappings is an incorrect size", 4, testStack.getDependencyConditionalServiceMap().size()); assertEquals("Incorrect service dependency for HCAT", "HIVE", testStack.getDependencyConditionalServiceMap().get(hCatDependency)); assertEquals("Incorrect service dependency for YARN_CLIENT", "YARN", testStack.getDependencyConditionalServiceMap().get(yarnClientDependency)); assertEquals("Incorrect service dependency for TEZ_CLIENT", "TEZ", testStack.getDependencyConditionalServiceMap().get(tezClientDependency)); assertEquals("Incorrect service dependency for MAPREDUCE2_CLIENT", "MAPREDUCE2", testStack.getDependencyConditionalServiceMap().get(mapReduceTwoClientDependency)); mockSupport.verifyAll(); }', 'output': 'public void testStackRegisterConditionalDependenciesNoOozieClient() throws Exception { EasyMockSupport mockSupport = new EasyMockSupport(); AmbariManagementController mockMgmtController = mockSupport.createMock(AmbariManagementController.class); expect(mockMgmtController.getStackServices(isA(Set.class))).andReturn(Collections.<StackServiceResponse>emptySet()); final DependencyInfo hCatDependency = new TestDependencyInfo("HIVE/HCAT"); final DependencyInfo yarnClientDependency = new TestDependencyInfo("YARN/YARN_CLIENT"); final DependencyInfo tezClientDependency = new TestDependencyInfo("TEZ/TEZ_CLIENT"); final DependencyInfo mapReduceTwoClientDependency = new TestDependencyInfo("YARN/MAPREDUCE2_CLIENT"); mockSupport.replayAll(); Stack stack = new BaseBlueprintProcessor.Stack("HDP", "2.1", mockMgmtController) { @Override public Collection<DependencyInfo> getDependenciesForComponent(String component) { if (component.equals("NAGIOS_SERVER")) { Set<DependencyInfo> setOfDependencies = new HashSet<DependencyInfo>(); setOfDependencies.add(hCatDependency); setOfDependencies.add(yarnClientDependency); setOfDependencies.add(tezClientDependency); setOfDependencies.add(mapReduceTwoClientDependency); return setOfDependencies; } return Collections.emptySet(); } }; assertEquals("Initial conditional dependency map should be empty", 0, stack.getDependencyConditionalServiceMap().size()); stack.registerConditionalDependencies(); assertEquals("Set of conditional service mappings is an incorrect size", 4, stack.getDependencyConditionalServiceMap().size()); assertEquals("Incorrect service dependency for HCAT", "HIVE", stack.getDependencyConditionalServiceMap().get(hCatDependency)); assertEquals("Incorrect service dependency for YARN_CLIENT", "YARN", stack.getDependencyConditionalServiceMap().get(yarnClientDependency)); assertEquals("Incorrect service dependency for TEZ_CLIENT", "TEZ", stack.getDependencyConditionalServiceMap().get(tezClientDependency)); assertEquals'}
{'instruction': 'Please refactor the following code:', 'input': 'public Object get(Object key) { Object ret = super.get(key); if (ret!=null) { XXX weak = (WeakReference) ret; ret = weak.get(); if (ret==null) remove(key); } return ret; }', 'ground_truth': 'public Object get(Object key) { Object ret = super.get(key); if (ret!=null) { SoftReference weak = (SoftReference) ret; ret = weak.get(); if (ret==null) remove(key); } return ret; }', 'output': 'public Object get(Object key) { Object ret = super.get(key); if (ret!=null) { WeakReference<Object> weak = (WeakReference<Object>) ret; ret = weak.get(); if (ret==null) remove(key); } return ret; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testConfigurationFromConfig() { XXX config = new FlinkJobServerDriver.ServerConfiguration(); FlinkJobServerDriver driver = FlinkJobServerDriver.fromConfig(config); assertThat(driver.configuration, is(config)); }', 'ground_truth': 'public void testConfigurationFromConfig() { FlinkJobServerDriver.FlinkServerConfiguration config = new FlinkJobServerDriver.FlinkServerConfiguration(); FlinkJobServerDriver driver = FlinkJobServerDriver.fromConfig(config); assertThat(driver.configuration, is(config)); }', 'output': 'public void testConfigurationFromConfig() { ServerConfiguration config = new ServerConfiguration(); FlinkJobServerDriver driver = FlinkJobServerDriver.fromConfig(config); assertThat(driver.configuration, is(config)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMapRun() throws Exception { int lineCnt = 10000; String fileName = "/testFile"; prepareFile(fileName, lineCnt); totalLineCnt.set(0); taskWorkDirs.clear(); Configuration cfg = new Configuration(); cfg.setStrings("fs.ggfs.impl", GridGgfsHadoopFileSystem.class.getName()); Job job = Job.getInstance(cfg); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setMapperClass(TestMapper.class); job.setNumReduceTasks(0); job.setInputFormatClass(TextInputFormat.class); FileInputFormat.setInputPaths(job, new Path("ggfs: FileOutputFormat.setOutputPath(job, new Path("ggfs: job.setJarByClass(getClass()); XXX fut = grid(0).hadoop().submit(new GridHadoopJobId(UUID.randomUUID(), 1), createJobInfo(job.getConfiguration())); fut.get(); assertEquals(lineCnt, totalLineCnt.get()); assertEquals(32, taskWorkDirs.size()); }', 'ground_truth': 'public void testMapRun() throws Exception { int lineCnt = 10000; String fileName = "/testFile"; prepareFile(fileName, lineCnt); totalLineCnt.set(0); taskWorkDirs.clear(); Configuration cfg = new Configuration(); cfg.setStrings("fs.ggfs.impl", GridGgfsHadoopFileSystem.class.getName()); Job job = Job.getInstance(cfg); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setMapperClass(TestMapper.class); job.setNumReduceTasks(0); job.setInputFormatClass(TextInputFormat.class); FileInputFormat.setInputPaths(job, new Path("ggfs: FileOutputFormat.setOutputPath(job, new Path("ggfs: job.setJarByClass(getClass()); IgniteFuture<?> fut = grid(0).hadoop().submit(new GridHadoopJobId(UUID.randomUUID(), 1), createJobInfo(job.getConfiguration())); fut.get(); assertEquals(lineCnt, totalLineCnt.get()); assertEquals(32, taskWorkDirs.size()); }', 'output': 'public void testMapRun() throws Exception { int lineCnt = 10000; String fileName = "/testFile"; prepareFile(fileName, lineCnt); totalLineCnt.set(0); taskWorkDirs.clear(); Configuration cfg = new Configuration(); cfg.setStrings("fs.ggfs.impl", GridGgfsHadoopFileSystem.class.getName()); Job job = Job.getInstance(cfg); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setMapperClass(TestMapper.class); job.setNumReduceTasks(0); job.setInputFormatClass(TextInputFormat.class); FileInputFormat.setInputPaths(job, new Path("ggfs: FileOutputFormat.setOutputPath(job, new Path("ggfs: job.setJarByClass(getClass()); IgniteFuture<?> fut = grid(0).hadoop().submit(new GridHadoopJobId(UUID.randomUUID(), 1), createJobInfo(job.getConfiguration())); fut.get(); assertEquals(lineCnt, totalLineCnt.get()); assertEquals(32, taskWorkDirs.size()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void validateFeature( Feature feature, SqlParserPos context) { if (feature.equals(disabledFeature)) { XXX ex = new EigenbaseException( FEATURE_DISABLED, null); if (context == null) { throw ex; } throw new EigenbaseContextException( "location", ex, context.getLineNum(), context.getColumnNum(), context.getEndLineNum(), context.getEndColumnNum()); } }', 'ground_truth': 'protected void validateFeature( Feature feature, SqlParserPos context) { if (feature.equals(disabledFeature)) { CalciteException ex = new CalciteException( FEATURE_DISABLED, null); if (context == null) { throw ex; } throw new CalciteContextException( "location", ex, context.getLineNum(), context.getColumnNum(), context.getEndLineNum(), context.getEndColumnNum()); } }', 'output': 'protected void validateFeature( Feature feature, SqlParserPos context) { if (feature.equals(disabledFeature)) { EigenbaseException ex = new EigenbaseException( FEATURE_DISABLED, null); if (context == null) { throw ex; } throw new EigenbaseContextException( "location", ex, context.getLineNum(), context.getColumnNum(), context.getEndLineNum(), context.getEndColumnNum()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSaveLoad() throws ExecutionException, InterruptedException, WriteTimeoutException { ColumnFamilyStore cfs = Keyspace.open(KEYSPACE1).getColumnFamilyStore(COUNTER1); cfs.truncateBlocking(); CacheService.instance.invalidateCounterCache(); new CounterMutation(new RowUpdateBuilder(cfs.metadata(), 0, bytes(1)).clustering(1).add("c", 1L).build(), ConsistencyLevel.ONE).apply(); new CounterMutation(new RowUpdateBuilder(cfs.metadata(), 0, bytes(1)).clustering(2).add("c", 2L).build(), ConsistencyLevel.ONE).apply(); new CounterMutation(new RowUpdateBuilder(cfs.metadata(), 0, bytes(2)).clustering(1).add("c", 1L).build(), ConsistencyLevel.ONE).apply(); new CounterMutation(new RowUpdateBuilder(cfs.metadata(), 0, bytes(2)).clustering(2).add("c", 2L).build(), ConsistencyLevel.ONE).apply(); assertEquals(4, CacheService.instance.counterCache.size()); CacheService.instance.counterCache.submitWrite(Integer.MAX_VALUE).get(); CacheService.instance.invalidateCounterCache(); assertEquals(0, CacheService.instance.counterCache.size()); CacheService.instance.counterCache.loadSaved(); assertEquals(4, CacheService.instance.counterCache.size()); Clustering c1 = CBuilder.create(cfs.metadata().comparator).add(ByteBufferUtil.bytes(1)).build(); XXX c2 = CBuilder.create(cfs.metadata().comparator).add(ByteBufferUtil.bytes(2)).build(); ColumnMetadata cd = cfs.metadata().getColumn(ByteBufferUtil.bytes("c")); assertEquals(1L, cfs.getCachedCounter(bytes(1), c1, cd, null).count); assertEquals(2L, cfs.getCachedCounter(bytes(1), c2, cd, null).count); assertEquals(1L, cfs.getCachedCounter(bytes(2), c1, cd, null).count); assertEquals(2L, cfs.getCachedCounter(bytes(2), c2, cd, null).count); }', 'ground_truth': 'public void testSaveLoad() throws ExecutionException, InterruptedException, WriteTimeoutException { ColumnFamilyStore cfs = Keyspace.open(KEYSPACE1).getColumnFamilyStore(COUNTER1); cfs.truncateBlocking(); CacheService.instance.invalidateCounterCache(); new CounterMutation(new RowUpdateBuilder(cfs.metadata(), 0, bytes(1)).clustering(1).add("c", 1L).build(), ConsistencyLevel.ONE).apply(); new CounterMutation(new RowUpdateBuilder(cfs.metadata(), 0, bytes(1)).clustering(2).add("c", 2L).build(), ConsistencyLevel.ONE).apply(); new CounterMutation(new RowUpdateBuilder(cfs.metadata(), 0, bytes(2)).clustering(1).add("c", 1L).build(), ConsistencyLevel.ONE).apply(); new CounterMutation(new RowUpdateBuilder(cfs.metadata(), 0, bytes(2)).clustering(2).add("c", 2L).build(), ConsistencyLevel.ONE).apply(); assertEquals(4, CacheService.instance.counterCache.size()); CacheService.instance.counterCache.submitWrite(Integer.MAX_VALUE).get(); CacheService.instance.invalidateCounterCache(); assertEquals(0, CacheService.instance.counterCache.size()); CacheService.instance.counterCache.loadSaved(); assertEquals(4, CacheService.instance.counterCache.size()); Clustering<?> c1 = CBuilder.create(cfs.metadata().comparator).add(ByteBufferUtil.bytes(1)).build(); Clustering<?> c2 = CBuilder.create(cfs.metadata().comparator).add(ByteBufferUtil.bytes(2)).build(); ColumnMetadata cd = cfs.metadata().getColumn(ByteBufferUtil.bytes("c")); assertEquals(1L, cfs.getCachedCounter(bytes(1), c1, cd, null).count); assertEquals(2L, cfs.getCachedCounter(bytes(1), c2, cd, null).count); assertEquals(1L, cfs.getCachedCounter(bytes(2), c1, cd, null).count); assertEquals(2L, cfs.getCachedCounter(bytes(2), c2, cd, null).count); }', 'output': 'public void testSaveLoad() throws ExecutionException, InterruptedException, WriteTimeoutException { ColumnFamilyStore cfs = Keyspace.open(KEYSPACE1).getColumnFamilyStore(COUNTER1); cfs.truncateBlocking(); CacheService.instance.invalidateCounterCache(); new CounterMutation(new RowUpdateBuilder(cfs.metadata(), 0, bytes(1)).clustering(1).add("c", 1L).build(), ConsistencyLevel.ONE).apply(); new CounterMutation(new RowUpdateBuilder(cfs.metadata(), 0, bytes(1)).clustering(2).add("c", 2L).build(), ConsistencyLevel.ONE).apply(); new CounterMutation(new RowUpdateBuilder(cfs.metadata(), 0, bytes(2)).clustering(1).add("c", 1L).build(), ConsistencyLevel.ONE).apply(); new CounterMutation(new RowUpdateBuilder(cfs.metadata(), 0, bytes(2)).clustering(2).add("c", 2L).build(), ConsistencyLevel.ONE).apply(); assertEquals(4, CacheService.instance.counterCache.size()); CacheService.instance.counterCache.submitWrite(Integer.MAX_VALUE).get(); CacheService.instance.invalidateCounterCache(); assertEquals(0, CacheService.instance.counterCache.size()); CacheService.instance.counterCache.loadSaved(); assertEquals(4, CacheService.instance.counterCache.size()); Clustering c1 = CBuilder.create(cfs.metadata().comparator).add(ByteBufferUtil.bytes(1)).build(); Clustering c2 = CBuilder.create(cfs.metadata().comparator).add(ByteBufferUtil.bytes(2)).build(); ColumnMetadata cd = cfs.metadata().getColumn(ByteBufferUtil.bytes("c")); assertEquals(1L, cfs.getCachedCounter(bytes(1), c1, cd, null).count); assertEquals(2L, cfs.getCachedCounter(bytes(1), c2, cd, null).count); assertEquals(1L, cfs.getCachedCounter(bytes(2), c1, cd, null).count); assertEquals'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAddDeleteUnbounded() throws Exception { String queueName = UUID.randomUUID().toString(); String val = UUID.randomUUID().toString(); XXX queue = grid().cache(null).dataStructures().queue(queueName, 0, false, true); assert queue.add(val); assert queue.remove(val); assert queue.isEmpty(); }', 'ground_truth': 'public void testAddDeleteUnbounded() throws Exception { String queueName = UUID.randomUUID().toString(); String val = UUID.randomUUID().toString(); IgniteQueue<String> queue = grid().cache(null).dataStructures().queue(queueName, 0, false, true); assert queue.add(val); assert queue.remove(val); assert queue.isEmpty(); }', 'output': 'public void testAddDeleteUnbounded() throws Exception { String queueName = UUID.randomUUID().toString(); String val = UUID.randomUUID().toString(); IgniteCache<String, String> queue = grid().jcache(null).dataStructures().queue(queueName, 0, false, true); assert queue.add(val); assert queue.remove(val); assert queue.isEmpty(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSimpleNot() { XXX p1 = new PropertyIdImpl("prop1", "cat1", false); Resource resource = new ResourceImpl(Resource.Type.Cluster); resource.setProperty(p1, "foo"); /*  ! p1 == "foo" */ PredicateBuilder pb = new PredicateBuilder(); Predicate predicate1 = pb.not().property(p1).equals("foo").toPredicate(); Assert.assertFalse(predicate1.evaluate(resource)); /*  ! p1 == "bar" */ PredicateBuilder pb2 = new PredicateBuilder(); Predicate predicate2 = pb2.not().property(p1).equals("bar").toPredicate(); Assert.assertTrue(predicate2.evaluate(resource)); }', 'ground_truth': 'public void testSimpleNot() { String p1 = PropertyHelper.getPropertyId("cat1", "prop1"); Resource resource = new ResourceImpl(Resource.Type.Cluster); resource.setProperty(p1, "foo"); /*  ! p1 == "foo" */ PredicateBuilder pb = new PredicateBuilder(); Predicate predicate1 = pb.not().property(p1).equals("foo").toPredicate(); Assert.assertFalse(predicate1.evaluate(resource)); /*  ! p1 == "bar" */ PredicateBuilder pb2 = new PredicateBuilder(); Predicate predicate2 = pb2.not().property(p1).equals("bar").toPredicate(); Assert.assertTrue(predicate2.evaluate(resource)); }', 'output': 'public void testSimpleNot() { PropertyIdImpl p1 = new PropertyIdImpl("prop1", "cat1", false); Resource resource = new ResourceImpl(Resource.Type.Cluster); resource.setProperty(p1, "foo"); /*  ! p1 == "foo" */ PredicateBuilder pb = new PredicateBuilder(); Predicate predicate1 = pb.not().property(p1).equals("foo").toPredicate(); Assert.assertFalse(predicate1.evaluate(resource)); /*  ! p1 == "bar" */ PredicateBuilder pb2 = new PredicateBuilder(); Predicate predicate2 = pb2.not().property(p1).equals("bar").toPredicate(); Assert.assertTrue(predicate2.evaluate(resource)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAtomicLongConstantMultipleTopologyChange() throws Exception { try { XXX s = cache().dataStructures().atomicLong(STRUCTURE_NAME, 1, true); IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new CA() { @Override public void apply() { try { for (int i = 0; i < TOP_CHANGE_CNT; i++) { Collection<String> names = new GridLeanSet<>(3); try { for (int j = 0; j < 3; j++) { String name = UUID.randomUUID().toString(); names.add(name); Ignite g = startGrid(name); assert g.cache(null).dataStructures().atomicLong(STRUCTURE_NAME, 1, true).get() > 0; } } finally { if (i != TOP_CHANGE_CNT - 1) for (String name : names) stopGrid(name); } } } catch (Exception e) { throw F.wrap(e); } } }, TOP_CHANGE_THREAD_CNT, "topology-change-thread"); long val = s.get(); while (!fut.isDone()) { assert s.get() == val; assert s.incrementAndGet() == val + 1; val++; } fut.get(); for (Ignite g : G.allGrids()) assert g.cache(null).dataStructures().atomicLong(STRUCTURE_NAME, 1, true).get() == val; } finally { cache().dataStructures().removeAtomicLong(STRUCTURE_NAME); } }', 'ground_truth': 'public void testAtomicLongConstantMultipleTopologyChange() throws Exception { try { IgniteAtomicLong s = cache().dataStructures().atomicLong(STRUCTURE_NAME, 1, true); IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new CA() { @Override public void apply() { try { for (int i = 0; i < TOP_CHANGE_CNT; i++) { Collection<String> names = new GridLeanSet<>(3); try { for (int j = 0; j < 3; j++) { String name = UUID.randomUUID().toString(); names.add(name); Ignite g = startGrid(name); assert g.cache(null).dataStructures().atomicLong(STRUCTURE_NAME, 1, true).get() > 0; } } finally { if (i != TOP_CHANGE_CNT - 1) for (String name : names) stopGrid(name); } } } catch (Exception e) { throw F.wrap(e); } } }, TOP_CHANGE_THREAD_CNT, "topology-change-thread"); long val = s.get(); while (!fut.isDone()) { assert s.get() == val; assert s.incrementAndGet() == val + 1; val++; } fut.get(); for (Ignite g : G.allGrids()) assert g.cache(null).dataStructures().atomicLong(STRUCTURE_NAME, 1, true).get() == val; } finally { cache().dataStructures().removeAtomicLong(STRUCTURE_NAME); } }', 'output': 'public void testAtomicLongConstantMultipleTopologyChange() throws Exception { try { AtomicLong s = cache().dataStructures().atomicLong(STRUCTURE_NAME, 1, true); IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new CA() { @Override public void apply() { try { for (int i = 0; i < TOP_CHANGE_CNT; i++) { Collection<String> names = new GridLeanSet<>(3); try { for (int j = 0; j < 3; j++) { String name = UUID.randomUUID().toString(); names.add(name); Ignite g = startGrid(name); assert g.cache(null).dataStructures().atomicLong(STRUCTURE_NAME, 1, true).get() > 0; } } finally { if (i != TOP_CHANGE_CNT - 1) for (String name : names) stopGrid(name); } } } catch (Exception e) { throw F.wrap(e); } } }, TOP_CHANGE_THREAD_CNT, "topology-change-thread"); long val = s.get(); while (!fut.isDone()) { assert s.get() == val; assert s.incrementAndGet() == val + 1; val++; } fut.get(); for (Ignite g : G.allGrids()) assert g.cache(null).dataStructures().atomicLong(STRUCTURE_NAME, 1, true).get() == val; } finally { cache().dataStructures().removeAtomicLong(STRUCTURE_NAME); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Object getValueAt (Object node, String columnID) throws UnknownTypeException { boolean showValue = columnID == Constants.WATCH_VALUE_COLUMN_ID; if (showValue || columnID == Constants.WATCH_TYPE_COLUMN_ID) { EvalWatch ew; synchronized (evalWatches) { ew = evalWatches.get(node); } if (ew != null) { ew.startEvaluate(); switch (ew.getStatus()) { case READY: XXX result = ew.getResult(); if (showValue) { return result.getValue(); } else { return result.getType(); } case FAILED: if (showValue) { EvaluateException exc = ew.getException(); return exc.getLocalizedMessage(); } } return ""; } } throw new UnknownTypeException (node); }', 'ground_truth': 'public Object getValueAt (Object node, String columnID) throws UnknownTypeException { boolean showValue = columnID == Constants.WATCH_VALUE_COLUMN_ID; if (showValue || columnID == Constants.WATCH_TYPE_COLUMN_ID) { EvalWatch ew; synchronized (evalWatches) { ew = evalWatches.get(node); } if (ew != null) { ew.startEvaluate(); switch (ew.getStatus()) { case READY: NIVariable result = ew.getResult(); if (showValue) { return result.getValue(); } else { return result.getType(); } case FAILED: if (showValue) { EvaluateException exc = ew.getException(); return exc.getLocalizedMessage(); } } return ""; } } throw new UnknownTypeException (node); }', 'output': 'public Object getValueAt (Object node, String columnID) throws UnknownTypeException { boolean showValue = columnID == Constants.WATCH_VALUE_COLUMN_ID; if (showValue || columnID == Constants.WATCH_TYPE_COLUMN_ID) { EvalWatch ew; synchronized (evalWatches) { ew = evalWatches.get(node); } if (ew != null) { ew.startEvaluate(); switch (ew.getStatus()) { case READY: Object result = ew.getResult(); if (showValue) { return result.getValue(); } else { return result.getType(); } case FAILED: if (showValue) { EvaluateException exc = ew.getException(); return exc.getLocalizedMessage(); } } return ""; } } throw new UnknownTypeException (node); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testEmptyGrid() throws Exception { XXX qry = grid(0).cache(null).queries().createSqlFieldsQuery("select name, " + "age from Person where age = 25"); List<?> res = F.first(qry.execute().get()); assert res != null; assert res.size() == 2; assert "John White".equals(res.get(0)); assert res.get(1).equals(25); }', 'ground_truth': 'public void testEmptyGrid() throws Exception { CacheQuery<List<?>> qry = grid(0).cache(null).queries().createSqlFieldsQuery("select name, " + "age from Person where age = 25"); List<?> res = F.first(qry.execute().get()); assert res != null; assert res.size() == 2; assert "John White".equals(res.get(0)); assert res.get(1).equals(25); }', 'output': 'public void testEmptyGrid() throws Exception { IgniteCache<Person, Person> qry = grid(0).cache(null).queries().createSqlFieldsQuery("select name, " + "age from Person where age = 25"); List<?> res = F.first(qry.execute().get()); assert res != null; assert res.size() == 2; assert "John White".equals(res.get(0)); assert res.get(1).equals(25); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void representationChanged(Class type) { if ( Util.THIS.isLoggable() ) /* then */ Util.THIS.debug ("" + System.identityHashCode(this) + " entering synchronizator: " + type); if (isInSync()) return; try { setSyncOp(JUST_SYNCHRONIZING); Representation master = null; Representation all[] = getRepresentations(null); if ( Util.THIS.isLoggable() ) /* then */ Util.THIS.debug ("\\tReps: " + Arrays.asList(all)); XXX modified = new Vector(); for (int i=0; i<all.length; i++) { if (all[i].represents(type)) { master = all[i]; modified.add(all[i]); continue; } if (all[i].isModified()) { modified.add(all[i]); } } if (modified.size() > 1) { master = selectMasterRepresentation((Representation[])modified.toArray(new Representation[0])); } if ( Util.THIS.isLoggable() ) /* then */ Util.THIS.debug ("\\t" + System.identityHashCode(this) + " master: " + master); if (master == null) return; for (int i=0; i<all.length; i++) { if (all[i] == master) continue; if ( Util.THIS.isLoggable() ) /* then */ Util.THIS.debug("\\tpreffered update class: " + all[i].getUpdateClass()); Object change = master.getChange(all[i].getUpdateClass()); if (change == null) change = master.getChange(null); if (change != null) { if ( Util.THIS.isLoggable() ) /* then */ Util.THIS.debug ("\\t" + System.identityHashCode(this) + " updating: " + all[i] + " with:" + change.getClass()); all[i].update(change); } } } finally { setSyncOp(NOP); } }', 'ground_truth': 'protected void representationChanged(Class type) { if ( Util.THIS.isLoggable() ) /* then */ Util.THIS.debug ("" + System.identityHashCode(this) + " entering synchronizator: " + type); if (isInSync()) return; try { setSyncOp(JUST_SYNCHRONIZING); Representation master = null; Representation all[] = getRepresentations(null); if ( Util.THIS.isLoggable() ) /* then */ Util.THIS.debug ("\\tReps: " + Arrays.asList(all)); Vector<Representation> modified = new Vector<>(); for (int i=0; i<all.length; i++) { if (all[i].represents(type)) { master = all[i]; modified.add(all[i]); continue; } if (all[i].isModified()) { modified.add(all[i]); } } if (modified.size() > 1) { master = selectMasterRepresentation((Representation[])modified.toArray(new Representation[0])); } if ( Util.THIS.isLoggable() ) /* then */ Util.THIS.debug ("\\t" + System.identityHashCode(this) + " master: " + master); if (master == null) return; for (int i=0; i<all.length; i++) { if (all[i] == master) continue; if ( Util.THIS.isLoggable() ) /* then */ Util.THIS.debug("\\tpreffered update class: " + all[i].getUpdateClass()); Object change = master.getChange(all[i].getUpdateClass()); if (change == null) change = master.getChange(null); if (change != null) { if ( Util.THIS.isLoggable() ) /* then */ Util.THIS.debug ("\\t" + System.identityHashCode(this) + " updating: " + all[i] + " with:" + change.getClass()); all[i].update(change); } } } finally { setSyncOp(NOP); } }', 'output': 'protected void representationChanged(Class type) { if ( Util.THIS.isLoggable() ) /* then */ Util.THIS.debug ("" + System.identityHashCode(this) + " entering synchronizator: " + type); if (isInSync()) return; try { setSyncOp(JUST_SYNCHRONIZING); Representation master = null; Representation all[] = getRepresentations(null); if ( Util.THIS.isLoggable() ) /* then */ Util.THIS.debug ("\\tReps: " + Arrays.asList(all)); Vector modified = new Vector(); for (int i=0; i<all.length; i++) { if (all[i].represents(type)) { master = all[i]; modified.add(all[i]); continue; } if (all[i].isModified()) { modified.add(all[i]); } } if (modified.size() > 1) { master = selectMasterRepresentation((Representation[])modified.toArray(new Representation[0])); } if ( Util.THIS.isLoggable() ) /* then */ Util.THIS.debug ("\\t" + System.identityHashCode(this) + " master: " + master); if (master == null) return; for (int i=0; i<all.length; i++) { if (all[i] == master) continue; if ( Util.THIS.isLoggable() ) /* then */ Util.THIS.debug("\\tpreffered update class: " + all[i].getUpdateClass()); Object change = master.getChange(all[i].getUpdateClass()); if (change == null) change = master.getChange(null); if (change != null) { if ( Util.THIS.isLoggable() ) /* then */ Util.THIS.debug ("\\t" + System.identityHashCode(this) + " updating: " + all[i] + " with:" + change.getClass()); all[i].update(change); } } } finally { setSyncOp(NOP); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void addAllProperties(TemporalInfo temporalInfo) { allProperties = true; if (temporalInfo != null) { m_mapCategoryTemporalInfo.put(null, temporalInfo); } for (Map.Entry<String, ResourceInstance> entry : m_resource.getSubResources().entrySet()) { String name = entry.getKey(); if (! m_mapSubResources.containsKey(name)) { m_mapSubResources.put(name, entry.getValue()); } } }', 'ground_truth': 'private void addAllProperties(TemporalInfo temporalInfo) { allProperties = true; if (temporalInfo != null) { temporalInfoMap.put(null, temporalInfo); } for (Map.Entry<String, QueryImpl> entry : ensureSubResources().entrySet()) { String name = entry.getKey(); if (! querySubResourceSet.containsKey(name)) { querySubResourceSet.put(name, entry.getValue()); } } }', 'output': 'public void addAllProperties(TemporalInfo temporalInfo) { allProperties = true; if (temporalInfo != null) { m_mapCategoryTemporalInfo.put(null, temporalInfo); } for (Map.Entry<String, ResourceInstance> entry : m_resource.getSubResources().entrySet()) { String name = entry.getKey(); if (! m_mapSubResources.containsKey(name)) { m_mapSubResources.put(name, entry.getValue()); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMultiThreadedSwapUnswapObject() throws Exception { int threadCnt = 50; final int keyCnt = 4000; final int valCnt = 10000; final Ignite g = grid(0); final GridCache<Integer, TestValue> c = g.cache(null); assertEquals(0, g.cache(null).size()); assertEquals(0, c.queries().createSqlQuery(String.class, "1 = 1").execute().get().size()); assertEquals(0, c.queries().createSqlQuery(Long.class, "1 = 1").execute().get().size()); Random rnd = new Random(); for (int i = 0; i < keyCnt; i += 1 + rnd.nextInt(3)) { c.putx(i, new TestValue(rnd.nextInt(valCnt))); if (evictsEnabled() && rnd.nextBoolean()) assertTrue(c.evict(i)); } final AtomicBoolean done = new AtomicBoolean(); IgniteFuture<?> fut = multithreadedAsync(new CAX() { @Override public void applyx() throws IgniteCheckedException { Random rnd = new Random(); while (!done.get()) { int key = rnd.nextInt(keyCnt); switch (rnd.nextInt(5)) { case 0: c.putx(key, new TestValue(rnd.nextInt(valCnt))); break; case 1: if (evictsEnabled()) c.evict(key); break; case 2: c.remove(key); break; case 3: c.get(key); break; case 4: CacheQuery<Map.Entry<Integer, TestValue>> qry = c.queries().createSqlQuery( Long.class, "TestValue.val between ? and ?"); int from = rnd.nextInt(valCnt); CacheQueryFuture<Map.Entry<Integer, TestValue>> f = qry.execute(from, from + 250); Collection<Map.Entry<Integer, TestValue>> res = f.get(); for (Map.Entry<Integer, TestValue> ignored : res) { } } } } }, threadCnt); Thread.sleep(DURATION); done.set(true); fut.get(); }', 'ground_truth': 'public void testMultiThreadedSwapUnswapObject() throws Exception { int threadCnt = 50; final int keyCnt = 4000; final int valCnt = 10000; final Ignite g = grid(0); final IgniteCache<Integer, TestValue> c = g.jcache(null); assertEquals(0, g.cache(null).size()); assertEquals(0, c.query(new QuerySqlPredicate<Integer, TestValue>("1 = 1")).getAll().size()); Random rnd = new Random(); for (int i = 0; i < keyCnt; i += 1 + rnd.nextInt(3)) { c.put(i, new TestValue(rnd.nextInt(valCnt))); if (evictsEnabled() && rnd.nextBoolean()) c.localEvict(Arrays.asList(i)); } final AtomicBoolean done = new AtomicBoolean(); IgniteFuture<?> fut = multithreadedAsync(new CAX() { @Override public void applyx() throws IgniteCheckedException { Random rnd = new Random(); while (!done.get()) { int key = rnd.nextInt(keyCnt); switch (rnd.nextInt(5)) { case 0: c.put(key, new TestValue(rnd.nextInt(valCnt))); break; case 1: if (evictsEnabled()) c.localEvict(Arrays.asList(key)); break; case 2: c.remove(key); break; case 3: c.get(key); break; case 4: int from = rnd.nextInt(valCnt); Collection<Cache.Entry<Integer, TestValue>> res = c.query(new QuerySqlPredicate<Integer, TestValue>("TestValue.val between ? and ?", from, from + 250)).getAll(); for (Cache.Entry<Integer, TestValue> ignored : res) { } } } } }, threadCnt); Thread.sleep(DURATION); done.set(true); fut.get(); }', 'output': 'public void testMultiThreadedSwapUnswapObject() throws Exception { int threadCnt = 50; final int keyCnt = 4000; final int valCnt = 10000; final Ignite g = grid(0); final GridCache<Integer, TestValue> c = g.cache(null); assertEquals(0, g.cache(null).size()); assertEquals(0, c.queries().createSqlQuery(String.class, "1 = 1").execute().get().size()); assertEquals(0, c.queries().createSqlQuery(Long.class, "1 = 1").execute().get().size()); Random rnd = new Random(); for (int i = 0; i < keyCnt; i += 1 + rnd.nextInt(3)) { c.putx(i, new TestValue(rnd.nextInt(valCnt))); if (evictsEnabled() && rnd.nextBoolean()) assertTrue(c.evict(i)); } final AtomicBoolean done = new AtomicBoolean(); IgniteFuture<?> fut = multithreadedAsync(new CAX() { @Override public void applyx() throws IgniteCheckedException { Random rnd = new Random(); while (!done.get()) { int key = rnd.nextInt(keyCnt); switch (rnd.nextInt(5)) { case 0: c.putx(key, new TestValue(rnd.nextInt(valCnt))); break; case 1: if (evictsEnabled()) c.evict(key); break; case 2: c.remove(key); break; case 3: c.get(key); break; case 4: CacheQuery<Map.Entry<Integer, TestValue>> qry = c.queries().createSqlQuery( Long.class, "TestValue.val between ? and ?"); int from = rnd.nextInt(valCnt); CacheQueryFuture<Map.Entry<Integer, TestValue>> f = qry.execute(from, from + 250); Collection<Map.Entry<Integer, TestValue>> res = f.get(); for (Map.Entry<Integer, TestValue> ignored : res) { } } } } }, threadCnt); Thread.sleep(D'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMergeLatestWatermarkIntoSource() throws Exception { WatermarkHoldState<BoundedWindow> value1 = underTest.state(NAMESPACE_1, WATERMARK_LATEST_ADDR); WatermarkHoldState<BoundedWindow> value2 = underTest.state(NAMESPACE_2, WATERMARK_LATEST_ADDR); XXX value3 = underTest.state(NAMESPACE_3, WATERMARK_LATEST_ADDR); value1.add(new Instant(3000)); value2.add(new Instant(5000)); value1.add(new Instant(4000)); value2.add(new Instant(2000)); StateMerging.mergeWatermarks(Arrays.asList(value1, value2), value3, WINDOW_1); assertThat(value3.read(), equalTo(new Instant(5000))); assertThat(value1.read(), equalTo(null)); assertThat(value2.read(), equalTo(null)); }', 'ground_truth': 'public void testMergeLatestWatermarkIntoSource() throws Exception { WatermarkHoldState value1 = underTest.state(NAMESPACE_1, WATERMARK_LATEST_ADDR); WatermarkHoldState value2 = underTest.state(NAMESPACE_2, WATERMARK_LATEST_ADDR); WatermarkHoldState value3 = underTest.state(NAMESPACE_3, WATERMARK_LATEST_ADDR); value1.add(new Instant(3000)); value2.add(new Instant(5000)); value1.add(new Instant(4000)); value2.add(new Instant(2000)); StateMerging.mergeWatermarks(Arrays.asList(value1, value2), value3, WINDOW_1); assertThat(value3.read(), equalTo(new Instant(5000))); assertThat(value1.read(), equalTo(null)); assertThat(value2.read(), equalTo(null)); }', 'output': 'public void testMergeLatestWatermarkIntoSource() throws Exception { WatermarkHoldState<BoundedWindow> value1 = underTest.state(NAMESPACE_1, WATERMARK_LATEST_ADDR); WatermarkHoldState<BoundedWindow> value2 = underTest.state(NAMESPACE_2, WATERMARK_LATEST_ADDR); WatermarkHoldState<BoundedWindow> value3 = underTest.state(NAMESPACE_3, WATERMARK_LATEST_ADDR); value1.add(new Instant(3000)); value2.add(new Instant(5000)); value1.add(new Instant(4000)); value2.add(new Instant(2000)); StateMerging.mergeWatermarks(Arrays.asList(value1, value2), value3, WINDOW_1); assertThat(value3.read(), equalTo(new Instant(5000))); assertThat(value1.read(), equalTo(null)); assertThat(value2.read(), equalTo(null)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSelectQuery() throws Exception { GridCache<String, String> cache = ignite.cache(null); cache.putx("key", "value"); CacheQuery<Map.Entry<String, String>> qry = cache.queries().createSqlQuery(String.class, "select * from String"); CacheQueryFuture<Map.Entry<String, String>> iter = qry.execute(); assert iter != null; assert iter.next() != null; }', 'ground_truth': 'public void testSelectQuery() throws Exception { IgniteCache<String, String> cache = ignite.jcache(null); cache.put("key", "value"); QueryCursor<Cache.Entry<String, String>> qry = cache.query(new QuerySqlPredicate<String, String>( "select * from String")); Iterator<Cache.Entry<String, String>> iter = qry.iterator(); assert iter != null; assert iter.next() != null; }', 'output': 'public void testSelectQuery() throws Exception { IgniteCache<String, String> cache = ignite.jcache(null); cache.putx("key", "value"); CacheQuery<Map.Entry<String, String>> qry = cache.queries().createSqlQuery(String.class, "select * from String"); CacheQueryFuture<Map.Entry<String, String>> iter = qry.execute(); assert iter != null; assert iter.next() != null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String getAsText() { try { List fileobjs = (List) this.getValue(); StringBuffer result = new StringBuffer (); boolean first = true; for (XXX it = fileobjs.iterator(); it.hasNext();) { FileObject fo = (FileObject) it.next (); File f = FileUtil.toFile(fo); if (f != null) { if (!first) { result.append (File.pathSeparator); } else { first = false; } result.append(f.getAbsolutePath()); } } return result.toString (); } catch (Exception e) { e.printStackTrace(); return ""; } }', 'ground_truth': 'public String getAsText() { try { List fileobjs = (List) this.getValue(); StringBuffer result = new StringBuffer (); boolean first = true; for (Iterator<FileObject> it = fileobjs.iterator(); it.hasNext();) { FileObject fo = it.next(); File f = FileUtil.toFile(fo); if (f != null) { if (!first) { result.append (File.pathSeparator); } else { first = false; } result.append(f.getAbsolutePath()); } } return result.toString (); } catch (Exception e) { e.printStackTrace(); return ""; } }', 'output': 'public String getAsText() { try { List<FileObject> fileobjs = (List<FileObject>) this.getValue(); StringBuffer result = new StringBuffer(); boolean first = true; for (Iterator<FileObject> it = fileobjs.iterator(); it.hasNext();) { FileObject fo = it.next(); File f = FileUtil.toFile(fo); if (f != null) { if (!first) { result.append(File.pathSeparator); } else { first = false; } result.append(f.getAbsolutePath()); } } return result.toString(); } catch (Exception e) { e.printStackTrace(); return ""; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean removeSet0(String name) throws IgniteCheckedException { GridCacheSetHeaderKey key = new GridCacheSetHeaderKey(name); GridCache cache = cctx.cache(); GridCacheSetHeader hdr = retryRemove(cache, key); if (hdr == null) return false; if (!cctx.isLocal()) { while (true) { long topVer = cctx.topologyVersionFuture().get(); Collection<ClusterNode> nodes = CU.affinityNodes(cctx, topVer); try { cctx.closures().callAsyncNoFailover(BROADCAST, new BlockSetCallable(cctx.name(), hdr.id()), nodes, true).get(); } catch (XXX e) { if (log.isDebugEnabled()) log.debug("BlockSet job failed, will retry: " + e); continue; } try { cctx.closures().callAsyncNoFailover(BROADCAST, new RemoveSetDataCallable(cctx.name(), hdr.id(), topVer), nodes, true).get(); } catch (XXX e) { if (log.isDebugEnabled()) log.debug("RemoveSetData job failed, will retry: " + e); continue; } if (cctx.topologyVersionFuture().get() == topVer) break; } } else { blockSet(hdr.id()); removeSetData(hdr.id(), 0); } return true; }', 'ground_truth': 'private boolean removeSet0(String name) throws IgniteCheckedException { GridCacheSetHeaderKey key = new GridCacheSetHeaderKey(name); GridCache cache = cctx.cache(); GridCacheSetHeader hdr = retryRemove(cache, key); if (hdr == null) return false; if (!cctx.isLocal()) { while (true) { long topVer = cctx.topologyVersionFuture().get(); Collection<ClusterNode> nodes = CU.affinityNodes(cctx, topVer); try { cctx.closures().callAsyncNoFailover(BROADCAST, new BlockSetCallable(cctx.name(), hdr.id()), nodes, true).get(); } catch (ClusterTopologyCheckedException e) { if (log.isDebugEnabled()) log.debug("BlockSet job failed, will retry: " + e); continue; } try { cctx.closures().callAsyncNoFailover(BROADCAST, new RemoveSetDataCallable(cctx.name(), hdr.id(), topVer), nodes, true).get(); } catch (ClusterTopologyCheckedException e) { if (log.isDebugEnabled()) log.debug("RemoveSetData job failed, will retry: " + e); continue; } if (cctx.topologyVersionFuture().get() == topVer) break; } } else { blockSet(hdr.id()); removeSetData(hdr.id(), 0); } return true; }', 'output': 'private boolean removeSet0(String name) throws IgniteCheckedException { CacheSetHeaderKey key = new CacheSetHeaderKey(name); Cache cache = cctx.cache(); CacheSetHeader hdr = retryRemove(cache, key); if (hdr == null) return false; if (!cctx.isLocal()) { while (true) { long topVer = cctx.topologyVersionFuture().get(); Collection<ClusterNode> nodes = CU.affinityNodes(cctx, topVer); try { cctx.closures().callAsyncNoFailover(BROADCAST, new BlockSetCallable(cctx.name(), hdr.id()), nodes, true).get(); } catch (IgniteCheckedException e) { if (log.isDebugEnabled()) log.debug("BlockSet job failed, will retry: " + e); continue; } try { cctx.closures().callAsyncNoFailover(BROADCAST, new RemoveSetDataCallable(cctx.name(), hdr.id(), topVer), nodes, true).get(); } catch (IgniteCheckedException e) { if (log.isDebugEnabled()) log.debug("RemoveSetData job failed, will retry: " + e); continue; } if (cctx.topologyVersionFuture().get() == topVer) break; } } else { blockSet(hdr.id()); removeSetData(hdr.id(), 0); } return true; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public ClientResponse process(ClientConnectionContext ctx) { XXX cur = ctx.resources().get(cursorId); return new ClientCacheScanQueryNextPageResponse(requestId(), cur); } } }', 'ground_truth': '@Override public ClientResponse process(ClientConnectionContext ctx) { ClientCacheQueryCursor cur = ctx.resources().get(cursorId); return new ClientCacheQueryNextPageResponse(requestId(), cur); }', 'output': '@Override public ClientResponse process(ClientConnectionContext ctx) { Cursor<T> cur = ctx.resources().get(cursorId); return new ClientCacheScanQueryNextPageResponse(requestId(), cur); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testVIntSmallBuffer() throws Exception { for(int ii = 0; ii < 10; ii++) { ByteBuffer buf = ByteBuffer.allocate(Math.max(1,  ii)); long value = 0; if (ii > 0) value = (1L << 7 * ii) - 1; BufferedDataOutputStreamPlus out = new DataOutputBufferFixed(buf); out.writeUnsignedVInt(value); buf.position(0); XXX in = new DataInputBuffer(buf, false); assertEquals(value, in.readUnsignedVInt()); boolean threw = false; try { in.readUnsignedVInt(); } catch (EOFException e) { threw = true; } assertTrue(threw); } }', 'ground_truth': 'public void testVIntSmallBuffer() throws Exception { for(int ii = 0; ii < 10; ii++) { ByteBuffer buf = ByteBuffer.allocate(Math.max(1,  ii)); long value = 0; if (ii > 0) value = (1L << 7 * ii) - 1; BufferedDataOutputStreamPlus out = new DataOutputBufferFixed(buf); out.writeUnsignedVInt(value); buf.position(0); RebufferingInputStream in = new DataInputBuffer(buf, false); assertEquals(value, in.readUnsignedVInt()); boolean threw = false; try { in.readUnsignedVInt(); } catch (EOFException e) { threw = true; } assertTrue(threw); } }', 'output': 'public void testVIntSmallBuffer() throws Exception { for(int ii = 0; ii < 10; ii++) { ByteBuffer buf = ByteBuffer.allocate(Math.max(1,  ii)); long value = 0; if (ii > 0) value = (1L << 7 * ii) - 1; BufferedDataOutputStreamPlus out = new DataOutputBufferFixed(buf); out.writeUnsignedVInt(value); buf.position(0); DataInputBuffer in = new DataInputBuffer(buf, false); assertEquals(value, in.readUnsignedVInt()); boolean threw = false; try { in.readUnsignedVInt(); } catch (EOFException e) { threw = true; } assertTrue(threw); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testLarge() throws Exception { CacheProjection<Integer, Integer> cache = grid().cache(null); for (int i = 0; i < LARGE_ENTRY_CNT; i++) assert cache.putx(i, i); assert cache.size() == LARGE_ENTRY_CNT; IgniteInClosure<CacheEntry<Integer, Integer>> c = emptyFilter(); for (int i = 0; i < 3; i++) iterate(cache, c); long start = System.currentTimeMillis(); iterate(cache, c); long time = System.currentTimeMillis() - start; X.println(">>>"); X.println(">>> Iterated over " + cache.size() + " entries."); X.println(">>> Iteration time: " + time + "ms."); X.println(">>>"); }', 'ground_truth': 'public void testLarge() throws Exception { IgniteCache<Integer, Integer> cache = grid().jcache(null); for (int i = 0; i < LARGE_ENTRY_CNT; i++) cache.put(i, i); assert cache.size() == LARGE_ENTRY_CNT; IgniteInClosure<Cache.Entry<Integer, Integer>> c = emptyFilter(); for (int i = 0; i < 3; i++) iterate(cache, c); long start = System.currentTimeMillis(); iterate(cache, c); long time = System.currentTimeMillis() - start; X.println(">>>"); X.println(">>> Iterated over " + cache.size() + " entries."); X.println(">>> Iteration time: " + time + "ms."); X.println(">>>"); }', 'output': 'public void testLarge() throws Exception { CacheProjection<Integer, Integer> cache = grid().cache(null); for (int i = 0; i < LARGE_ENTRY_CNT; i++) assert cache.putx(i, i); assert cache.size() == LARGE_ENTRY_CNT; IgniteInClosure<CacheEntry<Integer, Integer>> c = emptyFilter(); for (int i = 0; i < 3; i++) iterate(cache, c); long start = System.currentTimeMillis(); iterate(cache, c); long time = System.currentTimeMillis() - start; X.println(">>>"); X.println(">>> Iterated over " + cache.size() + " entries."); X.println(">>> Iteration time: " + time + "ms."); X.println(">>>"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testManyColumns() throws IOException { ColumnFamily cf; TreeMap<String, String> map = new TreeMap<String, String>(); for (int i = 100; i < 1000; ++i) { map.put(Integer.toString(i), "Avinash Lakshman is a good man: " + i); } cf = ColumnFamily.create("Keyspace1", "Standard1"); DataOutputBuffer bufOut = new DataOutputBuffer(); for (String cName : map.navigableKeySet()) { cf.addColumn(column(cName, map.get(cName), 314)); } ColumnFamily.serializer().serialize(cf, bufOut); XXX bufIn = new DataInputBuffer(); bufIn.reset(bufOut.getData(), bufOut.getLength()); cf = ColumnFamily.serializer().deserialize(bufIn); for (String cName : map.navigableKeySet()) { assert new String(cf.getColumn(cName.getBytes()).value()).equals(map.get(cName)); } assert cf.getColumnNames().size() == map.size(); }', 'ground_truth': 'public void testManyColumns() throws IOException { ColumnFamily cf; TreeMap<String, String> map = new TreeMap<String, String>(); for (int i = 100; i < 1000; ++i) { map.put(Integer.toString(i), "Avinash Lakshman is a good man: " + i); } cf = ColumnFamily.create("Keyspace1", "Standard1"); DataOutputBuffer bufOut = new DataOutputBuffer(); for (String cName : map.navigableKeySet()) { cf.addColumn(column(cName, map.get(cName), 314)); } ColumnFamily.serializer().serialize(cf, bufOut); ByteArrayInputStream bufIn = new ByteArrayInputStream(bufOut.getData(), 0, bufOut.getLength()); cf = ColumnFamily.serializer().deserialize(new DataInputStream(bufIn)); for (String cName : map.navigableKeySet()) { assert new String(cf.getColumn(cName.getBytes()).value()).equals(map.get(cName)); } assert cf.getColumnNames().size() == map.size(); }', 'output': 'public void testManyColumns() throws IOException { ColumnFamily cf; TreeMap<String, String> map = new TreeMap<String, String>(); for (int i = 100; i < 1000; ++i) { map.put(Integer.toString(i), "Avinash Lakshman is a good man: " + i); } cf = ColumnFamily.create("Keyspace1", "Standard1"); DataOutputBuffer bufOut = new DataOutputBuffer(); for (String cName : map.navigableKeySet()) { cf.addColumn(column(cName, map.get(cName), 314)); } ColumnFamily.serializer().serialize(cf, bufOut); DataInputBuffer bufIn = new DataInputBuffer(); bufIn.reset(bufOut.getData(), bufOut.getLength()); cf = ColumnFamily.serializer().deserialize(bufIn); for (String cName : map.navigableKeySet()) { assert new String(cf.getColumn(cName.getBytes()).value()).equals(map.get(cName)); } assert cf.getColumnNames().size() == map.size(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void onMatch(RelOptRuleCall call) { final FilterRelBase filter = call.rel(0); final JavaRules.EnumerableInterpreterRel interpreter = call.rel(1); final XXX scan = call.rel(2); final FilterableTable filterableTable = scan.getTable().unwrap(FilterableTable.class); final ProjectableFilterableTable projectableFilterableTable = scan.getTable().unwrap(ProjectableFilterableTable.class); final FilterSplit filterSplit; if (filterableTable != null) { filterSplit = FilterSplit.of(filterableTable, filter.getCondition(), null); } else if (projectableFilterableTable != null) { filterSplit = FilterSplit.of(projectableFilterableTable, filter.getCondition(), null); } else { throw new AssertionError(scan.getTable()); } final RelNode newFilter = RelOptUtil.createFilter(interpreter.getChild(), filterSplit.acceptedFilters, EnumerableRel.FILTER_FACTORY); final RelNode newInterpreter = new JavaRules.EnumerableInterpreterRel(interpreter.getCluster(), interpreter.getTraitSet(), newFilter, 0.15d); final RelNode residue = RelOptUtil.createFilter(newInterpreter, filterSplit.rejectedFilters); call.transformTo(residue); }', 'ground_truth': 'public void onMatch(RelOptRuleCall call) { final Filter filter = call.rel(0); final EnumerableInterpreter interpreter = call.rel(1); final TableScan scan = call.rel(2); final FilterableTable filterableTable = scan.getTable().unwrap(FilterableTable.class); final ProjectableFilterableTable projectableFilterableTable = scan.getTable().unwrap(ProjectableFilterableTable.class); final FilterSplit filterSplit; if (filterableTable != null) { filterSplit = FilterSplit.of(filterableTable, filter.getCondition(), null); } else if (projectableFilterableTable != null) { filterSplit = FilterSplit.of(projectableFilterableTable, filter.getCondition(), null); } else { throw new AssertionError(scan.getTable()); } final RelNode newFilter = RelOptUtil.createFilter(interpreter.getInput(), filterSplit.acceptedFilters, EnumerableRel.FILTER_FACTORY); final RelNode newInterpreter = new EnumerableInterpreter(interpreter.getCluster(), interpreter.getTraitSet(), newFilter, 0.15d); final RelNode residue = RelOptUtil.createFilter(newInterpreter, filterSplit.rejectedFilters); call.transformTo(residue); }', 'output': 'public void onMatch(RelOptRuleCall call) { final FilterRelBase filter = call.rel(0); final JavaRules.EnumerableInterpreterRel interpreter = call.rel(1); final Scan scan = call.rel(2); final FilterableTable filterableTable = scan.getTable().unwrap(FilterableTable.class); final ProjectableFilterableTable projectableFilterableTable = scan.getTable().unwrap(ProjectableFilterableTable.class); final FilterSplit filterSplit; if (filterableTable != null) { filterSplit = FilterSplit.of(filterableTable, filter.getCondition(), null); } else if (projectableFilterableTable != null) { filterSplit = FilterSplit.of(projectableFilterableTable, filter.getCondition(), null); } else { throw new AssertionError(scan.getTable()); } final RelNode newFilter = RelOptUtil.createFilter(interpreter.getChild(), filterSplit.acceptedFilters, EnumerableRel.FILTER_FACTORY); final RelNode newInterpreter = new JavaRules.EnumerableInterpreterRel(interpreter.getCluster(), interpreter.getTraitSet(), newFilter, 0.15d); final RelNode residue = RelOptUtil.createFilter(newInterpreter, filterSplit.rejectedFilters); call.transformTo(residue); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSingleColumn() throws IOException { ColumnFamily cf; cf = ColumnFamily.create("Keyspace1", "Standard1"); cf.addColumn(column("C", "v", 1)); DataOutputBuffer bufOut = new DataOutputBuffer(); ColumnFamily.serializer().serialize(cf, bufOut); XXX bufIn = new DataInputBuffer(); bufIn.reset(bufOut.getData(), bufOut.getLength()); cf = ColumnFamily.serializer().deserialize(bufIn); assert cf != null; assert cf.name().equals("Standard1"); assert cf.getSortedColumns().size() == 1; }', 'ground_truth': 'public void testSingleColumn() throws IOException { ColumnFamily cf; cf = ColumnFamily.create("Keyspace1", "Standard1"); cf.addColumn(column("C", "v", 1)); DataOutputBuffer bufOut = new DataOutputBuffer(); ColumnFamily.serializer().serialize(cf, bufOut); ByteArrayInputStream bufIn = new ByteArrayInputStream(bufOut.getData(), 0, bufOut.getLength()); cf = ColumnFamily.serializer().deserialize(new DataInputStream(bufIn)); assert cf != null; assert cf.name().equals("Standard1"); assert cf.getSortedColumns().size() == 1; }', 'output': 'public void testSingleColumn() throws IOException { ColumnFamily cf; cf = ColumnFamily.create("Keyspace1", "Standard1"); cf.addColumn(column("C", "v", 1)); DataOutputBuffer bufOut = new DataOutputBuffer(); ColumnFamily.serializer().serialize(cf, bufOut); DataInputBuffer bufIn = new DataInputBuffer(); bufIn.reset(bufOut.getData(), bufOut.getLength()); cf = ColumnFamily.serializer().deserialize(bufIn); assert cf != null; assert cf.name().equals("Standard1"); assert cf.getSortedColumns().size() == 1; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String getCanonicalForm() { if (dynamicValue == null) { return StringUtil.ftlQuote(value); } else { XXX sb = new StringBuffer(); sb.append(\'"\'); for (Enumeration childrenEnum = dynamicValue.children(); childrenEnum.hasMoreElements();) { TemplateElement child = (TemplateElement) childrenEnum.nextElement(); if (child instanceof Interpolation) { sb.append(((Interpolation) child).getCanonicalFormInStringLiteral()); } else { sb.append(StringUtil.FTLStringLiteralEnc(child.getCanonicalForm(), \'"\')); } } sb.append(\'"\'); return sb.toString(); } }', 'ground_truth': 'public String getCanonicalForm() { if (dynamicValue == null) { return StringUtil.ftlQuote(value); } else { StringBuilder sb = new StringBuilder(); sb.append(\'"\'); for (Enumeration childrenEnum = dynamicValue.children(); childrenEnum.hasMoreElements();) { TemplateElement child = (TemplateElement) childrenEnum.nextElement(); if (child instanceof Interpolation) { sb.append(((Interpolation) child).getCanonicalFormInStringLiteral()); } else { sb.append(StringUtil.FTLStringLiteralEnc(child.getCanonicalForm(), \'"\')); } } sb.append(\'"\'); return sb.toString(); } }', 'output': 'public String getCanonicalForm() { if (dynamicValue == null) { return StringUtil.ftlQuote(value); } else { StringBuilder sb = new StringBuilder(); sb.append(\'"\'); for (Enumeration childrenEnum = dynamicValue.children(); childrenEnum.hasMoreElements();) { TemplateElement child = (TemplateElement) childrenEnum.nextElement(); if (child instanceof Interpolation) { sb.append(((Interpolation) child).getCanonicalFormInStringLiteral()); } else { sb.append(StringUtil.FTLStringLiteralEnc(child.getCanonicalForm(), \'"\')); } } sb.append(\'"\'); return sb.toString(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static IFilter getFilter(SlicePredicate predicate, AbstractType comparator) { if (predicate.column_names != null) { final XXX columnNameSet = new TreeSet<byte[]>(comparator); columnNameSet.addAll(predicate.column_names); return new NamesQueryFilter(columnNameSet); } SliceRange range = predicate.slice_range; return new SliceQueryFilter(range.start, range.finish, range.reversed, range.count); }', 'ground_truth': 'public static IFilter getFilter(SlicePredicate predicate, AbstractType comparator) { if (predicate.column_names != null) { final SortedSet<ByteBuffer> columnNameSet = new TreeSet<ByteBuffer>(comparator); columnNameSet.addAll(predicate.column_names); return new NamesQueryFilter(columnNameSet); } SliceRange range = predicate.slice_range; return new SliceQueryFilter(range.start, range.finish, range.reversed, range.count); }', 'output': 'public static IFilter getFilter(SlicePredicate predicate, AbstractType comparator) { if (predicate.column_names != null) { final TreeSet<byte[]> columnNameSet = new TreeSet<byte[]>(comparator); columnNameSet.addAll(predicate.column_names); return new NamesQueryFilter(columnNameSet); } SliceRange range = predicate.slice_range; return new SliceQueryFilter(range.start, range.finish, range.reversed, range.count); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static XXX suite(Collection<Class> ignoredTests) { XXX suite = new TestSuite("Ignite Persistent Store Mvcc Test Suite 3"); addRealPageStoreTestsNotForDirectIo(suite, ignoredTests); return suite; }', 'ground_truth': 'public static List<Class<?>> suite(Collection<Class> ignoredTests) { List<Class<?>> suite = new ArrayList<>(); addRealPageStoreTestsNotForDirectIo(suite, ignoredTests); return suite; }', 'output': 'public static IgniteTest suite(Collection<Class> ignoredTests) { IgniteTest suite = new IgniteTest("Ignite Persistent Store Mvcc Test Suite 3"); addRealPageStoreTestsNotForDirectIo(suite, ignoredTests); return suite; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RexLocalRef makeInputRef(int index) { final RelDataTypeField [] fields = inputRowType.getFields(); assert index < fields.length; final RelDataTypeField field = fields[index]; return new RexLocalRef( index, field.getType()); }', 'ground_truth': 'public RexLocalRef makeInputRef(int index) { final List<RelDataTypeField> fields = inputRowType.getFieldList(); assert index < fields.size(); final RelDataTypeField field = fields.get(index); return new RexLocalRef( index, field.getType()); }', 'output': 'public RexInputRef makeInputRef(int index) { final RelDataTypeField [] fields = inputRowType.getFields(); assert index < fields.length; final RelDataTypeField field = fields[index]; return new RexInputRef( index, field.getType()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testClassFieldsMarshalling() throws Exception { PortableMarshaller marsh = new PortableMarshaller(); initPortableContext(marsh); ObjectWithClassFields obj = new ObjectWithClassFields(); obj.cls1 = GridPortableMarshallerSelfTest.class; byte[] marshal = marsh.marshal(obj); ObjectWithClassFields obj2 = marsh.unmarshal(marshal, null); assertEquals(obj.cls1, obj2.cls1); assertNull(obj2.cls2); XXX portObj = marshal(obj, marsh); Class cls1 = portObj.field("cls1"); assertEquals(obj.cls1, cls1); }', 'ground_truth': 'public void testClassFieldsMarshalling() throws Exception { PortableMarshaller marsh = new PortableMarshaller(); initPortableContext(marsh); ObjectWithClassFields obj = new ObjectWithClassFields(); obj.cls1 = GridPortableMarshallerSelfTest.class; byte[] marshal = marsh.marshal(obj); ObjectWithClassFields obj2 = marsh.unmarshal(marshal, null); assertEquals(obj.cls1, obj2.cls1); assertNull(obj2.cls2); BinaryObject portObj = marshal(obj, marsh); Class cls1 = portObj.field("cls1"); assertEquals(obj.cls1, cls1); }', 'output': 'public void testClassFieldsMarshalling() throws Exception { PortableMarshaller marsh = new PortableMarshaller(); initPortableContext(marsh); ObjectWithClassFields obj = new ObjectWithClassFields(); obj.cls1 = GridPortableMarshallerSelfTest.class; byte[] marshal = marsh.marshal(obj); ObjectWithClassFields obj2 = marsh.unmarshal(marshal, null); assertEquals(obj.cls1, obj2.cls1); assertNull(obj2.cls2); ObjectWithClassFields portObj = marsh.unmarshal(marshal, null); Class cls1 = portObj.field("cls1"); assertEquals(obj.cls1, cls1); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCheckpointHistory() throws Exception { Ignite ignite = startGrid(); ignite.cluster().active(true); try { GridCacheDatabaseSharedManager dbMgr = (GridCacheDatabaseSharedManager)((IgniteEx)ignite).context() .cache().context().database(); dbMgr.waitForCheckpoint("test"); int entries = WAL_HIST_SIZE * 2; IgniteCache<Integer, Integer> cache = ignite.cache(CACHE_NAME); for (int i = 0; i < entries; i++) { cache.put(i * PARTS, i * PARTS); cache.put(i * PARTS + 1, i * PARTS + 1); dbMgr.waitForCheckpoint("test"); } XXX hist = dbMgr.checkpointHistory(); assertTrue(hist.checkpoints().size() <= WAL_HIST_SIZE); File cpDir = dbMgr.checkpointDirectory(); File[] cpFiles = cpDir.listFiles(); assertTrue(cpFiles.length <= WAL_HIST_SIZE * 2 + 1); } finally { stopAllGrids(); } }', 'ground_truth': 'public void testCheckpointHistory() throws Exception { Ignite ignite = startGrid(); ignite.cluster().active(true); try { GridCacheDatabaseSharedManager dbMgr = (GridCacheDatabaseSharedManager)((IgniteEx)ignite).context() .cache().context().database(); dbMgr.waitForCheckpoint("test"); int entries = WAL_HIST_SIZE * 2; IgniteCache<Integer, Integer> cache = ignite.cache(CACHE_NAME); for (int i = 0; i < entries; i++) { cache.put(i * PARTS, i * PARTS); cache.put(i * PARTS + 1, i * PARTS + 1); dbMgr.waitForCheckpoint("test"); } CheckpointHistory hist = dbMgr.checkpointHistory(); assertTrue(hist.checkpoints().size() <= WAL_HIST_SIZE); File cpDir = dbMgr.checkpointDirectory(); File[] cpFiles = cpDir.listFiles(); assertTrue(cpFiles.length <= WAL_HIST_SIZE * 2 + 1); } finally { stopAllGrids(); } }', 'output': 'public void testCheckpointHistory() throws Exception { Ignite ignite = startGrid(); ignite.cluster().active(true); try { GridCacheDatabaseSharedManager dbMgr = (GridCacheDatabaseSharedManager)((IgniteEx)ignite).context() .cache().context().database(); dbMgr.waitForCheckpoint("test"); int entries = WAL_HIST_SIZE * 2; IgniteCache<Integer, Integer> cache = ignite.cache(CACHE_NAME); for (int i = 0; i < entries; i++) { cache.put(i * PARTS, i * PARTS); cache.put(i * PARTS + 1, i * PARTS + 1); dbMgr.waitForCheckpoint("test"); } CheckpointHistory hist = dbMgr.checkpointHistory(); assertTrue(hist.checkpoints().size() <= WAL_HIST_SIZE); File cpDir = dbMgr.checkpointDirectory(); File[] cpFiles = cpDir.listFiles(); assertTrue(cpFiles.length <= WAL_HIST_SIZE * 2 + 1); } finally { stopAllGrids(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<SqlMoniker> getAllSchemaObjectNames(List<String> names) { final OptiqSchema schema = getSchema(names); if (schema == null) { return ImmutableList.of(); } final List<SqlMoniker> result = new ArrayList<SqlMoniker>(); final Map<String, OptiqSchema> schemaMap = schema.getSubSchemaMap(); for (String subSchema : schemaMap.keySet()) { result.add( new SqlMonikerImpl(schema.path(subSchema), SqlMonikerType.SCHEMA)); } for (String table : schema.getTableNames()) { result.add( new SqlMonikerImpl(schema.path(table), SqlMonikerType.TABLE)); } final NavigableSet<String> functions = schema.getFunctionNames(); for (String function : functions) { result.add( new SqlMonikerImpl(schema.path(function), SqlMonikerType.FUNCTION)); } return result; }', 'ground_truth': 'public List<SqlMoniker> getAllSchemaObjectNames(List<String> names) { final CalciteSchema schema = getSchema(names); if (schema == null) { return ImmutableList.of(); } final List<SqlMoniker> result = new ArrayList<SqlMoniker>(); final Map<String, CalciteSchema> schemaMap = schema.getSubSchemaMap(); for (String subSchema : schemaMap.keySet()) { result.add( new SqlMonikerImpl(schema.path(subSchema), SqlMonikerType.SCHEMA)); } for (String table : schema.getTableNames()) { result.add( new SqlMonikerImpl(schema.path(table), SqlMonikerType.TABLE)); } final NavigableSet<String> functions = schema.getFunctionNames(); for (String function : functions) { result.add( new SqlMonikerImpl(schema.path(function), SqlMonikerType.FUNCTION)); } return result; }', 'output': 'public List<SqlMoniker> getAllSchemaObjectNames(List<String> names) { final OptiqSchema schema = getSchema(names); if (schema == null) { return ImmutableList.of(); } final List<SqlMoniker> result = new ArrayList<SqlMoniker>(); final Map<String, OptiqSchema> schemaMap = schema.getSubSchemaMap(); for (String subSchema : schemaMap.keySet()) { result.add( new SqlMonikerImpl(schema.path(subSchema), SqlMonikerType.SCHEMA)); } for (String table : schema.getTableNames()) { result.add( new SqlMonikerImpl(schema.path(table), SqlMonikerType.TABLE)); } final NavigableSet<String> functions = schema.getFunctionNames(); for (String function : functions) { result.add( new SqlMonikerImpl(schema.path(function), SqlMonikerType.FUNCTION)); } return result; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void run() { XXX verb = message_.getVerb(); IVerbHandler verbHandler = MessagingService.instance.getVerbHandler(verb); assert verbHandler != null : "unknown verb " + verb; verbHandler.doVerb(message_); }', 'ground_truth': 'public void run() { StorageService.Verb verb = message_.getVerb(); IVerbHandler verbHandler = MessagingService.instance.getVerbHandler(verb); assert verbHandler != null : "unknown verb " + verb; verbHandler.doVerb(message_); }', 'output': 'public void run() { Verb verb = message_.getVerb(); IVerbHandler verbHandler = MessagingService.instance.getVerbHandler(verb); assert verbHandler != null : "unknown verb " + verb; verbHandler.doVerb(message_); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void assertSplit(long suggestedStart, long suggestedLen, long expStart, long expLen, byte[] data, int len) throws Exception { write(data); GridGgfsFixedLengthRecordResolver rslvr = resolver(len); XXX split; try (IgniteFsInputStream is = read()) { split = rslvr.resolveRecords(ggfs, is, split(suggestedStart, suggestedLen)); } assert split != null : "Split is null."; assert split.start() == expStart : "Incorrect start [expected=" + expStart + ", actual=" + split.start() + \']\'; assert split.length() == expLen : "Incorrect length [expected=" + expLen + ", actual=" + split.length() + \']\'; }', 'ground_truth': 'public void assertSplit(long suggestedStart, long suggestedLen, long expStart, long expLen, byte[] data, int len) throws Exception { write(data); GridGgfsFixedLengthRecordResolver rslvr = resolver(len); IgniteFsFileRange split; try (IgniteFsInputStream is = read()) { split = rslvr.resolveRecords(ggfs, is, split(suggestedStart, suggestedLen)); } assert split != null : "Split is null."; assert split.start() == expStart : "Incorrect start [expected=" + expStart + ", actual=" + split.start() + \']\'; assert split.length() == expLen : "Incorrect length [expected=" + expLen + ", actual=" + split.length() + \']\'; }', 'output': 'public void assertSplit(long suggestedStart, long suggestedLen, long expStart, long expLen, byte[] data, int len) throws Exception { write(data); GridGgfsFixedLengthRecordResolver rslvr = resolver(len); IgniteFsInputStream is = read(); IgniteFsRecordReader rdr = new IgniteFsRecordReader(rslvr, is); Split split = rdr.split(suggestedStart, suggestedLen); assert split != null : "Split is null."; assert split.start() == expStart : "Incorrect start [expected=" + expStart + ", actual=" + split.start() + \']\'; assert split.length() == expLen : "Incorrect length [expected=" + expLen + ", actual=" + split.length() + \']\'; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGetFromPrimaryNode() throws Exception { for (int i = 0; i < GRID_CNT; i++) { GridCache<String, Integer> c = grid(i).cache(null); CacheEntry<String, Integer> e = c.entry(KEY); if (e.primary()) { info("Primary node: " + grid(i).localNode().id()); c.get(KEY); break; } } assert !await(); }', 'ground_truth': 'public void testGetFromPrimaryNode() throws Exception { for (int i = 0; i < GRID_CNT; i++) { GridCache<String, Integer> c = grid(i).cache(null); Entry<String, Integer> e = c.entry(KEY); if (e.primary()) { info("Primary node: " + grid(i).localNode().id()); c.get(KEY); break; } } assert !await(); }', 'output': 'public void testGetFromPrimaryNode() throws Exception { for (int i = 0; i < GRID_CNT; i++) { IgniteCache<String, Integer> c = grid(i).jcache(null); CacheEntry<String, Integer> e = c.entry(KEY); if (e.primary()) { info("Primary node: " + grid(i).localNode().id()); c.get(KEY); break; } } assert !await(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean primary(int gridIdx, Object key) { XXX aff = grid(0).cache(CACHE_NAME).affinity(); return aff.isPrimary(grid(gridIdx).cluster().localNode(), key); }', 'ground_truth': 'private boolean primary(int gridIdx, Object key) { CacheAffinity<Object> aff = grid(0).cache(CACHE_NAME).affinity(); return aff.isPrimary(grid(gridIdx).cluster().localNode(), key); }', 'output': 'private boolean primary(int gridIdx, Object key) { Cache<Object, Object> aff = grid(0).cache(CACHE_NAME); return aff.isPrimary(grid(gridIdx).cluster().localNode(), key); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAtomicReferenceReconnectInProgress() throws Exception { Ignite client = grid(serverCount()); assertTrue(client.cluster().localNode().isClient()); Ignite srv = clientRouter(client); final IgniteAtomicReference<String> clientAtomicRef = client.atomicReference("atomicRefInProg", "1st value", true); assertEquals("1st value", clientAtomicRef.get()); assertTrue(clientAtomicRef.compareAndSet("1st value", "2st value")); assertEquals("2st value", clientAtomicRef.get()); IgniteAtomicReference<String> srvAtomicRef = srv.atomicReference("atomicRefInProg", "1st value", false); assertEquals("2st value", srvAtomicRef.get()); assertTrue(srvAtomicRef.compareAndSet("2st value", "3st value")); assertEquals("3st value", srvAtomicRef.get()); XXX servCommSpi = commSpi(srv); servCommSpi.blockMessage(GridNearLockResponse.class); final IgniteInternalFuture<Object> fut = GridTestUtils.runAsync(new Callable<Object>() { @Override public Object call() throws Exception { try { clientAtomicRef.compareAndSet("3st value", "4st value"); } catch (IgniteClientDisconnectedException e) { checkAndWait(e); return true; } return false; } }); GridTestUtils.assertThrows(log, new Callable<Object>() { @Override public Object call() throws Exception { return fut.get(200); } }, IgniteFutureTimeoutCheckedException.class, null); assertNotDone(fut); servCommSpi.unblockMessage(); reconnectClientNode(client, srv, null); assertTrue((Boolean)fut.get(2, TimeUnit.SECONDS)); assertEquals("3st value", clientAtomicRef.get()); assertTrue(clientAtomicRef.compareAndSet("3st value", "4st value")); assertEquals("4st value", clientAtomicRef.get()); assertEquals("4st value", srvAtomicRef.get()); assertTrue(srvAtomicRef.compareAndSet("4st value", "5st value")); assertEquals("5st value", srvAtomicRef.get()); srvAtomicRef.close(); }', 'ground_truth': 'public void testAtomicReferenceReconnectInProgress() throws Exception { Ignite client = grid(serverCount()); assertTrue(client.cluster().localNode().isClient()); Ignite srv = clientRouter(client); final IgniteAtomicReference<String> clientAtomicRef = client.atomicReference("atomicRefInProg", "1st value", true); assertEquals("1st value", clientAtomicRef.get()); assertTrue(clientAtomicRef.compareAndSet("1st value", "2st value")); assertEquals("2st value", clientAtomicRef.get()); IgniteAtomicReference<String> srvAtomicRef = srv.atomicReference("atomicRefInProg", "1st value", false); assertEquals("2st value", srvAtomicRef.get()); assertTrue(srvAtomicRef.compareAndSet("2st value", "3st value")); assertEquals("3st value", srvAtomicRef.get()); BlockTcpCommunicationSpi servCommSpi = commSpi(srv); servCommSpi.blockMessage(GridNearLockResponse.class); final IgniteInternalFuture<Object> fut = GridTestUtils.runAsync(new Callable<Object>() { @Override public Object call() throws Exception { try { clientAtomicRef.compareAndSet("3st value", "4st value"); } catch (IgniteClientDisconnectedException e) { checkAndWait(e); return true; } return false; } }); GridTestUtils.assertThrows(log, new Callable<Object>() { @Override public Object call() throws Exception { return fut.get(200); } }, IgniteFutureTimeoutCheckedException.class, null); assertNotDone(fut); servCommSpi.unblockMessage(); reconnectClientNode(client, srv, null); assertTrue((Boolean)fut.get(2, TimeUnit.SECONDS)); assertEquals("3st value", clientAtomicRef.get()); assertTrue(clientAtomicRef.compareAndSet("3st value", "4st value")); assertEquals("4st value", clientAtomicRef.get()); assertEquals("4st value", srvAtomicRef.get()); assertTrue(srvAtomicRef.compareAndSet("4st value", "5st value")); assertEquals("5st value", srvAtomicRef.get()); srvAtomicRef.close(); }', 'output': 'public void testAtomicReferenceReconnectInProgress() throws Exception { Ignite client = grid(serverCount()); assertTrue(client.cluster().localNode().isClient()); Ignite srv = clientRouter(client); final IgniteAtomicReference<String> clientAtomicRef = client.atomicReference("atomicRefInProg", "1st value", true); assertEquals("1st value", clientAtomicRef.get()); assertTrue(clientAtomicRef.compareAndSet("1st value", "2st value")); assertEquals("2st value", clientAtomicRef.get()); IgniteAtomicReference<String> srvAtomicRef = srv.atomicReference("atomicRefInProg", "1st value", false); assertEquals("2st value", srvAtomicRef.get()); assertTrue(srvAtomicRef.compareAndSet("2st value", "3st value")); assertEquals("3st value", srvAtomicRef.get()); CommunicationSpi servCommSpi = commSpi(srv); servCommSpi.blockMessage(GridNearLockResponse.class); final IgniteInternalFuture<Object> fut = GridTestUtils.runAsync(new Callable<Object>() { @Override public Object call() throws Exception { try { clientAtomicRef.compareAndSet("3st value", "4st value"); } catch (IgniteClientDisconnectedException e) { checkAndWait(e); return true; } return false; } }); GridTestUtils.assertThrows(log, new Callable<Object>() { @Override public Object call() throws Exception { return fut.get(200); } }, IgniteFutureTimeoutCheckedException.class, null); assertNotDone(fut); servCommSpi.unblockMessage(); reconnectClientNode(client, srv, null); assertTrue((Boolean)fut.get(2, TimeUnit.SECONDS)); assertEquals("3st value", clientAtomicRef.get()); assertTrue(clientAtomicRef.compareAndSet("3st value", "4st value")); assertEquals("4st value", clientAtomicRef.get()); assertEquals("4st value", srvAtomicRef.get()); assertTrue(srvAtomicRef.compareAndSet("4st value", "5st value")); assertEquals("5st value",'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAtomicSequenceConstantMultipleTopologyChange() throws Exception { try { GridCacheAtomicSequence s = cache().dataStructures().atomicSequence(STRUCTURE_NAME, 1, true); XXX fut = GridTestUtils.runMultiThreadedAsync(new CA() { @Override public void apply() { try { for (int i = 0; i < TOP_CHANGE_CNT; i++) { Collection<String> names = new GridLeanSet<>(3); try { for (int j = 0; j < 3; j++) { String name = UUID.randomUUID().toString(); names.add(name); Ignite g = startGrid(name); assertTrue(g.cache(null).dataStructures().atomicSequence(STRUCTURE_NAME, 1, false) .get() > 0); } } finally { if (i != TOP_CHANGE_CNT - 1) for (String name : names) stopGrid(name); } } } catch (Exception e) { throw F.wrap(e); } } }, TOP_CHANGE_THREAD_CNT, "topology-change-thread"); long old = s.get(); while (!fut.isDone()) { assertEquals(old, s.get()); long val = s.incrementAndGet(); assertTrue(val > old); old = val; } fut.get(); } finally { cache().dataStructures().removeAtomicSequence(STRUCTURE_NAME); } }', 'ground_truth': 'public void testAtomicSequenceConstantMultipleTopologyChange() throws Exception { try { GridCacheAtomicSequence s = cache().dataStructures().atomicSequence(STRUCTURE_NAME, 1, true); IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new CA() { @Override public void apply() { try { for (int i = 0; i < TOP_CHANGE_CNT; i++) { Collection<String> names = new GridLeanSet<>(3); try { for (int j = 0; j < 3; j++) { String name = UUID.randomUUID().toString(); names.add(name); Ignite g = startGrid(name); assertTrue(g.cache(null).dataStructures().atomicSequence(STRUCTURE_NAME, 1, false) .get() > 0); } } finally { if (i != TOP_CHANGE_CNT - 1) for (String name : names) stopGrid(name); } } } catch (Exception e) { throw F.wrap(e); } } }, TOP_CHANGE_THREAD_CNT, "topology-change-thread"); long old = s.get(); while (!fut.isDone()) { assertEquals(old, s.get()); long val = s.incrementAndGet(); assertTrue(val > old); old = val; } fut.get(); } finally { cache().dataStructures().removeAtomicSequence(STRUCTURE_NAME); } }', 'output': 'public void testAtomicSequenceConstantMultipleTopologyChange() throws Exception { try { GridCacheAtomicSequence s = cache().dataStructures().atomicSequence(STRUCTURE_NAME, 1, true); IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new CA() { @Override public void apply() { try { for (int i = 0; i < TOP_CHANGE_CNT; i++) { Collection<String> names = new GridLeanSet<>(3); try { for (int j = 0; j < 3; j++) { String name = UUID.randomUUID().toString(); names.add(name); Ignite g = startGrid(name); assertTrue(g.cache(null).dataStructures().atomicSequence(STRUCTURE_NAME, 1, false) .get() > 0); } } finally { if (i != TOP_CHANGE_CNT - 1) for (String name : names) stopGrid(name); } } } catch (Exception e) { throw F.wrap(e); } } }, TOP_CHANGE_THREAD_CNT, "topology-change-thread"); long old = s.get(); while (!fut.isDone()) { assertEquals(old, s.get()); long val = s.incrementAndGet(); assertTrue(val > old); old = val; } fut.get(); } finally { cache().dataStructures().removeAtomicSequence(STRUCTURE_NAME); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void _testOptimisticSerializableConsistency() throws Exception { final GridCache<Integer, Long> cache = grid(0).cache(null); final int THREADS = 2; final int ITERATIONS = 100; final int key = 0; cache.put(key, 0L); List<IgniteFuture<Collection<Long>>> futs = new ArrayList<>(THREADS); for (int i = 0; i < THREADS; i++) { futs.add(GridTestUtils.runAsync(new Callable<Collection<Long>>() { @Override public Collection<Long> call() throws Exception { Collection<Long> res = new ArrayList<>(); for (int i = 0; i < ITERATIONS; i++) { while (true) { try (IgniteTx tx = cache.txStart(OPTIMISTIC, SERIALIZABLE)) { long val = cache.get(key); cache.put(key, val + 1); tx.commit(); assertTrue(res.add(val + 1)); break; } catch(IgniteTxOptimisticException e) { log.info("Got error, will retry: " + e); } } } return res; } })); } List<Collection<Long>> cols = new ArrayList<>(THREADS); for (XXX fut : futs) { Collection<Long> col = fut.get(); assertEquals(ITERATIONS, col.size()); cols.add(col); } Set<Long> duplicates = new HashSet<>(); for (Collection<Long> col1 : cols) { for (Long val1 : col1) { for (Collection<Long> col2 : cols) { if (col1 == col2) continue; for (Long val2 : col2) { if (val1.equals(val2)) { duplicates.add(val2); break; } } } } } assertTrue("Found duplicated values: " + duplicates, duplicates.isEmpty()); }', 'ground_truth': 'public void _testOptimisticSerializableConsistency() throws Exception { final GridCache<Integer, Long> cache = grid(0).cache(null); final int THREADS = 2; final int ITERATIONS = 100; final int key = 0; cache.put(key, 0L); List<IgniteInternalFuture<Collection<Long>>> futs = new ArrayList<>(THREADS); for (int i = 0; i < THREADS; i++) { futs.add(GridTestUtils.runAsync(new Callable<Collection<Long>>() { @Override public Collection<Long> call() throws Exception { Collection<Long> res = new ArrayList<>(); for (int i = 0; i < ITERATIONS; i++) { while (true) { try (IgniteTx tx = cache.txStart(OPTIMISTIC, SERIALIZABLE)) { long val = cache.get(key); cache.put(key, val + 1); tx.commit(); assertTrue(res.add(val + 1)); break; } catch(IgniteTxOptimisticException e) { log.info("Got error, will retry: " + e); } } } return res; } })); } List<Collection<Long>> cols = new ArrayList<>(THREADS); for (IgniteInternalFuture<Collection<Long>> fut : futs) { Collection<Long> col = fut.get(); assertEquals(ITERATIONS, col.size()); cols.add(col); } Set<Long> duplicates = new HashSet<>(); for (Collection<Long> col1 : cols) { for (Long val1 : col1) { for (Collection<Long> col2 : cols) { if (col1 == col2) continue; for (Long val2 : col2) { if (val1.equals(val2)) { duplicates.add(val2); break; } } } } } assertTrue("Found duplicated values: " + duplicates, duplicates.isEmpty()); }', 'output': 'public void _testOptimisticSerializableConsistency() throws Exception { final GridCache<Integer, Long> cache = grid(0).cache(null); final int THREADS = 2; final int ITERATIONS = 100; final int key = 0; cache.put(key, 0L); List<IgniteFuture<Collection<Long>>> futs = new ArrayList<>(THREADS); for (int i = 0; i < THREADS; i++) { futs.add(GridTestUtils.runAsync(new Callable<Collection<Long>>() { @Override public Collection<Long> call() throws Exception { Collection<Long> res = new ArrayList<>(); for (int i = 0; i < ITERATIONS; i++) { while (true) { try (IgniteTx tx = cache.txStart(OPTIMISTIC, SERIALIZABLE)) { long val = cache.get(key); cache.put(key, val + 1); tx.commit(); assertTrue(res.add(val + 1)); break; } catch(IgniteTxOptimisticException e) { log.info("Got error, will retry: " + e); } } } return res; } })); } List<Collection<Long>> cols = new ArrayList<>(THREADS); for (IgniteFuture<Collection<Long>> fut : futs) { Collection<Long> col = fut.get(); assertEquals(ITERATIONS, col.size()); cols.add(col); } Set<Long> duplicates = new HashSet<>(); for (Collection<Long> col1 : cols) { for (Long val1 : col1) { for (Collection<Long> col2 : cols) { if (col1 == col2) continue; for (Long val2 : col2) { if (val1.equals(val2)) { duplicates.add(val2); break; } } } } } assertTrue("Found duplicated values: " + duplicates, duplicates.isEmpty()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testReloadAllFiltered() throws Exception { GridCache<Integer, ValueObject> cache = cache(); for (int i = 0; i < PUT_CNT; i++) assert cache.putx(i, new ValueObject(i)); assert cache.size() == PUT_CNT; Integer[] keys = new Integer[PUT_CNT]; for (int i = 0; i < PUT_CNT; i++) keys[i] = i; for (Integer key : keys) cache.clear(key); assert cache.isEmpty(); assertEquals(0, cache.size()); cache.projection(new P1<GridCacheEntry<Integer, ValueObject>>() { @Override public boolean apply(GridCacheEntry<Integer, ValueObject> e) { return e.getKey() >= 5; } }).reloadAll(Arrays.asList(keys)); assert cache.size() == PUT_CNT - 5; Collection<Map.Entry<Integer, ValueObject>> res = cache.queries().createSqlQuery(ValueObject.class, "val >= 0").execute().get(); assert res != null; assert res.size() == PUT_CNT - 5; assert size(ValueObject.class) == PUT_CNT - 5; }', 'ground_truth': 'public void testReloadAllFiltered() throws Exception { Cache<Integer, ValueObject> cache = cache(); for (int i = 0; i < PUT_CNT; i++) assert cache.putx(i, new ValueObject(i)); assert cache.size() == PUT_CNT; Integer[] keys = new Integer[PUT_CNT]; for (int i = 0; i < PUT_CNT; i++) keys[i] = i; for (Integer key : keys) cache.clear(key); assert cache.isEmpty(); assertEquals(0, cache.size()); cache.projection(new P1<CacheEntry<Integer, ValueObject>>() { @Override public boolean apply(CacheEntry<Integer, ValueObject> e) { return e.getKey() >= 5; } }).reloadAll(Arrays.asList(keys)); assert cache.size() == PUT_CNT - 5; Collection<Map.Entry<Integer, ValueObject>> res = cache.queries().createSqlQuery(ValueObject.class, "val >= 0").execute().get(); assert res != null; assert res.size() == PUT_CNT - 5; assert size(ValueObject.class) == PUT_CNT - 5; }', 'output': 'public void testReloadAllFiltered() throws Exception { Cache<Integer, ValueObject> cache = cache(); for (int i = 0; i < PUT_CNT; i++) assert cache.putx(i, new ValueObject(i)); assert cache.size() == PUT_CNT; Integer[] keys = new Integer[PUT_CNT]; for (int i = 0; i < PUT_CNT; i++) keys[i] = i; for (Integer key : keys) cache.clear(key); assert cache.isEmpty(); assertEquals(0, cache.size()); cache.projection(new P1<GridCacheEntry<Integer, ValueObject>>() { @Override public boolean apply(GridCacheEntry<Integer, ValueObject> e) { return e.getKey() >= 5; } }).reloadAll(Arrays.asList(keys)); assert cache.size() == PUT_CNT - 5; Collection<Map.Entry<Integer, ValueObject>> res = cache.queries().createSqlQuery(ValueObject.class, "val >= 0").execute().get(); assert res != null; assert res.size() == PUT_CNT - 5; assert size(ValueObject.class) == PUT_CNT - 5; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void setReconnectInProgress(final CollectionConfiguration colCfg) throws Exception { Ignite client = grid(serverCount()); assertTrue(client.cluster().localNode().isClient()); final Ignite srv = clientRouter(client); final String setName = "set-in-progress-" + colCfg.getAtomicityMode(); final IgniteSet<String> clientSet = client.set(setName, colCfg); final IgniteSet<String> srvSet = srv.set(setName, null); assertTrue(clientSet.add("1")); assertFalse(srvSet.add("1")); XXX commSpi = commSpi(srv); if (colCfg.getAtomicityMode() == ATOMIC) commSpi.blockMessage(GridNearAtomicUpdateResponse.class); else commSpi.blockMessage(GridNearTxPrepareResponse.class); final IgniteInternalFuture<Object> fut = GridTestUtils.runAsync(new Callable<Object>() { @Override public Object call() throws Exception { try { for (int i = 0; i < 100; i++) clientSet.add("2"); } catch (IgniteClientDisconnectedException e) { checkAndWait(e); return true; } return false; } }); GridTestUtils.assertThrows(log, new Callable<Object>() { @Override public Object call() throws Exception { return fut.get(200); } }, IgniteFutureTimeoutCheckedException.class, null); assertNotDone(fut); commSpi.unblockMessage(); reconnectClientNode(client, srv, null); assertTrue((Boolean)fut.get(2, TimeUnit.SECONDS)); assertTrue(clientSet.add("3")); assertFalse(srvSet.add("3")); srvSet.close(); }', 'ground_truth': 'private void setReconnectInProgress(final CollectionConfiguration colCfg) throws Exception { Ignite client = grid(serverCount()); assertTrue(client.cluster().localNode().isClient()); final Ignite srv = clientRouter(client); final String setName = "set-in-progress-" + colCfg.getAtomicityMode(); final IgniteSet<String> clientSet = client.set(setName, colCfg); final IgniteSet<String> srvSet = srv.set(setName, null); assertTrue(clientSet.add("1")); assertFalse(srvSet.add("1")); BlockTcpCommunicationSpi commSpi = commSpi(srv); if (colCfg.getAtomicityMode() == ATOMIC) commSpi.blockMessage(GridNearAtomicUpdateResponse.class); else commSpi.blockMessage(GridNearTxPrepareResponse.class); final IgniteInternalFuture<Object> fut = GridTestUtils.runAsync(new Callable<Object>() { @Override public Object call() throws Exception { try { for (int i = 0; i < 100; i++) clientSet.add("2"); } catch (IgniteClientDisconnectedException e) { checkAndWait(e); return true; } return false; } }); GridTestUtils.assertThrows(log, new Callable<Object>() { @Override public Object call() throws Exception { return fut.get(200); } }, IgniteFutureTimeoutCheckedException.class, null); assertNotDone(fut); commSpi.unblockMessage(); reconnectClientNode(client, srv, null); assertTrue((Boolean)fut.get(2, TimeUnit.SECONDS)); assertTrue(clientSet.add("3")); assertFalse(srvSet.add("3")); srvSet.close(); }', 'output': 'private void setReconnectInProgress(final CollectionConfiguration colCfg) throws Exception { Ignite client = grid(serverCount()); assertTrue(client.cluster().localNode().isClient()); final Ignite srv = clientRouter(client); final String setName = "set-in-progress-" + colCfg.getAtomicityMode(); final IgniteSet<String> clientSet = client.set(setName, colCfg); final IgniteSet<String> srvSet = srv.set(setName, null); assertTrue(clientSet.add("1")); assertFalse(srvSet.add("1")); commSpi = commSpi(srv); if (colCfg.getAtomicityMode() == ATOMIC) commSpi.blockMessage(GridNearAtomicUpdateResponse.class); else commSpi.blockMessage(GridNearTxPrepareResponse.class); final IgniteInternalFuture<Object> fut = GridTestUtils.runAsync(new Callable<Object>() { @Override public Object call() throws Exception { try { for (int i = 0; i < 100; i++) clientSet.add("2"); } catch (IgniteClientDisconnectedException e) { checkAndWait(e); return true; } return false; } }); GridTestUtils.assertThrows(log, new Callable<Object>() { @Override public Object call() throws Exception { return fut.get(200); } }, IgniteFutureTimeoutCheckedException.class, null); assertNotDone(fut); commSpi.unblockMessage(); reconnectClientNode(client, srv, null); assertTrue((Boolean)fut.get(2, TimeUnit.SECONDS)); assertTrue(clientSet.add("3")); assertFalse(srvSet.add("3")); srvSet.close(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testReadXMLNoBundleSize() throws IOException { File file = tempFolder.newFile("trainXMLSmall"); Files.write(file.toPath(), trainXML.getBytes(StandardCharsets.UTF_8)); XXX source = XMLSource.<Train>from(file.toPath().toString()) .withRootElement("trains") .withRecordElement("train") .withRecordClass(Train.class); List<Train> expectedResults = ImmutableList.of(new Train("Thomas", 1, "blue", null), new Train("Henry", 3, "green", null), new Train("Toby", 7, "brown", null), new Train("Gordon", 4, "blue", null), new Train("Emily", -1, "red", null), new Train("Percy", 6, "green", null)); assertThat( trainsToStrings(expectedResults), containsInAnyOrder( trainsToStrings(readEverythingFromReader(source.createReader(null, null))).toArray())); }', 'ground_truth': 'public void testReadXMLNoBundleSize() throws IOException { File file = tempFolder.newFile("trainXMLSmall"); Files.write(file.toPath(), trainXML.getBytes(StandardCharsets.UTF_8)); XmlSource<Train> source = XmlSource.<Train>from(file.toPath().toString()) .withRootElement("trains") .withRecordElement("train") .withRecordClass(Train.class); List<Train> expectedResults = ImmutableList.of(new Train("Thomas", 1, "blue", null), new Train("Henry", 3, "green", null), new Train("Toby", 7, "brown", null), new Train("Gordon", 4, "blue", null), new Train("Emily", -1, "red", null), new Train("Percy", 6, "green", null)); assertThat( trainsToStrings(expectedResults), containsInAnyOrder( trainsToStrings(readEverythingFromReader(source.createReader(null, null))).toArray())); }', 'output': 'public void testReadXMLNoBundleSize() throws IOException { File file = tempFolder.newFile("trainXMLSmall"); Files.write(file.toPath(), trainXML.getBytes(StandardCharsets.UTF_8)); XMLSource<Train> source = XMLSource.<Train>from(file.toPath().toString()) .withRootElement("trains") .withRecordElement("train") .withRecordClass(Train.class); List<Train> expectedResults = ImmutableList.of(new Train("Thomas", 1, "blue", null), new Train("Henry", 3, "green", null), new Train("Toby", 7, "brown", null), new Train("Gordon", 4, "blue", null), new Train("Emily", -1, "red", null), new Train("Percy", 6, "green", null)); assertThat( trainsToStrings(expectedResults), containsInAnyOrder( trainsToStrings(readEverythingFromReader(source.createReader(null, null))).toArray())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSatisfiedByWithMultipleTerms() { final XXX comment = getColumn(UTF8Type.instance.decompose("comment")); Unfiltered row = buildRow(buildCell(comment,UTF8Type.instance.decompose("software engineer is working on a project"),System.currentTimeMillis())); Row staticRow = buildRow(Clustering.STATIC_CLUSTERING); Operation.Builder builder = new Operation.Builder(OperationType.AND, controller, new SimpleExpression(comment, Operator.LIKE_CONTAINS, UTF8Type.instance.decompose("eng is a work"))); Operation op = builder.complete(); Assert.assertTrue(op.satisfiedBy(row, staticRow, false)); builder = new Operation.Builder(OperationType.AND, controller, new SimpleExpression(comment, Operator.LIKE_CONTAINS, UTF8Type.instance.decompose("soft works fine"))); op = builder.complete(); Assert.assertTrue(op.satisfiedBy(row, staticRow, false)); }', 'ground_truth': 'public void testSatisfiedByWithMultipleTerms() { final ColumnMetadata comment = getColumn(UTF8Type.instance.decompose("comment")); Unfiltered row = buildRow(buildCell(comment,UTF8Type.instance.decompose("software engineer is working on a project"),System.currentTimeMillis())); Row staticRow = buildRow(Clustering.STATIC_CLUSTERING); Operation.Builder builder = new Operation.Builder(OperationType.AND, controller, new SimpleExpression(comment, Operator.LIKE_CONTAINS, UTF8Type.instance.decompose("eng is a work"))); Operation op = builder.complete(); Assert.assertTrue(op.satisfiedBy(row, staticRow, false)); builder = new Operation.Builder(OperationType.AND, controller, new SimpleExpression(comment, Operator.LIKE_CONTAINS, UTF8Type.instance.decompose("soft works fine"))); op = builder.complete(); Assert.assertTrue(op.satisfiedBy(row, staticRow, false)); }', 'output': 'public void testSatisfiedByWithMultipleTerms() { final String comment = getColumn(UTF8Type.instance.decompose("comment")); Unfiltered row = buildRow(buildCell(comment,UTF8Type.instance.decompose("software engineer is working on a project"),System.currentTimeMillis())); Row staticRow = buildRow(Clustering.STATIC_CLUSTERING); Operation.Builder builder = new Operation.Builder(OperationType.AND, controller, new SimpleExpression(comment, Operator.LIKE_CONTAINS, UTF8Type.instance.decompose("eng is a work"))); Operation op = builder.complete(); Assert.assertTrue(op.satisfiedBy(row, staticRow, false)); builder = new Operation.Builder(OperationType.AND, controller, new SimpleExpression(comment, Operator.LIKE_CONTAINS, UTF8Type.instance.decompose("soft works fine"))); op = builder.complete(); Assert.assertTrue(op.satisfiedBy(row, staticRow, false)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void processTimelineClusterMetric(TimelineClusterMetric clusterMetric, String hostname, Double metricValue) { String appId = clusterMetric.getAppId(); if (appId == null) { return; } if (appId.equalsIgnoreCase(HOST_APP_ID)) { if (hostMetadata.containsKey(hostname)) { updateAppAggregatesFromHostMetric(clusterMetric, hostname, metricValue); } } else { if (appIdsToAggregate.contains(appId)) { TimelineMetricHostMetadata timelineMetricHostMetadata = hostMetadata.get(hostname); XXX appIds; if (timelineMetricHostMetadata == null) { appIds = new HashSet<>(); hostMetadata.put(hostname, new TimelineMetricHostMetadata(appIds)); } else { appIds = timelineMetricHostMetadata.getHostedApps(); } if (!appIds.contains(appId)) { appIds.add(appId); LOG.info("Adding appId to hosted apps: appId = " + clusterMetric.getAppId() + ", hostname = " + hostname); } } } }', 'ground_truth': 'public void processTimelineClusterMetric(TimelineClusterMetric clusterMetric, String hostname, Double metricValue) { String appId = clusterMetric.getAppId(); if (appId == null) { return; } if (appId.equalsIgnoreCase(HOST_APP_ID)) { if (hostMetadata.containsKey(hostname)) { updateAppAggregatesFromHostMetric(clusterMetric, hostname, metricValue); } } else { if (appIdsToAggregate.contains(appId)) { TimelineMetricHostMetadata timelineMetricHostMetadata = hostMetadata.get(hostname); ConcurrentHashMap<String, String> appIds; if (timelineMetricHostMetadata == null) { appIds = new ConcurrentHashMap<>(); hostMetadata.put(hostname, new TimelineMetricHostMetadata(appIds)); } else { appIds = timelineMetricHostMetadata.getHostedApps(); } if (!appIds.containsKey(appId)) { appIds.put(appId, appId); LOG.info("Adding appId to hosted apps: appId = " + clusterMetric.getAppId() + ", hostname = " + hostname); } } } }', 'output': 'public void processTimelineClusterMetric(TimelineClusterMetric clusterMetric, String hostname, Double metricValue) { String appId = clusterMetric.getAppId(); if (appId == null) { return; } if (appId.equalsIgnoreCase(HOST_APP_ID)) { if (hostMetadata.containsKey(hostname)) { updateAppAggregatesFromHostMetric(clusterMetric, hostname, metricValue); } } else { if (appIdsToAggregate.contains(appId)) { TimelineMetricHostMetadata timelineMetricHostMetadata = hostMetadata.get(hostname); Set<String> appIds; if (timelineMetricHostMetadata == null) { appIds = new HashSet<>(); hostMetadata.put(hostname, new TimelineMetricHostMetadata(appIds)); } else { appIds = timelineMetricHostMetadata.getHostedApps(); } if (!appIds.contains(appId)) { appIds.add(appId); LOG.info("Adding appId to hosted apps: appId = " + clusterMetric.getAppId() + ", hostname = " + hostname); } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testIllegalBounds() throws Exception { IgniteCache<Integer, Integer> cache = ignite.jcache(null); cache.put(1, 1); cache.put(2, 2); XXX qry = cache.queryFields(new QuerySqlPredicate(Integer.class, "_key between 2 and 1")); assertTrue(qry.getAll().isEmpty()); }', 'ground_truth': 'public void testIllegalBounds() throws Exception { IgniteCache<Integer, Integer> cache = ignite.jcache(null); cache.put(1, 1); cache.put(2, 2); QueryCursor<Cache.Entry<Integer,Integer>> qry = cache.query(new QuerySqlPredicate(Integer.class, "_key between 2 and 1")); assertTrue(qry.getAll().isEmpty()); }', 'output': 'public void testIllegalBounds() throws Exception { IgniteCache<Integer, Integer> cache = ignite.jcache(null); cache.put(1, 1); cache.put(2, 2); CacheQuery<Integer, Integer> qry = cache.queryFields(new QuerySqlPredicate(Integer.class, "_key between 2 and 1")); assertTrue(qry.getAll().isEmpty()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void send(Collection<? extends ClusterNode> nodes, Object topic, int topicOrd, GridTcpCommunicationMessageAdapter msg, GridIoPolicy plc, long msgId, long timeout, boolean skipOnTimeout) throws GridException { assert nodes != null; assert topic != null; assert msg != null; assert plc != null; if (msgId < 0) assert F.find(nodes, null, F.localNode(locNodeId)) == null : "Internal GridGain code should never call the method with local node in a node list."; try { if (!nodes.isEmpty()) { boolean first = true; for (ClusterNode node : nodes) { GridTcpCommunicationMessageAdapter msg0 = first ? msg : msg.clone(); first = false; send(node, topic, topicOrd, msg0, plc, msgId, timeout, skipOnTimeout); } } else if (log.isDebugEnabled()) log.debug("Failed to send message to empty nodes collection [topic=" + topic + ", msg=" + msg + ", policy=" + plc + \']\'); } catch (XXX e) { throw new GridException("Failed to send message (nodes may have left the grid or " + "TCP connection cannot be established due to firewall issues) " + "[nodes=" + nodes + ", topic=" + topic + ", msg=" + msg + ", policy=" + plc + \']\', e); } }', 'ground_truth': 'private void send(Collection<? extends ClusterNode> nodes, Object topic, int topicOrd, GridTcpCommunicationMessageAdapter msg, GridIoPolicy plc, long msgId, long timeout, boolean skipOnTimeout) throws GridException { assert nodes != null; assert topic != null; assert msg != null; assert plc != null; if (msgId < 0) assert F.find(nodes, null, F.localNode(locNodeId)) == null : "Internal GridGain code should never call the method with local node in a node list."; try { if (!nodes.isEmpty()) { boolean first = true; for (ClusterNode node : nodes) { GridTcpCommunicationMessageAdapter msg0 = first ? msg : msg.clone(); first = false; send(node, topic, topicOrd, msg0, plc, msgId, timeout, skipOnTimeout); } } else if (log.isDebugEnabled()) log.debug("Failed to send message to empty nodes collection [topic=" + topic + ", msg=" + msg + ", policy=" + plc + \']\'); } catch (IgniteSpiException e) { throw new GridException("Failed to send message (nodes may have left the grid or " + "TCP connection cannot be established due to firewall issues) " + "[nodes=" + nodes + ", topic=" + topic + ", msg=" + msg + ", policy=" + plc + \']\', e); } }', 'output': 'private void send(Collection<? extends ClusterNode> nodes, Object topic, int topicOrd, GridTcpCommunicationMessageAdapter msg, GridIoPolicy plc, long msgId, long timeout, boolean skipOnTimeout) throws GridException { assert nodes != null; assert topic != null; assert msg != null; assert plc != null; if (msgId < 0) assert F.find(nodes, null, F.localNode(locNodeId)) == null : "Internal GridGain code should never call the method with local node in a node list."; try { if (!nodes.isEmpty()) { boolean first = true; for (ClusterNode node : nodes) { GridTcpCommunicationMessageAdapter msg0 = first ? msg : msg.clone(); first = false; send(node, topic, topicOrd, msg0, plc, msgId, timeout, skipOnTimeout); } } else if (log.isDebugEnabled()) log.debug("Failed to send message to empty nodes collection [topic=" + topic + ", msg=" + msg + ", policy=" + plc + \']\'); } catch (Throwable e) { throw new GridException("Failed to send message (nodes may have left the grid or " + "TCP connection cannot be established due to firewall issues) " + "[nodes=" + nodes + ", topic=" + topic + ", msg=" + msg + ", policy=" + plc + \']\', e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testImportOutOfRangeExtendedVerify() throws Throwable { createTable("create table %s (id int primary key, d int)"); for (int i = 0; i < 1000; i++) execute("insert into %s (id, d) values (?, ?)", i, i); getCurrentColumnFamilyStore().forceBlockingFlush(); Set<SSTableReader> sstables = getCurrentColumnFamilyStore().getLiveSSTables(); getCurrentColumnFamilyStore().clearUnsafe(); TokenMetadata tmd = StorageService.instance.getTokenMetadata(); tmd.updateNormalTokens(BootStrapper.getRandomTokens(tmd, 5), InetAddressAndPort.getByName("127.0.0.1")); tmd.updateNormalTokens(BootStrapper.getRandomTokens(tmd, 5), InetAddressAndPort.getByName("127.0.0.2")); tmd.updateNormalTokens(BootStrapper.getRandomTokens(tmd, 5), InetAddressAndPort.getByName("127.0.0.3")); File backupdir = moveToBackupDir(sstables); try { XXX options = ColumnFamilyStore.ImportOptions.options(backupdir.toString()) .verifySSTables(true) .verifyTokens(true) .extendedVerify(true).build(); getCurrentColumnFamilyStore().importNewSSTables(options); } finally { tmd.clearUnsafe(); } }', 'ground_truth': 'public void testImportOutOfRangeExtendedVerify() throws Throwable { createTable("create table %s (id int primary key, d int)"); for (int i = 0; i < 1000; i++) execute("insert into %s (id, d) values (?, ?)", i, i); getCurrentColumnFamilyStore().forceBlockingFlush(); Set<SSTableReader> sstables = getCurrentColumnFamilyStore().getLiveSSTables(); getCurrentColumnFamilyStore().clearUnsafe(); TokenMetadata tmd = StorageService.instance.getTokenMetadata(); tmd.updateNormalTokens(BootStrapper.getRandomTokens(tmd, 5), InetAddressAndPort.getByName("127.0.0.1")); tmd.updateNormalTokens(BootStrapper.getRandomTokens(tmd, 5), InetAddressAndPort.getByName("127.0.0.2")); tmd.updateNormalTokens(BootStrapper.getRandomTokens(tmd, 5), InetAddressAndPort.getByName("127.0.0.3")); File backupdir = moveToBackupDir(sstables); try { SSTableImporter.Options options = SSTableImporter.Options.options(backupdir.toString()) .verifySSTables(true) .verifyTokens(true) .extendedVerify(true).build(); SSTableImporter importer = new SSTableImporter(getCurrentColumnFamilyStore()); List<String> failedDirectories = importer.importNewSSTables(options); assertEquals(Collections.singletonList(backupdir.toString()), failedDirectories); } finally { tmd.clearUnsafe(); } }', 'output': 'public void testImportOutOfRangeExtendedVerify() throws Throwable { createTable("create table %s (id int primary key, d int)"); for (int i = 0; i < 1000; i++) execute("insert into %s (id, d) values (?, ?)", i, i); getCurrentColumnFamilyStore().forceBlockingFlush(); Set<SSTableReader> sstables = getCurrentColumnFamilyStore().getLiveSSTables(); getCurrentColumnFamilyStore().clearUnsafe(); TokenMetadata tmd = StorageService.instance.getTokenMetadata(); tmd.updateNormalTokens(BootStrapper.getRandomTokens(tmd, 5), InetAddressAndPort.getByName("127.0.0.1")); tmd.updateNormalTokens(BootStrapper.getRandomTokens(tmd, 5), InetAddressAndPort.getByName("127.0.0.2")); tmd.updateNormalTokens(BootStrapper.getRandomTokens(tmd, 5), InetAddressAndPort.getByName("127.0.0.3")); File backupdir = moveToBackupDir(sstables); try { ColumnFamilyStore.ImportOptions options = ColumnFamilyStore.ImportOptions.options(backupdir.toString()) .verifySSTables(true) .verifyTokens(true) .extendedVerify(true).build(); getCurrentColumnFamilyStore().importNewSSTables(options); } finally { tmd.clearUnsafe(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Collection<String> match(String spec) throws IOException { File file = new File(spec); File parent = file.getAbsoluteFile().getParentFile(); if (!parent.exists()) { throw new IOException("Unable to find parent directory of " + spec); } String pathToMatch = file.getAbsolutePath().replaceAll(Matcher.quoteReplacement("\\\\"), Matcher.quoteReplacement("\\\\\\\\")); final PathMatcher matcher = FileSystems.getDefault().getPathMatcher("glob:" + pathToMatch); XXX files = parent.listFiles(new FileFilter() { @Override public boolean accept(File pathname) { return matcher.matches(pathname.toPath()); } }); List<String> result = new LinkedList<>(); for (File match : files) { result.add(match.getPath()); } return result; }', 'ground_truth': 'public Collection<String> match(String spec) throws IOException { File file = new File(spec); File parent = file.getAbsoluteFile().getParentFile(); if (!parent.exists()) { throw new IOException("Unable to find parent directory of " + spec); } String pathToMatch = file.getAbsolutePath().replaceAll(Matcher.quoteReplacement("\\\\"), Matcher.quoteReplacement("\\\\\\\\")); final PathMatcher matcher = FileSystems.getDefault().getPathMatcher("glob:" + pathToMatch); Iterable<File> files = com.google.common.io.Files.fileTreeTraverser().preOrderTraversal(parent); Iterable<File> matchedFiles = Iterables.filter(files, Predicates.and( com.google.common.io.Files.isFile(), new Predicate<File>() { @Override public boolean apply(File input) { return matcher.matches(input.toPath()); } })); List<String> result = new LinkedList<>(); for (File match : matchedFiles) { result.add(match.getPath()); } return result; }', 'output': 'public Collection<String> match(String spec) throws IOException { File file = new File(spec); File parent = file.getAbsoluteFile().getParentFile(); if (!parent.exists()) { throw new IOException("Unable to find parent directory of " + spec); } String pathToMatch = file.getAbsolutePath().replaceAll(Matcher.quoteReplacement("\\\\"), Matcher.quoteReplacement("\\\\\\\\")); final PathMatcher matcher = FileSystems.getDefault().getPathMatcher("glob:" + pathToMatch); final List<File> files = parent.listFiles(new FileFilter() { @Override public boolean accept(File pathname) { return matcher.matches(pathname.toPath()); } }); List<String> result = new LinkedList<>(); for (File match : files) { result.add(match.getPath()); } return result; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public  void fireCatalogListeners() { XXX iter = catalogListeners.iterator(); while (iter.hasNext()) { CatalogListener l = (CatalogListener) iter.next(); l.notifyInvalidate(); } }', 'ground_truth': 'public  void fireCatalogListeners() { Iterator<CatalogListener> iter = catalogListeners.iterator(); while (iter.hasNext()) { CatalogListener l = iter.next(); l.notifyInvalidate(); } }', 'output': 'public  void fireCatalogListeners() { Iterator<CatalogListener> iter = catalogListeners.iterator(); while (iter.hasNext()) { CatalogListener l = (CatalogListener) iter.next(); l.notifyInvalidate(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private int readSyncMarker(CommitLogDescriptor descriptor, int offset, RandomAccessReader reader) throws IOException { if (offset > reader.length() - CommitLogSegment.SYNC_MARKER_SIZE) { return -1; } reader.seek(offset); XXX crc = CRC32Factory.instance.create(); crc.updateInt((int) (descriptor.id & 0xFFFFFFFFL)); crc.updateInt((int) (descriptor.id >>> 32)); crc.updateInt((int) reader.getPosition()); int end = reader.readInt(); long filecrc = reader.readInt() & 0xffffffffL; if (crc.getValue() != filecrc) { if (end != 0 || filecrc != 0) { logger.warn("Encountered bad header at position {} of commit log {}, with invalid CRC. The end of segment marker should be zero.", offset, reader.getPath()); } return -1; } else if (end < offset || end > reader.length()) { logger.warn("Encountered bad header at position {} of commit log {}, with bad position but valid CRC", offset, reader.getPath()); return -1; } return end; }', 'ground_truth': 'private int readSyncMarker(CommitLogDescriptor descriptor, int offset, RandomAccessReader reader) throws IOException { if (offset > reader.length() - CommitLogSegment.SYNC_MARKER_SIZE) { return -1; } reader.seek(offset); CRC32 crc = new CRC32(); updateChecksumInt(crc, (int) (descriptor.id & 0xFFFFFFFFL)); updateChecksumInt(crc, (int) (descriptor.id >>> 32)); updateChecksumInt(crc, (int) reader.getPosition()); int end = reader.readInt(); long filecrc = reader.readInt() & 0xffffffffL; if (crc.getValue() != filecrc) { if (end != 0 || filecrc != 0) { logger.warn("Encountered bad header at position {} of commit log {}, with invalid CRC. The end of segment marker should be zero.", offset, reader.getPath()); } return -1; } else if (end < offset || end > reader.length()) { logger.warn("Encountered bad header at position {} of commit log {}, with bad position but valid CRC", offset, reader.getPath()); return -1; } return end; }', 'output': 'private int readSyncMarker(CommitLogDescriptor descriptor, int offset, RandomAccessReader reader) throws IOException { if (offset > reader.length() - CommitLogSegment.SYNC_MARKER_SIZE) { return -1; } reader.seek(offset); final CRC32 crc = CRC32Factory.instance.create(); crc.updateInt((int) (descriptor.id & 0xFFFFFFFFL)); crc.updateInt((int) (descriptor.id >>> 32)); crc.updateInt((int) reader.getPosition()); int end = reader.readInt(); long filecrc = reader.readInt() & 0xffffffffL; if (crc.getValue() != filecrc) { if (end != 0 || filecrc != 0) { logger.warn("Encountered bad header at position {} of commit log {}, with invalid CRC. The end of segment marker should be zero.", offset, reader.getPath()); } return -1; } else if (end < offset || end > reader.length()) { logger.warn("Encountered bad header at position {} of commit log {}, with bad position but valid CRC", offset, reader.getPath()); return -1; } return end; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void close() { boolean close = false; if (useCnt.decrementAndGet() == 0) { logLock.writeLock().lock(); try { if (useCnt.get() == 0) { loggers.remove(endpoint); close = true; } } finally { logLock.writeLock().unlock(); } } if (close) { U.interrupt(flushWorker); try { U.join(flushWorker); } catch (XXX ignore) { } entries.clear(); } }', 'ground_truth': 'public void close() { boolean close = false; if (useCnt.decrementAndGet() == 0) { logLock.writeLock().lock(); try { if (useCnt.get() == 0) { loggers.remove(endpoint); close = true; } } finally { logLock.writeLock().unlock(); } } if (close) { U.interrupt(flushWorker); try { U.join(flushWorker); } catch (IgniteInterruptedException ignore) { } entries.clear(); } }', 'output': 'public void close() { boolean close = false; if (useCnt.decrementAndGet() == 0) { logLock.writeLock().lock(); try { if (useCnt.get() == 0) { loggers.remove(endpoint); close = true; } } finally { logLock.writeLock().unlock(); } } if (close) { U.interrupt(flushWorker); try { U.join(flushWorker); } catch (InterruptedException ignore) { } entries.clear(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testQueueSizeCounter() throws Exception { final ConcurrentLinkedQueue<Integer> q = new ConcurrentLinkedQueue<>(); final AtomicInteger sizeCnt = new AtomicInteger(); final AtomicBoolean done = new AtomicBoolean(); final AtomicBoolean guard = new AtomicBoolean(); final ReadWriteLock lock = new ReentrantReadWriteLock(); XXX fut1 = GridTestUtils.runMultiThreadedAsync( new Callable<Object>() { @SuppressWarnings( {"BusyWait"}) @Nullable @Override public Object call() throws Exception { int cleanUps = 0; while (!done.get()) { lock.readLock().lock(); try { q.add(1); sizeCnt.incrementAndGet(); } finally { lock.readLock().unlock(); } if (sizeCnt.get() > 100 && guard.compareAndSet(false, true)) { lock.writeLock().lock(); try { for (Integer i = q.poll(); i != null; i = q.poll()) sizeCnt.decrementAndGet(); cleanUps++; assert sizeCnt.get() == 0 : "Invalid count [cnt=" + sizeCnt.get() + ", size=" + q.size() + ", entries=" + q + \']\'; } finally { lock.writeLock().unlock(); guard.set(false); } } } X.println("Cleanups count (per thread): " + cleanUps); return null; } }, 100, "test-thread" ); Thread.sleep(3 * 60 * 1000); done.set(true); fut1.get(); }', 'ground_truth': 'public void testQueueSizeCounter() throws Exception { final ConcurrentLinkedQueue<Integer> q = new ConcurrentLinkedQueue<>(); final AtomicInteger sizeCnt = new AtomicInteger(); final AtomicBoolean done = new AtomicBoolean(); final AtomicBoolean guard = new AtomicBoolean(); final ReadWriteLock lock = new ReentrantReadWriteLock(); IgniteFuture fut1 = GridTestUtils.runMultiThreadedAsync( new Callable<Object>() { @SuppressWarnings( {"BusyWait"}) @Nullable @Override public Object call() throws Exception { int cleanUps = 0; while (!done.get()) { lock.readLock().lock(); try { q.add(1); sizeCnt.incrementAndGet(); } finally { lock.readLock().unlock(); } if (sizeCnt.get() > 100 && guard.compareAndSet(false, true)) { lock.writeLock().lock(); try { for (Integer i = q.poll(); i != null; i = q.poll()) sizeCnt.decrementAndGet(); cleanUps++; assert sizeCnt.get() == 0 : "Invalid count [cnt=" + sizeCnt.get() + ", size=" + q.size() + ", entries=" + q + \']\'; } finally { lock.writeLock().unlock(); guard.set(false); } } } X.println("Cleanups count (per thread): " + cleanUps); return null; } }, 100, "test-thread" ); Thread.sleep(3 * 60 * 1000); done.set(true); fut1.get(); }', 'output': 'public void testQueueSizeCounter() throws Exception { final ConcurrentLinkedQueue<Integer> q = new ConcurrentLinkedQueue<>(); final AtomicInteger sizeCnt = new AtomicInteger(); final AtomicBoolean done = new AtomicBoolean(); final AtomicBoolean guard = new AtomicBoolean(); final ReadWriteLock lock = new ReentrantReadWriteLock(); IgniteFuture<?> fut1 = GridTestUtils.runMultiThreadedAsync( new Callable<Object>() { @SuppressWarnings( {"BusyWait"}) @Nullable @Override public Object call() throws Exception { int cleanUps = 0; while (!done.get()) { lock.readLock().lock(); try { q.add(1); sizeCnt.incrementAndGet(); } finally { lock.readLock().unlock(); } if (sizeCnt.get() > 100 && guard.compareAndSet(false, true)) { lock.writeLock().lock(); try { for (Integer i = q.poll(); i != null; i = q.poll()) sizeCnt.decrementAndGet(); cleanUps++; assert sizeCnt.get() == 0 : "Invalid count [cnt=" + sizeCnt.get() + ", size=" + q.size() + ", entries=" + q + \']\'; } finally { lock.writeLock().unlock(); guard.set(false); } } } X.println("Cleanups count (per thread): " + cleanUps); return null; } }, 100, "test-thread" ); Thread.sleep(3 * 60 * 1000); done.set(true); fut1.get(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Iterator<RangeTombstone> forwardIterator(final Slice slice) { int startIdx = slice.start() == Slice.Bound.BOTTOM ? 0 : searchInternal(slice.start(), 0, size); final int start = startIdx < 0 ? -startIdx-1 : startIdx; if (start >= size) return Collections.emptyIterator(); int finishIdx = slice.end() == Slice.Bound.TOP ? size - 1 : searchInternal(slice.end(), start, size); final int finish = finishIdx < 0 ? -finishIdx-2 : finishIdx; if (start > finish) return Collections.emptyIterator(); if (start == finish) { Slice.Bound s = comparator.compare(starts[start], slice.start()) < 0 ? slice.start() : starts[start]; XXX e = comparator.compare(slice.end(), ends[start]) < 0 ? slice.end() : ends[start]; return Iterators.<RangeTombstone>singletonIterator(rangeTombstoneWithNewBounds(start, s, e)); } return new AbstractIterator<RangeTombstone>() { private int idx = start; protected RangeTombstone computeNext() { if (idx >= size || idx > finish) return endOfData(); if (idx == start && comparator.compare(starts[idx], slice.start()) < 0) return rangeTombstoneWithNewStart(idx++, slice.start()); if (idx == finish && comparator.compare(slice.end(), ends[idx]) < 0) return rangeTombstoneWithNewEnd(idx++, slice.end()); return rangeTombstone(idx++); } }; }', 'ground_truth': 'private Iterator<RangeTombstone> forwardIterator(final Slice slice) { int startIdx = slice.start() == ClusteringBound.BOTTOM ? 0 : searchInternal(slice.start(), 0, size); final int start = startIdx < 0 ? -startIdx-1 : startIdx; if (start >= size) return Collections.emptyIterator(); int finishIdx = slice.end() == ClusteringBound.TOP ? size - 1 : searchInternal(slice.end(), start, size); final int finish = finishIdx < 0 ? -finishIdx-2 : finishIdx; if (start > finish) return Collections.emptyIterator(); if (start == finish) { ClusteringBound s = comparator.compare(starts[start], slice.start()) < 0 ? slice.start() : starts[start]; ClusteringBound e = comparator.compare(slice.end(), ends[start]) < 0 ? slice.end() : ends[start]; return Iterators.<RangeTombstone>singletonIterator(rangeTombstoneWithNewBounds(start, s, e)); } return new AbstractIterator<RangeTombstone>() { private int idx = start; protected RangeTombstone computeNext() { if (idx >= size || idx > finish) return endOfData(); if (idx == start && comparator.compare(starts[idx], slice.start()) < 0) return rangeTombstoneWithNewStart(idx++, slice.start()); if (idx == finish && comparator.compare(slice.end(), ends[idx]) < 0) return rangeTombstoneWithNewEnd(idx++, slice.end()); return rangeTombstone(idx++); } }; }', 'output': 'private Iterator<RangeTombstone> forwardIterator(final Slice slice) { int startIdx = slice.start() == Slice.Bound.BOTTOM ? 0 : searchInternal(slice.start(), 0, size); final int start = startIdx < 0 ? -startIdx-1 : startIdx; if (start >= size) return Collections.emptyIterator(); int finishIdx = slice.end() == Slice.Bound.TOP ? size - 1 : searchInternal(slice.end(), start, size); final int finish = finishIdx < 0 ? -finishIdx-2 : finishIdx; if (start > finish) return Collections.emptyIterator(); if (start == finish) { Slice.Bound s = comparator.compare(starts[start], slice.start()) < 0 ? slice.start() : starts[start]; Slice.Bound e = comparator.compare(slice.end(), ends[start]) < 0 ? slice.end() : ends[start]; return Iterators.<RangeTombstone>singletonIterator(rangeTombstoneWithNewBounds(start, s, e)); } return new AbstractIterator<RangeTombstone>() { private int idx = start; protected RangeTombstone computeNext() { if (idx >= size || idx > finish) return endOfData(); if (idx == start && comparator.compare(starts[idx], slice.start()) < 0) return rangeTombstoneWithNewStart(idx++, slice.start()); if (idx == finish && comparator.compare(slice.end(), ends[idx]) < 0) return rangeTombstoneWithNewEnd(idx++, slice.end()); return rangeTombstone(idx++); } }; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public DigestResolverDebugResult[] getDigestsByEndpoint() { DigestResolverDebugResult[] ret = new DigestResolverDebugResult[responses.size()]; for (int i = 0; i < responses.size(); i++) { XXX message = responses.get(i); ReadResponse response = message.payload; String digestHex = ByteBufferUtil.bytesToHex(response.digest(command)); ret[i] = new DigestResolverDebugResult(message.from, digestHex, message.payload.isDigestResponse()); } return ret; }', 'ground_truth': 'public DigestResolverDebugResult[] getDigestsByEndpoint() { DigestResolverDebugResult[] ret = new DigestResolverDebugResult[responses.size()]; for (int i = 0; i < responses.size(); i++) { Message<ReadResponse> message = responses.get(i); ReadResponse response = message.payload; String digestHex = ByteBufferUtil.bytesToHex(response.digest(command)); ret[i] = new DigestResolverDebugResult(message.from(), digestHex, message.payload.isDigestResponse()); } return ret; }', 'output': 'public DigestResolverDebugResult[] getDigestsByEndpoint() { DigestResolverDebugResult[] ret = new DigestResolverDebugResult[responses.size()]; for (int i = 0; i < responses.size(); i++) { Message message = responses.get(i); ReadResponse response = message.payload; String digestHex = ByteBufferUtil.bytesToHex(response.digest(command)); ret[i] = new DigestResolverDebugResult(message.from, digestHex, message.payload.isDigestResponse()); } return ret; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void close() throws IOException { if (writer != null) { writer.close(); SSTableLoader.LoaderFuture future = loader.stream(); while (true) { try { future.get(1000, TimeUnit.MILLISECONDS); break; } catch (XXX te) { progress.progress(); } catch (InterruptedException e) { throw new IOException(e); } } if (future.hadFailures()) { if (future.getFailedHosts().size() > maxFailures) throw new IOException("Too many hosts failed: " + future.getFailedHosts()); else logger.warn("Some hosts failed: " + future.getFailedHosts()); } } }', 'ground_truth': 'private void close() throws IOException { if (writer != null) { writer.close(); Future<StreamState> future = loader.stream(); while (true) { try { future.get(1000, TimeUnit.MILLISECONDS); break; } catch (ExecutionException | TimeoutException te) { progress.progress(); } catch (InterruptedException e) { throw new IOException(e); } } if (loader.getFailedHosts().size() > 0) { if (loader.getFailedHosts().size() > maxFailures) throw new IOException("Too many hosts failed: " + loader.getFailedHosts()); else logger.warn("Some hosts failed: " + loader.getFailedHosts()); } } }', 'output': 'private void close() throws IOException { if (writer != null) { writer.close(); SSTableLoader.LoaderFuture future = loader.stream(); while (true) { try { future.get(1000, TimeUnit.MILLISECONDS); break; } catch (TimeoutException te) { progress.progress(); } catch (InterruptedException e) { throw new IOException(e); } } if (future.hadFailures()) { if (future.getFailedHosts().size() > maxFailures) throw new IOException("Too many hosts failed: " + future.getFailedHosts()); else logger.warn("Some hosts failed: " + future.getFailedHosts()); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAvroProhibitsShadowing() { try { ReflectData.get().getSchema(SubclassHidingParent.class); fail("Expected AvroTypeException"); } catch (XXX e) { assertThat(e.getMessage(), containsString("mapField")); assertThat(e.getMessage(), containsString("two fields named")); } }', 'ground_truth': 'public void testAvroProhibitsShadowing() { try { ReflectData.get().getSchema(SubclassHidingParent.class); fail("Expected AvroTypeException"); } catch (AvroRuntimeException e) { assertThat(e.getMessage(), containsString("mapField")); assertThat(e.getMessage(), containsString("two fields named")); } }', 'output': 'public void testAvroProhibitsShadowing() { try { ReflectData.get().getSchema(SubclassHidingParent.class); fail("Expected AvroTypeException"); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("mapField")); assertThat(e.getMessage(), containsString("two fields named")); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void onClassLoaderReleased(ClassLoader clsLdr) { XXX tmp = lsnr; if (tmp != null) tmp.onUnregistered(clsLdr); }', 'ground_truth': 'private void onClassLoaderReleased(ClassLoader clsLdr) { DeploymentListener tmp = lsnr; if (tmp != null) tmp.onUnregistered(clsLdr); }', 'output': 'private void onClassLoaderReleased(ClassLoader clsLdr) { if (lsnr != null) lsnr.onUnregistered(clsLdr); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void outputASTInVariousFormsIfNeeded(SourceUnit sourceUnit, SourceBuffer sourceBuffer) { if ("xml".equals(System.getProperty("antlr.ast"))) { saveAsXML(sourceUnit.getName(), ast); } if ("groovy".equals(System.getProperty("antlr.ast"))) { try { PrintStream out = new PrintStream(new FileOutputStream(sourceUnit.getName() + ".pretty.groovy")); Visitor visitor = new SourcePrinter(out,tokenNames); AntlrASTProcessor treewalker = new SourceCodeTraversal(visitor); treewalker.process(ast); } catch (FileNotFoundException e) { System.out.println("Cannot create " + sourceUnit.getName() + ".pretty.groovy"); } } if ("mindmap".equals(System.getProperty("antlr.ast"))) { try { PrintStream out = new PrintStream(new FileOutputStream(sourceUnit.getName() + ".mm")); Visitor visitor = new MindMapPrinter(out,tokenNames); AntlrASTProcessor treewalker = new PreOrderTraversal(visitor); treewalker.process(ast); } catch (FileNotFoundException e) { System.out.println("Cannot create " + sourceUnit.getName() + ".mm"); } } if ("extendedMindmap".equals(System.getProperty("antlr.ast"))) { try { PrintStream out = new PrintStream(new FileOutputStream(sourceUnit.getName() + ".mm")); Visitor visitor = new MindMapPrinter(out,tokenNames,sourceBuffer); AntlrASTProcessor treewalker = new PreOrderTraversal(visitor); treewalker.process(ast); } catch (FileNotFoundException e) { System.out.println("Cannot create " + sourceUnit.getName() + ".mm"); } } if ("html".equals(System.getProperty("antlr.ast"))) { try { PrintStream out = new PrintStream(new FileOutputStream(sourceUnit.getName() + ".html")); XXX v = new ArrayList(); v.add(new NodeAsHTMLPrinter(out,tokenNames)); v.add(new SourcePrinter(out,tokenNames)); Visitor visitors = new CompositeVisitor(v); AntlrASTProcessor treewalker = new SourceCodeTraversal(visitors); treewalker.process(ast); } catch (FileNotFoundException e) { System.out.println("Cannot create " + sourceUnit.getName() + ".html"); } } }', 'ground_truth': 'private void outputASTInVariousFormsIfNeeded(SourceUnit sourceUnit, SourceBuffer sourceBuffer) { if ("xml".equals(System.getProperty("antlr.ast"))) { saveAsXML(sourceUnit.getName(), ast); } if ("groovy".equals(System.getProperty("antlr.ast"))) { try { PrintStream out = new PrintStream(new FileOutputStream(sourceUnit.getName() + ".pretty.groovy")); Visitor visitor = new SourcePrinter(out,tokenNames); AntlrASTProcessor treewalker = new SourceCodeTraversal(visitor); treewalker.process(ast); } catch (FileNotFoundException e) { System.out.println("Cannot create " + sourceUnit.getName() + ".pretty.groovy"); } } if ("mindmap".equals(System.getProperty("antlr.ast"))) { try { PrintStream out = new PrintStream(new FileOutputStream(sourceUnit.getName() + ".mm")); Visitor visitor = new MindMapPrinter(out,tokenNames); AntlrASTProcessor treewalker = new PreOrderTraversal(visitor); treewalker.process(ast); } catch (FileNotFoundException e) { System.out.println("Cannot create " + sourceUnit.getName() + ".mm"); } } if ("extendedMindmap".equals(System.getProperty("antlr.ast"))) { try { PrintStream out = new PrintStream(new FileOutputStream(sourceUnit.getName() + ".mm")); Visitor visitor = new MindMapPrinter(out,tokenNames,sourceBuffer); AntlrASTProcessor treewalker = new PreOrderTraversal(visitor); treewalker.process(ast); } catch (FileNotFoundException e) { System.out.println("Cannot create " + sourceUnit.getName() + ".mm"); } } if ("html".equals(System.getProperty("antlr.ast"))) { try { PrintStream out = new PrintStream(new FileOutputStream(sourceUnit.getName() + ".html")); List<VisitorAdapter> v = new ArrayList<VisitorAdapter>(); v.add(new NodeAsHTMLPrinter(out,tokenNames)); v.add(new SourcePrinter(out,tokenNames)); Visitor visitors = new CompositeVisitor(v); AntlrASTProcessor treewalker = new SourceCodeTraversal(visitors); treewalker.process(ast); } catch (FileNotFoundException e) { System.out.println("Cannot create " + sourceUnit.getName() + ".html"); } } }', 'output': 'private void outputASTInVariousFormsIfNeeded(SourceUnit sourceUnit, SourceBuffer sourceBuffer) { if ("xml".equals(System.getProperty("antlr.ast"))) { saveAsXML(sourceUnit.getName(), ast); } if ("groovy".equals(System.getProperty("antlr.ast"))) { try { PrintStream out = new PrintStream(new FileOutputStream(sourceUnit.getName() + ".pretty.groovy")); Visitor visitor = new SourcePrinter(out,tokenNames); AntlrASTProcessor treewalker = new SourceCodeTraversal(visitor); treewalker.process(ast); } catch (FileNotFoundException e) { System.out.println("Cannot create " + sourceUnit.getName() + ".pretty.groovy"); } } if ("mindmap".equals(System.getProperty("antlr.ast"))) { try { PrintStream out = new PrintStream(new FileOutputStream(sourceUnit.getName() + ".mm")); Visitor visitor = new MindMapPrinter(out,tokenNames); AntlrASTProcessor treewalker = new PreOrderTraversal(visitor); treewalker.process(ast); } catch (FileNotFoundException e) { System.out.println("Cannot create " + sourceUnit.getName() + ".mm"); } } if ("extendedMindmap".equals(System.getProperty("antlr.ast"))) { try { PrintStream out = new PrintStream(new FileOutputStream(sourceUnit.getName() + ".mm")); Visitor visitor = new MindMapPrinter(out,tokenNames,sourceBuffer); AntlrASTProcessor treewalker = new PreOrderTraversal(visitor); treewalker.process(ast); } catch (FileNotFoundException e) { System.out.println("Cannot create " + sourceUnit.getName() + ".mm"); } } if ("html".equals(System.getProperty("antlr.ast"))) { try { PrintStream out = new PrintStream(new FileOutputStream(sourceUnit.getName() + ".html")); Visitor visitor = new HTMLPrinter(out,tokenNames); AntlrASTProcessor treewalker = new SourceCodeTraversal(visitor); treewalker.process(ast); } catch (FileNotFoundException e) { System.out.println("Cannot create "'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean isEmpty() { XXX view = data.getView(); return view.sstables.isEmpty() && view.getCurrentMemtable().getOperations() == 0 && view.getCurrentMemtable() == view.getOldestMemtable(); }', 'ground_truth': 'public boolean isEmpty() { View view = data.getView(); return view.sstables.isEmpty() && view.getCurrentMemtable().getOperations() == 0 && view.liveMemtables.size() <= 1 && view.flushingMemtables.size() == 0; }', 'output': 'public boolean isEmpty() { View view = data.getView(); return view.sstables.isEmpty() && view.getCurrentMemtable().getOperations() == 0 && view.getCurrentMemtable() == view.getOldestMemtable(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static IgniteSet<String> initializeSet(Ignite ignite, String setName) throws IgniteException { XXX setCfg = new IgniteCollectionConfiguration(); setCfg.setCacheName(CACHE_NAME); IgniteSet<String> set = ignite.set(setName, setCfg); for (int i = 0; i < 10; i++) set.add(Integer.toString(i)); System.out.println("Set size after initializing: " + set.size()); return set; }', 'ground_truth': 'private static IgniteSet<String> initializeSet(Ignite ignite, String setName) throws IgniteException { CollectionConfiguration setCfg = new CollectionConfiguration(); setCfg.setCacheName(CACHE_NAME); IgniteSet<String> set = ignite.set(setName, setCfg); for (int i = 0; i < 10; i++) set.add(Integer.toString(i)); System.out.println("Set size after initializing: " + set.size()); return set; }', 'output': 'private static IgniteSet<String> initializeSet(Ignite ignite, String setName) throws IgniteException { CollectionConfiguration cfg = new CollectionConfiguration(); cfg.setCacheName(CACHE_NAME); IgniteSet<String> set = ignite.set(setName, cfg); for (int i = 0; i < 10; i++) set.add(Integer.toString(i)); System.out.println("Set size after initializing: " + set.size()); return set; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private State getStateForUpdate(WID window) { return states.computeIfAbsent(window, w -> { XXX col = allowEarlyEmitting ? new ElementCollectContext(collector, w) : null; return stateFactory.createState(stateStorageProvider, col); }); }', 'ground_truth': 'private State getStateForUpdate(WID window) { return states.computeIfAbsent(window, w -> { ElementCollector col = allowEarlyEmitting ? new ElementCollector(collector, w) : null; return stateFactory.createState(stateStorageProvider, col); }); }', 'output': 'private State getStateForUpdate(WID window) { return states.computeIfAbsent(window, w -> { ElementCollectContext col = allowEarlyEmitting ? new ElementCollectContext(collector, w) : null; return stateFactory.createState(stateStorageProvider, col); }); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void assertContentEquals(SSTableReader lhs, SSTableReader rhs) throws Exception { try (ISSTableScanner slhs = lhs.getScanner(); ISSTableScanner srhs = rhs.getScanner()) { while (slhs.hasNext()) { OnDiskAtomIterator ilhs = slhs.next(); assert srhs.hasNext() : "LHS contained more rows than RHS"; XXX irhs = srhs.next(); assertContentEquals(ilhs, irhs); } assert !srhs.hasNext() : "RHS contained more rows than LHS"; } }', 'ground_truth': 'public static void assertContentEquals(UnfilteredRowIterator lhs, UnfilteredRowIterator rhs) { assertEquals(lhs.partitionKey(), rhs.partitionKey()); assertEquals(lhs.partitionLevelDeletion(), rhs.partitionLevelDeletion()); while (lhs.hasNext()) { Unfiltered clhs = lhs.next(); assert rhs.hasNext() : "LHS contained more columns than RHS for " + lhs.partitionKey(); Unfiltered crhs = rhs.next(); assertEquals("Mismatched row/tombstone for " + lhs.partitionKey(), clhs, crhs); } assert !rhs.hasNext() : "RHS contained more columns than LHS for " + lhs.partitionKey(); }', 'output': 'public static void assertContentEquals(SSTableReader lhs, SSTableReader rhs) throws Exception { try (ISSTableScanner slhs = lhs.getScanner(); ISSTableScanner srhs = rhs.getScanner()) { while (slhs.hasNext()) { OnDiskAtomIterator ilhs = slhs.next(); assert srhs.hasNext() : "LHS contained more rows than RHS"; OnDiskAtomIterator irhs = srhs.next(); assertContentEquals(ilhs, irhs); } assert !srhs.hasNext() : "RHS contained more rows than LHS"; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testLockAsyncWithTimeoutEntry() throws Exception { if (lockingEnabled()) { ClusterNode node = F.first(cache().affinity().mapKeyToPrimaryAndBackups("key")); if (node == null) throw new IgniteCheckedException("Failed to map key."); GridCache<String, Integer> cache = G.ignite(node.id()).cache(null); final CacheEntry<String, Integer> e = cache.entry("key"); info("Entry [e=" + e + ", primary=" + e.primary() + ", backup=" + e.backup() + \']\'); assert e != null; assert !e.isLocked(); e.lockAsync(2000).get(); assert e.isLocked(); final CountDownLatch syncLatch = new CountDownLatch(1); IgniteCompute comp = forLocal(dfltIgnite).withAsync(); comp.call(new Callable<Boolean>() { @Override public Boolean call() throws Exception { syncLatch.countDown(); IgniteInternalFuture<Boolean> f = e.lockAsync(1000); try { f.get(100); fail(); } catch (IgniteFutureTimeoutCheckedException ex) { info("Caught expected exception: " + ex); } try { assert f.get(); } finally { e.unlock(); } return true; } }); IgniteFuture<Boolean> f = comp.future(); syncLatch.await(); Thread.sleep(300); assert e.isLocked(); assert e.isLockedByThread(); cache.unlock("key"); assert f.get(); for (int i = 0; i < 100; i++) if (cache.isLocked("key") || cache.isLockedByThread("key")) Thread.sleep(10); else break; assert !cache.isLocked("key"); assert !cache.isLockedByThread("key"); } }', 'ground_truth': 'public void testLockAsyncWithTimeoutEntry() throws Exception { if (lockingEnabled()) { ClusterNode node = F.first(cache().affinity().mapKeyToPrimaryAndBackups("key")); if (node == null) throw new IgniteCheckedException("Failed to map key."); GridCache<String, Integer> cache = G.ignite(node.id()).cache(null); final Entry<String, Integer> e = cache.entry("key"); info("Entry [e=" + e + ", primary=" + e.primary() + ", backup=" + e.backup() + \']\'); assert e != null; assert !e.isLocked(); e.lockAsync(2000).get(); assert e.isLocked(); final CountDownLatch syncLatch = new CountDownLatch(1); IgniteCompute comp = forLocal(dfltIgnite).withAsync(); comp.call(new Callable<Boolean>() { @Override public Boolean call() throws Exception { syncLatch.countDown(); IgniteInternalFuture<Boolean> f = e.lockAsync(1000); try { f.get(100); fail(); } catch (IgniteFutureTimeoutCheckedException ex) { info("Caught expected exception: " + ex); } try { assert f.get(); } finally { e.unlock(); } return true; } }); IgniteFuture<Boolean> f = comp.future(); syncLatch.await(); Thread.sleep(300); assert e.isLocked(); assert e.isLockedByThread(); cache.unlock("key"); assert f.get(); for (int i = 0; i < 100; i++) if (cache.isLocked("key") || cache.isLockedByThread("key")) Thread.sleep(10); else break; assert !cache.isLocked("key"); assert !cache.isLockedByThread("key"); } }', 'output': 'public void testLockAsyncWithTimeoutEntry() throws Exception { if (lockingEnabled()) { ClusterNode node = F.first(cache().affinity().mapKeyToPrimaryAndBackups("key")); if (node == null) throw new IgniteCheckedException("Failed to map key."); GridCache<String, Integer> cache = G.ignite(node.id()).cache(null); final CacheEntry<String, Integer> e = cache.entry("key"); info("Entry [e=" + e + ", primary=" + e.primary() + ", backup=" + e.backup() + \']\'); assert e != null; assert !e.isLocked(); e.lockAsync(2000).get(); assert e.isLocked(); final CountDownLatch syncLatch = new CountDownLatch(1); IgniteCompute comp = forLocal(dfltIgnite).withAsync(); comp.call(new Callable<Boolean>() { @Override public Boolean call() throws Exception { syncLatch.countDown(); IgniteInternalFuture<Boolean> f = e.lockAsync(1000); try { f.get(100); fail(); } catch (IgniteFutureTimeoutCheckedException ex) { info("Caught expected exception: " + ex); } try { assert f.get(); } finally { e.unlock(); } return true; } }); IgniteFuture<Boolean> f = comp.future(); syncLatch.await(); Thread.sleep(300); assert e.isLocked(); assert e.isLockedByThread(); cache.unlock("key"); assert f.get(); for (int i = 0; i < 100; i++) if (cache.isLocked("key") || cache.isLockedByThread("key")) Thread.sleep(10); else break; assert !cache.isLocked("key"); assert !cache.isLockedByThread("key"); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void doTestEviction(GridCacheTxConcurrency concurrency, GridCacheTxIsolation isolation) throws Exception { assert concurrency != null; assert isolation != null; assert KEY_CNT >= EVICT_CACHE_SIZE; GridDhtCacheAdapter<String, Integer> dht0 = dht(cache(0)); GridDhtCacheAdapter<String, Integer> dht1 = dht(cache(1)); GridCacheAffinity<String> aff = dht0.affinity(); for (int kv = 0; kv < KEY_CNT; kv++) { String key = String.valueOf(kv); GridCacheProjection<String, Integer> c = cache(aff.mapKeyToNode(key)); try (GridCacheTx tx = c.txStart(concurrency, isolation)) { assert c.get(key) == null; c.put(key, kv); c.entry(key).timeToLive(10); assertEquals(Integer.valueOf(kv), c.get(key)); tx.commit(); } } if (TEST_INFO) { info("Printing keys in dht0..."); for (String key : dht0.keySet()) info("[key=" + key + ", primary=" + F.eqNodes(grid(0).localNode(), aff.mapKeyToNode(key)) + \']\'); info("Printing keys in dht1..."); for (String key : dht1.keySet()) info("[key=" + key + ", primary=" + F.eqNodes(grid(1).localNode(), aff.mapKeyToNode(key)) + \']\'); } assertEquals(EVICT_CACHE_SIZE, dht0.size()); assertEquals(EVICT_CACHE_SIZE, dht1.size()); assertEquals(0, near(cache(0)).nearSize()); assertEquals(0, near(cache(1)).nearSize()); }', 'ground_truth': 'private void doTestEviction(GridCacheTxConcurrency concurrency, GridCacheTxIsolation isolation) throws Exception { assert concurrency != null; assert isolation != null; assert KEY_CNT >= EVICT_CACHE_SIZE; GridDhtCacheAdapter<String, Integer> dht0 = dht(cache(0)); GridDhtCacheAdapter<String, Integer> dht1 = dht(cache(1)); GridCacheAffinity<String> aff = dht0.affinity(); TouchedExpiryPolicy plc = new TouchedExpiryPolicy(new Duration(TimeUnit.MILLISECONDS, 10)); for (int kv = 0; kv < KEY_CNT; kv++) { String key = String.valueOf(kv); ClusterNode node = aff.mapKeyToNode(key); IgniteCache<String, Integer> c = cache(node); IgniteTransactions txs = G.ignite(node.id()).transactions(); try (GridCacheTx tx = txs.txStart(concurrency, isolation)) { assert c.get(key) == null; c.withExpiryPolicy(plc).put(key, 1); assertEquals(Integer.valueOf(kv), c.get(key)); tx.commit(); } } if (TEST_INFO) { info("Printing keys in dht0..."); for (String key : dht0.keySet()) info("[key=" + key + ", primary=" + F.eqNodes(grid(0).localNode(), aff.mapKeyToNode(key)) + \']\'); info("Printing keys in dht1..."); for (String key : dht1.keySet()) info("[key=" + key + ", primary=" + F.eqNodes(grid(1).localNode(), aff.mapKeyToNode(key)) + \']\'); } assertEquals(EVICT_CACHE_SIZE, dht0.size()); assertEquals(EVICT_CACHE_SIZE, dht1.size()); assertEquals(0, near(cache(0)).nearSize()); assertEquals(0, near(cache(1)).nearSize()); }', 'output': 'private void doTestEviction(GridCacheTxConcurrency concurrency, GridCacheTxIsolation isolation) throws Exception { assert concurrency != null; assert isolation != null; assert KEY_CNT >= EVICT_CACHE_SIZE; GridDhtCacheAdapter<String, Integer> dht0 = dht(cache(0)); GridDhtCacheAdapter<String, Integer> dht1 = dht(cache(1)); GridCacheAffinity<String> aff = dht0.affinity(); for (int kv = 0; kv < KEY_CNT; kv++) { String key = String.valueOf(kv); GridCacheProjection<String, Integer> c = cache(aff.mapKeyToNode(key)); try (GridCacheTx tx = c.txStart(concurrency, isolation)) { assert c.get(key) == null; c.put(key, kv); c.entry(key).timeToLive(10); assertEquals(Integer.valueOf(kv), c.get(key)); tx.commit(); } } if (TEST_INFO) { info("Printing keys in dht0..."); for (String key : dht0.keySet()) info("[key=" + key + ", primary=" + F.eqNodes(grid(0).localNode(), aff.mapKeyToNode(key)) + \']\'); info("Printing keys in dht1..."); for (String key : dht1.keySet()) info("[key=" + key + ", primary=" + F.eqNodes(grid(1).localNode(), aff.mapKeyToNode(key)) + \']\'); } assertEquals(EVICT_CACHE_SIZE, dht0.size()); assertEquals(EVICT_CACHE_SIZE, dht1.size()); assertEquals(0, near(cache(0)).nearSize()); assertEquals(0, near(cache(1)).nearSize()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCacheAndEvictAll() throws Exception { GridCache<Object, String> c = grid().cache(DATA_CACHE_NAME); assertEquals("value1", svc.cacheable(1)); assertEquals("value2", svc.cacheable(2)); assertEquals(4, c.size()); assertEquals("value1", c.get(key("testCache1", 1))); assertEquals("value1", c.get(key("testCache2", 1))); assertEquals("value2", c.get(key("testCache1", 2))); assertEquals("value2", c.get(key("testCache2", 2))); svc.cacheEvictAll(); assertEquals(2, c.size()); assertEquals(null, c.get(key("testCache1", 1))); assertEquals("value1", c.get(key("testCache2", 1))); assertEquals(null, c.get(key("testCache1", 2))); assertEquals("value2", c.get(key("testCache2", 2))); }', 'ground_truth': 'public void testCacheAndEvictAll() throws Exception { Cache<Object, String> c = grid().cache(DATA_CACHE_NAME); assertEquals("value1", svc.cacheable(1)); assertEquals("value2", svc.cacheable(2)); assertEquals(4, c.size()); assertEquals("value1", c.get(key("testCache1", 1))); assertEquals("value1", c.get(key("testCache2", 1))); assertEquals("value2", c.get(key("testCache1", 2))); assertEquals("value2", c.get(key("testCache2", 2))); svc.cacheEvictAll(); assertEquals(2, c.size()); assertEquals(null, c.get(key("testCache1", 1))); assertEquals("value1", c.get(key("testCache2", 1))); assertEquals(null, c.get(key("testCache1", 2))); assertEquals("value2", c.get(key("testCache2", 2))); }', 'output': 'public void testCacheAndEvictAll() throws Exception { Cache<Object, String> c = grid().cache(DATA_CACHE_NAME); assertEquals("value1", svc.cacheable(1)); assertEquals("value2", svc.cacheable(2)); assertEquals(4, c.size()); assertEquals("value1", c.get(key("testCache1", 1))); assertEquals("value1", c.get(key("testCache2", 1))); assertEquals("value2", c.get(key("testCache1", 2))); assertEquals("value2", c.get(key("testCache2", 2))); svc.cacheEvictAll(); assertEquals(2, c.size()); assertEquals(null, c.get(key("testCache1", 1))); assertEquals("value1", c.get(key("testCache2", 1))); assertEquals(null, c.get(key("testCache1", 2))); assertEquals("value2", c.get(key("testCache2", 2))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Map<String, String> getVolumesTable() { Map<String, String> result = new TreeMap<>(); DefaultTableModel model = (DefaultTableModel) volumesTable.getModel(); if (volumesTable.isEditing()) { volumesTable.getCellEditor().stopCellEditing(); } for (Object o : model.getDataVector()) { XXX v = (Vector) o; String target = (String) v.elementAt(0); String source = (String) v.elementAt(1); if (target != null && source != null) { result.put(target, source); } } return result; }', 'ground_truth': 'public Map<String, String> getVolumesTable() { Map<String, String> result = new TreeMap<>(); DefaultTableModel model = (DefaultTableModel) volumesTable.getModel(); if (volumesTable.isEditing()) { volumesTable.getCellEditor().stopCellEditing(); } for (Object o : model.getDataVector()) { Vector<String> v = (Vector<String>)o; String target = v.elementAt(0); String source = v.elementAt(1); if (target != null && source != null) { result.put(target, source); } } return result; }', 'output': 'public Map<String, String> getVolumesTable() { Map<String, String> result = new TreeMap<>(); DefaultTableModel model = (DefaultTableModel) volumesTable.getModel(); if (volumesTable.isEditing()) { volumesTable.getCellEditor().stopCellEditing(); } for (Object o : model.getDataVector()) { Vector v = (Vector) o; String target = (String) v.elementAt(0); String source = (String) v.elementAt(1); if (target != null && source != null) { result.put(target, source); } } return result; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ForeignKeyElement[] getForeignKeys() { XXX keys = getKeys(ForeignKeyElement.class); if (keys == null) return null; int count = keys.size(); return ((ForeignKeyElement[]) keys.toArray(new ForeignKeyElement[count])); }', 'ground_truth': 'public ForeignKeyElement[] getForeignKeys() { ArrayList keys = getKeys(ForeignKeyElement.class); if (keys == null) return null; int count = keys.size(); return ((ForeignKeyElement[]) keys.toArray(new ForeignKeyElement[count])); }', 'output': 'public ForeignKeyElement[] getForeignKeys() { List<ForeignKeyElement> keys = getKeys(ForeignKeyElement.class); if (keys == null) return null; int count = keys.size(); return keys.toArray(new ForeignKeyElement[count]); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void doTestSerializeAndLoadKeyCache() throws Exception { ColumnFamilyStore cfs = Keyspace.open(KEYSPACE1).getColumnFamilyStore(CF_STANDARD1); cfs.truncateBlocking(); for (int i = 0; i < 2; i++) { XXX colDef = ColumnDefinition.regularDef(cfs.metadata, ByteBufferUtil.bytes("col1"), AsciiType.instance); RowUpdateBuilder rowBuilder = new RowUpdateBuilder(cfs.metadata, System.currentTimeMillis(), "key1"); rowBuilder.add(colDef, "val1"); rowBuilder.build().apply(); cfs.forceBlockingFlush(); } Assert.assertEquals(2, cfs.getLiveSSTables().size()); for (SSTableReader sstable : cfs.getLiveSSTables()) sstable.getPosition(Util.dk("key1"), SSTableReader.Operator.EQ); AutoSavingCache<KeyCacheKey, RowIndexEntry> keyCache = CacheService.instance.keyCache; keyCache.submitWrite(keyCache.size()).get(); keyCache.clear(); Assert.assertEquals(0, keyCache.size()); keyCache.loadSavedAsync().get(); for (SSTableReader sstable : cfs.getLiveSSTables()) Assert.assertNotNull(keyCache.get(new KeyCacheKey(cfs.metadata.ksAndCFName, sstable.descriptor, ByteBufferUtil.bytes("key1")))); }', 'ground_truth': 'private static void doTestSerializeAndLoadKeyCache() throws Exception { ColumnFamilyStore cfs = Keyspace.open(KEYSPACE1).getColumnFamilyStore(CF_STANDARD1); cfs.truncateBlocking(); for (int i = 0; i < 2; i++) { ColumnMetadata colDef = ColumnMetadata.regularColumn(cfs.metadata(), ByteBufferUtil.bytes("col1"), AsciiType.instance); RowUpdateBuilder rowBuilder = new RowUpdateBuilder(cfs.metadata(), System.currentTimeMillis(), "key1"); rowBuilder.add(colDef, "val1"); rowBuilder.build().apply(); cfs.forceBlockingFlush(); } Assert.assertEquals(2, cfs.getLiveSSTables().size()); for (SSTableReader sstable : cfs.getLiveSSTables()) sstable.getPosition(Util.dk("key1"), SSTableReader.Operator.EQ); AutoSavingCache<KeyCacheKey, RowIndexEntry> keyCache = CacheService.instance.keyCache; keyCache.submitWrite(keyCache.size()).get(); keyCache.clear(); Assert.assertEquals(0, keyCache.size()); keyCache.loadSavedAsync().get(); for (SSTableReader sstable : cfs.getLiveSSTables()) Assert.assertNotNull(keyCache.get(new KeyCacheKey(cfs.metadata(), sstable.descriptor, ByteBufferUtil.bytes("key1")))); }', 'output': 'private static void doTestSerializeAndLoadKeyCache() throws Exception { ColumnFamilyStore cfs = Keyspace.open(KEYSPACE1).getColumnFamilyStore(CF_STANDARD1); cfs.truncateBlocking(); for (int i = 0; i < 2; i++) { ColumnDefinition colDef = ColumnDefinition.regularDef(cfs.metadata, ByteBufferUtil.bytes("col1"), AsciiType.instance); RowUpdateBuilder rowBuilder = new RowUpdateBuilder(cfs.metadata, System.currentTimeMillis(), "key1"); rowBuilder.add(colDef, "val1"); rowBuilder.build().apply(); cfs.forceBlockingFlush(); } Assert.assertEquals(2, cfs.getLiveSSTables().size()); for (SSTableReader sstable : cfs.getLiveSSTables()) sstable.getPosition(Util.dk("key1"), SSTableReader.Operator.EQ); AutoSavingCache<KeyCacheKey, RowIndexEntry> keyCache = CacheService.instance.keyCache; keyCache.submitWrite(keyCache.size()).get(); keyCache.clear(); Assert.assertEquals(0, keyCache.size()); keyCache.loadSavedAsync().get(); for (SSTableReader sstable : cfs.getLiveSSTables()) Assert.assertNotNull(keyCache.get(new KeyCacheKey(cfs.metadata.ksAndCFName, sstable.descriptor, ByteBufferUtil.bytes("key1")))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void emptyResultForEmptyCollection() { FlinkBatchSideInputHandlerFactory factory = FlinkBatchSideInputHandlerFactory.forStage(EXECUTABLE_STAGE, context); MultimapSideInputHandler<Integer, GlobalWindow> handler = factory.forSideInput( TRANSFORM_ID, SIDE_INPUT_NAME, MULTIMAP_ACCESS, KvCoder.of(VoidCoder.of(), VarIntCoder.of()), GlobalWindow.Coder.INSTANCE); Iterable<Integer> result = handler.get(ENCODED_NULL, GlobalWindow.INSTANCE); assertThat(result, emptyIterable()); }', 'ground_truth': 'public void emptyResultForEmptyCollection() { FlinkBatchSideInputHandlerFactory factory = FlinkBatchSideInputHandlerFactory.forStage(EXECUTABLE_STAGE, context); SideInputHandler<Integer, GlobalWindow> handler = factory.forSideInput( TRANSFORM_ID, SIDE_INPUT_NAME, MULTIMAP_ACCESS, KvCoder.of(VoidCoder.of(), VarIntCoder.of()), GlobalWindow.Coder.INSTANCE); Iterable<Integer> result = handler.get(ENCODED_NULL, GlobalWindow.INSTANCE); assertThat(result, emptyIterable()); }', 'output': 'public void emptyResultForEmptyCollection() { BatchSideInputHandlerFactory factory = BatchSideInputHandlerFactory.forStage(EXECUTABLE_STAGE, context); MultimapSideInputHandler<Integer, GlobalWindow> handler = factory.forSideInput( TRANSFORM_ID, SIDE_INPUT_NAME, MULTIMAP_ACCESS, KvCoder.of(VoidCoder.of(), VarIntCoder.of()), GlobalWindow.Coder.INSTANCE); Iterable<Integer> result = handler.get(ENCODED_NULL, GlobalWindow.INSTANCE); assertThat(result, emptyIterable()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPrimaryPut() throws Exception { Ignite g0 = grid(0); GridCache<Integer, Integer> cache0 = g0.cache(null); int key; for (int i = 0; ; i++) { if (cache0.affinity().isPrimary(g0.cluster().localNode(), i)) { cache0.put(i, i); cache0.get(i); key = i; info("Puts: " + cache0.metrics().getCachePuts()); info("Reads: " + cache0.metrics().getCacheGets()); info("Affinity nodes: " + U.nodes2names(cache0.affinity().mapKeyToPrimaryAndBackups(i))); break; } } for (int j = 0; j < gridCount(); j++) { Ignite g = grid(j); info("Checking grid: " + g.name()); info("Puts: " + g.cache(null).metrics().getCachePuts()); info("Reads: " + g.cache(null).metrics().getCacheGets()); if (g.cache(null).affinity().isPrimaryOrBackup(g.cluster().localNode(), key)) assertEquals(1, g.cache(null).metrics().getCachePuts()); else assertEquals(0, g.cache(null).metrics().getCachePuts()); if (g.cache(null).affinity().isPrimary(g.cluster().localNode(), key)) { assertEquals(2, g.cache(null).metrics().getCacheGets()); assertEquals(1, g.cache(null).metrics().getCacheHits()); assertEquals(1, g.cache(null).metrics().getCacheHits()); } else { assertEquals(0, g.cache(null).metrics().getCacheGets()); assertEquals(0, g.cache(null).metrics().getCacheHits()); assertEquals(0, g.cache(null).metrics().getCacheMisses()); } } }', 'ground_truth': 'public void testPrimaryPut() throws Exception { Ignite g0 = grid(0); IgniteCache<Integer, Integer> cache0 = g0.jcache(null); int key; for (int i = 0; ; i++) { if (affinity(cache0).isPrimary(g0.cluster().localNode(), i)) { cache0.put(i, i); cache0.get(i); key = i; info("Puts: " + cache0.metrics().getCachePuts()); info("Reads: " + cache0.metrics().getCacheGets()); info("Affinity nodes: " + U.nodes2names(affinity(cache0).mapKeyToPrimaryAndBackups(i))); break; } } for (int j = 0; j < gridCount(); j++) { Ignite g = grid(j); info("Checking grid: " + g.name()); IgniteCache<Object, Object> jcache = g.jcache(null); info("Puts: " + jcache.metrics().getCachePuts()); info("Reads: " + jcache.metrics().getCacheGets()); if (affinity(jcache).isPrimaryOrBackup(g.cluster().localNode(), key)) assertEquals(1, jcache.metrics().getCachePuts()); else assertEquals(0, jcache.metrics().getCachePuts()); if (affinity(jcache).isPrimary(g.cluster().localNode(), key)) { assertEquals(2, jcache.metrics().getCacheGets()); assertEquals(1, jcache.metrics().getCacheHits()); assertEquals(1, jcache.metrics().getCacheHits()); } else { assertEquals(0, jcache.metrics().getCacheGets()); assertEquals(0, jcache.metrics().getCacheHits()); assertEquals(0, jcache.metrics().getCacheMisses()); } } }', 'output': 'public void testPrimaryPut() throws Exception { Ignite g0 = grid(0); Cache<Integer, Integer> cache0 = g0.cache(null); int key; for (int i = 0; ; i++) { if (cache0.affinity().isPrimary(g0.cluster().localNode(), i)) { cache0.put(i, i); cache0.get(i); key = i; info("Puts: " + cache0.metrics().getCachePuts()); info("Reads: " + cache0.metrics().getCacheGets()); info("Affinity nodes: " + U.nodes2names(cache0.affinity().mapKeyToPrimaryAndBackups(i))); break; } } for (int j = 0; j < gridCount(); j++) { Ignite g = grid(j); info("Checking grid: " + g.name()); info("Puts: " + g.cache(null).metrics().getCachePuts()); info("Reads: " + g.cache(null).metrics().getCacheGets()); if (g.cache(null).affinity().isPrimaryOrBackup(g.cluster().localNode(), key)) assertEquals(1, g.cache(null).metrics().getCachePuts()); else assertEquals(0, g.cache(null).metrics().getCachePuts()); if (g.cache(null).affinity().isPrimary(g.cluster().localNode(), key)) { assertEquals(2, g.cache(null).metrics().getCacheGets()); assertEquals(1, g.cache(null).metrics().getCacheHits()); assertEquals(1, g.cache(null).metrics().getCacheHits()); } else { assertEquals(0, g.cache(null).metrics().getCacheGets()); assertEquals(0, g.cache(null).metrics().getCacheHits()); assertEquals(0, g.cache(null).metrics().getCacheMisses()); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void cliHelpDump() { XXX result = jmxtool("help", "dump"); result.assertCleanStdErr().assertOnExitCode(); Assertions.assertThat(result.getStdout()) .isEqualTo("NAME\\n" + "        jmxtool dump - Dump the Apache Cassandra JMX objects and metadata.\\n" + "\\n" + "SYNOPSIS\\n" + "        jmxtool dump [(-f <format> | --format <format>)] [(-h | --help)]\\n" + "                [(-u <url> | --url <url>)]\\n" + "\\n" + "OPTIONS\\n" + "        -f <format>, --format <format>\\n" + "            What format to dump content as; supported values are console\\n" + "            (default), json, and yaml\\n" + "\\n" + "        -h, --help\\n" + "            Display help information\\n" + "\\n" + "        -u <url>, --url <url>\\n" + "            JMX url to target\\n" + "\\n" + "\\n"); }', 'ground_truth': 'public void cliHelpDump() { ToolResult result = jmxtool("help", "dump"); result.assertOnCleanExit(); Assertions.assertThat(result.getStdout()) .isEqualTo("NAME\\n" + "        jmxtool dump - Dump the Apache Cassandra JMX objects and metadata.\\n" + "\\n" + "SYNOPSIS\\n" + "        jmxtool dump [(-f <format> | --format <format>)] [(-h | --help)]\\n" + "                [(-u <url> | --url <url>)]\\n" + "\\n" + "OPTIONS\\n" + "        -f <format>, --format <format>\\n" + "            What format to dump content as; supported values are console\\n" + "            (default), json, and yaml\\n" + "\\n" + "        -h, --help\\n" + "            Display help information\\n" + "\\n" + "        -u <url>, --url <url>\\n" + "            JMX url to target\\n" + "\\n" + "\\n"); }', 'output': 'public void cliHelpDump() { Result result = jmxtool("help", "dump"); result.assertCleanStdErr().assertOnExitCode(); Assertions.assertThat(result.getStdout()) .isEqualTo("NAME\\n" + "        jmxtool dump - Dump the Apache Cassandra JMX objects and metadata.\\n" + "\\n" + "SYNOPSIS\\n" + "        jmxtool dump [(-f <format> | --format <format>)] [(-h | --help)]\\n" + "                [(-u <url> | --url <url>)]\\n" + "\\n" + "OPTIONS\\n" + "        -f <format>, --format <format>\\n" + "            What format to dump content as; supported values are console\\n" + "            (default), json, and yaml\\n" + "\\n" + "        -h, --help\\n" + "            Display help information\\n" + "\\n" + "        -u <url>, --url <url>\\n" + "            JMX url to target\\n" + "\\n" + "\\n"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private ByteBuffer rowListValue(ListType<?> type, Row row) { if (column.type.isMultiCell()) return cellValueAtIndex(getCells(row, column), getListIndex(collectionElement)); XXX cell = getCell(row, column); return cell == null ? null : type.getSerializer().getElement(cell.value(), getListIndex(collectionElement)); }', 'ground_truth': 'private ByteBuffer rowListValue(ListType<?> type, Row row) { if (column.type.isMultiCell()) return cellValueAtIndex(getCells(row, column), getListIndex(collectionElement)); Cell<?> cell = getCell(row, column); return cell == null ? null : type.getSerializer().getElement(cell.buffer(), getListIndex(collectionElement)); }', 'output': 'private ByteBuffer rowListValue(ListType<?> type, Row row) { if (column.type.isMultiCell()) return cellValueAtIndex(getCells(row, column), getListIndex(collectionElement)); Cell cell = getCell(row, column); return cell == null ? null : type.getSerializer().getElement(cell.value(), getListIndex(collectionElement)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void endElement(String nsUri, String localName, String qName) throws TldParsingException { if (E_TAGLIB_URI.equals(qName)) { taglibUriCData = cDataCollector.toString().trim(); cDataCollector = null; } else if (E_TAGLIB_LOCATION.equals(qName)) { taglibLocationCData = cDataCollector.toString().trim(); try { if (getUriType(taglibLocationCData) == RELATIVE_URL_PATH) { taglibLocationCData = "/WEB-INF/" + taglibLocationCData; } } catch (XXX e) { throw new TldParsingException("Failed to detect URI type for: " + taglibLocationCData, locator, e); } cDataCollector = null; } else if (E_TAGLIB.equals(qName)) { final String jarEntryPath = isJarPath(taglibLocationCData) ? STANDARD_TLD_JAR_ENTRY_PATH : null; addTldLocation(new ServletContextTldLocation(taglibLocationCData, jarEntryPath), taglibUriCData); } }', 'ground_truth': 'public void endElement(String nsUri, String localName, String qName) throws TldParsingException { if (E_TAGLIB_URI.equals(qName)) { taglibUriCData = cDataCollector.toString().trim(); cDataCollector = null; } else if (E_TAGLIB_LOCATION.equals(qName)) { taglibLocationCData = cDataCollector.toString().trim(); if (taglibLocationCData.length() == 0) { throw new TldParsingException("Required \\"" + E_TAGLIB_URI + "\\" element was missing or empty", locator); } try { if (getUriType(taglibLocationCData) == RELATIVE_URL_PATH) { taglibLocationCData = "/WEB-INF/" + taglibLocationCData; } } catch (MalformedURLException e) { throw new TldParsingException("Failed to detect URI type for: " + taglibLocationCData, locator, e); } cDataCollector = null; } else if (E_TAGLIB.equals(qName)) { final String jarEntryPath = isJarPath(taglibLocationCData) ? STANDARD_TLD_JAR_ENTRY_PATH : null; addTldLocation(new ServletContextTldLocation(taglibLocationCData, jarEntryPath), taglibUriCData); } }', 'output': 'public void endElement(String namespace, String localName, String qName) throws TldParsingException { if (E_TAGLIB_URI.equals(qName)) { taglibUriCData = cDataCollector.toString().trim(); cDataCollector = null; } else if (E_TAGLIB_LOCATION.equals(qName)) { taglibLocationCData = cDataCollector.toString().trim(); try { if (getUriType(taglibLocationCData) == RELATIVE_URL_PATH) { taglibLocationCData = "/WEB-INF/" + taglibLocationCData; } } catch (IOException e) { throw new TldParsingException("Failed to detect URI type for: " + taglibLocationCData, locator, e); } cDataCollector = null; } else if (E_TAGLIB.equals(qName)) { final String jarEntryPath = isJarPath(taglibLocationCData) ? STANDARD_TLD_JAR_ENTRY_PATH : null; addTldLocation(new ServletContextTldLocation(taglibLocationCData, jarEntryPath), taglibUriCData); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCallAsyncErrorNoFailover() throws Exception { IgniteCompute comp = compute(grid(0).forPredicate(F.notEqualTo(grid(0).localNode()))).withAsync(); comp.withNoFailover().call(new TestCallableError()); XXX fut = comp.future(); try { fut.get(); assert false : "Exception should have been thrown."; } catch (IgniteCheckedException e) { info("Caught expected exception: " + e); } }', 'ground_truth': 'public void testCallAsyncErrorNoFailover() throws Exception { IgniteCompute comp = compute(grid(0).forPredicate(F.notEqualTo(grid(0).localNode()))).withAsync(); comp.withNoFailover().call(new TestCallableError()); IgniteInternalFuture<Integer> fut = comp.future(); try { fut.get(); assert false : "Exception should have been thrown."; } catch (IgniteCheckedException e) { info("Caught expected exception: " + e); } }', 'output': 'public void testCallAsyncErrorNoFailover() throws Exception { IgniteCompute comp = compute(grid(0).forPredicate(F.notEqualTo(grid(0).localNode()))).withAsync(); comp.withNoFailover().call(new TestCallableError()); IgniteFuture<?> fut = comp.future(); try { fut.get(); assert false : "Exception should have been thrown."; } catch (IgniteCheckedException e) { info("Caught expected exception: " + e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static List<TreeRange> difference(MerkleTree ltree, MerkleTree rtree) { if (!ltree.fullRange.equals(rtree.fullRange)) throw new IllegalArgumentException("Difference only make sense on tree covering the same range (but " + ltree.fullRange + " != " + rtree.fullRange + ")"); List<TreeRange> diff = new ArrayList<TreeRange>(); XXX active = new TreeRange(null, ltree.fullRange.left, ltree.fullRange.right, (byte)0, null); byte[] lhash = ltree.hash(active); byte[] rhash = rtree.hash(active); if (lhash != null && rhash != null && !Arrays.equals(lhash, rhash)) { if (FULLY_INCONSISTENT == differenceHelper(ltree, rtree, diff, active)) diff.add(active); } else if (lhash == null || rhash == null) diff.add(active); return diff; }', 'ground_truth': 'public static List<TreeRange> difference(MerkleTree ltree, MerkleTree rtree) { if (!ltree.fullRange.equals(rtree.fullRange)) throw new IllegalArgumentException("Difference only make sense on tree covering the same range (but " + ltree.fullRange + " != " + rtree.fullRange + ")"); List<TreeRange> diff = new ArrayList<>(); TreeDifference active = new TreeDifference(ltree.fullRange.left, ltree.fullRange.right, (byte)0); Hashable lnode = ltree.find(active); Hashable rnode = rtree.find(active); byte[] lhash = lnode.hash(); byte[] rhash = rnode.hash(); active.setSize(lnode.sizeOfRange(), rnode.sizeOfRange()); if (lhash != null && rhash != null && !Arrays.equals(lhash, rhash)) { if (FULLY_INCONSISTENT == differenceHelper(ltree, rtree, diff, active)) diff.add(active); } else if (lhash == null || rhash == null) diff.add(active); return diff; }', 'output': 'public static List<TreeRange> difference(MerkleTree ltree, MerkleTree rtree) { if (!ltree.fullRange.equals(rtree.fullRange)) throw new IllegalArgumentException("Difference only make sense on tree covering the same range (but " + ltree.fullRange + " != " + rtree.fullRange + ")"); List<TreeRange> diff = new ArrayList<TreeRange>(); TreeRange active = new TreeRange(null, ltree.fullRange.left, ltree.fullRange.right, (byte)0, null); byte[] lhash = ltree.hash(active); byte[] rhash = rtree.hash(active); if (lhash != null && rhash != null && !Arrays.equals(lhash, rhash)) { if (FULLY_INCONSISTENT == differenceHelper(ltree, rtree, diff, active)) diff.add(active); } else if (lhash == null || rhash == null) diff.add(active); return diff; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RelDataType validateImpl() { table = validator.catalogReader.getTable(Arrays.asList(id.names)); if (table == null) { throw validator.newValidationError( id, EigenbaseResource.instance().TableNameNotFound.ex( id.toString())); } if (validator.shouldExpandIdentifiers()) { List<String> qualifiedNames = table.getQualifiedName(); if (qualifiedNames != null) { XXX poses = new SqlParserPos[qualifiedNames.size()]; Arrays.fill( poses, id.getParserPosition()); int offset = qualifiedNames.size() - id.names.length; if (offset >= 0) { for (int i = 0; i < id.names.length; i++) { poses[i + offset] = id.getComponentParserPosition(i); } } final String[] names = qualifiedNames.toArray(new String[qualifiedNames.size()]); id.setNames(names, poses); } } monotonicExprs = new ArrayList<Pair<SqlNode, SqlMonotonicity>>(); RelDataType rowType = table.getRowType(); List<RelDataTypeField> fields = rowType.getFieldList(); for (int i = 0; i < fields.size(); i++) { final String fieldName = fields.get(i).getName(); final SqlMonotonicity monotonicity = table.getMonotonicity(fieldName); if (monotonicity != SqlMonotonicity.NotMonotonic) { monotonicExprs.add( new Pair<SqlNode, SqlMonotonicity>( new SqlIdentifier(fieldName, SqlParserPos.ZERO), monotonicity)); } } return rowType; }', 'ground_truth': 'public RelDataType validateImpl() { table = validator.catalogReader.getTable(id.names); if (table == null) { throw validator.newValidationError( id, EigenbaseResource.instance().TableNameNotFound.ex( id.toString())); } if (validator.shouldExpandIdentifiers()) { List<String> qualifiedNames = table.getQualifiedName(); if (qualifiedNames != null) { List<SqlParserPos> poses = new ArrayList<SqlParserPos>( Collections.nCopies( qualifiedNames.size(), id.getParserPosition())); int offset = qualifiedNames.size() - id.names.size(); if (offset >= 0) { for (int i = 0; i < id.names.size(); i++) { poses.set(i + offset, id.getComponentParserPosition(i)); } } id.setNames(qualifiedNames, poses); } } monotonicExprs = new ArrayList<Pair<SqlNode, SqlMonotonicity>>(); RelDataType rowType = table.getRowType(); List<RelDataTypeField> fields = rowType.getFieldList(); for (int i = 0; i < fields.size(); i++) { final String fieldName = fields.get(i).getName(); final SqlMonotonicity monotonicity = table.getMonotonicity(fieldName); if (monotonicity != SqlMonotonicity.NotMonotonic) { monotonicExprs.add( new Pair<SqlNode, SqlMonotonicity>( new SqlIdentifier(fieldName, SqlParserPos.ZERO), monotonicity)); } } return rowType; }', 'output': 'public RelDataType validateImpl() { table = validator.catalogReader.getTable(Arrays.asList(id.names)); if (table == null) { throw validator.newValidationError( id, EigenbaseResource.instance().TableNameNotFound.ex( id.toString())); } if (validator.shouldExpandIdentifiers()) { List<String> qualifiedNames = table.getQualifiedName(); if (qualifiedNames != null) { SqlParserPos[] poses = new SqlParserPos[qualifiedNames.size()]; Arrays.fill( poses, id.getParserPosition()); int offset = qualifiedNames.size() - id.names.length; if (offset >= 0) { for (int i = 0; i < id.names.length; i++) { poses[i + offset] = id.getComponentParserPosition(i); } } final String[] names = qualifiedNames.toArray(new String[qualifiedNames.size()]); id.setNames(names, poses); } } monotonicExprs = new ArrayList<Pair<SqlNode, SqlMonotonicity>>(); RelDataType rowType = table.getRowType(); List<RelDataTypeField> fields = rowType.getFieldList(); for (int i = 0; i < fields.size(); i++) { final String fieldName = fields.get(i).getName(); final SqlMonotonicity monotonicity = table.getMonotonicity(fieldName); if (monotonicity != SqlMonotonicity.NotMonotonic) { monotonicExprs.add( new Pair<SqlNode, SqlMonotonicity>( new SqlIdentifier(fieldName, SqlParserPos.ZERO), monotonicity)); } } return rowType; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void rewriteRel(SortRel rel) { RelFieldCollation [] oldCollations = rel.getCollations(); RelFieldCollation [] newCollations = new RelFieldCollation[oldCollations.length]; for (int i = 0; i < oldCollations.length; ++i) { int oldInput = oldCollations[i].getFieldIndex(); RelDataType sortFieldType = rel.getChild().getRowType().getFields()[oldInput].getType(); if (sortFieldType.isStruct()) { throw Util.needToImplement("sorting on structured types"); } newCollations[i] = new RelFieldCollation( getNewForOldInput(oldInput), oldCollations[i].getDirection()); } SortRel newRel = new SortRel( rel.getCluster(), getNewForOldRel(rel.getChild()), newCollations); setNewForOldRel(rel, newRel); }', 'ground_truth': 'public void rewriteRel(SortRel rel) { List<RelFieldCollation> oldCollations = rel.getCollations(); List<RelFieldCollation> newCollations = new ArrayList<RelFieldCollation>(oldCollations.size()); for (RelFieldCollation oldCollation : oldCollations) { int oldInput = oldCollation.getFieldIndex(); RelDataType sortFieldType = rel.getChild().getRowType().getFields()[oldInput].getType(); if (sortFieldType.isStruct()) { throw Util.needToImplement("sorting on structured types"); } newCollations.add( new RelFieldCollation( getNewForOldInput(oldInput), oldCollation.getDirection())); } SortRel newRel = new SortRel( rel.getCluster(), rel.getCluster().traitSetOf(CallingConvention.NONE), getNewForOldRel(rel.getChild()), newCollations); setNewForOldRel(rel, newRel); }', 'output': 'public void rewriteRel(SortRel rel) { RelFieldCollation [] oldCollations = rel.getCollations(); RelFieldCollation [] newCollations = new RelFieldCollation[oldCollations.length]; for (int i = 0; i < oldCollations.length; ++i) { int oldInput = oldCollations[i].getFieldIndex(); RelDataType sortFieldType = rel.getChild().getRowType().getFields()[oldInput].getType(); if (sortFieldType.isStruct()) { throw Util.needToImplement("sorting on structured types"); } newCollations[i] = new RelFieldCollation( getNewForOldInput(oldInput), oldCollations[i].getDirection()); } SortRel newRel = new SortRel( rel.getCluster(), getNewForOldRel(rel.getChild()), newCollations); setNewForOldRel(rel, newRel); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private long getId() throws IgniteCheckedException { GridCache<Object, Object> cache = ignite.cache(cacheName); assert cache != null; XXX seq = cache.dataStructures().atomicSequence("ID", 0, true); return seq.incrementAndGet(); }', 'ground_truth': 'private long getId() throws IgniteCheckedException { GridCache<Object, Object> cache = ignite.cache(cacheName); assert cache != null; IgniteAtomicSequence seq = cache.dataStructures().atomicSequence("ID", 0, true); return seq.incrementAndGet(); }', 'output': 'private long getId() throws IgniteCheckedException { GridCache<Object, Object> cache = ignite.cache(cacheName); assert cache != null; AtomicLong seq = cache.dataStructures().atomicSequence("ID", 0, true); return seq.incrementAndGet(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void udtSerDeserTest(int version) throws Exception { ListType<?> lt = ListType.getInstance(Int32Type.instance, true); SetType<?> st = SetType.getInstance(UTF8Type.instance, true); MapType<?, ?> mt = MapType.getInstance(UTF8Type.instance, LongType.instance, true); UserType udt = new UserType("ks", bb("myType"), Arrays.asList(bb("f1"), bb("f2"), bb("f3"), bb("f4")), Arrays.asList(LongType.instance, lt, st, mt), true); Map<ColumnIdentifier, Term.Raw> value = new HashMap<>(); value.put(ci("f1"), lit(42)); value.put(ci("f2"), new Lists.Literal(Arrays.<Term.Raw>asList(lit(3), lit(1)))); value.put(ci("f3"), new Sets.Literal(Arrays.<Term.Raw>asList(lit("foo"), lit("bar")))); value.put(ci("f4"), new Maps.Literal(Arrays.<Pair<Term.Raw, Term.Raw>>asList( Pair.<Term.Raw, Term.Raw>create(lit("foo"), lit(24)), Pair.<Term.Raw, Term.Raw>create(lit("bar"), lit(12))))); UserTypes.Literal u = new UserTypes.Literal(value); Term t = u.prepare("ks", columnSpec("myValue", udt)); QueryOptions options = QueryOptions.DEFAULT; ByteBuffer serialized = t.bindAndGet(options); ByteBuffer[] fields = udt.split(serialized); assertEquals(4, fields.length); assertEquals(bytes(42L), fields[0]); assertEquals(Arrays.asList(3, 1), lt.getSerializer().deserializeForNativeProtocol(fields[1], 3)); LinkedHashSet<String> s = new LinkedHashSet<>(); s.addAll(Arrays.asList("bar", "foo")); assertEquals(s, st.getSerializer().deserializeForNativeProtocol(fields[2], 3)); LinkedHashMap<String, Long> m = new LinkedHashMap<>(); m.put("bar", 12L); m.put("foo", 24L); assertEquals(m, mt.getSerializer().deserializeForNativeProtocol(fields[3], 3)); }', 'ground_truth': 'public void udtSerDeserTest(int version) throws Exception { ListType<?> lt = ListType.getInstance(Int32Type.instance, true); SetType<?> st = SetType.getInstance(UTF8Type.instance, true); MapType<?, ?> mt = MapType.getInstance(UTF8Type.instance, LongType.instance, true); UserType udt = new UserType("ks", bb("myType"), Arrays.asList(field("f1"), field("f2"), field("f3"), field("f4")), Arrays.asList(LongType.instance, lt, st, mt), true); Map<FieldIdentifier, Term.Raw> value = new HashMap<>(); value.put(field("f1"), lit(42)); value.put(field("f2"), new Lists.Literal(Arrays.<Term.Raw>asList(lit(3), lit(1)))); value.put(field("f3"), new Sets.Literal(Arrays.<Term.Raw>asList(lit("foo"), lit("bar")))); value.put(field("f4"), new Maps.Literal(Arrays.<Pair<Term.Raw, Term.Raw>>asList( Pair.<Term.Raw, Term.Raw>create(lit("foo"), lit(24)), Pair.<Term.Raw, Term.Raw>create(lit("bar"), lit(12))))); UserTypes.Literal u = new UserTypes.Literal(value); Term t = u.prepare("ks", columnSpec("myValue", udt)); QueryOptions options = QueryOptions.DEFAULT; ByteBuffer serialized = t.bindAndGet(options); ByteBuffer[] fields = udt.split(serialized); assertEquals(4, fields.length); assertEquals(bytes(42L), fields[0]); assertEquals(Arrays.asList(3, 1), lt.getSerializer().deserializeForNativeProtocol(fields[1], 3)); LinkedHashSet<String> s = new LinkedHashSet<>(); s.addAll(Arrays.asList("bar", "foo")); assertEquals(s, st.getSerializer().deserializeForNativeProtocol(fields[2], 3)); LinkedHashMap<String, Long> m = new LinkedHashMap<>(); m.put("bar", 12L); m.put("foo", 24L); assertEquals(m, mt.getSerializer().deserializeForNativeProtocol(fields[3], 3)); }', 'output': 'public void udtSerDeserTest(int version) throws Exception { ListType<?> lt = ListType.getInstance(Int32Type.instance, true); SetType<?> st = SetType.getInstance(UTF8Type.instance, true); MapType<?, ?> mt = MapType.getInstance(UTF8Type.instance, LongType.instance, true); UserType udt = new UserType("ks", bb("myType"), Arrays.asList(bb("f1"), bb("f2"), bb("f3"), bb("f4")), Arrays.asList(LongType.instance, lt, st, mt), true); Map<ColumnIdentifier, Term.Raw> value = new HashMap<>(); value.put(ci("f1"), lit(42)); value.put(ci("f2"), new Lists.Literal(Arrays.<Term.Raw>asList(lit(3), lit(1)))); value.put(ci("f3"), new Sets.Literal(Arrays.<Term.Raw>asList(lit("foo"), lit("bar")))); value.put(ci("f4"), new Maps.Literal(Arrays.<Pair<Term.Raw, Term.Raw>>asList( Pair.<Term.Raw, Term.Raw>create(lit("foo"), lit(24)), Pair.<Term.Raw, Term.Raw>create(lit("bar"), lit(12))))); UserTypes.Literal u = new UserTypes.Literal(value); Term t = u.prepare("ks", columnSpec("myValue", udt)); QueryOptions options = QueryOptions.DEFAULT; ByteBuffer serialized = t.bindAndGet(options); ByteBuffer[] fields = udt.split(serialized); assertEquals(4, fields.length); assertEquals(bytes(42L), fields[0]); assertEquals(Arrays.asList(3, 1), lt.getSerializer().deserializeForNativeProtocol(fields[1], 3)); LinkedHashSet<String> s = new LinkedHashSet<>(); s.addAll(Arrays.asList("bar", "foo")); assertEquals(s, st.getSerializer().deserializeForNativeProtocol(fields[2], 3)); LinkedHashMap<String, Long> m = new Linked'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSlicePredicate() { long columnValue = 1271253600000l; byte[] columnBytes = FBUtilities.toByteArray(columnValue); XXX columnNames = new ArrayList<byte[]>(); columnNames.add(columnBytes); SlicePredicate originalPredicate = new SlicePredicate().setColumn_names(columnNames); Configuration conf = new Configuration(); ConfigHelper.setInputSlicePredicate(conf, originalPredicate); SlicePredicate rtPredicate = ConfigHelper.getInputSlicePredicate(conf); assert rtPredicate.column_names.size() == 1; assert Arrays.equals(originalPredicate.column_names.get(0), rtPredicate.column_names.get(0)); }', 'ground_truth': 'public void testSlicePredicate() { long columnValue = 1271253600000l; ByteBuffer columnBytes = FBUtilities.toByteArray(columnValue); List<ByteBuffer> columnNames = new ArrayList<ByteBuffer>(); columnNames.add(columnBytes); SlicePredicate originalPredicate = new SlicePredicate().setColumn_names(columnNames); Configuration conf = new Configuration(); ConfigHelper.setInputSlicePredicate(conf, originalPredicate); SlicePredicate rtPredicate = ConfigHelper.getInputSlicePredicate(conf); assert rtPredicate.column_names.size() == 1; assert originalPredicate.column_names.get(0).equals(rtPredicate.column_names.get(0)); }', 'output': 'public void testSlicePredicate() { long columnValue = 1271253600000l; byte[] columnBytes = FBUtilities.toByteArray(columnValue); List<byte[]> columnNames = new ArrayList<>(); columnNames.add(columnBytes); SlicePredicate originalPredicate = new SlicePredicate().setColumn_names(columnNames); Configuration conf = new Configuration(); ConfigHelper.setInputSlicePredicate(conf, originalPredicate); SlicePredicate rtPredicate = ConfigHelper.getInputSlicePredicate(conf); assert rtPredicate.column_names.size() == 1; assert Arrays.equals(originalPredicate.column_names.get(0), rtPredicate.column_names.get(0)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testHelp() { injectTestSystemOut(); assertEquals(EXIT_CODE_OK, execute("--help")); for (XXX cmd : Command.values()) assertTrue(cmd.text(), testOut.toString().contains(cmd.toString())); }', 'ground_truth': 'public void testHelp() { injectTestSystemOut(); assertEquals(EXIT_CODE_OK, execute("--help")); for (CommandList cmd : CommandList.values()) assertTrue(cmd.text(), testOut.toString().contains(cmd.toString())); }', 'output': 'public void testHelp() { injectTestSystemOut(); assertEquals(EXIT_CODE_OK, execute("--help")); for (Command cmd : Command.values()) assertTrue(cmd.text(), testOut.toString().contains(cmd.toString())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static boolean isAffinityNode(CacheConfiguration cfg) { if (cfg.getCacheMode() == LOCAL) return true; XXX partTax = cfg.getDistributionMode(); if (partTax == null) partTax = distributionMode(cfg); return partTax == GridCacheDistributionMode.PARTITIONED_ONLY || partTax == GridCacheDistributionMode.NEAR_PARTITIONED; }', 'ground_truth': 'public static boolean isAffinityNode(CacheConfiguration cfg) { if (cfg.getCacheMode() == LOCAL) return true; CacheDistributionMode partTax = cfg.getDistributionMode(); if (partTax == null) partTax = distributionMode(cfg); return partTax == CacheDistributionMode.PARTITIONED_ONLY || partTax == CacheDistributionMode.NEAR_PARTITIONED; }', 'output': 'public static boolean isAffinityNode(CacheConfiguration cfg) { if (cfg.getCacheMode() == LOCAL) return true; CacheDistributionMode partTax = cfg.getDistributionMode(); if (partTax == null) partTax = distributionMode(cfg); return partTax == CacheDistributionMode.PARTITIONED_ONLY || partTax == CacheDistributionMode.NEAR_PARTITIONED; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static int validateEntry(UUID hostId, ByteBuffer buffer, long baseTimestamp, UUID[] load) throws IOException { CRC32 crc = new CRC32(); DataInputPlus di = new DataInputBuffer(buffer, true); int hintSize = di.readInt(); assertEquals(hintSize + HintsBuffer.ENTRY_OVERHEAD_SIZE, buffer.remaining()); updateChecksum(crc, buffer, buffer.position(), 4); assertEquals((int) crc.getValue(), di.readInt()); Hint hint = Hint.serializer.deserialize(di, MessagingService.current_version); updateChecksum(crc, buffer, buffer.position() + 8, hintSize); assertEquals((int) crc.getValue(), di.readInt()); int idx = (int) (hint.creationTime - baseTimestamp); assertEquals(hostId, load[idx]); Row row = hint.mutation.getPartitionUpdates().iterator().next().iterator().next(); assertEquals(1, Iterables.size(row.cells())); assertEquals(bytes(idx), row.clustering().get(0)); XXX cell = row.cells().iterator().next(); assertEquals(TimeUnit.MILLISECONDS.toMicros(baseTimestamp + idx), cell.timestamp()); assertEquals(bytes(idx), cell.value()); return idx; }', 'ground_truth': 'private static int validateEntry(UUID hostId, ByteBuffer buffer, long baseTimestamp, UUID[] load) throws IOException { CRC32 crc = new CRC32(); DataInputPlus di = new DataInputBuffer(buffer, true); int hintSize = di.readInt(); assertEquals(hintSize + HintsBuffer.ENTRY_OVERHEAD_SIZE, buffer.remaining()); updateChecksum(crc, buffer, buffer.position(), 4); assertEquals((int) crc.getValue(), di.readInt()); Hint hint = Hint.serializer.deserialize(di, MessagingService.current_version); updateChecksum(crc, buffer, buffer.position() + 8, hintSize); assertEquals((int) crc.getValue(), di.readInt()); int idx = (int) (hint.creationTime - baseTimestamp); assertEquals(hostId, load[idx]); Row row = hint.mutation.getPartitionUpdates().iterator().next().iterator().next(); assertEquals(1, Iterables.size(row.cells())); ValueAccessors.assertDataEquals(bytes(idx), row.clustering().get(0)); Cell<?> cell = row.cells().iterator().next(); assertEquals(TimeUnit.MILLISECONDS.toMicros(baseTimestamp + idx), cell.timestamp()); ValueAccessors.assertDataEquals(bytes(idx), cell.buffer()); return idx; }', 'output': 'private static int validateEntry(UUID hostId, ByteBuffer buffer, long baseTimestamp, UUID[] load) throws IOException { CRC32 crc = new CRC32(); DataInputPlus di = new DataInputBuffer(buffer, true); int hintSize = di.readInt(); assertEquals(hintSize + HintsBuffer.ENTRY_OVERHEAD_SIZE, buffer.remaining()); updateChecksum(crc, buffer, buffer.position(), 4); assertEquals((int) crc.getValue(), di.readInt()); Hint hint = Hint.serializer.deserialize(di, MessagingService.current_version); updateChecksum(crc, buffer, buffer.position() + 8, hintSize); assertEquals((int) crc.getValue(), di.readInt()); int idx = (int) (hint.creationTime - baseTimestamp); assertEquals(hostId, load[idx]); Row row = hint.mutation.getPartitionUpdates().iterator().next().iterator().next(); assertEquals(1, Iterables.size(row.cells())); assertEquals(bytes(idx), row.clustering().get(0)); Cell cell = row.cells().iterator().next(); assertEquals(TimeUnit.MILLISECONDS.toMicros(baseTimestamp + idx), cell.timestamp()); assertEquals(bytes(idx), cell.value()); return idx; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMapEquality() { StateTag<?, ?> fooStringVarInt1 = StateTags.map("foo", StringUtf8Coder.of(), VarIntCoder.of()); StateTag<?, ?> fooStringVarInt2 = StateTags.map("foo", StringUtf8Coder.of(), VarIntCoder.of()); StateTag<?, ?> fooStringBigEndian = StateTags.map("foo", StringUtf8Coder.of(), BigEndianIntegerCoder.of()); StateTag<?, ?> fooVarIntBigEndian = StateTags.map("foo", VarIntCoder.of(), BigEndianIntegerCoder.of()); StateTag<?, ?> barStringVarInt = StateTags.map("bar", StringUtf8Coder.of(), VarIntCoder.of()); assertEquals(fooStringVarInt1, fooStringVarInt2); assertNotEquals(fooStringVarInt1, fooStringBigEndian); assertNotEquals(fooStringBigEndian, fooVarIntBigEndian); assertNotEquals(fooStringVarInt1, fooVarIntBigEndian); assertNotEquals(fooStringVarInt1, barStringVarInt); }', 'ground_truth': 'public void testMapEquality() { StateTag<?> fooStringVarInt1 = StateTags.map("foo", StringUtf8Coder.of(), VarIntCoder.of()); StateTag<?> fooStringVarInt2 = StateTags.map("foo", StringUtf8Coder.of(), VarIntCoder.of()); StateTag<?> fooStringBigEndian = StateTags.map("foo", StringUtf8Coder.of(), BigEndianIntegerCoder.of()); StateTag<?> fooVarIntBigEndian = StateTags.map("foo", VarIntCoder.of(), BigEndianIntegerCoder.of()); StateTag<?> barStringVarInt = StateTags.map("bar", StringUtf8Coder.of(), VarIntCoder.of()); assertEquals(fooStringVarInt1, fooStringVarInt2); assertNotEquals(fooStringVarInt1, fooStringBigEndian); assertNotEquals(fooStringBigEndian, fooVarIntBigEndian); assertNotEquals(fooStringVarInt1, fooVarIntBigEndian); assertNotEquals(fooStringVarInt1, barStringVarInt); }', 'output': 'public void testMapEquality() { StateTag<?, ?> fooStringVarInt1 = StateTags.map("foo", StringUtf8Coder.of(), VarIntCoder.of()); StateTag<?, ?> fooStringVarInt2 = StateTags.map("foo", StringUtf8Coder.of(), VarIntCoder.of()); StateTag<?, ?> fooStringBigEndian = StateTags.map("foo", StringUtf8Coder.of(), BigEndianIntegerCoder.of()); StateTag<?, ?> fooVarIntBigEndian = StateTags.map("foo", VarIntCoder.of(), BigEndianIntegerCoder.of()); StateTag<?, ?> barStringVarInt = StateTags.map("bar", StringUtf8Coder.of(), VarIntCoder.of()); assertEquals(fooStringVarInt1, fooStringVarInt2); assertNotEquals(fooStringVarInt1, fooStringBigEndian); assertNotEquals(fooStringBigEndian, fooVarIntBigEndian); assertNotEquals(fooStringVarInt1, fooVarIntBigEndian); assertNotEquals(fooStringVarInt1, barStringVarInt); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testParseAndPredict() { URL url = XGBoostModelParserTest.class.getClassLoader().getResource(TEST_MODEL_RESOURCE); if (url == null) throw new IllegalStateException("File not found [resource_name=" + TEST_MODEL_RESOURCE + "]"); InfModelReader reader = new FileSystemInfModelReader(url.getPath()); try (InfModel<XGObject, Double> mdl = mdlBuilder.build(reader, parser); Scanner testDataScanner = new Scanner(XGBoostModelParserTest.class.getClassLoader() .getResourceAsStream("datasets/agaricus-test-data.txt")); Scanner testExpResultsScanner = new Scanner(XGBoostModelParserTest.class.getClassLoader() .getResourceAsStream("datasets/agaricus-test-expected-results.txt"))) { while (testDataScanner.hasNextLine()) { assertTrue(testExpResultsScanner.hasNextLine()); String testDataStr = testDataScanner.nextLine(); String testExpResultsStr = testExpResultsScanner.nextLine(); XXX testObj = new MapBasedXGObject(); for (String keyValueString : testDataStr.split(" ")) { String[] keyVal = keyValueString.split(":"); if (keyVal.length == 2) testObj.put("f" + keyVal[0], Double.parseDouble(keyVal[1])); } double prediction = mdl.predict(testObj); double expPrediction = Double.parseDouble(testExpResultsStr); assertEquals(expPrediction, prediction, 1e-6); } } }', 'ground_truth': 'public void testParseAndPredict() { URL url = XGBoostModelParserTest.class.getClassLoader().getResource(TEST_MODEL_RESOURCE); if (url == null) throw new IllegalStateException("File not found [resource_name=" + TEST_MODEL_RESOURCE + "]"); InfModelReader reader = new FileSystemInfModelReader(url.getPath()); try (XGModelComposition mdl = mdlBuilder.build(reader, parser); Scanner testDataScanner = new Scanner(XGBoostModelParserTest.class.getClassLoader() .getResourceAsStream("datasets/agaricus-test-data.txt")); Scanner testExpResultsScanner = new Scanner(XGBoostModelParserTest.class.getClassLoader() .getResourceAsStream("datasets/agaricus-test-expected-results.txt"))) { while (testDataScanner.hasNextLine()) { assertTrue(testExpResultsScanner.hasNextLine()); String testDataStr = testDataScanner.nextLine(); String testExpResultsStr = testExpResultsScanner.nextLine(); HashMap<String, Double> testObj = new HashMap<>(); for (String keyValueString : testDataStr.split(" ")) { String[] keyVal = keyValueString.split(":"); if (keyVal.length == 2) testObj.put("f" + keyVal[0], Double.parseDouble(keyVal[1])); } double prediction = mdl.apply(testObj); double expPrediction = Double.parseDouble(testExpResultsStr); assertEquals(expPrediction, prediction, 1e-6); } } }', 'output': 'public void testParseAndPredict() { URL url = XGBoostModelParserTest.class.getClassLoader().getResource(TEST_MODEL_RESOURCE); if (url == null) throw new IllegalStateException("File not found [resource_name=" + TEST_MODEL_RESOURCE + "]"); InfModelReader reader = new FileSystemInfModelReader(url.getPath()); try (InfModel<XGObject, Double> mdl = mdlBuilder.build(reader, parser); Scanner testDataScanner = new Scanner(XGBoostModelParserTest.class.getClassLoader() .getResourceAsStream("datasets/agaricus-test-data.txt")); Scanner testExpResultsScanner = new Scanner(XGBoostModelParserTest.class.getClassLoader() .getResourceAsStream("datasets/agaricus-test-expected-results.txt"))) { while (testDataScanner.hasNextLine()) { assertTrue(testExpResultsScanner.hasNextLine()); String testDataStr = testDataScanner.nextLine(); String testExpResultsStr = testExpResultsScanner.nextLine(); MapBasedXGObject testObj = new MapBasedXGObject(); for (String keyValueString : testDataStr.split(" ")) { String[] keyVal = keyValueString.split(":"); if (keyVal.length == 2) testObj.put("f" + keyVal[0], Double.parseDouble(keyVal[1])); } double prediction = mdl.predict(testObj); double expPrediction = Double.parseDouble(testExpResultsStr); assertEquals(expPrediction, prediction, 1e-6); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean canRemoveRow(Row row, Columns requestedColumns, long sstableTimestamp) { if (row.primaryKeyLivenessInfo().isEmpty() || row.primaryKeyLivenessInfo().timestamp() <= sstableTimestamp) return false; for (ColumnMetadata column : requestedColumns) { XXX cell = row.getCell(column); if (cell == null || cell.timestamp() <= sstableTimestamp) return false; } return true; }', 'ground_truth': 'private boolean canRemoveRow(Row row, Columns requestedColumns, long sstableTimestamp) { if (row.primaryKeyLivenessInfo().isEmpty() || row.primaryKeyLivenessInfo().timestamp() <= sstableTimestamp) return false; for (ColumnMetadata column : requestedColumns) { Cell<?> cell = row.getCell(column); if (cell == null || cell.timestamp() <= sstableTimestamp) return false; } return true; }', 'output': 'private boolean canRemoveRow(Row row, Columns requestedColumns, long sstableTimestamp) { if (row.primaryKeyLivenessInfo().isEmpty() || row.primaryKeyLivenessInfo().timestamp() <= sstableTimestamp) return false; for (ColumnMetadata column : requestedColumns) { Cell cell = row.getCell(column); if (cell == null || cell.timestamp() <= sstableTimestamp) return false; } return true; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public IgniteInternalFuture<HadoopJobId> submit(HadoopJobId jobId, HadoopJobInfo info) { if (!busyLock.tryReadLock()) { return new GridFinishedFuture<>(new IgniteCheckedException("Failed to execute map-reduce job " + "(grid is stopping): " + info)); } try { long jobPrepare = U.currentTimeMillis(); if (jobs.containsKey(jobId) || jobMetaCache().containsKey(jobId)) throw new IgniteCheckedException("Failed to submit job. Job with the same ID already exists: " + jobId); XXX job = job(jobId, info); HadoopMapReducePlan mrPlan = mrPlanner.preparePlan(job, ctx.nodes(), null); logPlan(info, mrPlan); HadoopJobMetadata meta = new HadoopJobMetadata(ctx.localNodeId(), jobId, info); meta.mapReducePlan(mrPlan); meta.pendingSplits(allSplits(mrPlan)); meta.pendingReducers(allReducers(mrPlan)); GridFutureAdapter<HadoopJobId> completeFut = new GridFutureAdapter<>(); GridFutureAdapter<HadoopJobId> old = activeFinishFuts.put(jobId, completeFut); assert old == null : "Duplicate completion future [jobId=" + jobId + ", old=" + old + \']\'; if (log.isDebugEnabled()) log.debug("Submitting job metadata [jobId=" + jobId + ", meta=" + meta + \']\'); long jobStart = U.currentTimeMillis(); HadoopPerformanceCounter perfCntr = HadoopPerformanceCounter.getCounter(meta.counters(), ctx.localNodeId()); perfCntr.clientSubmissionEvents(info); perfCntr.onJobPrepare(jobPrepare); perfCntr.onJobStart(jobStart); if (jobMetaCache().getAndPutIfAbsent(jobId, meta) != null) throw new IgniteCheckedException("Failed to submit job. Job with the same ID already exists: " + jobId); return completeFut; } catch (IgniteCheckedException e) { U.error(log, "Failed to submit job: " + jobId, e); return new GridFinishedFuture<>(e); } finally { busyLock.readUnlock(); } }', 'ground_truth': 'public IgniteInternalFuture<HadoopJobId> submit(HadoopJobId jobId, HadoopJobInfo info) { if (!busyLock.tryReadLock()) { return new GridFinishedFuture<>(new IgniteCheckedException("Failed to execute map-reduce job " + "(grid is stopping): " + info)); } try { long jobPrepare = U.currentTimeMillis(); if (jobs.containsKey(jobId) || jobMetaCache().containsKey(jobId)) throw new IgniteCheckedException("Failed to submit job. Job with the same ID already exists: " + jobId); HadoopJobEx job = job(jobId, info); HadoopMapReducePlan mrPlan = mrPlanner.preparePlan(job, ctx.nodes(), null); logPlan(info, mrPlan); HadoopJobMetadata meta = new HadoopJobMetadata(ctx.localNodeId(), jobId, info); meta.mapReducePlan(mrPlan); meta.pendingSplits(allSplits(mrPlan)); meta.pendingReducers(allReducers(mrPlan)); GridFutureAdapter<HadoopJobId> completeFut = new GridFutureAdapter<>(); GridFutureAdapter<HadoopJobId> old = activeFinishFuts.put(jobId, completeFut); assert old == null : "Duplicate completion future [jobId=" + jobId + ", old=" + old + \']\'; if (log.isDebugEnabled()) log.debug("Submitting job metadata [jobId=" + jobId + ", meta=" + meta + \']\'); long jobStart = U.currentTimeMillis(); HadoopPerformanceCounter perfCntr = HadoopPerformanceCounter.getCounter(meta.counters(), ctx.localNodeId()); perfCntr.clientSubmissionEvents(info); perfCntr.onJobPrepare(jobPrepare); perfCntr.onJobStart(jobStart); if (jobMetaCache().getAndPutIfAbsent(jobId, meta) != null) throw new IgniteCheckedException("Failed to submit job. Job with the same ID already exists: " + jobId); return completeFut; } catch (IgniteCheckedException e) { U.error(log, "Failed to submit job: " + jobId, e); return new GridFinishedFuture<>(e); } finally { busyLock.readUnlock(); } }', 'output': 'public IgniteInternalFuture<HadoopJobId> submit(HadoopJobId jobId, HadoopJobInfo info) { if (!busyLock.tryReadLock()) { return new GridFinishedFuture<>(new IgniteCheckedException("Failed to execute map-reduce job " + "(grid is stopping): " + info)); } try { long jobPrepare = U.currentTimeMillis(); if (jobs.containsKey(jobId) || jobMetaCache().containsKey(jobId)) throw new IgniteCheckedException("Failed to submit job. Job with the same ID already exists: " + jobId); HadoopJob job = job(jobId, info); HadoopMapReducePlan mrPlan = mrPlanner.preparePlan(job, ctx.nodes(), null); logPlan(info, mrPlan); HadoopJobMetadata meta = new HadoopJobMetadata(ctx.localNodeId(), jobId, info); meta.mapReducePlan(mrPlan); meta.pendingSplits(allSplits(mrPlan)); meta.pendingReducers(allReducers(mrPlan)); GridFutureAdapter<HadoopJobId> completeFut = new GridFutureAdapter<>(); GridFutureAdapter<HadoopJobId> old = activeFinishFuts.put(jobId, completeFut); assert old == null : "Duplicate completion future [jobId=" + jobId + ", old=" + old + \']\'; if (log.isDebugEnabled()) log.debug("Submitting job metadata [jobId=" + jobId + ", meta=" + meta + \']\'); long jobStart = U.currentTimeMillis(); HadoopPerformanceCounter perfCntr = HadoopPerformanceCounter.getCounter(meta.counters(), ctx.localNodeId()); perfCntr.clientSubmissionEvents(info); perfCntr.onJobPrepare(jobPrepare); perfCntr.onJobStart(jobStart); if (jobMetaCache().getAndPutIfAbsent(jobId, meta) != null) throw new IgniteCheckedException("Failed to submit job. Job with the same ID already exists: " + jobId); return completeFut; } catch (IgniteCheckedException e) { U.error(log, "Failed to submit job: "'}
{'instruction': 'Please refactor the following code:', 'input': 'private void advanceSegment() throws IgniteCheckedException { ReadFileHandle cur0 = curHandle; if (cur0 != null) { cur0.close(); if (cur0.workDir) releaseWorkSegment(cur0.idx); curHandle = null; } if (end != null && curIdx + 1 > end.index()) return; curIdx++; FileDescriptor fd; boolean readArchive = canReadArchiveOrReserveWork(curIdx); if (readArchive) { fd = new FileDescriptor(new File(walArchiveDir, FileDescriptor.fileName(curIdx, serializer.version()))); } else { XXX workIdx = curIdx % dbCfg.getWalSegments(); fd = new FileDescriptor( new File(walWorkDir, FileDescriptor.fileName(workIdx, serializer.version())), curIdx); } if (log.isDebugEnabled()) log.debug("Reading next file [absIdx=" + curIdx + ", file=" + fd.file.getAbsolutePath() + \']\'); assert fd != null; try { curHandle = initReadHandle(fd, start != null && curIdx == start.index() ? start : null); } catch (FileNotFoundException e) { if (readArchive) throw new IgniteCheckedException("Missing WAL segment in the archive", e); else curHandle = null; } if (curHandle != null) curHandle.workDir = !readArchive; else releaseWorkSegment(curIdx); curRec = null; }', 'ground_truth': 'private void advanceSegment() throws IgniteCheckedException { ReadFileHandle cur0 = curHandle; if (cur0 != null) { cur0.close(); if (cur0.workDir) releaseWorkSegment(cur0.idx); curHandle = null; } if (end != null && curIdx + 1 > end.index()) return; curIdx++; FileDescriptor fd; boolean readArchive = canReadArchiveOrReserveWork(curIdx); if (readArchive) { fd = new FileDescriptor(new File(walArchiveDir, FileDescriptor.fileName(curIdx, serializer.version()))); } else { long workIdx = curIdx % dbCfg.getWalSegments(); fd = new FileDescriptor( new File(walWorkDir, FileDescriptor.fileName(workIdx, serializer.version())), curIdx); } if (log.isDebugEnabled()) log.debug("Reading next file [absIdx=" + curIdx + ", file=" + fd.file.getAbsolutePath() + \']\'); assert fd != null; try { curHandle = initReadHandle(fd, start != null && curIdx == start.index() ? start : null); } catch (FileNotFoundException e) { if (readArchive) throw new IgniteCheckedException("Missing WAL segment in the archive", e); else curHandle = null; } if (curHandle != null) curHandle.workDir = !readArchive; else releaseWorkSegment(curIdx); curRec = null; }', 'output': 'private void advanceSegment() throws IgniteCheckedException { ReadFileHandle cur0 = curHandle; if (cur0 != null) { cur0.close(); if (cur0.workDir) releaseWorkSegment(cur0.idx); curHandle = null; } if (end != null && curIdx + 1 > end.index()) return; curIdx++; FileDescriptor fd; boolean readArchive = canReadArchiveOrReserveWork(curIdx); if (readArchive) { fd = new FileDescriptor(new File(walArchiveDir, FileDescriptor.fileName(curIdx, serializer.version()))); } else { int workIdx = curIdx % dbCfg.getWalSegments(); fd = new FileDescriptor( new File(walWorkDir, FileDescriptor.fileName(workIdx, serializer.version())), curIdx); } if (log.isDebugEnabled()) log.debug("Reading next file [absIdx=" + curIdx + ", file=" + fd.file.getAbsolutePath() + \']\'); assert fd != null; try { curHandle = initReadHandle(fd, start != null && curIdx == start.index() ? start : null); } catch (FileNotFoundException e) { if (readArchive) throw new IgniteCheckedException("Missing WAL segment in the archive", e); else curHandle = null; } if (curHandle != null) curHandle.workDir = !readArchive; else releaseWorkSegment(curIdx); curRec = null; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ViewProviderModule getViewProviderModule(ProviderModule module) { Map<Resource.Type, ResourceProvider> resourceProviders = new HashMap<Resource.Type, ResourceProvider>(); ViewRegistry registry = ViewRegistry.getInstance(); for (XXX definition : registry.getDefinitions()) { for (Resource.Type type : definition.getViewResourceTypes()){ ResourceProvider provider = definition.getResourceProvider(type); resourceProviders.put(type, provider); } } return new ViewProviderModule(module, resourceProviders); }', 'ground_truth': 'public static ViewProviderModule getViewProviderModule(ProviderModule module) { Map<Resource.Type, ResourceProvider> resourceProviders = new HashMap<Resource.Type, ResourceProvider>(); ViewRegistry registry = ViewRegistry.getInstance(); for (ViewEntity definition : registry.getDefinitions()) { for (Resource.Type type : definition.getViewResourceTypes()){ ResourceProvider provider = definition.getResourceProvider(type); resourceProviders.put(type, provider); } } return new ViewProviderModule(module, resourceProviders); }', 'output': 'public static ViewProviderModule getViewProviderModule(ProviderModule module) { Map<Resource.Type, ResourceProvider> resourceProviders = new HashMap<Resource.Type, ResourceProvider>(); ViewRegistry registry = ViewRegistry.getInstance(); for (ViewDefinition definition : registry.getDefinitions()) { for (Resource.Type type : definition.getViewResourceTypes()){ ResourceProvider provider = definition.getResourceProvider(type); resourceProviders.put(type, provider); } } return new ViewProviderModule(module, resourceProviders); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testLocalNode() throws Exception { assert getDiscoverySpi().getRemoteNodes().isEmpty(); GridNode locNode = getDiscoverySpi().getLocalNode(); XXX node = getSpi().getBalancedNode(new GridTestTaskSession(GridUuid.randomUuid()), Collections.singletonList(locNode), new GridTestJob()); assert  node == locNode; node = getSpi().getBalancedNode(new GridTestTaskSession(GridUuid.randomUuid()), Collections.singletonList(locNode), new GridTestJob()); assert node == locNode; }', 'ground_truth': 'public void testLocalNode() throws Exception { assert getDiscoverySpi().getRemoteNodes().isEmpty(); ClusterNode locNode = getDiscoverySpi().getLocalNode(); ClusterNode node = getSpi().getBalancedNode(new GridTestTaskSession(GridUuid.randomUuid()), Collections.singletonList(locNode), new GridTestJob()); assert  node == locNode; node = getSpi().getBalancedNode(new GridTestTaskSession(GridUuid.randomUuid()), Collections.singletonList(locNode), new GridTestJob()); assert node == locNode; }', 'output': 'public void testLocalNode() throws Exception { assert getDiscoverySpi().getRemoteNodes().isEmpty(); GridNode locNode = getDiscoverySpi().getLocalNode(); ClusterNode node = getSpi().getBalancedNode(new GridTestTaskSession(GridUuid.randomUuid()), Collections.singletonList(locNode), new GridTestJob()); assert  node == locNode; node = getSpi().getBalancedNode(new GridTestTaskSession(GridUuid.randomUuid()), Collections.singletonList(locNode), new GridTestJob()); assert node == locNode; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void finishBundle(FinishBundleContext context) throws Exception { List<ValueInSingleWindow<TableRow>> failedInserts = Lists.newArrayList(); BigQueryOptions options = context.getPipelineOptions().as(BigQueryOptions.class); for (Map.Entry<String, List<ValueInSingleWindow<TableRow>>> entry : tableRows.entrySet()) { TableReference tableReference = BigQueryHelpers.parseTableSpec(entry.getKey()); flushRows( tableReference, entry.getValue(), uniqueIdsForTableRows.get(entry.getKey()), options, failedInserts); } tableRows.clear(); uniqueIdsForTableRows.clear(); for (XXX row : failedInserts) { context.output(failedOutputTag, row.getValue(), row.getTimestamp(), row.getWindow()); } }', 'ground_truth': 'public void finishBundle(FinishBundleContext context) throws Exception { List<ValueInSingleWindow<T>> failedInserts = Lists.newArrayList(); BigQueryOptions options = context.getPipelineOptions().as(BigQueryOptions.class); for (Map.Entry<String, List<ValueInSingleWindow<TableRow>>> entry : tableRows.entrySet()) { TableReference tableReference = BigQueryHelpers.parseTableSpec(entry.getKey()); flushRows( tableReference, entry.getValue(), uniqueIdsForTableRows.get(entry.getKey()), options, failedInserts); } tableRows.clear(); uniqueIdsForTableRows.clear(); for (ValueInSingleWindow<T> row : failedInserts) { context.output(failedOutputTag, row.getValue(), row.getTimestamp(), row.getWindow()); } }', 'output': 'public void finishBundle(FinishBundleContext context) throws Exception { List<ValueInSingleWindow<TableRow>> failedInserts = Lists.newArrayList(); BigQueryOptions options = context.getPipelineOptions().as(BigQueryOptions.class); for (Map.Entry<String, List<ValueInSingleWindow<TableRow>>> entry : tableRows.entrySet()) { TableReference tableReference = BigQueryHelpers.parseTableSpec(entry.getKey()); flushRows( tableReference, entry.getValue(), uniqueIdsForTableRows.get(entry.getKey()), options, failedInserts); } tableRows.clear(); uniqueIdsForTableRows.clear(); for (ValueInSingleWindow<TableRow> row : failedInserts) { context.output(failedOutputTag, row.getValue(), row.getTimestamp(), row.getWindow()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private GridCloseableIterator<T> iterator0() { try { GridCacheQuery qry = new GridCacheQueryAdapter<>(ctx, SET, null, null, null, new GridSetQueryPredicate<>(id, collocated), false, false); XXX nodes = dataNodes(ctx.affinity().affinityTopologyVersion()); qry.projection(ctx.grid().forNodes(nodes)); GridCacheQueryFuture<T> fut = qry.execute(); SetIterator<T> it = new SetIterator<>(fut); itFuts.put(it.weakReference(), fut); if (rmvd) { itFuts.remove(it.weakReference()); it.close(); checkRemoved(); } return it; } catch (GridException e) { throw new GridRuntimeException(e); } }', 'ground_truth': 'private GridCloseableIterator<T> iterator0() { try { GridCacheQuery qry = new GridCacheQueryAdapter<>(ctx, SET, null, null, null, new GridSetQueryPredicate<>(id, collocated), false, false); Collection<ClusterNode> nodes = dataNodes(ctx.affinity().affinityTopologyVersion()); qry.projection(ctx.grid().forNodes(nodes)); GridCacheQueryFuture<T> fut = qry.execute(); SetIterator<T> it = new SetIterator<>(fut); itFuts.put(it.weakReference(), fut); if (rmvd) { itFuts.remove(it.weakReference()); it.close(); checkRemoved(); } return it; } catch (GridException e) { throw new GridRuntimeException(e); } }', 'output': 'private GridCloseableIterator<T> iterator0() { try { GridCacheQuery qry = new GridCacheQueryAdapter<>(ctx, SET, null, null, null, new GridSetQueryPredicate<>(id, collocated), false, false); Collection<ClusterNode> nodes = dataNodes(ctx.affinity().affinityTopologyVersion()); qry.projection(ctx.grid().forNodes(nodes)); GridCacheQueryFuture<T> fut = qry.execute(); SetIterator<T> it = new SetIterator<>(fut); itFuts.put(it.weakReference(), fut); if (rmvd) { itFuts.remove(it.weakReference()); it.close(); checkRemoved(); } return it; } catch (GridException e) { throw new GridRuntimeException(e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void happyCase() throws Exception { final XXX applicationFeatureId = ApplicationFeatureId.newMember("com.mycompany.Bar#foo"); assertThat(applicationFeatureId.title(), is("com.mycompany.Bar#foo")); }', 'ground_truth': 'public void happyCase() throws Exception { val applicationFeatureId = ApplicationFeatureId.newMember("com.mycompany.Bar#foo"); assertThat(applicationFeatureId.title(), is("com.mycompany.Bar#foo")); }', 'output': 'public void happyCase() throws Exception { final ApplicationFeatureId applicationFeatureId = ApplicationFeatureId.newMember("com.mycompany.Bar#foo"); assertThat(applicationFeatureId.title(), is("com.mycompany.Bar#foo")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static void syncWriteToBatchlog(List<RowMutation> mutations, String localDataCenter, Collection<InetAddress> endpoints, UUID uuid) throws WriteTimeoutException { RowMutation rm = BatchlogManager.getBatchlogMutationFor(mutations, uuid); XXX handler = WriteResponseHandler.create(endpoints, ConsistencyLevel.ONE, Table.SYSTEM_KS, null); try { sendMessagesToOneDC(rm.createMessage(), endpoints, true, handler); } catch (IOException e) { throw new RuntimeException("Error writing to batchlog", e); } try { handler.get(); } catch (WriteTimeoutException e) { throw new WriteTimeoutException(e.consistency, 0, e.blockFor, false); } }', 'ground_truth': 'private static void syncWriteToBatchlog(List<RowMutation> mutations, String localDataCenter, Collection<InetAddress> endpoints, UUID uuid) throws WriteTimeoutException { RowMutation rm = BatchlogManager.getBatchlogMutationFor(mutations, uuid); AbstractWriteResponseHandler handler = WriteResponseHandler.create(endpoints, ConsistencyLevel.ONE, Table.SYSTEM_KS, null); try { sendMessagesToOneDC(rm.createMessage(), endpoints, true, handler); } catch (IOException e) { throw new RuntimeException("Error writing to batchlog", e); } try { handler.get(); } catch (WriteTimeoutException e) { throw new WriteTimeoutException(e.consistency, 0, e.blockFor, false); } }', 'output': 'private static void syncWriteToBatchlog(List<RowMutation> mutations, String localDataCenter, Collection<InetAddress> endpoints, UUID uuid) throws WriteTimeoutException { RowMutation rm = BatchlogManager.getBatchlogMutationFor(mutations, uuid); WriteResponseHandler handler = WriteResponseHandler.create(endpoints, ConsistencyLevel.ONE, Table.SYSTEM_KS, null); try { sendMessagesToOneDC(rm.createMessage(), endpoints, true, handler); } catch (IOException e) { throw new RuntimeException("Error writing to batchlog", e); } try { handler.get(); } catch (WriteTimeoutException e) { throw new WriteTimeoutException(e.consistency, 0, e.blockFor, false); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void initPageMemoryDataStructures(DataStorageConfiguration dbCfg) throws IgniteCheckedException { freeListMap = U.newHashMap(dataRegionMap.size()); String dfltMemPlcName = dbCfg.getDefaultDataRegionConfiguration().getName(); for (DataRegion memPlc : dataRegionMap.values()) { DataRegionConfiguration memPlcCfg = memPlc.config(); DataRegionMetricsImpl memMetrics = (DataRegionMetricsImpl) memMetricsMap.get(memPlcCfg.getName()); boolean persistenceEnabled = memPlcCfg.isPersistenceEnabled(); XXX freeList = new CacheFreeListImpl(0, cctx.igniteInstanceName(), memMetrics, memPlc, null, persistenceEnabled ? cctx.wal() : null, 0L, true); freeListMap.put(memPlcCfg.getName(), freeList); } dfltFreeList = freeListMap.get(dfltMemPlcName); }', 'ground_truth': 'protected void initPageMemoryDataStructures(DataStorageConfiguration dbCfg) throws IgniteCheckedException { freeListMap = U.newHashMap(dataRegionMap.size()); String dfltMemPlcName = dbCfg.getDefaultDataRegionConfiguration().getName(); for (DataRegion memPlc : dataRegionMap.values()) { DataRegionConfiguration memPlcCfg = memPlc.config(); DataRegionMetricsImpl memMetrics = (DataRegionMetricsImpl)memMetricsMap.get(memPlcCfg.getName()); boolean persistenceEnabled = memPlcCfg.isPersistenceEnabled(); String freeListName = memPlcCfg.getName() + "##FreeList"; PageLockListener lsnr = cctx.diagnostic().pageLockTracker().createPageLockTracker(freeListName); CacheFreeList freeList = new CacheFreeList( 0, freeListName, memMetrics, memPlc, null, persistenceEnabled ? cctx.wal() : null, 0L, true, lsnr ); freeListMap.put(memPlcCfg.getName(), freeList); } dfltFreeList = freeListMap.get(dfltMemPlcName); }', 'output': 'protected void initPageMemoryDataStructures(DataStorageConfiguration dbCfg) throws IgniteCheckedException { freeListMap = U.newHashMap(dataRegionMap.size()); String dfltMemPlcName = dbCfg.getDefaultDataRegionConfiguration().getName(); for (DataRegion memPlc : dataRegionMap.values()) { DataRegionConfiguration memPlcCfg = memPlc.config(); DataRegionMetricsImpl memMetrics = (DataRegionMetricsImpl) memMetricsMap.get(memPlcCfg.getName()); boolean persistenceEnabled = memPlcCfg.isPersistenceEnabled(); CacheFreeListImpl freeList = new CacheFreeListImpl(0, cctx.igniteInstanceName(), memMetrics, memPlc, null, persistenceEnabled ? cctx.wal() : null, 0L, true); freeListMap.put(memPlcCfg.getName(), freeList); } dfltFreeList = freeListMap.get(dfltMemPlcName); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static FileObject resolveConvertor(Object obj) throws IOException { String prefix = "xml/memory"; FileObject memContext = FileUtil.getConfigFile(prefix); if (memContext == null) { throw new FileNotFoundException("SFS:xml/memory while converting a " + obj.getClass().getName()); } XXX clazz = obj.getClass(); Class c = clazz; while (c != null) { String className = c.getName(); String convertorPath = new StringBuffer(200).append(prefix).append(\'/\'). append(className.replace(\'.\', \'/\')).toString(); FileObject fo = FileUtil.getConfigFile(convertorPath); if (fo != null) { String providerPath = (String) fo.getAttribute(EA_PROVIDER_PATH); if (providerPath == null) { c = c.getSuperclass(); continue; } if (c.equals(clazz) || Object.class.equals(c)) { FileObject ret = FileUtil.getConfigFile(providerPath); if (ret == null) { throw new FileNotFoundException("Invalid " + EA_PROVIDER_PATH + "=" + providerPath + " under SFS/xml/memory/ for " + clazz); } else { return ret; } } else { Object inheritAttribute = fo.getAttribute(EA_SUBCLASSES); if (inheritAttribute instanceof Boolean) { boolean subclasses = ((Boolean)inheritAttribute).booleanValue(); if (subclasses) { FileObject ret = FileUtil.getConfigFile(providerPath); if (ret == null) { throw new FileNotFoundException("Invalid settings.providerPath under SFS/xml/memory/ for " + clazz); } else { return ret; } } } } } c = c.getSuperclass(); } throw new FileNotFoundException("None convertor was found under SFS/xml/memory/ for " + clazz); }', 'ground_truth': 'private static FileObject resolveConvertor(Object obj) throws IOException { String prefix = "xml/memory"; FileObject memContext = FileUtil.getConfigFile(prefix); if (memContext == null) { throw new FileNotFoundException("SFS:xml/memory while converting a " + obj.getClass().getName()); } Class<?> clazz = obj.getClass(); Class c = clazz; while (c != null) { String className = c.getName(); String convertorPath = new StringBuffer(200).append(prefix).append(\'/\'). append(className.replace(\'.\', \'/\')).toString(); FileObject fo = FileUtil.getConfigFile(convertorPath); if (fo != null) { String providerPath = (String) fo.getAttribute(EA_PROVIDER_PATH); if (providerPath == null) { c = c.getSuperclass(); continue; } if (c.equals(clazz) || Object.class.equals(c)) { FileObject ret = FileUtil.getConfigFile(providerPath); if (ret == null) { throw new FileNotFoundException("Invalid " + EA_PROVIDER_PATH + "=" + providerPath + " under SFS/xml/memory/ for " + clazz); } else { return ret; } } else { Object inheritAttribute = fo.getAttribute(EA_SUBCLASSES); if (inheritAttribute instanceof Boolean) { boolean subclasses = ((Boolean)inheritAttribute).booleanValue(); if (subclasses) { FileObject ret = FileUtil.getConfigFile(providerPath); if (ret == null) { throw new FileNotFoundException("Invalid settings.providerPath under SFS/xml/memory/ for " + clazz); } else { return ret; } } } } } c = c.getSuperclass(); } throw new FileNotFoundException("None convertor was found under SFS/xml/memory/ for " + clazz); }', 'output': 'private static FileObject resolveConvertor(Object obj) throws IOException { String prefix = "xml/memory"; FileObject memContext = FileUtil.getConfigFile(prefix); if (memContext == null) { throw new FileNotFoundException("SFS:xml/memory while converting a " + obj.getClass().getName()); } Class<?> clazz = obj.getClass(); Class<?> c = clazz; while (c != null) { String className = c.getName(); String convertorPath = new StringBuffer(200).append(prefix).append(\'/\'). append(className.replace(\'.\', \'/\')).toString(); FileObject fo = FileUtil.getConfigFile(convertorPath); if (fo != null) { String providerPath = (String) fo.getAttribute(EA_PROVIDER_PATH); if (providerPath == null) { c = c.getSuperclass(); continue; } if (c.equals(clazz) || Object.class.equals(c)) { FileObject ret = FileUtil.getConfigFile(providerPath); if (ret == null) { throw new FileNotFoundException("Invalid " + EA_PROVIDER_PATH + "=" + providerPath + " under SFS/xml/memory/ for " + clazz); } else { return ret; } } else { Object inheritAttribute = fo.getAttribute(EA_SUBCLASSES); if (inheritAttribute instanceof Boolean) { boolean subclasses = ((Boolean)inheritAttribute).booleanValue(); if (subclasses) { FileObject ret = FileUtil.getConfigFile(providerPath); if (ret == null) { throw new FileNotFoundException("Invalid settings.providerPath under SFS/xml/memory/ for " + clazz); } else { return ret; } } } } c = c.getSuperclass(); } throw new FileNotFoundException("None convertor was found under SFS/xml/memory/ for " + clazz); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void open() { state.ensureNotOpened(); if (log.isDebugEnabled()) { log.debug("opening {}", this); } persistenceManager = jdoPersistenceManagerFactory.getPersistenceManager(); final XXX psLifecycleMgmt = this; final IsisLifecycleListener isisLifecycleListener = new IsisLifecycleListener(psLifecycleMgmt); persistenceManager.addInstanceLifecycleListener(isisLifecycleListener, (Class[]) null); val loadLifecycleListener = new LoadLifecycleListenerForIsis(); val storeLifecycleListener = new JdoStoreLifecycleListenerForIsis(); getServiceInjector().injectServicesInto(loadLifecycleListener); getServiceInjector().injectServicesInto(storeLifecycleListener); persistenceManager.addInstanceLifecycleListener(loadLifecycleListener, (Class[]) null); persistenceManager.addInstanceLifecycleListener(storeLifecycleListener, (Class[]) null); this.unregisterLifecycleListeners = ()->{ persistenceManager.removeInstanceLifecycleListener(loadLifecycleListener); persistenceManager.removeInstanceLifecycleListener(storeLifecycleListener); }; this.state = State.OPEN; }', 'ground_truth': 'public void open() { state.ensureNotOpened(); if (log.isDebugEnabled()) { log.debug("opening {}", this); } persistenceManager = jdoPersistenceManagerFactory.getPersistenceManager(); final IsisLifecycleListener.EntityChangeEmitter psLifecycleMgmt = this; final IsisLifecycleListener isisLifecycleListener = new IsisLifecycleListener(psLifecycleMgmt); persistenceManager.addInstanceLifecycleListener(isisLifecycleListener, (Class[]) null); val loadLifecycleListener = new LoadLifecycleListenerForIsis(); val storeLifecycleListener = new JdoStoreLifecycleListenerForIsis(); getServiceInjector().injectServicesInto(loadLifecycleListener); getServiceInjector().injectServicesInto(storeLifecycleListener); persistenceManager.addInstanceLifecycleListener(loadLifecycleListener, (Class[]) null); persistenceManager.addInstanceLifecycleListener(storeLifecycleListener, (Class[]) null); this.unregisterLifecycleListeners = ()->{ persistenceManager.removeInstanceLifecycleListener(loadLifecycleListener); persistenceManager.removeInstanceLifecycleListener(storeLifecycleListener); }; this.state = State.OPEN; }', 'output': 'public void open() { state.ensureNotOpened(); if (log.isDebugEnabled()) { log.debug("opening {}", this); } persistenceManager = jdoPersistenceManagerFactory.getPersistenceManager(); final PersistenceState psLifecycleMgmt = this; final IsisLifecycleListener isisLifecycleListener = new IsisLifecycleListener(psLifecycleMgmt); persistenceManager.addInstanceLifecycleListener(isisLifecycleListener, (Class[]) null); val loadLifecycleListener = new LoadLifecycleListenerForIsis(); val storeLifecycleListener = new JdoStoreLifecycleListenerForIsis(); getServiceInjector().injectServicesInto(loadLifecycleListener); getServiceInjector().injectServicesInto(storeLifecycleListener); persistenceManager.addInstanceLifecycleListener(loadLifecycleListener, (Class[]) null); persistenceManager.addInstanceLifecycleListener(storeLifecycleListener, (Class[]) null); this.unregisterLifecycleListeners = ()->{ persistenceManager.removeInstanceLifecycleListener(loadLifecycleListener); persistenceManager.removeInstanceLifecycleListener(storeLifecycleListener); }; this.state = State.OPEN; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testBoundsAsClusteringWithMultiInRestrictions() { TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC); ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); Restriction in = newMultiIN(tableMetadata, 0, asList(value1, value2), asList(value2, value3)); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(in); XXX bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertStartBound(get(bounds, 0), true, value1, value2); assertStartBound(get(bounds, 1), true, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value2); assertEndBound(get(bounds, 1), true, value2, value3); }', 'ground_truth': 'public void testBoundsAsClusteringWithMultiInRestrictions() { TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC); ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); Restriction in = newMultiIN(tableMetadata, 0, asList(value1, value2), asList(value2, value3)); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(in); SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertStartBound(get(bounds, 0), true, value1, value2); assertStartBound(get(bounds, 1), true, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value2); assertEndBound(get(bounds, 1), true, value2, value3); }', 'output': 'public void testBoundsAsClusteringWithMultiInRestrictions() { TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC); ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); Restriction in = newMultiIN(tableMetadata, 0, asList(value1, value2), asList(value2, value3)); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(in); Bounds bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertStartBound(get(bounds, 0), true, value1, value2); assertStartBound(get(bounds, 1), true, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value2); assertEndBound(get(bounds, 1), true, value2, value3); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private List<Pair<CFDefinition.Name, ColumnIdentifier>> getExpandedSelection() { if (selectedNames.isEmpty()) { List<Pair<CFDefinition.Name, ColumnIdentifier>> selection = new ArrayList<Pair<CFDefinition.Name, ColumnIdentifier>>(); for (CFDefinition.Name name : cfDef) selection.add(Pair.create(name, name.name)); return selection; } else { return selectedNames; } }', 'ground_truth': 'private List<Pair<CFDefinition.Name, Selector>> getExpandedSelection() { if (selectedNames.isEmpty()) { List<Pair<CFDefinition.Name, Selector>> selection = new ArrayList<Pair<CFDefinition.Name, Selector>>(); for (CFDefinition.Name name : cfDef) selection.add(Pair.<CFDefinition.Name, Selector>create(name, name.name)); return selection; } else { return selectedNames; } }', 'output': 'private List<ColumnIdentifier> getExpandedSelection() { if (selectedNames.isEmpty()) { List<ColumnIdentifier> selection = new ArrayList<ColumnIdentifier>(); for (CFDefinition.Name name : cfDef) selection.add(name.name); return selection; } else { return selectedNames; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RecordReader<Map<String, ByteBuffer>, Map<String, ByteBuffer>> getRecordReader(InputSplit split, JobConf jobConf, final Reporter reporter) throws IOException { TaskAttemptContext tac = new TaskAttemptContext(jobConf, TaskAttemptID.forName(jobConf.get(MAPRED_TASK_ID))) { @Override public void progress() { reporter.progress(); } }; XXX recordReader = new ColumnFamilyRecordReader(); recordReader.initialize((org.apache.hadoop.mapreduce.InputSplit)split, tac); return recordReader; }', 'ground_truth': 'public RecordReader<Map<String, ByteBuffer>, Map<String, ByteBuffer>> getRecordReader(InputSplit split, JobConf jobConf, final Reporter reporter) throws IOException { TaskAttemptContext tac = new TaskAttemptContext(jobConf, TaskAttemptID.forName(jobConf.get(MAPRED_TASK_ID))) { @Override public void progress() { reporter.progress(); } }; CqlPagingRecordReader recordReader = new CqlPagingRecordReader(); recordReader.initialize((org.apache.hadoop.mapreduce.InputSplit)split, tac); return recordReader; }', 'output': 'public RecordReader<Map<String, ByteBuffer>, Map<String, ByteBuffer>> getRecordReader(InputSplit split, JobConf jobConf, final Reporter reporter) throws IOException { TaskAttemptContext tac = new TaskAttemptContext(jobConf, TaskAttemptID.forName(jobConf.get(MAPRED_TASK_ID))) { @Override public void progress() { reporter.progress(); } }; ColumnFamilyRecordReader recordReader = new ColumnFamilyRecordReader(); recordReader.initialize((org.apache.hadoop.mapreduce.InputSplit)split, tac); return recordReader; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testReadXMLNoRootElement() throws IOException { File file = tempFolder.newFile("trainXMLSmall"); Files.write(file.toPath(), trainXML.getBytes(StandardCharsets.UTF_8)); XXX source = XMLSource.<Train>from(file.toPath().toString()) .withRecordElement("train") .withRecordClass(Train.class); exception.expect(NullPointerException.class); exception.expectMessage( "rootElement is null. Use builder method withRootElement() to set this."); readEverythingFromReader(source.createReader(null, null)); }', 'ground_truth': 'public void testReadXMLNoRootElement() throws IOException { File file = tempFolder.newFile("trainXMLSmall"); Files.write(file.toPath(), trainXML.getBytes(StandardCharsets.UTF_8)); XmlSource<Train> source = XmlSource.<Train>from(file.toPath().toString()) .withRecordElement("train") .withRecordClass(Train.class); exception.expect(NullPointerException.class); exception.expectMessage( "rootElement is null. Use builder method withRootElement() to set this."); readEverythingFromReader(source.createReader(null, null)); }', 'output': 'public void testReadXMLNoRootElement() throws IOException { File file = tempFolder.newFile("trainXMLSmall"); Files.write(file.toPath(), trainXML.getBytes(StandardCharsets.UTF_8)); XMLSource<Train> source = XMLSource.from(file.toPath().toString()) .withRecordElement("train") .withRecordClass(Train.class); exception.expect(NullPointerException.class); exception.expectMessage( "rootElement is null. Use builder method withRootElement() to set this."); readEverythingFromReader(source.createReader(null, null)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void fullStreamPlan() throws Exception { XXX sessionID = registerSession(cfs, false, true); ActiveRepairService.ParentRepairSession prs = ActiveRepairService.instance.getParentRepairSession(sessionID); RepairJobDesc desc = new RepairJobDesc(sessionID, UUIDGen.getTimeUUID(), ks, tbl, prs.getRanges()); SyncRequest request = new SyncRequest(desc, PARTICIPANT1, PARTICIPANT2, PARTICIPANT3, prs.getRanges(), PreviewKind.NONE, false); StreamingRepairTask task = new StreamingRepairTask(desc, request.initiator, request.src, request.dst, request.ranges, null, PreviewKind.NONE, false); StreamPlan plan = task.createStreamPlan(request.dst); Assert.assertTrue(plan.getFlushBeforeTransfer()); }', 'ground_truth': 'public void fullStreamPlan() throws Exception { TimeUUID sessionID = registerSession(cfs, false, true); ActiveRepairService.ParentRepairSession prs = ActiveRepairService.instance.getParentRepairSession(sessionID); RepairJobDesc desc = new RepairJobDesc(sessionID, nextTimeUUID(), ks, tbl, prs.getRanges()); SyncRequest request = new SyncRequest(desc, PARTICIPANT1, PARTICIPANT2, PARTICIPANT3, prs.getRanges(), PreviewKind.NONE, false); StreamingRepairTask task = new StreamingRepairTask(desc, request.initiator, request.src, request.dst, request.ranges, null, PreviewKind.NONE, false); StreamPlan plan = task.createStreamPlan(request.dst); Assert.assertTrue(plan.getFlushBeforeTransfer()); }', 'output': 'public void fullStreamPlan() throws Exception { String sessionID = registerSession(cfs, false, true); ActiveRepairService.ParentRepairSession prs = ActiveRepairService.instance.getParentRepairSession(sessionID); RepairJobDesc desc = new RepairJobDesc(sessionID, UUIDGen.getTimeUUID(), ks, tbl, prs.getRanges()); SyncRequest request = new SyncRequest(desc, PARTICIPANT1, PARTICIPANT2, PARTICIPANT3, prs.getRanges(), PreviewKind.NONE, false); StreamingRepairTask task = new StreamingRepairTask(desc, request.initiator, request.src, request.dst, request.ranges, null, PreviewKind.NONE, false); StreamPlan plan = task.createStreamPlan(request.dst); Assert.assertTrue(plan.getFlushBeforeTransfer()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void main(String[] args) throws IgniteCheckedException { try (Ignite g = Ignition.start("examples/config/example-cache.xml")) { System.out.println(); System.out.println(">>> Cache atomic long example started."); String atomicName = UUID.randomUUID().toString(); final XXX atomicLong = g.cache(CACHE_NAME).dataStructures().atomicLong(atomicName, 0, true); System.out.println(); System.out.println("Atomic long initial value : " + atomicLong.get() + \'.\'); g.compute(g.cluster().forCache(CACHE_NAME)).call(new IgniteCallable<Object>() { @Override public Object call() throws  Exception { for (int i = 0; i < RETRIES; i++) System.out.println("AtomicLong value has been incremented: " + atomicLong.incrementAndGet()); return null; } }); System.out.println(); System.out.println("Atomic long value after successful CAS: " + atomicLong.get()); } }', 'ground_truth': 'public static void main(String[] args) throws IgniteCheckedException { try (Ignite g = Ignition.start("examples/config/example-cache.xml")) { System.out.println(); System.out.println(">>> Cache atomic long example started."); String atomicName = UUID.randomUUID().toString(); final CacheAtomicLong atomicLong = g.cache(CACHE_NAME).dataStructures().atomicLong(atomicName, 0, true); System.out.println(); System.out.println("Atomic long initial value : " + atomicLong.get() + \'.\'); g.compute(g.cluster().forCache(CACHE_NAME)).call(new IgniteCallable<Object>() { @Override public Object call() throws  Exception { for (int i = 0; i < RETRIES; i++) System.out.println("AtomicLong value has been incremented: " + atomicLong.incrementAndGet()); return null; } }); System.out.println(); System.out.println("Atomic long value after successful CAS: " + atomicLong.get()); } }', 'output': 'public static void main(String[] args) throws IgniteCheckedException { try (Ignite g = Ignition.start("examples/config/example-cache.xml")) { System.out.println(); System.out.println(">>> Cache atomic long example started."); String atomicName = UUID.randomUUID().toString(); final AtomicLong atomicLong = g.cache(CACHE_NAME).dataStructures().atomicLong(atomicName, 0, true); System.out.println(); System.out.println("Atomic long initial value : " + atomicLong.get() + \'.\'); g.compute(g.cluster().forCache(CACHE_NAME)).call(new IgniteCallable<Object>() { @Override public Object call() throws  Exception { for (int i = 0; i < RETRIES; i++) System.out.println("AtomicLong value has been incremented: " + atomicLong.incrementAndGet()); return null; } }); System.out.println(); System.out.println("Atomic long value after successful CAS: " + atomicLong.get()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void revokeAll(String droppedUser) { try { process(String.format("DELETE FROM %s.%s WHERE username = \'%s\'", Auth.AUTH_KS, PERMISSIONS_CF, escape(droppedUser))); } catch (XXX e) { logger.warn("CassandraAuthorizer failed to revoke all permissions of {}: {}", droppedUser, e); } }', 'ground_truth': 'public void revokeAll(String droppedUser) { try { process(String.format("DELETE FROM %s.%s WHERE username = \'%s\'", Auth.AUTH_KS, PERMISSIONS_CF, escape(droppedUser))); } catch (RequestExecutionException e) { logger.warn("CassandraAuthorizer failed to revoke all permissions of {}: {}", droppedUser, e); } }', 'output': 'public void revokeAll(String droppedUser) { try { process(String.format("DELETE FROM %s.%s WHERE username = \'%s\'", Auth.AUTH_KS, PERMISSIONS_CF, escape(droppedUser))); } catch (Exception e) { logger.warn("CassandraAuthorizer failed to revoke all permissions of {}: {}", droppedUser, e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void processTest(boolean isExpectUndeploy) throws Exception { try { Ignite ignite1 = startGrid(1); Ignite ignite2 = startGrid(2); Ignite ignite3 = startGrid(3); Class task1 = urlClsLdr1.loadClass("org.gridgain.grid.tests.p2p.GridP2PTestTaskExternalPath1"); int[] res1 = (int[]) ignite1.compute().execute(task1, ignite2.cluster().localNode().id()); stopGrid(1); Thread.sleep(1000); XXX res2 = (int[]) ignite3.compute().execute(task1, ignite2.cluster().localNode().id()); if (isExpectUndeploy) assert isNotSame(res1, res2); else assert Arrays.equals(res1, res2); } finally { stopGrid(1); stopGrid(2); stopGrid(3); } }', 'ground_truth': 'private void processTest(boolean isExpectUndeploy) throws Exception { try { Ignite ignite1 = startGrid(1); Ignite ignite2 = startGrid(2); Ignite ignite3 = startGrid(3); Class task1 = urlClsLdr1.loadClass("org.gridgain.grid.tests.p2p.GridP2PTestTaskExternalPath1"); Integer res1 = ignite1.compute().execute(task1, ignite2.cluster().localNode().id()); stopGrid(1); Thread.sleep(1000); Integer res2 = ignite3.compute().execute(task1, ignite2.cluster().localNode().id()); if (isExpectUndeploy) assert !res1.equals(res2); else assert res1.equals(res2); } finally { stopGrid(1); stopGrid(2); stopGrid(3); } }', 'output': 'private void processTest(boolean isExpectUndeploy) throws Exception { try { Ignite ignite1 = startGrid(1); Ignite ignite2 = startGrid(2); Ignite ignite3 = startGrid(3); Class task1 = urlClsLdr1.loadClass("org.gridgain.grid.tests.p2p.GridP2PTestTaskExternalPath1"); int[] res1 = (int[]) ignite1.compute().execute(task1, ignite2.cluster().localNode().id()); stopGrid(1); Thread.sleep(1000); int[] res2 = (int[]) ignite3.compute().execute(task1, ignite2.cluster().localNode().id()); if (isExpectUndeploy) assert isNotSame(res1, res2); else assert Arrays.equals(res1, res2); } finally { stopGrid(1); stopGrid(2); stopGrid(3); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public synchronized void displaySuiteRunning(final TestSession session, final TestSuite suite) { final XXX displayHandler = getDisplayHandler(session); displayHandler.displaySuiteRunning(suite); displayInWindow(session, displayHandler); }', 'ground_truth': 'public synchronized void displaySuiteRunning(final TestSession session, final TestSuite suite) { final TestResultDisplayHandler displayHandler = getDisplayHandler(session); displayHandler.displaySuiteRunning(suite); displayInWindow(session, displayHandler); }', 'output': 'public synchronized void displaySuiteRunning(final TestSession session, final TestSuite suite) { final DisplayHandler displayHandler = getDisplayHandler(session); displayHandler.displaySuiteRunning(suite); displayInWindow(session, displayHandler); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static JLabel findLabel(JComponent comp, String labelText) { Vector allComponents = new Vector(); getAllComponents(comp.getComponents(), allComponents); XXX iterator = allComponents.iterator(); while (iterator.hasNext()) { Component c = (Component) iterator.next(); if (c instanceof JLabel) { JLabel label = (JLabel) c; if (label.getText().equals(labelText)) { return label; } } } return null; }', 'ground_truth': 'public static JLabel findLabel(JComponent comp, String labelText) { Vector allComponents = new Vector(); getAllComponents(comp.getComponents(), allComponents); Iterator<Component> iterator = allComponents.iterator(); while (iterator.hasNext()) { Component c = iterator.next(); if (c instanceof JLabel) { JLabel label = (JLabel) c; if (label.getText().equals(labelText)) { return label; } } } return null; }', 'output': 'public static JLabel findLabel(JComponent comp, String labelText) { Vector allComponents = new Vector(); getAllComponents(comp.getComponents(), allComponents); Iterator<Component> iterator = allComponents.iterator(); while (iterator.hasNext()) { Component c = iterator.next(); if (c instanceof JLabel) { JLabel label = (JLabel) c; if (label.getText().equals(labelText)) { return label; } } } return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testBigIntegerEndpointsA() throws UnknownHostException { XXX endpointSnitch = new EndpointSnitch(); AbstractReplicationStrategy strategy = new RackAwareStrategy(tmd, endpointSnitch); addEndpoint("0", "5", "254.0.0.1"); addEndpoint("10", "15", "254.0.0.2"); addEndpoint("20", "25", "254.0.0.3"); addEndpoint("30", "35", "254.0.0.4"); expectedResults.put("5", buildResult("254.0.0.2", "254.0.0.3", "254.0.0.4")); expectedResults.put("15", buildResult("254.0.0.3", "254.0.0.4", "254.0.0.1")); expectedResults.put("25", buildResult("254.0.0.4", "254.0.0.1", "254.0.0.2")); expectedResults.put("35", buildResult("254.0.0.1", "254.0.0.2", "254.0.0.3")); runTestForReplicatedTables(strategy); }', 'ground_truth': 'public void testBigIntegerEndpointsA() throws UnknownHostException { RackInferringSnitch endpointSnitch = new RackInferringSnitch(); AbstractReplicationStrategy strategy = new RackAwareStrategy(tmd, endpointSnitch); addEndpoint("0", "5", "254.0.0.1"); addEndpoint("10", "15", "254.0.0.2"); addEndpoint("20", "25", "254.0.0.3"); addEndpoint("30", "35", "254.0.0.4"); expectedResults.put("5", buildResult("254.0.0.2", "254.0.0.3", "254.0.0.4")); expectedResults.put("15", buildResult("254.0.0.3", "254.0.0.4", "254.0.0.1")); expectedResults.put("25", buildResult("254.0.0.4", "254.0.0.1", "254.0.0.2")); expectedResults.put("35", buildResult("254.0.0.1", "254.0.0.2", "254.0.0.3")); runTestForReplicatedTables(strategy); }', 'output': 'public void testBigIntegerEndpointsA() throws UnknownHostException { EndpointSnitch endpointSnitch = new EndpointSnitch(); AbstractReplicationStrategy strategy = new RackAwareStrategy(tmd, endpointSnitch); addEndpoint("0", "5", "254.0.0.1"); addEndpoint("10", "15", "254.0.0.2"); addEndpoint("20", "25", "254.0.0.3"); addEndpoint("30", "35", "254.0.0.4"); expectedResults.put("5", buildResult("254.0.0.2", "254.0.0.3", "254.0.0.4")); expectedResults.put("15", buildResult("254.0.0.3", "254.0.0.4", "254.0.0.1")); expectedResults.put("25", buildResult("254.0.0.4", "254.0.0.1", "254.0.0.2")); expectedResults.put("35", buildResult("254.0.0.1", "254.0.0.2", "254.0.0.3")); runTestForReplicatedTables(strategy); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testInstallViewInstance_invalid() throws Exception { ViewRegistry registry = ViewRegistry.getInstance(); Properties properties = new Properties(); properties.put("p1", "v1"); Configuration ambariConfig = new Configuration(properties); ViewConfig config = ViewConfigTest.getConfig(xml_invalid_instance); ViewEntity viewEntity = getViewEntity(config, ambariConfig, getClass().getClassLoader(), ""); ViewInstanceEntity viewInstanceEntity = getViewInstanceEntity(viewEntity, config.getInstances().get(0)); replay(viewDAO, viewInstanceDAO, securityHelper, resourceTypeDAO); registry.addDefinition(viewEntity); try { registry.installViewInstance(viewInstanceEntity); Assert.fail("expected an IllegalStateException"); } catch (XXX e) { } verify(viewDAO, viewInstanceDAO, securityHelper, resourceTypeDAO); }', 'ground_truth': 'public void testInstallViewInstance_invalid() throws Exception { ViewRegistry registry = ViewRegistry.getInstance(); Properties properties = new Properties(); properties.put("p1", "v1"); Configuration ambariConfig = new Configuration(properties); ViewConfig config = ViewConfigTest.getConfig(xml_invalid_instance); ViewEntity viewEntity = getViewEntity(config, ambariConfig, getClass().getClassLoader(), ""); ViewInstanceEntity viewInstanceEntity = getViewInstanceEntity(viewEntity, config.getInstances().get(0)); replay(viewDAO, viewInstanceDAO, securityHelper, resourceTypeDAO); registry.addDefinition(viewEntity); try { registry.installViewInstance(viewInstanceEntity); Assert.fail("expected an IllegalStateException"); } catch (ValidationException e) { } verify(viewDAO, viewInstanceDAO, securityHelper, resourceTypeDAO); }', 'output': 'public void testInstallViewInstance_invalid() throws Exception { ViewRegistry registry = ViewRegistry.getInstance(); Properties properties = new Properties(); properties.put("p1", "v1"); Configuration ambariConfig = new Configuration(properties); ViewConfig config = ViewConfigTest.getConfig(xml_invalid_instance); ViewEntity viewEntity = getViewEntity(config, ambariConfig, getClass().getClassLoader(), ""); ViewInstanceEntity viewInstanceEntity = getViewInstanceEntity(viewEntity, config.getInstances().get(0)); replay(viewDAO, viewInstanceDAO, securityHelper, resourceTypeDAO); registry.addDefinition(viewEntity); try { registry.installViewInstance(viewInstanceEntity); Assert.fail("expected an IllegalStateException"); } catch (IllegalStateException e) { } verify(viewDAO, viewInstanceDAO, securityHelper, resourceTypeDAO); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void fullRepairStreamPlan() throws Exception { UUID sessionID = registerSession(cfs, true, true); ActiveRepairService.ParentRepairSession prs = ActiveRepairService.instance.getParentRepairSession(sessionID); RepairJobDesc desc = new RepairJobDesc(sessionID, UUIDGen.getTimeUUID(), KEYSPACE1, CF_STANDARD, prs.getRanges()); TreeResponse r1 = new TreeResponse(PARTICIPANT1, createInitialTree(desc, DatabaseDescriptor.getPartitioner())); TreeResponse r2 = new TreeResponse(PARTICIPANT2, createInitialTree(desc, DatabaseDescriptor.getPartitioner())); XXX task = new LocalSyncTask(desc, r1, r2, NO_PENDING_REPAIR, false, PreviewKind.NONE); StreamPlan plan = task.createStreamPlan(PARTICIPANT1, Lists.newArrayList(RANGE1)); assertEquals(NO_PENDING_REPAIR, plan.getPendingRepair()); assertTrue(plan.getFlushBeforeTransfer()); }', 'ground_truth': 'public void fullRepairStreamPlan() throws Exception { UUID sessionID = registerSession(cfs, true, true); ActiveRepairService.ParentRepairSession prs = ActiveRepairService.instance.getParentRepairSession(sessionID); RepairJobDesc desc = new RepairJobDesc(sessionID, UUIDGen.getTimeUUID(), KEYSPACE1, CF_STANDARD, prs.getRanges()); TreeResponse r1 = new TreeResponse(PARTICIPANT1, createInitialTree(desc, DatabaseDescriptor.getPartitioner())); TreeResponse r2 = new TreeResponse(PARTICIPANT2, createInitialTree(desc, DatabaseDescriptor.getPartitioner())); SymmetricLocalSyncTask task = new SymmetricLocalSyncTask(desc, r1, r2, false, NO_PENDING_REPAIR, false, PreviewKind.NONE); StreamPlan plan = task.createStreamPlan(PARTICIPANT1, Lists.newArrayList(RANGE1)); assertEquals(NO_PENDING_REPAIR, plan.getPendingRepair()); assertTrue(plan.getFlushBeforeTransfer()); }', 'output': 'public void fullRepairStreamPlan() throws Exception { UUID sessionID = registerSession(cfs, true, true); ActiveRepairService.ParentRepairSession prs = ActiveRepairService.instance.getParentRepairSession(sessionID); RepairJobDesc desc = new RepairJobDesc(sessionID, UUIDGen.getTimeUUID(), KEYSPACE1, CF_STANDARD, prs.getRanges()); TreeResponse r1 = new TreeResponse(PARTICIPANT1, createInitialTree(desc, DatabaseDescriptor.getPartitioner())); TreeResponse r2 = new TreeResponse(PARTICIPANT2, createInitialTree(desc, DatabaseDescriptor.getPartitioner())); LocalSyncTask task = new LocalSyncTask(desc, r1, r2, NO_PENDING_REPAIR, false, PreviewKind.NONE); StreamPlan plan = task.createStreamPlan(PARTICIPANT1, Lists.newArrayList(RANGE1)); assertEquals(NO_PENDING_REPAIR, plan.getPendingRepair()); assertTrue(plan.getFlushBeforeTransfer()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void main(String[] args) throws IgniteCheckedException, InterruptedException { try (Ignite g = Ignition.start("examples/config/example-cache.xml")) { System.out.println(); System.out.println(">>> Cache events example started."); final GridCache<Integer, String> cache = g.cache(CACHE_NAME); cache.globalClearAll(0); IgniteBiPredicate<UUID, IgniteCacheEvent> locLsnr = new IgniteBiPredicate<UUID, IgniteCacheEvent>() { @Override public boolean apply(UUID uuid, IgniteCacheEvent evt) { System.out.println("Received event [evt=" + evt.name() + ", key=" + evt.key() + ", oldVal=" + evt.oldValue() + ", newVal=" + evt.newValue()); return true; } }; IgnitePredicate<IgniteCacheEvent> rmtLsnr = new IgnitePredicate<IgniteCacheEvent>() { @Override public boolean apply(IgniteCacheEvent evt) { System.out.println("Cache event [name=" + evt.name() + ", key=" + evt.key() + \']\'); int key = evt.key(); return key >= 10 && cache.affinity().isPrimary(g.cluster().localNode(), key); } }; g.events(g.cluster().forCache(CACHE_NAME)).remoteListen(locLsnr, rmtLsnr, EVT_CACHE_OBJECT_PUT, EVT_CACHE_OBJECT_READ, EVT_CACHE_OBJECT_REMOVED); for (int i = 0; i < 20; i++) cache.putx(i, Integer.toString(i)); Thread.sleep(2000); } }', 'ground_truth': 'public static void main(String[] args) throws IgniteCheckedException, InterruptedException { try (Ignite g = Ignition.start("examples/config/example-cache.xml")) { System.out.println(); System.out.println(">>> Cache events example started."); final IgniteCache<Integer, String> cache = g.jcache(CACHE_NAME); cache.clear(); IgniteBiPredicate<UUID, IgniteCacheEvent> locLsnr = new IgniteBiPredicate<UUID, IgniteCacheEvent>() { @Override public boolean apply(UUID uuid, IgniteCacheEvent evt) { System.out.println("Received event [evt=" + evt.name() + ", key=" + evt.key() + ", oldVal=" + evt.oldValue() + ", newVal=" + evt.newValue()); return true; } }; IgnitePredicate<IgniteCacheEvent> rmtLsnr = new IgnitePredicate<IgniteCacheEvent>() { @Override public boolean apply(IgniteCacheEvent evt) { System.out.println("Cache event [name=" + evt.name() + ", key=" + evt.key() + \']\'); int key = evt.key(); return key >= 10 && g.affinity(CACHE_NAME).isPrimary(g.cluster().localNode(), key); } }; g.events(g.cluster().forCache(CACHE_NAME)).remoteListen(locLsnr, rmtLsnr, EVT_CACHE_OBJECT_PUT, EVT_CACHE_OBJECT_READ, EVT_CACHE_OBJECT_REMOVED); for (int i = 0; i < 20; i++) cache.put(i, Integer.toString(i)); Thread.sleep(2000); } }', 'output': 'public static void main(String[] args) throws IgniteCheckedException, InterruptedException { try (Ignite g = Ignition.start("examples/config/example-cache.xml")) { System.out.println(); System.out.println(">>> Cache events example started."); final GridCache<Integer, String> cache = g.cache(CACHE_NAME); cache.globalClearAll(0); IgniteBiPredicate<UUID, IgniteCacheEvent> locLsnr = new IgniteBiPredicate<UUID, IgniteCacheEvent>() { @Override public boolean apply(UUID uuid, IgniteCacheEvent evt) { System.out.println("Received event [evt=" + evt.name() + ", key=" + evt.key() + ", oldVal=" + evt.oldValue() + ", newVal=" + evt.newValue()); return true; } }; IgnitePredicate<IgniteCacheEvent> rmtLsnr = new IgnitePredicate<IgniteCacheEvent>() { @Override public boolean apply(IgniteCacheEvent evt) { System.out.println("Cache event [name=" + evt.name() + ", key=" + evt.key() + \']\'); int key = evt.key(); return key >= 10 && cache.affinity().isPrimary(g.cluster().localNode(), key); } }; g.events(g.cluster().forCache(CACHE_NAME)).remoteListen(locLsnr, rmtLsnr, EVT_CACHE_OBJECT_PUT, EVT_CACHE_OBJECT_READ, EVT_CACHE_OBJECT_REMOVED); for (int i = 0; i < 20; i++) cache.putx(i, Integer.toString(i)); Thread.sleep(2000); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Collection<ApplicationPermission> findOrphaned() { final Collection<String> packageNames = applicationFeatureRepository.packageNames(); final Set<String> availableClasses = _Sets.newTreeSet(); for (String packageName : packageNames) { appendClasses(packageName, ApplicationMemberType.PROPERTY, availableClasses); appendClasses(packageName, ApplicationMemberType.COLLECTION, availableClasses); appendClasses(packageName, ApplicationMemberType.ACTION, availableClasses); } val orphaned = _Lists.<ApplicationPermission>newArrayList(); val permissions = allPermissions(); for (val permission : permissions) { final XXX featureType = permission.getFeatureType(); final String featureFqn = permission.getFeatureFqn(); switch (featureType) { case NAMESPACE: if(!packageNames.contains(featureFqn)) { orphaned.add(permission); } break; case TYPE: if(!availableClasses.contains(featureFqn)) { orphaned.add(permission); } break; case MEMBER: final List<String> split = _Strings.splitThenStream(featureFqn, "#") .collect(Collectors.toList()); final String fqClassName = split.get(0); final String memberName = split.get(1); final int lastDot = fqClassName.lastIndexOf(\'.\'); final String packageName = fqClassName.substring(0, lastDot); final String className = fqClassName.substring(lastDot + 1); final List<String> memberNames = memberNamesOf(packageName, className); if(!memberNames.contains(memberName)) { orphaned.add(permission); } break; } } return orphaned; }', 'ground_truth': 'public Collection<ApplicationPermission> findOrphaned() { final Collection<String> packageNames = applicationFeatureRepository.packageNames(); final Set<String> availableClasses = _Sets.newTreeSet(); for (String packageName : packageNames) { appendClasses(packageName, ApplicationMemberType.PROPERTY, availableClasses); appendClasses(packageName, ApplicationMemberType.COLLECTION, availableClasses); appendClasses(packageName, ApplicationMemberType.ACTION, availableClasses); } val orphaned = _Lists.<ApplicationPermission>newArrayList(); val permissions = allPermissions(); for (val permission : permissions) { final ApplicationFeatureSort featureType = permission.getFeatureType(); final String featureFqn = permission.getFeatureFqn(); switch (featureType) { case NAMESPACE: if(!packageNames.contains(featureFqn)) { orphaned.add(permission); } break; case TYPE: if(!availableClasses.contains(featureFqn)) { orphaned.add(permission); } break; case MEMBER: final List<String> split = _Strings.splitThenStream(featureFqn, "#") .collect(Collectors.toList()); final String fqClassName = split.get(0); final String memberName = split.get(1); final int lastDot = fqClassName.lastIndexOf(\'.\'); final String packageName = fqClassName.substring(0, lastDot); final String className = fqClassName.substring(lastDot + 1); final List<String> memberNames = memberNamesOf(packageName, className); if(!memberNames.contains(memberName)) { orphaned.add(permission); } break; } } return orphaned; }', 'output': 'public Collection<ApplicationPermission> findOrphaned() { final Collection<String> packageNames = applicationFeatureRepository.packageNames(); final Set<String> availableClasses = _Sets.newTreeSet(); for (String packageName : packageNames) { appendClasses(packageName, ApplicationMemberType.PROPERTY, availableClasses); appendClasses(packageName, ApplicationMemberType.COLLECTION, availableClasses); appendClasses(packageName, ApplicationMemberType.ACTION, availableClasses); } val orphaned = _Lists.<ApplicationPermission>newArrayList(); val permissions = allPermissions(); for (val permission : permissions) { final FeatureType featureType = permission.getFeatureType(); final String featureFqn = permission.getFeatureFqn(); switch (featureType) { case NAMESPACE: if (!packageNames.contains(featureFqn)) { orphaned.add(permission); } break; case TYPE: if (!availableClasses.contains(featureFqn)) { orphaned.add(permission); } break; case MEMBER: final List<String> split = _Strings.splitThenStream(featureFqn, "#") .collect(Collectors.toList()); final String fqClassName = split.get(0); final String memberName = split.get(1); final int lastDot = fqClassName.lastIndexOf(\'.\'); final String packageName = fqClassName.substring(0, lastDot); final String className = fqClassName.substring(lastDot + 1); final List<String> memberNames = memberNamesOf(packageName, className); if (!memberNames.contains(memberName)) { orphaned.add(permission); } break; } } return orphaned; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public T copy(T t) { XXX bao = new ByteArrayOutputStream(); try { coder.encode(t, bao, Coder.Context.OUTER); } catch (IOException e) { throw new RuntimeException("Could not copy.", e); } try { return coder.decode(new ByteArrayInputStream(bao.toByteArray()), Coder.Context.OUTER); } catch (IOException e) { throw new RuntimeException("Could not copy.", e); } } public T copy(T t, T reuse) { return copy(t); }', 'ground_truth': 'public T copy(T t) { byteBuffer.reset(); try { coder.encode(t, byteBuffer, Coder.Context.OUTER); } catch (IOException e) { throw new RuntimeException("Could not copy.", e); } try { return coder.decode( new ByteArrayInputStream(byteBuffer.getBuffer(), 0, byteBuffer.size()), Coder.Context.OUTER); } catch (IOException e) { throw new RuntimeException("Could not copy.", e); } }', 'output': 'public T copy(T t) { ByteArrayOutputStream bao = new ByteArrayOutputStream(); try { coder.encode(t, bao, Coder.Context.OUTER); } catch (IOException e) { throw new RuntimeException("Could not copy.", e); } try { return coder.decode(new ByteArrayInputStream(bao.toByteArray()), Coder.Context.OUTER); } catch (IOException e) { throw new RuntimeException("Could not copy.", e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public EndPoint getPrimary(String key) { EndPoint endpoint = StorageService.tcpAddr_; XXX token = hash(key); Map<BigInteger, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap(); List<BigInteger> tokens = new ArrayList<BigInteger>(tokenToEndPointMap.keySet()); if (tokens.size() > 0) { Collections.sort(tokens); int index = Collections.binarySearch(tokens, token); if (index >= 0) { /* * retrieve the endpoint based on the token at this index in the * tokens list */ endpoint = tokenToEndPointMap.get(tokens.get(index)); } else { index = (index + 1) * (-1); if (index < tokens.size()) endpoint = tokenToEndPointMap.get(tokens.get(index)); else endpoint = tokenToEndPointMap.get(tokens.get(0)); } } return endpoint; }', 'ground_truth': 'public EndPoint getPrimary(String key) { EndPoint endpoint = StorageService.tcpAddr_; Token token = token(key); Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap(); List tokens = new ArrayList<Token>(tokenToEndPointMap.keySet()); if (tokens.size() > 0) { Collections.sort(tokens); int index = Collections.binarySearch(tokens, token); if (index >= 0) { /* * retrieve the endpoint based on the token at this index in the * tokens list */ endpoint = tokenToEndPointMap.get(tokens.get(index)); } else { index = (index + 1) * (-1); if (index < tokens.size()) endpoint = tokenToEndPointMap.get(tokens.get(index)); else endpoint = tokenToEndPointMap.get(tokens.get(0)); } } return endpoint; }', 'output': 'public EndPoint getPrimary(String key) { EndPoint endpoint = StorageService.tcpAddr_; BigInteger token = hash(key); Map<BigInteger, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap(); List<BigInteger> tokens = new ArrayList<BigInteger>(tokenToEndPointMap.keySet()); if (tokens.size() > 0) { Collections.sort(tokens); int index = Collections.binarySearch(tokens, token); if (index >= 0) { /* * retrieve the endpoint based on the token at this index in the * tokens list */ endpoint = tokenToEndPointMap.get(tokens.get(index)); } else { index = (index + 1) * (-1); if (index < tokens.size()) endpoint = tokenToEndPointMap.get(tokens.get(index)); else endpoint = tokenToEndPointMap.get(tokens.get(0)); } } return endpoint; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void readSimpleColumn(ColumnMetadata column, DataInputPlus in, SerializationHeader header, DeserializationHelper helper, Row.Builder builder, LivenessInfo rowLiveness) throws IOException { if (helper.includes(column)) { XXX cell = Cell.serializer.deserialize(in, rowLiveness, column, header, helper); if (helper.includes(cell, rowLiveness) && !helper.isDropped(cell, false)) builder.addCell(cell); } else { Cell.serializer.skip(in, column, header); } }', 'ground_truth': 'private void readSimpleColumn(ColumnMetadata column, DataInputPlus in, SerializationHeader header, DeserializationHelper helper, Row.Builder builder, LivenessInfo rowLiveness) throws IOException { if (helper.includes(column)) { Cell<byte[]> cell = Cell.serializer.deserialize(in, rowLiveness, column, header, helper, ByteArrayAccessor.instance); if (helper.includes(cell, rowLiveness) && !helper.isDropped(cell, false)) builder.addCell(cell); } else { Cell.serializer.skip(in, column, header); } }', 'output': 'private void readSimpleColumn(ColumnMetadata column, DataInputPlus in, SerializationHeader header, DeserializationHelper helper, Row.Builder builder, LivenessInfo rowLiveness) throws IOException { if (helper.includes(column)) { Cell cell = Cell.serializer.deserialize(in, rowLiveness, column, header, helper); if (helper.includes(cell, rowLiveness) && !helper.isDropped(cell, false)) builder.addCell(cell); } else { Cell.serializer.skip(in, column, header); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public <K> boolean remove(String spi, String space, K key, @Nullable byte[] keyBytes) throws GridException { assert key != null; if (!busyLock.enterBusy()) throw new IllegalStateException("Failed to remove from index (grid is stopping)."); try { XXX k = entry(key, keyBytes); return getSpi(spi).remove(space, k); } finally { busyLock.leaveBusy(); } }', 'ground_truth': 'public <K> boolean remove(String spi, String space, K key, @Nullable byte[] keyBytes) throws GridException { assert key != null; if (!busyLock.enterBusy()) throw new IllegalStateException("Failed to remove from index (grid is stopping)."); try { IndexingEntity<K> k = entry(key, keyBytes); return getSpi(spi).remove(space, k); } finally { busyLock.leaveBusy(); } }', 'output': 'public <K> boolean remove(String spi, String space, K key, @Nullable byte[] keyBytes) throws GridException { assert key != null; if (!busyLock.enterBusy()) throw new IllegalStateException("Failed to remove from index (grid is stopping)."); try { Entry<K, byte[]> k = entry(key, keyBytes); return getSpi(spi).remove(space, k); } finally { busyLock.leaveBusy(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCreatingAndProcessingDoFlatten() throws Exception { String pTransformId = "pTransformId"; String mainOutputId = "101"; RunnerApi.FunctionSpec functionSpec = RunnerApi.FunctionSpec.newBuilder() .setUrn(PTransformTranslation.FLATTEN_TRANSFORM_URN) .build(); RunnerApi.PTransform pTransform = RunnerApi.PTransform.newBuilder() .setSpec(functionSpec) .putInputs("inputA", "inputATarget") .putInputs("inputB", "inputBTarget") .putInputs("inputC", "inputCTarget") .putOutputs(mainOutputId, "mainOutputTarget") .build(); List<WindowedValue<String>> mainOutputValues = new ArrayList<>(); ListMultimap<String, FnDataReceiver<WindowedValue<?>>> consumers = ArrayListMultimap.create(); consumers.put( "mainOutputTarget", (FnDataReceiver) (FnDataReceiver<WindowedValue<String>>) mainOutputValues::add); new FlattenRunner.Factory<>() .createRunnerForPTransform( PipelineOptionsFactory.create(), null /* beamFnDataClient */, null /* beamFnStateClient */, pTransformId, pTransform, Suppliers.ofInstance("57L")::get, Collections.emptyMap(), Collections.emptyMap(), Collections.emptyMap(), consumers, null /* startFunctionRegistry */, null, /* finishFunctionRegistry */ null /* splitListener */); mainOutputValues.clear(); assertThat( consumers.keySet(), containsInAnyOrder("inputATarget", "inputBTarget", "inputCTarget", "mainOutputTarget")); Iterables.getOnlyElement(consumers.get("inputATarget")).accept(valueInGlobalWindow("A1")); Iterables.getOnlyElement(consumers.get("inputATarget")).accept(valueInGlobalWindow("A2")); Iterables.getOnlyElement(consumers.get("inputBTarget")).accept(valueInGlobalWindow("B")); Iterables.getOnlyElement(consumers.get("inputCTarget")).accept(valueInGlobalWindow("C")); assertThat( mainOutputValues, contains( valueInGlobalWindow("A1"), valueInGlobalWindow("A2"), valueInGlobalWindow("B"), valueInGlobalWindow("C"))); mainOutputValues.clear(); }', 'ground_truth': 'public void testCreatingAndProcessingDoFlatten() throws Exception { String pTransformId = "pTransformId"; String mainOutputId = "101"; RunnerApi.FunctionSpec functionSpec = RunnerApi.FunctionSpec.newBuilder() .setUrn(PTransformTranslation.FLATTEN_TRANSFORM_URN) .build(); RunnerApi.PTransform pTransform = RunnerApi.PTransform.newBuilder() .setSpec(functionSpec) .putInputs("inputA", "inputATarget") .putInputs("inputB", "inputBTarget") .putInputs("inputC", "inputCTarget") .putOutputs(mainOutputId, "mainOutputTarget") .build(); List<WindowedValue<String>> mainOutputValues = new ArrayList<>(); PCollectionConsumerRegistry consumers = new PCollectionConsumerRegistry(); consumers.register( "mainOutputTarget", (FnDataReceiver) (FnDataReceiver<WindowedValue<String>>) mainOutputValues::add); new FlattenRunner.Factory<>() .createRunnerForPTransform( PipelineOptionsFactory.create(), null /* beamFnDataClient */, null /* beamFnStateClient */, pTransformId, pTransform, Suppliers.ofInstance("57L")::get, Collections.emptyMap(), Collections.emptyMap(), Collections.emptyMap(), consumers, null /* startFunctionRegistry */, null, /* finishFunctionRegistry */ null /* splitListener */); mainOutputValues.clear(); assertThat( consumers.keySet(), containsInAnyOrder("inputATarget", "inputBTarget", "inputCTarget", "mainOutputTarget")); consumers.getMultiplexingConsumer("inputATarget").accept(valueInGlobalWindow("A1")); consumers.getMultiplexingConsumer("inputATarget").accept(valueInGlobalWindow("A2")); consumers.getMultiplexingConsumer("inputBTarget").accept(valueInGlobalWindow("B")); consumers.getMultiplexingConsumer("inputCTarget").accept(valueInGlobalWindow("C")); assertThat( mainOutputValues, contains( valueInGlobalWindow("A1"), valueInGlobalWindow("A2"), valueInGlobalWindow("B"), valueInGlobalWindow("C"))); mainOutputValues.clear(); }', 'output': 'public void testCreatingAndProcessingDoFlatten() throws Exception { String pTransformId = "pTransformId"; String mainOutputId = "101"; RunnerApi.FunctionSpec functionSpec = RunnerApi.FunctionSpec.newBuilder() .setUrn(PTransformTranslation.FLATTEN_TRANSFORM_URN) .build(); RunnerApi.PTransform pTransform = RunnerApi.PTransform.newBuilder() .setSpec(functionSpec) .putInputs("inputA", "inputATarget") .putInputs("inputB", "inputBTarget") .putInputs("inputC", "inputCTarget") .putOutputs(mainOutputId, "mainOutputTarget") .build(); List<WindowedValue<String>> mainOutputValues = new ArrayList<>(); ListMultimap<String, FnDataReceiver<WindowedValue<?>>> consumers = ArrayListMultimap.create(); consumers.put( "mainOutputTarget", (FnDataReceiver) (FnDataReceiver<WindowedValue<String>>) mainOutputValues::add); new FlattenRunner.Factory<>() .createRunnerForPTransform( PipelineOptionsFactory.create(), null /* beamFnDataClient */, null /* beamFnStateClient */, pTransformId, pTransform, Suppliers.ofInstance("57L")::get, Collections.emptyMap(), Collections.emptyMap(), Collections.emptyMap(), consumers, null /* startFunctionRegistry */, null, /* finishFunctionRegistry */ null /* splitListener */); mainOutputValues.clear(); assertThat( consumers.keySet(), containsInAnyOrder("inputATarget", "inputBTarget", "inputCTarget", "mainOutputTarget")); Iterables.getOnlyElement(consumers.get("inputATarget")).accept(valueInGlobalWindow("A1")); Iterables.getOnlyElement(consumers.get("inputATarget")).accept(valueInGlobalWindow("A2")); Iterables.getOnlyElement(consumers.get("inputBTarget")).accept(valueInGlobalWindow("B")); Iterables.getOnlyElement(consumers.get("inputCTarget")).accept(valueInGlobalWindow("C")); assertThat( mainOutputValues, contains( valueInGlobalWindow("A1"), valueInGlobalWindow("A2"), valueInGlobalWindow("B'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testExplicitLocks() throws Exception { try { startGridsMultiThreaded(2); GridKernal[] nodes = new GridKernal[] {(GridKernal)grid(0), (GridKernal)grid(1)}; Collection<IgniteFuture> futs = new ArrayList<>(); final CountDownLatch startLatch = new CountDownLatch(1); for (final GridKernal node : nodes) { List<Integer> parts = partitions(node, PARTITION_PRIMARY); Map<Integer, Integer> keyMap = keysFor(node, parts); for (final Integer key : keyMap.values()) { futs.add(multithreadedAsync(new Runnable() { @Override public void run() { try { try { node.jcache(null).lock(key).lock(); info(">>> Acquired explicit lock for key: " + key); startLatch.await(); info(">>> Acquiring explicit lock for key: " + key * 10); node.jcache(null).lock(key * 10).lock(); info(">>> Releasing locks [key1=" + key + ", key2=" + key * 10 + \']\'); } finally { node.cache(null).unlock(key * 10); node.cache(null).unlock(key); } } catch (IgniteCheckedException e) { info(">>> Failed to perform lock [key=" + key + ", e=" + e + \']\'); } catch (InterruptedException ignored) { info(">>> Interrupted while waiting for start latch."); Thread.currentThread().interrupt(); } } }, 1)); } } IgniteFuture<?> startFut = multithreadedAsync(new Runnable() { @Override public void run() { try { startGrid(2); info(">>> Started grid2."); } catch (Exception e) { info(">>> Failed to start grid: " + e); } } }, 1); U.sleep(5000); assertFalse(startFut.isDone()); info(">>> Waiting for all locks to be released."); startLatch.countDown(); for (XXX fut : futs) fut.get(1000); startFut.get(); } finally { stopAllGrids(); } }', 'ground_truth': 'public void testExplicitLocks() throws Exception { try { startGridsMultiThreaded(2); GridKernal[] nodes = new GridKernal[] {(GridKernal)grid(0), (GridKernal)grid(1)}; Collection<IgniteInternalFuture> futs = new ArrayList<>(); final CountDownLatch startLatch = new CountDownLatch(1); for (final GridKernal node : nodes) { List<Integer> parts = partitions(node, PARTITION_PRIMARY); Map<Integer, Integer> keyMap = keysFor(node, parts); for (final Integer key : keyMap.values()) { futs.add(multithreadedAsync(new Runnable() { @Override public void run() { try { try { node.jcache(null).lock(key).lock(); info(">>> Acquired explicit lock for key: " + key); startLatch.await(); info(">>> Acquiring explicit lock for key: " + key * 10); node.jcache(null).lock(key * 10).lock(); info(">>> Releasing locks [key1=" + key + ", key2=" + key * 10 + \']\'); } finally { node.cache(null).unlock(key * 10); node.cache(null).unlock(key); } } catch (IgniteCheckedException e) { info(">>> Failed to perform lock [key=" + key + ", e=" + e + \']\'); } catch (InterruptedException ignored) { info(">>> Interrupted while waiting for start latch."); Thread.currentThread().interrupt(); } } }, 1)); } } IgniteInternalFuture<?> startFut = multithreadedAsync(new Runnable() { @Override public void run() { try { startGrid(2); info(">>> Started grid2."); } catch (Exception e) { info(">>> Failed to start grid: " + e); } } }, 1); U.sleep(5000); assertFalse(startFut.isDone()); info(">>> Waiting for all locks to be released."); startLatch.countDown(); for (IgniteInternalFuture fut : futs) fut.get(1000); startFut.get(); } finally { stopAllGrids(); } }', 'output': 'public void testExplicitLocks() throws Exception { try { startGridsMultiThreaded(2); GridKernal[] nodes = new GridKernal[] {(GridKernal)grid(0), (GridKernal)grid(1)}; Collection<IgniteFuture> futs = new ArrayList<>(); final CountDownLatch startLatch = new CountDownLatch(1); for (final GridKernal node : nodes) { List<Integer> parts = partitions(node, PARTITION_PRIMARY); Map<Integer, Integer> keyMap = keysFor(node, parts); for (final Integer key : keyMap.values()) { futs.add(multithreadedAsync(new Runnable() { @Override public void run() { try { try { node.jcache(null).lock(key).lock(); info(">>> Acquired explicit lock for key: " + key); startLatch.await(); info(">>> Acquiring explicit lock for key: " + key * 10); node.jcache(null).lock(key * 10).lock(); info(">>> Releasing locks [key1=" + key + ", key2=" + key * 10 + \']\'); } finally { node.cache(null).unlock(key * 10); node.cache(null).unlock(key); } } catch (IgniteCheckedException e) { info(">>> Failed to perform lock [key=" + key + ", e=" + e + \']\'); } catch (InterruptedException ignored) { info(">>> Interrupted while waiting for start latch."); Thread.currentThread().interrupt(); } } }, 1)); } } IgniteFuture<?> startFut = multithreadedAsync(new Runnable() { @Override public void run() { try { startGrid(2); info(">>> Started grid2."); } catch (Exception e) { info(">>> Failed to start grid: " + e); } } }, 1); U.sleep(5000); assertFalse(startFut.isDone()); info(">>> Waiting for all locks to be released."); startLatch.countDown(); for (XXX fut : futs) fut.get(1000); startFut.get(); } finally { stopAllGrids(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testTypeConfigurationsWithGlobalMapperJar() throws Exception { BinaryMarshaller marsh = binaryMarshaller(new BinaryIdMapper() { @SuppressWarnings("IfMayBeConditional") @Override public int typeId(String clsName) { if (clsName.endsWith("1")) return 300; else if (clsName.endsWith("2")) return 400; else return -500; } @Override public int fieldId(int typeId, String fieldName) { return 0; } }, Arrays.asList( new BinaryTypeConfiguration("org.apache.ignite.internal.binary.test.*"), new BinaryTypeConfiguration("unknown.*") )); BinaryContext ctx = binaryContext(marsh); Map<String, BinaryIdMapper> typeMappers = U.field(ctx, "typeMappers"); assertEquals(3, typeMappers.size()); assertEquals(300, typeMappers.get("GridBinaryTestClass1").typeId("GridBinaryTestClass1")); assertEquals(400, typeMappers.get("GridBinaryTestClass2").typeId("GridBinaryTestClass2")); }', 'ground_truth': 'public void testTypeConfigurationsWithGlobalMapperJar() throws Exception { BinaryMarshaller marsh = binaryMarshaller(new BinaryBasicNameMapper(false), new BinaryIdMapper() { @SuppressWarnings("IfMayBeConditional") @Override public int typeId(String clsName) { if (clsName.endsWith("1")) return 300; else if (clsName.endsWith("2")) return 400; else return -500; } @Override public int fieldId(int typeId, String fieldName) { return 0; } }, Arrays.asList( new BinaryTypeConfiguration("org.apache.ignite.internal.binary.test.*"), new BinaryTypeConfiguration("unknown.*") )); BinaryContext ctx = binaryContext(marsh); Map<String, org.apache.ignite.internal.binary.BinaryInternalMapper> typeMappers = U.field(ctx, "cls2Mappers"); assertEquals(3, typeMappers.size()); assertFalse(((BinaryBasicNameMapper)typeMappers.get(CLASS1_FULL_NAME).nameMapper()).isSimpleName()); assertEquals(300, typeMappers.get(CLASS1_FULL_NAME).idMapper().typeId(CLASS1_FULL_NAME)); assertFalse(((BinaryBasicNameMapper)typeMappers.get(CLASS2_FULL_NAME).nameMapper()).isSimpleName()); assertEquals(400, typeMappers.get(CLASS2_FULL_NAME).idMapper().typeId(CLASS2_FULL_NAME)); }', 'output': 'public void testTypeConfigurationsWithGlobalMapperJar() throws Exception { BinaryMarshaller marsh = binaryMarshaller(new BinaryIdMapper() { @SuppressWarnings("IfMayBeConditional") @Override public int typeId(String clsName) { if (clsName.endsWith("1")) return 300; else if (clsName.endsWith("2")) return 400; else return -500; } @Override public int fieldId(int typeId, String fieldName) { return 0; } }, Arrays.asList( new BinaryTypeConfiguration("org.apache.ignite.internal.binary.test.*"), new BinaryTypeConfiguration("unknown.*") )); BinaryContext ctx = binaryContext(marsh); Map<String, BinaryIdMapper> typeMappers = U.field(ctx, "typeMappers"); assertEquals(3, typeMappers.size()); assertEquals(300, typeMappers.get("GridBinaryTestClass1").typeId("GridBinaryTestClass1")); assertEquals(400, typeMappers.get("GridBinaryTestClass2").typeId("GridBinaryTestClass2")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testOpenPrefetch() throws Exception { create(ggfsSecondary, paths(DIR, SUBDIR), paths(FILE)); final int blockSize = ggfs.info(FILE).blockSize(); XXX out = ggfsSecondary.append(FILE, false); int totalWritten = 0; while (totalWritten < blockSize * 2 + chunk.length) { out.write(chunk); totalWritten += chunk.length; } out.close(); awaitFileClose(ggfsSecondary, FILE); int totalRead = 0; IgfsInputStream in = ggfs.open(FILE, blockSize); final byte[] readBuf = new byte[1024]; while (totalRead + readBuf.length <= blockSize * 2) { in.read(readBuf); totalRead += readBuf.length; } IgfsMetaManager meta = ggfs.context().meta(); IgfsFileInfo info = meta.info(meta.fileId(FILE)); IgfsBlockKey key = new IgfsBlockKey(info.id(), info.affinityKey(), info.evictExclude(), 2); GridCache<IgfsBlockKey, byte[]> dataCache = ggfs.context().kernalContext().cache().cache( ggfs.configuration().getDataCacheName()); for (int i = 0; i < 10; i++) { if (dataCache.containsKey(key)) break; else U.sleep(100); } ggfsSecondary.delete(FILE, false); U.sleep(300); totalRead = 0; in.seek(blockSize * 2); while (totalRead + readBuf.length <= blockSize) { in.read(readBuf); totalRead += readBuf.length; } in.close(); }', 'ground_truth': 'public void testOpenPrefetch() throws Exception { create(ggfsSecondary, paths(DIR, SUBDIR), paths(FILE)); final int blockSize = ggfs.info(FILE).blockSize(); IgfsOutputStream out = ggfsSecondary.append(FILE, false); int totalWritten = 0; while (totalWritten < blockSize * 2 + chunk.length) { out.write(chunk); totalWritten += chunk.length; } out.close(); awaitFileClose(ggfsSecondary, FILE); int totalRead = 0; IgfsInputStream in = ggfs.open(FILE, blockSize); final byte[] readBuf = new byte[1024]; while (totalRead + readBuf.length <= blockSize * 2) { in.read(readBuf); totalRead += readBuf.length; } IgfsMetaManager meta = ggfs.context().meta(); IgfsFileInfo info = meta.info(meta.fileId(FILE)); IgfsBlockKey key = new IgfsBlockKey(info.id(), info.affinityKey(), info.evictExclude(), 2); GridCache<IgfsBlockKey, byte[]> dataCache = ggfs.context().kernalContext().cache().cache( ggfs.configuration().getDataCacheName()); for (int i = 0; i < 10; i++) { if (dataCache.containsKey(key)) break; else U.sleep(100); } ggfsSecondary.delete(FILE, false); U.sleep(300); totalRead = 0; in.seek(blockSize * 2); while (totalRead + readBuf.length <= blockSize) { in.read(readBuf); totalRead += readBuf.length; } in.close(); }', 'output': 'public void testOpenPrefetch() throws Exception { create(ggfsSecondary, paths(DIR, SUBDIR), paths(FILE)); final int blockSize = ggfs.info(FILE).blockSize(); IgfsOutputStream out = ggfsSecondary.append(FILE, false); int totalWritten = 0; while (totalWritten < blockSize * 2 + chunk.length) { out.write(chunk); totalWritten += chunk.length; } out.close(); awaitFileClose(ggfsSecondary, FILE); int totalRead = 0; IgfsInputStream in = ggfs.open(FILE, blockSize); final byte[] readBuf = new byte[1024]; while (totalRead + readBuf.length <= blockSize * 2) { in.read(readBuf); totalRead += readBuf.length; } IgfsMetaManager meta = ggfs.context().meta(); IgfsFileInfo info = meta.info(meta.fileId(FILE)); IgfsBlockKey key = new IgfsBlockKey(info.id(), info.affinityKey(), info.evictExclude(), 2); GridCache<IgfsBlockKey, byte[]> dataCache = ggfs.context().kernalContext().cache().cache( ggfs.configuration().getDataCacheName()); for (int i = 0; i < 10; i++) { if (dataCache.containsKey(key)) break; else U.sleep(100); } ggfsSecondary.delete(FILE, false); U.sleep(300); totalRead = 0; in.seek(blockSize * 2); while (totalRead + readBuf.length <= blockSize) { in.read(readBuf); totalRead += readBuf.length; } in.close(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Collection<ClusterNode> keyNodes(Object key) { XXX aff = affinity(0); return aff.nodes(aff.partition(key), grid(0).nodes(), 1); }', 'ground_truth': 'private Collection<ClusterNode> keyNodes(Object key) { CacheConsistentHashAffinityFunction aff = affinity(0); return aff.nodes(aff.partition(key), grid(0).nodes(), 1); }', 'output': 'private Collection<ClusterNode> keyNodes(Object key) { IgniteAffinity aff = affinity(0); return aff.nodes(aff.partition(key), grid(0).nodes(), 1); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPutAllManyKeysOneReader() throws Exception { aff.backups(1); grids = 4; aff.partitions(grids); startGrids(); try { GridCache<Object, Object> prj0 = grid(0).cache(null); GridCache<Object, Object> prj1 = grid(1).cache(null); Map<Integer, Integer> putMap = new HashMap<>(); int size = 100; for (int i = 0; i < size; i++) putMap.put(i, i); prj0.putAll(putMap); for (int i = 0; i < size; i++) putMap.put(i, i * i); prj1.putAll(putMap); for (int i = 0; i < size; i++) { assertEquals(i * i, prj0.get(i)); assertEquals(i * i, prj1.get(i)); } } finally { stopAllGrids(); } }', 'ground_truth': 'public void testPutAllManyKeysOneReader() throws Exception { aff.backups(1); grids = 4; aff.partitions(grids); startGrids(); try { IgniteCache<Object, Object> prj0 = grid(0).jcache(null); IgniteCache<Object, Object> prj1 = grid(1).jcache(null); Map<Integer, Integer> putMap = new HashMap<>(); int size = 100; for (int i = 0; i < size; i++) putMap.put(i, i); prj0.putAll(putMap); for (int i = 0; i < size; i++) putMap.put(i, i * i); prj1.putAll(putMap); for (int i = 0; i < size; i++) { assertEquals(i * i, prj0.get(i)); assertEquals(i * i, prj1.get(i)); } } finally { stopAllGrids(); } }', 'output': 'public void testPutAllManyKeysOneReader() throws Exception { aff.backups(1); grids = 4; aff.partitions(grids); startGrids(); try { Cache<Object, Object> prj0 = grid(0).cache(null); Cache<Object, Object> prj1 = grid(1).cache(null); Map<Integer, Integer> putMap = new HashMap<>(); int size = 100; for (int i = 0; i < size; i++) putMap.put(i, i); prj0.putAll(putMap); for (int i = 0; i < size; i++) putMap.put(i, i * i); prj1.putAll(putMap); for (int i = 0; i < size; i++) { assertEquals(i * i, prj0.get(i)); assertEquals(i * i, prj1.get(i)); } } finally { stopAllGrids(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Dataset<Pair<KEY, Long>> output() { return new OutputBuilder<>(name, input, keyExtractor, valueExtractor, Batch.get(), this) .output(); } public Dataset<Pair<KEY, Long>> output() { return (Dataset) outputWindowed(); } public Dataset<WindowedPair<WLABEL, KEY, Long>> outputWindowed() { Flow flow = input.getFlow(); SumByKey<IN, KEY, WLABEL, W, WindowedPair<WLABEL, KEY, Long>> sumByKey = new SumByKey<>(name, flow, input, keyExtractor, valueExtractor, windowing, getPartitioning()); flow.add(sumByKey); return sumByKey.output(); }', 'ground_truth': 'public Dataset<Pair<KEY, Long>> output() { Flow flow = input.getFlow(); SumByKey<IN, KEY, WLABEL, W> sumByKey = new SumByKey<>(name, flow, input, keyExtractor, valueExtractor, windowing, getPartitioning()); flow.add(sumByKey); return sumByKey.output(); }', 'output': 'public Dataset<WindowedPair<WLABEL, KEY, Long>> outputWindowed() { Flow flow = input.getFlow(); SumByKey<IN, KEY, WLABEL, W, WindowedPair<WLABEL, KEY, Long>> sumByKey = new SumByKey<>(name, flow, input, keyExtractor, valueExtractor, windowing, getPartitioning()); flow.add(sumByKey); return sumByKey.output(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public PartitionUpdate.SimpleBuilder update(String tableName) { XXX metadata = Schema.instance.getCFMetaData(keyspaceName, tableName); assert metadata != null : "Unknown table " + tableName + " in keyspace " + keyspaceName; return update(metadata); }', 'ground_truth': 'public PartitionUpdate.SimpleBuilder update(String tableName) { TableMetadata metadata = Schema.instance.getTableMetadata(keyspaceName, tableName); assert metadata != null : "Unknown table " + tableName + " in keyspace " + keyspaceName; return update(metadata); }', 'output': 'public PartitionUpdate.SimpleBuilder update(String keyspaceName, String tableName) { TableMetadata metadata = Schema.instance.getTableMetadata(keyspaceName, tableName); assert metadata != null : "Unknown table " + tableName + " in keyspace " + keyspaceName; return update(metadata); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void rebuildSecondaryIndex(String ksName, String cfName, String... idxNames) { ColumnFamilyStore cfs = Keyspace.open(ksName).getColumnFamilyStore(cfName); Set<String> indexes = new HashSet<String>(Arrays.asList(idxNames)); XXX sstables = cfs.getSSTables(); try (Refs<SSTableReader> refs = Refs.ref(sstables)) { cfs.indexManager.setIndexRemoved(indexes); logger.info(String.format("User Requested secondary index re-build for %s/%s indexes", ksName, cfName)); cfs.indexManager.maybeBuildSecondaryIndexes(sstables, indexes); cfs.indexManager.setIndexBuilt(indexes); } }', 'ground_truth': 'public static void rebuildSecondaryIndex(String ksName, String cfName, String... idxNames) { ColumnFamilyStore cfs = Keyspace.open(ksName).getColumnFamilyStore(cfName); Set<String> indexes = new HashSet<String>(Arrays.asList(idxNames)); Iterable<SSTableReader> sstables = cfs.getSSTables(SSTableSet.CANONICAL); try (Refs<SSTableReader> refs = Refs.ref(sstables)) { cfs.indexManager.setIndexRemoved(indexes); logger.info(String.format("User Requested secondary index re-build for %s/%s indexes", ksName, cfName)); cfs.indexManager.maybeBuildSecondaryIndexes(refs, indexes); cfs.indexManager.setIndexBuilt(indexes); } }', 'output': 'public static void rebuildSecondaryIndex(String ksName, String cfName, String... idxNames) { ColumnFamilyStore cfs = Keyspace.open(ksName).getColumnFamilyStore(cfName); Set<String> indexes = new HashSet<String>(Arrays.asList(idxNames)); Collection<SSTableReader> sstables = cfs.getSSTables(); try (Refs<SSTableReader> refs = Refs.ref(sstables)) { cfs.indexManager.setIndexRemoved(indexes); logger.info(String.format("User Requested secondary index re-build for %s/%s indexes", ksName, cfName)); cfs.indexManager.maybeBuildSecondaryIndexes(sstables, indexes); cfs.indexManager.setIndexBuilt(indexes); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testClearSnapshot_RemoveByName() throws IOException { XXX tool = runner.invokeNodetool("snapshot","-t","some-name").waitAndAssertOnCleanExit(); assertTrue(!tool.getStdout().isEmpty()); Map<String, TabularData> snapshots_before = probe.getSnapshotDetails(); Assert.assertTrue(snapshots_before.containsKey("some-name")); tool = runner.invokeNodetool("clearsnapshot","-t","some-name").waitAndAssertOnCleanExit(); assertTrue(!tool.getStdout().isEmpty()); Map<String, TabularData> snapshots_after = probe.getSnapshotDetails(); Assert.assertFalse(snapshots_after.containsKey("some-name")); }', 'ground_truth': 'public void testClearSnapshot_RemoveByName() { ToolResult tool = ToolRunner.invokeNodetool("snapshot","-t","some-name"); tool.assertOnCleanExit(); assertTrue(!tool.getStdout().isEmpty()); Map<String, TabularData> snapshots_before = probe.getSnapshotDetails(); Assert.assertTrue(snapshots_before.containsKey("some-name")); tool = ToolRunner.invokeNodetool("clearsnapshot","-t","some-name"); tool.assertOnCleanExit(); assertTrue(!tool.getStdout().isEmpty()); Map<String, TabularData> snapshots_after = probe.getSnapshotDetails(); Assert.assertFalse(snapshots_after.containsKey("some-name")); }', 'output': 'public void testClearSnapshot_RemoveByName() throws IOException { NodeTool tool = runner.invokeNodetool("snapshot","-t","some-name").waitAndAssertOnCleanExit(); assertTrue(!tool.getStdout().isEmpty()); Map<String, TabularData> snapshots_before = probe.getSnapshotDetails(); Assert.assertTrue(snapshots_before.containsKey("some-name")); tool = runner.invokeNodetool("clearsnapshot","-t","some-name").waitAndAssertOnCleanExit(); assertTrue(!tool.getStdout().isEmpty()); Map<String, TabularData> snapshots_after = probe.getSnapshotDetails(); Assert.assertFalse(snapshots_after.containsKey("some-name")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testBoundsAsClusteringWithOneInRestrictionsAndOneClusteringColumn() { ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC); Restriction in = newSingleIN(tableMetadata, 0, value1, value2, value3); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(in); XXX bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(3, bounds.size()); assertStartBound(get(bounds, 0), true, value1); assertStartBound(get(bounds, 1), true, value2); assertStartBound(get(bounds, 2), true, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(3, bounds.size()); assertEndBound(get(bounds, 0), true, value1); assertEndBound(get(bounds, 1), true, value2); assertEndBound(get(bounds, 2), true, value3); }', 'ground_truth': 'public void testBoundsAsClusteringWithOneInRestrictionsAndOneClusteringColumn() { ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC); Restriction in = newSingleIN(tableMetadata, 0, value1, value2, value3); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(in); SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(3, bounds.size()); assertStartBound(get(bounds, 0), true, value1); assertStartBound(get(bounds, 1), true, value2); assertStartBound(get(bounds, 2), true, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(3, bounds.size()); assertEndBound(get(bounds, 0), true, value1); assertEndBound(get(bounds, 1), true, value2); assertEndBound(get(bounds, 2), true, value3); }', 'output': 'public void testBoundsAsClusteringWithOneInRestrictionsAndOneClusteringColumn() { ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC); Restriction in = newSingleIN(tableMetadata, 0, value1, value2, value3); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(in); Bounds bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(3, bounds.size()); assertStartBound(get(bounds, 0), true, value1); assertStartBound(get(bounds, 1), true, value2); assertStartBound(get(bounds, 2), true, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(3, bounds.size()); assertEndBound(get(bounds, 0), true, value1); assertEndBound(get(bounds, 1), true, value2); assertEndBound(get(bounds, 2), true, value3); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void whenEmptyString() { class Customer { @Property( regexPattern = "" ) public String getName() { return null; } public void setName(final String name) { } } final Class<?> cls = Customer.class; propertyMethod = findMethod(Customer.class, "getName"); final XXX processMethodContext = new FacetFactory.ProcessMethodContext(cls, null, propertyMethod, mockMethodRemover, facetedMethod); processRegEx(facetFactory, processMethodContext); final RegExFacet regExFacet = facetedMethod.getFacet(RegExFacet.class); Assert.assertNull(regExFacet); }', 'ground_truth': 'public void whenEmptyString() { class Customer { @Property( regexPattern = "" ) @Getter @Setter private String name; } final Class<?> cls = Customer.class; propertyMethod = findMethod(Customer.class, "getName"); val processMethodContext = new FacetFactory.ProcessMethodContext(cls, null, propertyMethod, mockMethodRemover, facetedMethod); processRegEx(facetFactory, processMethodContext); final RegExFacet regExFacet = facetedMethod.getFacet(RegExFacet.class); Assert.assertNull(regExFacet); }', 'output': 'public void whenEmptyString() { class Customer { @Property( regexPattern = "" ) public String getName() { return null; } public void setName(final String name) { } } final Class<?> cls = Customer.class; propertyMethod = findMethod(Customer.class, "getName"); final FacetFactory.ProcessMethodContext processMethodContext = new FacetFactory.ProcessMethodContext(cls, null, propertyMethod, mockMethodRemover, facetedMethod); processRegEx(facetFactory, processMethodContext); final RegExFacet regExFacet = facetedMethod.getFacet(RegExFacet.class); Assert.assertNull(regExFacet); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public SSTableWriter read(ReadableByteChannel channel) throws IOException { logger.debug("reading file from {}, repairedAt = {}", session.peer, repairedAt); long totalSize = totalSize(); Pair<String, String> kscf = Schema.instance.getCF(cfId); if (kscf == null) { throw new IOException("CF " + cfId + " was dropped during streaming"); } ColumnFamilyStore cfs = Keyspace.open(kscf.left).getColumnFamilyStore(kscf.right); CompressedInputStream cis = new CompressedInputStream(Channels.newInputStream(channel), compressionInfo); BytesReadTracker in = new BytesReadTracker(new DataInputStream(cis)); SSTableWriter writer = null; try { writer = createWriter(cfs, totalSize, repairedAt, format); for (Pair<Long, Long> section : sections) { assert cis.getTotalCompressedBytesRead() <= totalSize; XXX sectionLength = (int) (section.right - section.left); cis.position(section.left); in.reset(0); while (in.getBytesRead() < sectionLength) { writeRow(writer, in, cfs); session.progress(desc, ProgressInfo.Direction.IN, cis.getTotalCompressedBytesRead(), totalSize); } } return writer; } catch (Throwable e) { if (writer != null) { try { writer.abort(); } catch (Throwable e2) { e.addSuppressed(e2); } } drain(cis, in.getBytesRead()); if (e instanceof IOException) throw (IOException) e; else throw Throwables.propagate(e); } finally { cis.close(); } }', 'ground_truth': 'public SSTableWriter read(ReadableByteChannel channel) throws IOException { long totalSize = totalSize(); Pair<String, String> kscf = Schema.instance.getCF(cfId); ColumnFamilyStore cfs = null; if (kscf != null) cfs = Keyspace.open(kscf.left).getColumnFamilyStore(kscf.right); if (kscf == null || cfs == null) { throw new IOException("CF " + cfId + " was dropped during streaming"); } logger.debug("[Stream #{}] Start receiving file #{} from {}, repairedAt = {}, size = {}, ks = \'{}\', table = \'{}\'.", session.planId(), fileSeqNum, session.peer, repairedAt, totalSize, cfs.keyspace.getName(), cfs.getColumnFamilyName()); CompressedInputStream cis = new CompressedInputStream(Channels.newInputStream(channel), compressionInfo); BytesReadTracker in = new BytesReadTracker(new DataInputStream(cis)); SSTableWriter writer = null; DecoratedKey key = null; try { writer = createWriter(cfs, totalSize, repairedAt, format); int sectionIdx = 0; for (Pair<Long, Long> section : sections) { assert cis.getTotalCompressedBytesRead() <= totalSize; long sectionLength = section.right - section.left; logger.trace("[Stream #{}] Reading section {} with length {} from stream.", session.planId(), sectionIdx++, sectionLength); cis.position(section.left); in.reset(0); while (in.getBytesRead() < sectionLength) { key = StorageService.getPartitioner().decorateKey(ByteBufferUtil.readWithShortLength(in)); writeRow(key, writer, in, cfs); session.progress(desc, ProgressInfo.Direction.IN, cis.getTotalCompressedBytesRead(), totalSize); } } logger.debug("[Stream #{}] Finished receiving file #{} from {} readBytes = {}, totalSize = {}", session.planId(), fileSeqNum, session.peer, cis.getTotalCompressedBytesRead(), totalSize); return writer; } catch (Throwable e) { if (key != null) logger.warn("[Stream {}] Error while reading partition {} from stream on ks=\'{}\' and table=\'{}\'.", session.planId(), key, cfs.keyspace.getName(), cfs.getColumnFamilyName()); if (writer != null) { try { writer.abort(); } catch (Throwable e2) { e.addSuppressed(e2); } } drain(cis, in.getBytesRead()); if (e instanceof IOException) throw (IOException) e; else throw Throwables.propagate(e); } }', 'output': 'public SSTableWriter read(ReadableByteChannel channel) throws IOException { logger.debug("reading file from {}, repairedAt = {}", session.peer, repairedAt); long totalSize = totalSize(); Pair<String, String> kscf = Schema.instance.getCF(cfId); if (kscf == null) { throw new IOException("CF " + cfId + " was dropped during streaming"); } ColumnFamilyStore cfs = Keyspace.open(kscf.left).getColumnFamilyStore(kscf.right); CompressedInputStream cis = new CompressedInputStream(Channels.newInputStream(channel), compressionInfo); BytesReadTracker in = new BytesReadTracker(new DataInputStream(cis)); SSTableWriter writer = null; try { writer = createWriter(cfs, totalSize, repairedAt, format); for (Pair<Long, Long> section : sections) { assert cis.getTotalCompressedBytesRead() <= totalSize; int sectionLength = (int) (section.right - section.left); cis.position(section.left); in.reset(0); while (in.getBytesRead() < sectionLength) { writeRow(writer, in, cfs); session.progress(desc, ProgressInfo.Direction.IN, cis.getTotalCompressedBytesRead(), totalSize); } } return writer; } catch (Throwable e) { if (writer != null) { try { writer.abort(); } catch (Throwable e2) { e.addSuppressed(e2); } } drain(cis, in.getBytesRead()); if (e instanceof IOException) throw (IOException) e; else throw Throwables.propagate(e); } finally { cis.close(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ColumnFilter build() { boolean isFetchAll = metadata != null; XXX queried = queriedBuilder == null ? null : queriedBuilder.build(); if (!isFetchAll && queried == null) queried = PartitionColumns.NONE; SortedSetMultimap<ColumnIdentifier, ColumnSubselection> s = null; if (subSelections != null) { s = TreeMultimap.create(Comparator.<ColumnIdentifier>naturalOrder(), Comparator.<ColumnSubselection>naturalOrder()); for (ColumnSubselection subSelection : subSelections) s.put(subSelection.column().name, subSelection); } return new ColumnFilter(isFetchAll, metadata, queried, s); }', 'ground_truth': 'public ColumnFilter build() { boolean isFetchAll = metadata != null; RegularAndStaticColumns queried = queriedBuilder == null ? null : queriedBuilder.build(); if (!isFetchAll && queried == null) queried = RegularAndStaticColumns.NONE; SortedSetMultimap<ColumnIdentifier, ColumnSubselection> s = null; if (subSelections != null) { s = TreeMultimap.create(Comparator.<ColumnIdentifier>naturalOrder(), Comparator.<ColumnSubselection>naturalOrder()); for (ColumnSubselection subSelection : subSelections) s.put(subSelection.column().name, subSelection); } return new ColumnFilter(isFetchAll, metadata, queried, s); }', 'output': 'public ColumnFilter build() { boolean isFetchAll = metadata != null; List<ColumnSubselection> queried = queriedBuilder == null ? null : queriedBuilder.build(); if (!isFetchAll && queried == null) queried = Collections.emptyList(); SortedSetMultimap<ColumnIdentifier, ColumnSubselection> s = null; if (subSelections != null) { s = TreeMultimap.create(Comparator.<ColumnIdentifier>naturalOrder(), Comparator.<ColumnSubselection>naturalOrder()); for (ColumnSubselection subSelection : subSelections) s.put(subSelection.column().name, subSelection); } return new ColumnFilter(isFetchAll, metadata, queried, s); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void onlyBlockOnQuorum() { Map<InetAddressAndPort, Mutation> repairs = new HashMap<>(); repairs.put(target1, mutation(cell1)); repairs.put(target2, mutation(cell2)); repairs.put(target3, mutation(cell3)); Assert.assertEquals(3, repairs.size()); InstrumentedReadRepairHandler handler = createRepairHandler(repairs, 2); handler.sendInitialRepairs(); Assert.assertFalse(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); handler.ack(target1); Assert.assertFalse(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); handler.ack(target2); Assert.assertTrue(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); }', 'ground_truth': 'public void onlyBlockOnQuorum() { Map<Replica, Mutation> repairs = new HashMap<>(); repairs.put(replica1, mutation(cell1)); repairs.put(replica2, mutation(cell2)); repairs.put(replica3, mutation(cell3)); Assert.assertEquals(3, repairs.size()); InstrumentedReadRepairHandler handler = createRepairHandler(repairs, 2); handler.sendInitialRepairs(); Assert.assertFalse(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); handler.ack(target1); Assert.assertFalse(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); handler.ack(target2); Assert.assertTrue(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); }', 'output': 'public void onlyBlockOnQuorum() { List<Mutation> repairs = new ArrayList<>(); repairs.add(mutation(cell1)); repairs.add(mutation(cell2)); repairs.add(mutation(cell3)); Assert.assertEquals(3, repairs.size()); InstrumentedReadRepairHandler handler = createRepairHandler(repairs, 2); handler.sendInitialRepairs(); Assert.assertFalse(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); handler.ack(target1); Assert.assertFalse(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); handler.ack(target2); Assert.assertTrue(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void announceTypeDrop(UserType droppedType, boolean announceLocally) { XXX ksm = Schema.instance.getKSMetaData(droppedType.keyspace); announce(LegacySchemaTables.dropTypeFromSchemaMutation(ksm, droppedType, FBUtilities.timestampMicros()), announceLocally); }', 'ground_truth': 'public static void announceTypeDrop(UserType droppedType, boolean announceLocally) { KeyspaceMetadata ksm = Schema.instance.getKSMetaData(droppedType.keyspace); announce(LegacySchemaTables.dropTypeFromSchemaMutation(ksm, droppedType, FBUtilities.timestampMicros()), announceLocally); }', 'output': 'public static void announceTypeDrop(UserType droppedType, boolean announceLocally) { KeyspaceMetadata ksm = Schema.instance.getKSMetaData(droppedType.keyspace); announce(LegacySchemaTables.dropTypeFromSchemaMutation(ksm, droppedType, FBUtilities.timestampMicros()), announceLocally); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testRemoteIfDataCacheNameEquals() throws Exception { XXX g2Cfg = getConfiguration("g2"); GridGgfsConfiguration g2GgfsCfg1 = new GridGgfsConfiguration(g1GgfsCfg1); GridGgfsConfiguration g2GgfsCfg2 = new GridGgfsConfiguration(g1GgfsCfg2); g2GgfsCfg1.setName("g2GgfsCfg1"); g2GgfsCfg2.setName("g2GgfsCfg2"); g2GgfsCfg1.setMetaCacheName("g2MetaCache1"); g2GgfsCfg2.setMetaCacheName("g2MetaCache2"); g1Cfg.setCacheConfiguration(concat(dataCaches(1024), metaCaches(), GridCacheConfiguration.class)); g2Cfg.setCacheConfiguration(concat(dataCaches(1024), metaCaches("g2MetaCache1", "g2MetaCache2"), GridCacheConfiguration.class)); g2Cfg.setGgfsConfiguration(g2GgfsCfg1, g2GgfsCfg2); G.start(g1Cfg); checkGridStartFails(g2Cfg, "Data cache names should be different for different GGFS instances", false); }', 'ground_truth': 'public void testRemoteIfDataCacheNameEquals() throws Exception { IgniteConfiguration g2Cfg = getConfiguration("g2"); GridGgfsConfiguration g2GgfsCfg1 = new GridGgfsConfiguration(g1GgfsCfg1); GridGgfsConfiguration g2GgfsCfg2 = new GridGgfsConfiguration(g1GgfsCfg2); g2GgfsCfg1.setName("g2GgfsCfg1"); g2GgfsCfg2.setName("g2GgfsCfg2"); g2GgfsCfg1.setMetaCacheName("g2MetaCache1"); g2GgfsCfg2.setMetaCacheName("g2MetaCache2"); g1Cfg.setCacheConfiguration(concat(dataCaches(1024), metaCaches(), GridCacheConfiguration.class)); g2Cfg.setCacheConfiguration(concat(dataCaches(1024), metaCaches("g2MetaCache1", "g2MetaCache2"), GridCacheConfiguration.class)); g2Cfg.setGgfsConfiguration(g2GgfsCfg1, g2GgfsCfg2); G.start(g1Cfg); checkGridStartFails(g2Cfg, "Data cache names should be different for different GGFS instances", false); }', 'output': 'public void testRemoteIfDataCacheNameEquals() throws Exception { IgniteConfiguration g2Cfg = getConfiguration("g2"); GridGgfsConfiguration g2GgfsCfg1 = new GridGgfsConfiguration(g1GgfsCfg1); GridGgfsConfiguration g2GgfsCfg2 = new GridGgfsConfiguration(g1GgfsCfg2); g2GgfsCfg1.setName("g2GgfsCfg1"); g2GgfsCfg2.setName("g2GgfsCfg2"); g2GgfsCfg1.setMetaCacheName("g2MetaCache1"); g2GgfsCfg2.setMetaCacheName("g2MetaCache2"); g1Cfg.setCacheConfiguration(concat(dataCaches(1024), metaCaches(), GridCacheConfiguration.class)); g2Cfg.setCacheConfiguration(concat(dataCaches(1024), metaCaches("g2MetaCache1", "g2MetaCache2"), GridCacheConfiguration.class)); g2Cfg.setGgfsConfiguration(g2GgfsCfg1, g2GgfsCfg2); G.start(g1Cfg); checkGridStartFails(g2Cfg, "Data cache names should be different for different GGFS instances", false); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void run() throws ClassNotFoundException, SQLException { Class.forName("net.hydromatic.optiq.jdbc.Driver"); Connection connection = DriverManager.getConnection("jdbc:optiq:"); OptiqConnection optiqConnection = connection.unwrap(OptiqConnection.class); XXX rootSchema = optiqConnection.getRootSchema(); ReflectiveSchema.create(rootSchema, "hr", new HrSchema()); ReflectiveSchema.create(rootSchema, "foodmart", new FoodmartSchema()); Statement statement = connection.createStatement(); ResultSet resultSet = statement.executeQuery( "select *\\n" + "from \\"foodmart\\".\\"sales_fact_1997\\" as s\\n" + "join \\"hr\\".\\"emps\\" as e\\n" + "on e.\\"empid\\" = s.\\"cust_id\\""); while (resultSet.next()) { int n = resultSet.getMetaData().getColumnCount(); for (int i = 1; i <= n; i++) { new StringBuilder().append( (i > 1 ? "; " : "") + resultSet.getMetaData().getColumnLabel(i) + "=" + resultSet.getObject(i)); } System.out.println(); } resultSet.close(); statement.close(); connection.close(); }', 'ground_truth': 'public void run() throws ClassNotFoundException, SQLException { Class.forName("net.hydromatic.optiq.jdbc.Driver"); Connection connection = DriverManager.getConnection("jdbc:optiq:"); OptiqConnection optiqConnection = connection.unwrap(OptiqConnection.class); SchemaPlus rootSchema = optiqConnection.getRootSchema(); rootSchema.add(new ReflectiveSchema(rootSchema, "hr", new Hr())); rootSchema.add( new ReflectiveSchema(rootSchema, "foodmart", new Foodmart())); Statement statement = connection.createStatement(); ResultSet resultSet = statement.executeQuery( "select *\\n" + "from \\"foodmart\\".\\"sales_fact_1997\\" as s\\n" + "join \\"hr\\".\\"emps\\" as e\\n" + "on e.\\"empid\\" = s.\\"cust_id\\""); final StringBuilder buf = new StringBuilder(); while (resultSet.next()) { int n = resultSet.getMetaData().getColumnCount(); for (int i = 1; i <= n; i++) { buf.append(i > 1 ? "; " : "") .append(resultSet.getMetaData().getColumnLabel(i)) .append("=") .append(resultSet.getObject(i)); } System.out.println(buf.toString()); buf.setLength(0); } resultSet.close(); statement.close(); connection.close(); }', 'output': 'public void run() throws ClassNotFoundException, SQLException { Class.forName("net.hydromatic.optiq.jdbc.Driver"); Connection connection = DriverManager.getConnection("jdbc:optiq:"); OptiqConnection optiqConnection = connection.unwrap(OptiqConnection.class); Schema rootSchema = optiqConnection.getRootSchema(); ReflectiveSchema.create(rootSchema, "hr", new HrSchema()); ReflectiveSchema.create(rootSchema, "foodmart", new FoodmartSchema()); Statement statement = connection.createStatement(); ResultSet resultSet = statement.executeQuery( "select *\\n" + "from \\"foodmart\\".\\"sales_fact_1997\\" as s\\n" + "join \\"hr\\".\\"emps\\" as e\\n" + "on e.\\"empid\\" = s.\\"cust_id\\""); while (resultSet.next()) { int n = resultSet.getMetaData().getColumnCount(); for (int i = 1; i <= n; i++) { new StringBuilder().append( (i > 1 ? "; " : "") + resultSet.getMetaData().getColumnLabel(i) + "=" + resultSet.getObject(i)); } System.out.println(); } resultSet.close(); statement.close(); connection.close(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void happyCase_countOnly() throws Exception { final XXX q = queryDefault.withCount(20L); assertThat(q, is(queryDefault)); assertThat(q.getStart(), is(0L)); assertThat(q.getCount(), is(20L)); }', 'ground_truth': 'public void happyCase_countOnly() throws Exception { final NamedQuery<Customer> q = namedQuery.withCount(20L); assertThat(q, is(namedQuery)); assertThat(q.getStart(), is(0L)); assertThat(q.getCount(), is(20L)); }', 'output': 'public void happyCase_countOnly() throws Exception { final CountQuery q = queryDefault.withCount(20L); assertThat(q, is(queryDefault)); assertThat(q.getStart(), is(0L)); assertThat(q.getCount(), is(20L)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testReloadAllFiltered() throws Exception { GridCache<String, Integer> cache = cache(); Collection<String> keys = primaryKeysForCache(cache, 3); for (String key : keys) assertNull(cache.get(key)); Map<String, Integer> vals = new HashMap<>(keys.size()); int i = 0; for (String key : keys) { cache.put(key, i); vals.put(key, i); i++; } for (String key : keys) assertEquals(vals.get(key), cache.peek(key)); cache.clearAll(); for (String key : keys) assertNull(cache.peek(key)); String first = F.first(keys); cache.put(first, 0); assertEquals((Integer)0, cache.peek(first)); cache.projection(F.<String, Integer>cacheHasPeekValue()).reloadAll(keys); assertEquals((Integer)0, cache.peek(first)); for (String key : keys) { if (!first.equals(key)) assertNull(cache.peek(key)); } }', 'ground_truth': 'public void testReloadAllFiltered() throws Exception { Cache<String, Integer> cache = cache(); Collection<String> keys = primaryKeysForCache(cache, 3); for (String key : keys) assertNull(cache.get(key)); Map<String, Integer> vals = new HashMap<>(keys.size()); int i = 0; for (String key : keys) { cache.put(key, i); vals.put(key, i); i++; } for (String key : keys) assertEquals(vals.get(key), cache.peek(key)); cache.clearAll(); for (String key : keys) assertNull(cache.peek(key)); String first = F.first(keys); cache.put(first, 0); assertEquals((Integer)0, cache.peek(first)); cache.projection(F.<String, Integer>cacheHasPeekValue()).reloadAll(keys); assertEquals((Integer)0, cache.peek(first)); for (String key : keys) { if (!first.equals(key)) assertNull(cache.peek(key)); } }', 'output': 'public void testReloadAllFiltered() throws Exception { Cache<String, Integer> cache = cache(); Collection<String> keys = primaryKeysForCache(cache, 3); for (String key : keys) assertNull(cache.get(key)); Map<String, Integer> vals = new HashMap<>(keys.size()); int i = 0; for (String key : keys) { cache.put(key, i); vals.put(key, i); i++; } for (String key : keys) assertEquals(vals.get(key), cache.peek(key)); cache.clearAll(); for (String key : keys) assertNull(cache.peek(key)); String first = F.first(keys); cache.put(first, 0); assertEquals((Integer)0, cache.peek(first)); cache.projection(F.<String, Integer>cacheHasPeekValue()).reloadAll(keys); assertEquals((Integer)0, cache.peek(first)); for (String key : keys) { if (!first.equals(key)) assertNull(cache.peek(key)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkLatch(final String cacheName) throws Exception { checkAutoDelete(cacheName); checkAwait(cacheName); checkCountDown(cacheName); XXX latch1 = grid(0).cache(cacheName).dataStructures().countDownLatch("latch", 2, false, true); assert latch1.count() == 2; IgniteCompute comp = grid(0).compute().enableAsync(); comp.call(new IgniteCallable<Object>() { @IgniteInstanceResource private Ignite ignite; @IgniteLoggerResource private IgniteLogger log; @Nullable @Override public Object call() throws Exception { IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync( new Callable<Object>() { @Nullable @Override public Object call() throws Exception { GridCacheCountDownLatch latch = ignite.cache(cacheName).dataStructures() .countDownLatch("latch", 2, false, true); assert latch != null && latch.count() == 2; log.info("Thread is going to wait on latch: " + Thread.currentThread().getName()); assert latch.await(1, MINUTES); log.info("Thread is again runnable: " + Thread.currentThread().getName()); return null; } }, 5, "test-thread" ); fut.get(); return null; } }); IgniteFuture<Object> fut = comp.future(); Thread.sleep(3000); assert latch1.countDown() == 1; assert latch1.countDown() == 0; fut.get(); grid(0).cache(cacheName).dataStructures().removeCountDownLatch("latch"); checkRemovedLatch(latch1); }', 'ground_truth': 'private void checkLatch(final String cacheName) throws Exception { checkAutoDelete(cacheName); checkAwait(cacheName); checkCountDown(cacheName); CacheCountDownLatch latch1 = grid(0).cache(cacheName).dataStructures().countDownLatch("latch", 2, false, true); assert latch1.count() == 2; IgniteCompute comp = grid(0).compute().enableAsync(); comp.call(new IgniteCallable<Object>() { @IgniteInstanceResource private Ignite ignite; @IgniteLoggerResource private IgniteLogger log; @Nullable @Override public Object call() throws Exception { IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync( new Callable<Object>() { @Nullable @Override public Object call() throws Exception { CacheCountDownLatch latch = ignite.cache(cacheName).dataStructures() .countDownLatch("latch", 2, false, true); assert latch != null && latch.count() == 2; log.info("Thread is going to wait on latch: " + Thread.currentThread().getName()); assert latch.await(1, MINUTES); log.info("Thread is again runnable: " + Thread.currentThread().getName()); return null; } }, 5, "test-thread" ); fut.get(); return null; } }); IgniteFuture<Object> fut = comp.future(); Thread.sleep(3000); assert latch1.countDown() == 1; assert latch1.countDown() == 0; fut.get(); grid(0).cache(cacheName).dataStructures().removeCountDownLatch("latch"); checkRemovedLatch(latch1); }', 'output': 'private void checkLatch(final String cacheName) throws Exception { checkAutoDelete(cacheName); checkAwait(cacheName); checkCountDown(cacheName); CountDownLatch latch1 = grid(0).cache(cacheName).dataStructures().countDownLatch("latch", 2, false, true); assert latch1.count() == 2; IgniteCompute comp = grid(0).compute().enableAsync(); comp.call(new IgniteCallable<Object>() { @IgniteInstanceResource private Ignite ignite; @IgniteLoggerResource private IgniteLogger log; @Nullable @Override public Object call() throws Exception { IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync( new Callable<Object>() { @Nullable @Override public Object call() throws Exception { GridCacheCountDownLatch latch = ignite.cache(cacheName).dataStructures() .countDownLatch("latch", 2, false, true); assert latch != null && latch.count() == 2; log.info("Thread is going to wait on latch: " + Thread.currentThread().getName()); assert latch.await(1, MINUTES); log.info("Thread is again runnable: " + Thread.currentThread().getName()); return null; } }, 5, "test-thread" ); fut.get(); return null; } }); IgniteFuture<Object> fut = comp.future(); Thread.sleep(3000); assert latch1.countDown() == 1; assert latch1.countDown() == 0; fut.get(); grid(0).cache(cacheName).dataStructures().removeCountDownLatch("latch"); checkRemovedLatch(latch1); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void processElement(ProcessContext processContext) throws Exception { DoFnProcessContext<KV<K, Iterable<WindowedValue<V>>>, KV<K, Iterable<V>>> context = (DoFnProcessContext<KV<K, Iterable<WindowedValue<V>>>, KV<K, Iterable<V>>>) processContext; if (windowFn instanceof NonMergingWindowFn) { processElementViaIterators(context); } else { processElementViaWindowSet(context); } }', 'ground_truth': 'public void processElement(ProcessContext processContext) throws Exception { DoFnProcessContext<KV<K, Iterable<WindowedValue<VI>>>, KV<K, VO>> context = (DoFnProcessContext<KV<K, Iterable<WindowedValue<VI>>>, KV<K, VO>>) processContext; if (windowFn instanceof NonMergingWindowFn && combineFn == null) { processElementViaIterators(context); } else { processElementViaWindowSet(context); } }', 'output': 'public void processElement(ProcessContext processContext) throws Exception { DoFnProcessContext<KV<K, Iterable<WindowedValue<V>>>, KV<K, Iterable<V>>> context = (DoFnProcessContext<KV<K, Iterable<WindowedValue<V>>>, KV<K, Iterable<V>>>) processContext; if (windowFn instanceof NonMergingWindowFn) { processElementViaIterators(context); } else { processElementViaWindowSet(context); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void checkTask(String taskName) throws Exception { assert taskName != null; XXX task = getSpi().findResource(taskName); assert task != null; info("Deployed task [task=" + task + \']\'); }', 'ground_truth': 'protected void checkTask(String taskName) throws Exception { assert taskName != null; DeploymentResource task = getSpi().findResource(taskName); assert task != null; info("Deployed task [task=" + task + \']\'); }', 'output': 'protected void checkTask(String taskName) throws Exception { assert taskName != null; Resource task = getSpi().findResource(taskName); assert task != null; info("Deployed task [task=" + task + \']\'); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void startUpSecondary() throws Exception { IgniteFsConfiguration ggfsCfg = new IgniteFsConfiguration(); ggfsCfg.setDataCacheName("partitioned"); ggfsCfg.setMetaCacheName("replicated"); ggfsCfg.setName("ggfs-secondary"); ggfsCfg.setBlockSize(512 * 1024); ggfsCfg.setDefaultMode(PRIMARY); ggfsCfg.setIpcEndpointConfiguration(GridGgfsTestUtils.jsonToMap("{type:\'tcp\', port:11500}")); GridCacheConfiguration cacheCfg = defaultCacheConfiguration(); cacheCfg.setName("partitioned"); cacheCfg.setCacheMode(PARTITIONED); cacheCfg.setDistributionMode(GridCacheDistributionMode.PARTITIONED_ONLY); cacheCfg.setWriteSynchronizationMode(GridCacheWriteSynchronizationMode.FULL_SYNC); cacheCfg.setAffinityMapper(new IgniteFsGroupDataBlocksKeyMapper(128)); cacheCfg.setBackups(0); cacheCfg.setQueryIndexEnabled(false); cacheCfg.setAtomicityMode(TRANSACTIONAL); GridCacheConfiguration metaCacheCfg = defaultCacheConfiguration(); metaCacheCfg.setName("replicated"); metaCacheCfg.setCacheMode(REPLICATED); metaCacheCfg.setWriteSynchronizationMode(GridCacheWriteSynchronizationMode.FULL_SYNC); metaCacheCfg.setQueryIndexEnabled(false); metaCacheCfg.setAtomicityMode(TRANSACTIONAL); IgniteConfiguration cfg = new IgniteConfiguration(); cfg.setGridName("ggfs-grid-secondary"); XXX discoSpi = new GridTcpDiscoverySpi(); discoSpi.setIpFinder(new GridTcpDiscoveryVmIpFinder(true)); cfg.setDiscoverySpi(discoSpi); cfg.setCacheConfiguration(metaCacheCfg, cacheCfg); cfg.setGgfsConfiguration(ggfsCfg); cfg.setLocalHost("127.0.0.1"); G.start(cfg); }', 'ground_truth': 'private void startUpSecondary() throws Exception { IgniteFsConfiguration ggfsCfg = new IgniteFsConfiguration(); ggfsCfg.setDataCacheName("partitioned"); ggfsCfg.setMetaCacheName("replicated"); ggfsCfg.setName("ggfs-secondary"); ggfsCfg.setBlockSize(512 * 1024); ggfsCfg.setDefaultMode(PRIMARY); ggfsCfg.setIpcEndpointConfiguration(GridGgfsTestUtils.jsonToMap("{type:\'tcp\', port:11500}")); GridCacheConfiguration cacheCfg = defaultCacheConfiguration(); cacheCfg.setName("partitioned"); cacheCfg.setCacheMode(PARTITIONED); cacheCfg.setDistributionMode(GridCacheDistributionMode.PARTITIONED_ONLY); cacheCfg.setWriteSynchronizationMode(GridCacheWriteSynchronizationMode.FULL_SYNC); cacheCfg.setAffinityMapper(new IgniteFsGroupDataBlocksKeyMapper(128)); cacheCfg.setBackups(0); cacheCfg.setQueryIndexEnabled(false); cacheCfg.setAtomicityMode(TRANSACTIONAL); GridCacheConfiguration metaCacheCfg = defaultCacheConfiguration(); metaCacheCfg.setName("replicated"); metaCacheCfg.setCacheMode(REPLICATED); metaCacheCfg.setWriteSynchronizationMode(GridCacheWriteSynchronizationMode.FULL_SYNC); metaCacheCfg.setQueryIndexEnabled(false); metaCacheCfg.setAtomicityMode(TRANSACTIONAL); IgniteConfiguration cfg = new IgniteConfiguration(); cfg.setGridName("ggfs-grid-secondary"); TcpDiscoverySpi discoSpi = new TcpDiscoverySpi(); discoSpi.setIpFinder(new GridTcpDiscoveryVmIpFinder(true)); cfg.setDiscoverySpi(discoSpi); cfg.setCacheConfiguration(metaCacheCfg, cacheCfg); cfg.setGgfsConfiguration(ggfsCfg); cfg.setLocalHost("127.0.0.1"); G.start(cfg); }', 'output': 'private void startUpSecondary() throws Exception { IgniteFsConfiguration ggfsCfg = new IgniteFsConfiguration(); ggfsCfg.setDataCacheName("partitioned"); ggfsCfg.setMetaCacheName("replicated"); ggfsCfg.setName("ggfs-secondary"); ggfsCfg.setBlockSize(512 * 1024); ggfsCfg.setDefaultMode(PRIMARY); ggfsCfg.setIpcEndpointConfiguration(GridGgfsTestUtils.jsonToMap("{type:\'tcp\', port:11500}")); GridCacheConfiguration cacheCfg = defaultCacheConfiguration(); cacheCfg.setName("partitioned"); cacheCfg.setCacheMode(PARTITIONED); cacheCfg.setDistributionMode(GridCacheDistributionMode.PARTITIONED_ONLY); cacheCfg.setWriteSynchronizationMode(GridCacheWriteSynchronizationMode.FULL_SYNC); cacheCfg.setAffinityMapper(new IgniteFsGroupDataBlocksKeyMapper(128)); cacheCfg.setBackups(0); cacheCfg.setQueryIndexEnabled(false); cacheCfg.setAtomicityMode(TRANSACTIONAL); GridCacheConfiguration metaCacheCfg = defaultCacheConfiguration(); metaCacheCfg.setName("replicated"); metaCacheCfg.setCacheMode(REPLICATED); metaCacheCfg.setWriteSynchronizationMode(GridCacheWriteSynchronizationMode.FULL_SYNC); metaCacheCfg.setQueryIndexEnabled(false); metaCacheCfg.setAtomicityMode(TRANSACTIONAL); IgniteConfiguration cfg = new IgniteConfiguration(); cfg.setGridName("ggfs-grid-secondary"); TcpDiscoverySpi discoSpi = new TcpDiscoverySpi(); discoSpi.setIpFinder(new TcpDiscoveryVmIpFinder(true)); cfg.setDiscoverySpi(discoSpi); cfg.setCacheConfiguration(metaCacheCfg, cacheCfg); cfg.setGgfsConfiguration(ggfsCfg); cfg.setLocalHost("127.0.0.1"); G.start(cfg); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Object chooseEmptyMethodParams(List methods) { Object vargsMethod = null; for (Iterator iter = methods.iterator(); iter.hasNext();) { Object method = iter.next(); final ParameterTypes pt = getParameterTypes(method); XXX paramTypes = pt.getParameterTypes(); int paramLength = paramTypes.length; if (paramLength == 0) { return method; } else if (paramLength == 1 && isVargsMethod(pt, EMPTY_ARRAY)) { vargsMethod = method; } } return vargsMethod; }', 'ground_truth': 'public static Object chooseEmptyMethodParams(List methods) { Object vargsMethod = null; for (Iterator iter = methods.iterator(); iter.hasNext();) { Object method = iter.next(); final ParameterTypes pt = getParameterTypes(method); CachedClass[] paramTypes = pt.getParameterTypes(); int paramLength = paramTypes.length; if (paramLength == 0) { return method; } else if (paramLength == 1 && isVargsMethod(pt, EMPTY_ARRAY)) { vargsMethod = method; } } return vargsMethod; }', 'output': 'public static Object chooseEmptyMethodParams(List methods) { Object vargsMethod = null; for (Iterator iter = methods.iterator(); iter.hasNext();) { Object method = iter.next(); final ParameterTypes pt = getParameterTypes(method); Class<?>[] paramTypes = pt.getParameterTypes(); int paramLength = paramTypes.length; if (paramLength == 0) { return method; } else if (paramLength == 1 && isVargsMethod(pt, EMPTY_ARRAY)) { vargsMethod = method; } } return vargsMethod; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) { assert e.getMessage() instanceof Message : "Expecting message, got " + e.getMessage(); if (e.getMessage() instanceof Response) throw new ProtocolException("Invalid response message received, expecting requests"); Request request = (Request)e.getMessage(); try { assert request.connection() instanceof ServerConnection; ServerConnection connection = (ServerConnection)request.connection(); connection.validateNewMessage(request.type); logger.debug("Received: {}", request); Response response = request.execute(connection.getQueryState(request.getStreamId())); response.setStreamId(request.getStreamId()); response.attach(connection); connection.applyStateTransition(request.type, response.type); logger.debug("Responding: {}", response); ctx.getChannel().write(response); } catch (XXX ex) { ctx.getChannel().write(ErrorMessage.fromException(ex).setStreamId(request.getStreamId())); } }', 'ground_truth': 'public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) { assert e.getMessage() instanceof Message : "Expecting message, got " + e.getMessage(); if (e.getMessage() instanceof Response) throw new ProtocolException("Invalid response message received, expecting requests"); Request request = (Request)e.getMessage(); try { assert request.connection() instanceof ServerConnection; ServerConnection connection = (ServerConnection)request.connection(); connection.validateNewMessage(request.type); logger.debug("Received: {}", request); Response response = request.execute(connection.getQueryState(request.getStreamId())); response.setStreamId(request.getStreamId()); response.attach(connection); connection.applyStateTransition(request.type, response.type); logger.debug("Responding: {}", response); ctx.getChannel().write(response); } catch (Throwable ex) { ctx.getChannel().write(ErrorMessage.fromException(ex).setStreamId(request.getStreamId())); } }', 'output': 'public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) { assert e.getMessage() instanceof Message : "Expecting message, got " + e.getMessage(); if (e.getMessage() instanceof Response) throw new ProtocolException("Invalid response message received, expecting requests"); Request request = (Request)e.getMessage(); try { assert request.connection() instanceof ServerConnection; ServerConnection connection = (ServerConnection)request.connection(); connection.validateNewMessage(request.type); logger.debug("Received: {}", request); Response response = request.execute(connection.getQueryState(request.getStreamId())); response.setStreamId(request.getStreamId()); response.attach(connection); connection.applyStateTransition(request.type, response.type); logger.debug("Responding: {}", response); ctx.getChannel().write(response); } catch (Throwable ex) { ctx.getChannel().write(ErrorMessage.fromException(ex).setStreamId(request.getStreamId())); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public TransformResult<KeyedWorkItem<K, V>> finishBundle() throws Exception { XXX state = stepContext.commitState(); return StepTransformResult.<KeyedWorkItem<K, V>>withHold( application, state.getEarliestWatermarkHold()) .withState(state) .addOutput(outputBundles) .withTimerUpdate(stepContext.getTimerUpdate()) .addUnprocessedElements(unprocessedElements.build()) .build(); }', 'ground_truth': 'public TransformResult<KeyedWorkItem<K, V>> finishBundle() throws Exception { CopyOnAccessInMemoryStateInternals<?> state = stateInternals.commit(); return StepTransformResult.<KeyedWorkItem<K, V>>withHold( application, state.getEarliestWatermarkHold()) .withState(state) .addOutput(outputBundles) .withTimerUpdate(timerInternals.getTimerUpdate()) .build(); }', 'output': 'public TransformResult<KeyedWorkItem<K, V>> finishBundle() throws Exception { State state = stepContext.commitState(); return StepTransformResult.<KeyedWorkItem<K, V>>withHold( application, state.getEarliestWatermarkHold()) .withState(state) .addOutput(outputBundles) .withTimerUpdate(stepContext.getTimerUpdate()) .addUnprocessedElements(unprocessedElements.build()) .build(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkAppendFragmentizing(int chunkSize, boolean rotate) throws Exception { XXX path = new GridGgfsPath("/someFile"); long written = 0; int i = 0; int ggfsIdx = 0; int fileSize = 30 * GGFS_GROUP_SIZE * GGFS_BLOCK_SIZE; while (written < fileSize) { IgniteFs ggfs = grid(ggfsIdx).fileSystem("ggfs"); try (GridGgfsOutputStream out = ggfs.append(path, true)) { byte[] data = new byte[chunkSize]; Arrays.fill(data, (byte)i); out.write(data); } System.out.println("Written [start=" + written + ", filler=" + i + \']\'); written += chunkSize; i++; if (rotate && i % 5 == 0) { ggfsIdx++; if (ggfsIdx >= NODE_CNT) ggfsIdx = 0; } } IgniteFs ggfs = grid(0).fileSystem("ggfs"); try (GridGgfsInputStream in = ggfs.open(path)) { i = 0; int read = 0; byte[] chunk = new byte[chunkSize]; while (read < fileSize) { readFully(in, chunk); for (byte b : chunk) assertEquals("For read offset [start=" + read + ", filler=" + (i & 0xFF) + \']\', i & 0xFF, b & 0xFF); i++; read += chunkSize; } } }', 'ground_truth': 'private void checkAppendFragmentizing(int chunkSize, boolean rotate) throws Exception { IgniteFsPath path = new IgniteFsPath("/someFile"); long written = 0; int i = 0; int ggfsIdx = 0; int fileSize = 30 * GGFS_GROUP_SIZE * GGFS_BLOCK_SIZE; while (written < fileSize) { IgniteFs ggfs = grid(ggfsIdx).fileSystem("ggfs"); try (GridGgfsOutputStream out = ggfs.append(path, true)) { byte[] data = new byte[chunkSize]; Arrays.fill(data, (byte)i); out.write(data); } System.out.println("Written [start=" + written + ", filler=" + i + \']\'); written += chunkSize; i++; if (rotate && i % 5 == 0) { ggfsIdx++; if (ggfsIdx >= NODE_CNT) ggfsIdx = 0; } } IgniteFs ggfs = grid(0).fileSystem("ggfs"); try (GridGgfsInputStream in = ggfs.open(path)) { i = 0; int read = 0; byte[] chunk = new byte[chunkSize]; while (read < fileSize) { readFully(in, chunk); for (byte b : chunk) assertEquals("For read offset [start=" + read + ", filler=" + (i & 0xFF) + \']\', i & 0xFF, b & 0xFF); i++; read += chunkSize; } } }', 'output': 'private void checkAppendFragmentizing(int chunkSize, boolean rotate) throws Exception { String path = "/someFile"; long written = 0; int i = 0; int ggfsIdx = 0; int fileSize = 30 * GGFS_GROUP_SIZE * GGFS_BLOCK_SIZE; while (written < fileSize) { IgniteFs ggfs = grid(ggfsIdx).fileSystem("ggfs"); try (GridGgfsOutputStream out = ggfs.append(path, true)) { byte[] data = new byte[chunkSize]; Arrays.fill(data, (byte)i); out.write(data); } System.out.println("Written [start=" + written + ", filler=" + i + \']\'); written += chunkSize; i++; if (rotate && i % 5 == 0) { ggfsIdx++; if (ggfsIdx >= NODE_CNT) ggfsIdx = 0; } } IgniteFs ggfs = grid(0).fileSystem("ggfs"); try (GridGgfsInputStream in = ggfs.open(path)) { i = 0; int read = 0; byte[] chunk = new byte[chunkSize]; while (read < fileSize) { readFully(in, chunk); for (byte b : chunk) assertEquals("For read offset [start=" + read + ", filler=" + (i & 0xFF) + \']\', i & 0xFF, b & 0xFF); i++; read += chunkSize; } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testClearSnapshot_AllAndName() { XXX tool = ToolRunner.invokeNodetool("clearsnapshot", "-t", "some-name", "--all"); assertEquals(2, tool.getExitCode()); assertThat(tool.getCleanedStderr(), CoreMatchers.containsStringIgnoringCase("Specify only one of snapshot name or --all")); }', 'ground_truth': 'public void testClearSnapshot_AllAndName() { ToolRunner.ToolResult tool = ToolRunner.invokeNodetool("clearsnapshot", "-t", "some-name", "--all"); assertThat(tool.getExitCode()).isEqualTo(2); assertThat(tool.getCleanedStderr()).contains("Specify only one of snapshot name or --all"); }', 'output': 'public void testClearSnapshot_AllAndName() { NodeTool tool = ToolRunner.invokeNodetool("clearsnapshot", "-t", "some-name", "--all"); assertEquals(2, tool.getExitCode()); assertThat(tool.getCleanedStderr(), CoreMatchers.containsStringIgnoringCase("Specify only one of snapshot name or --all")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void onFormGroupCreated(final FormGroup formGroup) { super.onFormGroupCreated(formGroup); formGroup.add(createScalarValueContainer(ID_SCALAR_VALUE_CONTAINER)); setTextFieldSizeAndMaxLengthIfSpecified(); }', 'ground_truth': 'protected void onFormGroupCreated(final FormGroup formGroup) { super.onFormGroupCreated(formGroup); val textFieldFragment = new Fragment(ID_SCALAR_VALUE_CONTAINER, getTextFieldFragmentId(), this); textFieldFragment.add(getFormComponent()); formGroup.add(textFieldFragment); setTextFieldSizeAndMaxLengthIfSpecified(); }', 'output': 'public void onFormGroupCreated(final FormGroup formGroup) { super.onFormGroupCreated(formGroup); formGroup.add(createScalarValueContainer(ID_SCALAR_VALUE_CONTAINER)); setTextFieldSizeAndMaxLengthIfSpecified(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testContinuation() throws Exception { XXX firstElementPlus1 = AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardHours(1)); assertEquals( new AfterSynchronizedProcessingTime<>(), firstElementPlus1.getContinuationTrigger()); }', 'ground_truth': 'public void testContinuation() throws Exception { TimeTrigger<?> firstElementPlus1 = AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardHours(1)); assertEquals( new AfterSynchronizedProcessingTime<>(), firstElementPlus1.getContinuationTrigger()); }', 'output': 'public void testContinuation() throws Exception { AfterProcessingTime firstElementPlus1 = AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardHours(1)); assertEquals( new AfterSynchronizedProcessingTime<>(), firstElementPlus1.getContinuationTrigger()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void buildGui() { final RepeatingViewWithDynamicallyVisibleContent rv = new RepeatingViewWithDynamicallyVisibleContent(ID_ROW_CONTENTS); for(final XXX bs3RowContent: bs3Row.getCols()) { final String id = rv.newChildId(); final WebMarkupContainer rowContent; if(bs3RowContent instanceof BS3Col) { final BS3Col bs3Col = (BS3Col) bs3RowContent; final Col col = new Col(id, getModel(), bs3Col); visible = visible || col.isVisible(); rowContent = col; } else if (bs3RowContent instanceof BS3ClearFix) { final BS3ClearFix bs3ClearFix = (BS3ClearFix) bs3RowContent; rowContent = new ClearFix(id, getModel(), bs3ClearFix); } else { throw new IllegalStateException("Unrecognized implementation of BS3RowContent"); } rv.add(rowContent); } final WebMarkupContainer panel = this; if(visible) { Wkt.cssAppend(panel, "row"); Wkt.cssAppend(panel, bs3Row.getCssClass()); panel.add(rv); } else { WktComponents.permanentlyHide(panel, rv.getId()); } }', 'ground_truth': 'private void buildGui() { final RepeatingViewWithDynamicallyVisibleContent rv = new RepeatingViewWithDynamicallyVisibleContent(ID_ROW_CONTENTS); for(final BSRowContent bs3RowContent: bs3Row.getCols()) { final String id = rv.newChildId(); final WebMarkupContainer rowContent; if(bs3RowContent instanceof BSCol) { final BSCol bs3Col = (BSCol) bs3RowContent; final Col col = new Col(id, getModel(), bs3Col); visible = visible || col.isVisible(); rowContent = col; } else if (bs3RowContent instanceof BSClearFix) { final BSClearFix bs3ClearFix = (BSClearFix) bs3RowContent; rowContent = new ClearFix(id, getModel(), bs3ClearFix); } else { throw new IllegalStateException("Unrecognized implementation of BSRowContent"); } rv.add(rowContent); } final WebMarkupContainer panel = this; if(visible) { Wkt.cssAppend(panel, "row"); Wkt.cssAppend(panel, bs3Row.getCssClass()); panel.add(rv); } else { WktComponents.permanentlyHide(panel, rv.getId()); } }', 'output': 'private void buildGui() { final RepeatingViewWithDynamicallyVisibleContent rv = new RepeatingViewWithDynamicallyVisibleContent(ID_ROW_CONTENTS); for(final BS3RowContent bs3RowContent: bs3Row.getCols()) { final String id = rv.newChildId(); final WebMarkupContainer rowContent; if(bs3RowContent instanceof BS3Col) { final BS3Col bs3Col = (BS3Col) bs3RowContent; final Col col = new Col(id, getModel(), bs3Col); visible = visible || col.isVisible(); rowContent = col; } else if (bs3RowContent instanceof BS3ClearFix) { final BS3ClearFix bs3ClearFix = (BS3ClearFix) bs3RowContent; rowContent = new ClearFix(id, getModel(), bs3ClearFix); } else { throw new IllegalStateException("Unrecognized implementation of BS3RowContent"); } rv.add(rowContent); } final WebMarkupContainer panel = this; if(visible) { Wkt.cssAppend(panel, "row"); Wkt.cssAppend(panel, bs3Row.getCssClass()); panel.add(rv); } else { WktComponents.permanentlyHide(panel, rv.getId()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void checkCache(Object[] keys) throws Exception { preloadMode = SYNC; lifecycleBean = lifecycleBean(keys); for (int i = 0; i < gridCnt; i++) { startGrid(i); info("Checking \'" + (i + 1) + "\' nodes..."); for (int j = 0; j < G.allGrids().size(); j++) { GridCache<String, MyValue> c1 = grid(j).cache("one"); GridCache<String, MyValue> c2 = grid(j).cache("two"); int size1 = c1.size(); int size2 = c2.size(); assert size1 == keys.length : " Invalid cache1 size [i=" + i + ", j=" + j + ", size=" + size1 + ", cache=" + c1.entrySet() + \']\'; assert size2 == keys.length / 2 : " Invalid cache2 size [i=" + i + ", j=" + j + ", size=" + size2 + ", cache=" + c2.entrySet() + \']\'; } } }', 'ground_truth': 'public void checkCache(Object[] keys) throws Exception { preloadMode = SYNC; lifecycleBean = lifecycleBean(keys); for (int i = 0; i < gridCnt; i++) { startGrid(i); info("Checking \'" + (i + 1) + "\' nodes..."); for (int j = 0; j < G.allGrids().size(); j++) { Cache<String, MyValue> c1 = grid(j).cache("one"); Cache<String, MyValue> c2 = grid(j).cache("two"); int size1 = c1.size(); int size2 = c2.size(); assert size1 == keys.length : " Invalid cache1 size [i=" + i + ", j=" + j + ", size=" + size1 + ", cache=" + c1.entrySet() + \']\'; assert size2 == keys.length / 2 : " Invalid cache2 size [i=" + i + ", j=" + j + ", size=" + size2 + ", cache=" + c2.entrySet() + \']\'; } } }', 'output': 'public void checkCache(Object[] keys) throws Exception { preloadMode = SYNC; lifecycleBean = lifecycleBean(keys); for (int i = 0; i < gridCnt; i++) { startGrid(i); info("Checking \'" + (i + 1) + "\' nodes..."); for (int j = 0; j < G.allGrids().size(); j++) { Cache<String, MyValue> c1 = grid(j).cache("one"); Cache<String, MyValue> c2 = grid(j).cache("two"); int size1 = c1.size(); int size2 = c2.size(); assert size1 == keys.length : " Invalid cache1 size [i=" + i + ", j=" + j + ", size=" + size1 + ", cache=" + c1.entrySet() + \']\'; assert size2 == keys.length / 2 : " Invalid cache2 size [i=" + i + ", j=" + j + ", size=" + size2 + ", cache=" + c2.entrySet() + \']\'; } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected Composite makeWith(ByteBuffer[] components, int size, Composite.EOC eoc, boolean isStatic) { assert !isStatic; XXX c = new CompoundDenseCellName(components, size); return eoc != Composite.EOC.NONE ? c.withEOC(eoc) : c; }', 'ground_truth': 'protected Composite makeWith(ByteBuffer[] components, int size, Composite.EOC eoc, boolean isStatic) { assert !isStatic; CompoundDenseCellName c = new CompoundDenseCellName(components, size); return eoc != Composite.EOC.NONE ? c.withEOC(eoc) : c; }', 'output': 'protected Composite makeWith(ByteBuffer[] components, int size, Composite.EOC eoc, boolean isStatic) { assert !isStatic; CompoundDenseCellName c = new CompoundDenseCellName(components, size); return eoc != Composite.EOC.NONE ? c.withEOC(eoc) : c; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGetProgressNoEndOffset() throws Exception { File tmpFile = initTestFile(); TextReader<String> textReader = new TextReader<>(tmpFile.getPath(), false, 0L, null, new WholeLineVerifyingCoder(), TextIO.CompressionType.UNCOMPRESSED); try (XXX iterator = textReader.iterator()) { ApproximateReportedProgress progress = readerProgressToCloudProgress(iterator.getProgress()); assertEquals(0L, progress.getPosition().getByteOffset().longValue()); iterator.next(); progress = readerProgressToCloudProgress(iterator.getProgress()); assertEquals(13L, progress.getPosition().getByteOffset().longValue()); iterator.next(); progress = readerProgressToCloudProgress(iterator.getProgress()); assertEquals(28L, progress.getPosition().getByteOffset().longValue()); assertNull(progress.getFractionConsumed()); iterator.next(); progress = readerProgressToCloudProgress(iterator.getProgress()); assertEquals(40L, progress.getPosition().getByteOffset().longValue()); assertFalse(iterator.hasNext()); } }', 'ground_truth': 'public void testGetProgressNoEndOffset() throws Exception { File tmpFile = initTestFile(); TextReader<String> textReader = new TextReader<>(tmpFile.getPath(), false, 0L, null, new WholeLineVerifyingCoder(), TextIO.CompressionType.UNCOMPRESSED); try (NativeReaderIterator<String> iterator = textReader.iterator()) { ApproximateReportedProgress progress = readerProgressToCloudProgress(iterator.getProgress()); assertEquals(0L, progress.getPosition().getByteOffset().longValue()); assertTrue(iterator.start()); progress = readerProgressToCloudProgress(iterator.getProgress()); assertEquals(13L, progress.getPosition().getByteOffset().longValue()); assertTrue(iterator.advance()); progress = readerProgressToCloudProgress(iterator.getProgress()); assertEquals(28L, progress.getPosition().getByteOffset().longValue()); assertNull(progress.getFractionConsumed()); assertTrue(iterator.advance()); progress = readerProgressToCloudProgress(iterator.getProgress()); assertEquals(40L, progress.getPosition().getByteOffset().longValue()); assertFalse(iterator.advance()); } }', 'output': 'public void testGetProgressNoEndOffset() throws Exception { File tmpFile = initTestFile(); TextReader<String> textReader = new TextReader<>(tmpFile.getPath(), false, 0L, null, new WholeLineVerifyingCoder(), TextIO.CompressionType.UNCOMPRESSED); try (Iterator<String> iterator = textReader.iterator()) { ApproximateReportedProgress progress = readerProgressToCloudProgress(iterator.getProgress()); assertEquals(0L, progress.getPosition().getByteOffset().longValue()); iterator.next(); progress = readerProgressToCloudProgress(iterator.getProgress()); assertEquals(13L, progress.getPosition().getByteOffset().longValue()); iterator.next(); progress = readerProgressToCloudProgress(iterator.getProgress()); assertEquals(28L, progress.getPosition().getByteOffset().longValue()); assertNull(progress.getFractionConsumed()); iterator.next(); progress = readerProgressToCloudProgress(iterator.getProgress()); assertEquals(40L, progress.getPosition().getByteOffset().longValue()); assertFalse(iterator.hasNext()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Slices makeSlices(QueryOptions options) throws InvalidRequestException { SortedSet<ClusteringBound> startBounds = restrictions.getClusteringColumnsBounds(Bound.START, options); SortedSet<ClusteringBound> endBounds = restrictions.getClusteringColumnsBounds(Bound.END, options); assert startBounds.size() == endBounds.size(); if (startBounds.size() == 1) { ClusteringBound start = startBounds.first(); ClusteringBound end = endBounds.first(); return Slice.isEmpty(table.comparator, start, end) ? Slices.NONE : Slices.with(table.comparator, Slice.make(start, end)); } Slices.Builder builder = new Slices.Builder(table.comparator, startBounds.size()); Iterator<ClusteringBound> startIter = startBounds.iterator(); XXX endIter = endBounds.iterator(); while (startIter.hasNext() && endIter.hasNext()) { ClusteringBound start = startIter.next(); ClusteringBound end = endIter.next(); if (Slice.isEmpty(table.comparator, start, end)) continue; builder.add(start, end); } return builder.build(); }', 'ground_truth': 'public Slices makeSlices(QueryOptions options) throws InvalidRequestException { SortedSet<ClusteringBound<?>> startBounds = restrictions.getClusteringColumnsBounds(Bound.START, options); SortedSet<ClusteringBound<?>> endBounds = restrictions.getClusteringColumnsBounds(Bound.END, options); assert startBounds.size() == endBounds.size(); if (startBounds.size() == 1) { ClusteringBound<?> start = startBounds.first(); ClusteringBound<?> end = endBounds.first(); return Slice.isEmpty(table.comparator, start, end) ? Slices.NONE : Slices.with(table.comparator, Slice.make(start, end)); } Slices.Builder builder = new Slices.Builder(table.comparator, startBounds.size()); Iterator<ClusteringBound<?>> startIter = startBounds.iterator(); Iterator<ClusteringBound<?>> endIter = endBounds.iterator(); while (startIter.hasNext() && endIter.hasNext()) { ClusteringBound<?> start = startIter.next(); ClusteringBound<?> end = endIter.next(); if (Slice.isEmpty(table.comparator, start, end)) continue; builder.add(start, end); } return builder.build(); }', 'output': 'public Slices makeSlices(QueryOptions options) throws InvalidRequestException { SortedSet<ClusteringBound> startBounds = restrictions.getClusteringColumnsBounds(Bound.START, options); SortedSet<ClusteringBound> endBounds = restrictions.getClusteringColumnsBounds(Bound.END, options); assert startBounds.size() == endBounds.size(); if (startBounds.size() == 1) { ClusteringBound start = startBounds.first(); ClusteringBound end = endBounds.first(); return Slice.isEmpty(table.comparator, start, end) ? Slices.NONE : Slices.with(table.comparator, Slice.make(start, end)); } Slices.Builder builder = new Slices.Builder(table.comparator, startBounds.size()); Iterator<ClusteringBound> startIter = startBounds.iterator(); Iterator<ClusteringBound> endIter = endBounds.iterator(); while (startIter.hasNext() && endIter.hasNext()) { ClusteringBound start = startIter.next(); ClusteringBound end = endIter.next(); if (Slice.isEmpty(table.comparator, start, end)) continue; builder.add(start, end); } return builder.build(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testLocal() throws Exception { mode = LOCAL; try { GridCache<Integer, Value> cache = startGrid().cache(null); assertTrue(cache.putx(1, new Value(new byte[1024]))); assertTrue(cache.putx(2, new Value(new byte[2048]))); assertEquals(KEY_SIZE + NULL_REF_SIZE + ENTRY_OVERHEAD + extrasSize(cache.entry(0)), cache.entry(0).memorySize()); assertEquals(KEY_SIZE + ONE_KB_VAL_SIZE + ENTRY_OVERHEAD + extrasSize(cache.entry(1)), cache.entry(1).memorySize()); assertEquals(KEY_SIZE + TWO_KB_VAL_SIZE + ENTRY_OVERHEAD + extrasSize(cache.entry(2)), cache.entry(2).memorySize()); } finally { stopAllGrids(); } }', 'ground_truth': 'public void testLocal() throws Exception { mode = LOCAL; try { Cache<Integer, Value> cache = startGrid().cache(null); assertTrue(cache.putx(1, new Value(new byte[1024]))); assertTrue(cache.putx(2, new Value(new byte[2048]))); assertEquals(KEY_SIZE + NULL_REF_SIZE + ENTRY_OVERHEAD + extrasSize(cache.entry(0)), cache.entry(0).memorySize()); assertEquals(KEY_SIZE + ONE_KB_VAL_SIZE + ENTRY_OVERHEAD + extrasSize(cache.entry(1)), cache.entry(1).memorySize()); assertEquals(KEY_SIZE + TWO_KB_VAL_SIZE + ENTRY_OVERHEAD + extrasSize(cache.entry(2)), cache.entry(2).memorySize()); } finally { stopAllGrids(); } }', 'output': 'public void testLocal() throws Exception { mode = LOCAL; try { Cache<Integer, Value> cache = startGrid().cache(null); assertTrue(cache.putx(1, new Value(new byte[1024]))); assertTrue(cache.putx(2, new Value(new byte[2048]))); assertEquals(KEY_SIZE + NULL_REF_SIZE + ENTRY_OVERHEAD + extrasSize(cache.entry(0)), cache.entry(0).memorySize()); assertEquals(KEY_SIZE + ONE_KB_VAL_SIZE + ENTRY_OVERHEAD + extrasSize(cache.entry(1)), cache.entry(1).memorySize()); assertEquals(KEY_SIZE + TWO_KB_VAL_SIZE + ENTRY_OVERHEAD + extrasSize(cache.entry(2)), cache.entry(2).memorySize()); } finally { stopAllGrids(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void testAddInternal(boolean reversed) { XXX map = ArrayBackedSortedColumns.factory().create(BytesType.instance, reversed); int[] values = new int[]{ 1, 2, 2, 3 }; for (int i = 0; i < values.length; ++i) map.addColumn(new Column(ByteBufferUtil.bytes(values[reversed ? values.length - 1 - i : i])), HeapAllocator.instance); Iterator<Column> iter = map.iterator(); assertEquals("1st column", 1, iter.next().name().getInt(0)); assertEquals("2nd column", 2, iter.next().name().getInt(0)); assertEquals("3rd column", 3, iter.next().name().getInt(0)); }', 'ground_truth': 'private void testAddInternal(boolean reversed) { ColumnFamily map = ArrayBackedSortedColumns.factory.create(metadata(), reversed); int[] values = new int[]{ 1, 2, 2, 3 }; for (int i = 0; i < values.length; ++i) map.addColumn(new Column(ByteBufferUtil.bytes(values[reversed ? values.length - 1 - i : i])), HeapAllocator.instance); Iterator<Column> iter = map.iterator(); assertEquals("1st column", 1, iter.next().name().getInt(0)); assertEquals("2nd column", 2, iter.next().name().getInt(0)); assertEquals("3rd column", 3, iter.next().name().getInt(0)); }', 'output': 'private void testAddInternal(boolean reversed) { List<Column> map = ArrayBackedSortedColumns.factory().create(BytesType.instance, reversed); int[] values = new int[]{ 1, 2, 2, 3 }; for (int i = 0; i < values.length; ++i) map.add(new Column(ByteBufferUtil.bytes(values[reversed ? values.length - 1 - i : i]))); Iterator<Column> iter = map.iterator(); assertEquals("1st column", 1, iter.next().name().getInt(0)); assertEquals("2nd column", 2, iter.next().name().getInt(0)); assertEquals("3rd column", 3, iter.next().name().getInt(0)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void onMatch(RelOptRuleCall call) { final AggregateRelBase aggregate = call.rel(0); final FilterRelBase filter = call.rel(1); final BitSet filterColumns = RelOptUtil.InputFinder.bits(filter.getCondition()); final BitSet newGroupSet = BitSets.union(aggregate.getGroupSet(), filterColumns); final RelNode input = filter.getChild(); final Boolean unique = RelMetadataQuery.areColumnsUnique(input, newGroupSet); if (unique != null && unique) { return; } final AggregateRelBase newAggregate = aggregate.copy(aggregate.getTraitSet(), input, newGroupSet, aggregate.getAggCallList()); final Mappings.TargetMapping mapping = Mappings.target( new Function<Integer, Integer>() { public Integer apply(Integer a0) { return BitSets.toList(newGroupSet).indexOf(a0); } }, input.getRowType().getFieldCount(), newGroupSet.cardinality()); final RexNode newCondition = RexUtil.apply(mapping, filter.getCondition()); final FilterRelBase newFilter = filter.copy(filter.getTraitSet(), newAggregate, newCondition); if (BitSets.contains(aggregate.getGroupSet(), filterColumns)) { assert newGroupSet.equals(aggregate.getGroupSet()); call.transformTo(newFilter); } else { final BitSet topGroupSet = new BitSet(); for (int c : BitSets.toIter(aggregate.getGroupSet())) { topGroupSet.set(BitSets.toList(newGroupSet).indexOf(c)); } final List<AggregateCall> topAggCallList = Lists.newArrayList(); int i = newGroupSet.cardinality(); for (AggregateCall aggregateCall : aggregate.getAggCallList()) { final Aggregation rollup = SubstitutionVisitor.getRollup(aggregateCall.getAggregation()); if (rollup == null) { return; } if (aggregateCall.isDistinct()) { return; } topAggCallList.add( new AggregateCall(rollup, aggregateCall.isDistinct(), ImmutableList.of(i++), aggregateCall.type, aggregateCall.name)); } final XXX topAggregate = aggregate.copy(aggregate.getTraitSet(), newFilter, topGroupSet, topAggCallList); call.transformTo(topAggregate); } }', 'ground_truth': 'public void onMatch(RelOptRuleCall call) { final Aggregate aggregate = call.rel(0); final Filter filter = call.rel(1); final BitSet filterColumns = RelOptUtil.InputFinder.bits(filter.getCondition()); final BitSet newGroupSet = BitSets.union(aggregate.getGroupSet(), filterColumns); final RelNode input = filter.getInput(); final Boolean unique = RelMetadataQuery.areColumnsUnique(input, newGroupSet); if (unique != null && unique) { return; } final Aggregate newAggregate = aggregate.copy(aggregate.getTraitSet(), input, newGroupSet, aggregate.getAggCallList()); final Mappings.TargetMapping mapping = Mappings.target( new Function<Integer, Integer>() { public Integer apply(Integer a0) { return BitSets.toList(newGroupSet).indexOf(a0); } }, input.getRowType().getFieldCount(), newGroupSet.cardinality()); final RexNode newCondition = RexUtil.apply(mapping, filter.getCondition()); final Filter newFilter = filter.copy(filter.getTraitSet(), newAggregate, newCondition); if (BitSets.contains(aggregate.getGroupSet(), filterColumns)) { assert newGroupSet.equals(aggregate.getGroupSet()); call.transformTo(newFilter); } else { final BitSet topGroupSet = new BitSet(); for (int c : BitSets.toIter(aggregate.getGroupSet())) { topGroupSet.set(BitSets.toList(newGroupSet).indexOf(c)); } final List<AggregateCall> topAggCallList = Lists.newArrayList(); int i = newGroupSet.cardinality(); for (AggregateCall aggregateCall : aggregate.getAggCallList()) { final SqlAggFunction rollup = SubstitutionVisitor.getRollup(aggregateCall.getAggregation()); if (rollup == null) { return; } if (aggregateCall.isDistinct()) { return; } topAggCallList.add( new AggregateCall(rollup, aggregateCall.isDistinct(), ImmutableList.of(i++), aggregateCall.type, aggregateCall.name)); } final Aggregate topAggregate = aggregate.copy(aggregate.getTraitSet(), newFilter, topGroupSet, topAggCallList); call.transformTo(topAggregate); } }', 'output': 'public void onMatch(RelOptRuleCall call) { final AggregateRelBase aggregate = call.rel(0); final FilterRelBase filter = call.rel(1); final BitSet filterColumns = RelOptUtil.InputFinder.bits(filter.getCondition()); final BitSet newGroupSet = BitSets.union(aggregate.getGroupSet(), filterColumns); final RelNode input = filter.getChild(); final Boolean unique = RelMetadataQuery.areColumnsUnique(input, newGroupSet); if (unique != null && unique) { return; } final AggregateRelBase newAggregate = aggregate.copy(aggregate.getTraitSet(), input, newGroupSet, aggregate.getAggCallList()); final Mappings.TargetMapping mapping = Mappings.target( new Function<Integer, Integer>() { public Integer apply(Integer a0) { return BitSets.toList(newGroupSet).indexOf(a0); } }, input.getRowType().getFieldCount(), newGroupSet.cardinality()); final RexNode newCondition = RexUtil.apply(mapping, filter.getCondition()); final FilterRelBase newFilter = filter.copy(filter.getTraitSet(), newAggregate, newCondition); if (BitSets.contains(aggregate.getGroupSet(), filterColumns)) { assert newGroupSet.equals(aggregate.getGroupSet()); call.transformTo(newFilter); } else { final BitSet topGroupSet = new BitSet(); for (int c : BitSets.toIter(aggregate.getGroupSet())) { topGroupSet.set(BitSets.toList(newGroupSet).indexOf(c)); } final List<AggregateCall> topAggCallList = Lists.newArrayList(); int i = newGroupSet.cardinality(); for (AggregateCall aggregateCall : aggregate.getAggCallList()) { final Aggregation rollup = SubstitutionVisitor.getRollup(aggregateCall.getAggregation()); if (rollup == null) { return; } if (aggregateCall.isDistinct()) { return; } topAggCallList.add( new AggregateCall(rollup, aggregateCall.isDistinct(), ImmutableList.of(i++), aggregateCall.type, aggregate'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAsync() throws Exception { preloadMode = ASYNC; batchSize = 256; try { GridCache<Integer, String> cache1 = startGrid(1).cache(null); int keyCnt = 2000; for (int i = 0; i < keyCnt; i++) cache1.put(i, "val" + i); IgniteCache<Integer, String> cache2 = startGrid(2).jcache(null); int size = cache2.localSize(); info("Size of cache2: " + size); assert waitCacheSize(cache2, keyCnt, getTestTimeout()) : "Actual cache size: " + cache2.localSize(); } finally { stopAllGrids(); } }', 'ground_truth': 'public void testAsync() throws Exception { preloadMode = ASYNC; batchSize = 256; try { IgniteCache<Integer, String> cache1 = startGrid(1).jcache(null); int keyCnt = 2000; for (int i = 0; i < keyCnt; i++) cache1.put(i, "val" + i); IgniteCache<Integer, String> cache2 = startGrid(2).jcache(null); int size = cache2.localSize(); info("Size of cache2: " + size); assert waitCacheSize(cache2, keyCnt, getTestTimeout()) : "Actual cache size: " + cache2.localSize(); } finally { stopAllGrids(); } }', 'output': 'public void testAsync() throws Exception { preloadMode = ASYNC; batchSize = 256; try { Cache<Integer, String> cache1 = startGrid(1).cache(null); int keyCnt = 2000; for (int i = 0; i < keyCnt; i++) cache1.put(i, "val" + i); IgniteCache<Integer, String> cache2 = startGrid(2).jcache(null); int size = cache2.localSize(); info("Size of cache2: " + size); assert waitCacheSize(cache2, keyCnt, getTestTimeout()) : "Actual cache size: " + cache2.localSize(); } finally { stopAllGrids(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void registerCoder() throws IOException { Coder<?> coder = KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(SetCoder.of(ByteArrayCoder.of()))); String id = components.registerCoder(coder); assertThat(components.registerCoder(coder), equalTo(id)); assertThat(id, not(isEmptyOrNullString())); XXX otherCoder = VarLongCoder.of(); assertThat(components.registerCoder(otherCoder), not(equalTo(id))); components.toComponents().getCodersOrThrow(id); components.toComponents().getCodersOrThrow(components.registerCoder(otherCoder)); } public void registerCoderEqualsNotSame() throws IOException { Coder<?> coder = KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(SetCoder.of(ByteArrayCoder.of()))); Coder<?> otherCoder = KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(SetCoder.of(ByteArrayCoder.of()))); assertThat(coder, equalTo(otherCoder)); String id = components.registerCoder(coder); String otherId = components.registerCoder(otherCoder); assertThat(otherId, not(equalTo(id))); components.toComponents().getCodersOrThrow(id); components.toComponents().getCodersOrThrow(otherId); }', 'ground_truth': 'public void registerCoder() throws IOException { Coder<?> coder = KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(SetCoder.of(ByteArrayCoder.of()))); String id = components.registerCoder(coder); assertThat(components.registerCoder(coder), equalTo(id)); assertThat(id, not(isEmptyOrNullString())); Coder<?> equalCoder = KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(SetCoder.of(ByteArrayCoder.of()))); assertThat(components.registerCoder(equalCoder), equalTo(id)); Coder<?> otherCoder = VarLongCoder.of(); assertThat(components.registerCoder(otherCoder), not(equalTo(id))); components.toComponents().getCodersOrThrow(id); components.toComponents().getCodersOrThrow(components.registerCoder(otherCoder)); }', 'output': 'public void registerCoder() throws IOException { Coder<?> coder = KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(SetCoder.of(ByteArrayCoder.of()))); String id = components.registerCoder(coder); assertThat(components.registerCoder(coder), equalTo(id)); assertThat(id, not(isEmptyOrNullString())); ByteArrayCoder otherCoder = ByteArrayCoder.of(); assertThat(components.registerCoder(otherCoder), not(equalTo(id))); components.toComponents().getCodersOrThrow(id); components.toComponents().getCodersOrThrow(components.registerCoder(otherCoder)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private List<Pair<Long, Long>> getTransferSections(CompressionMetadata.Chunk[] chunks) { List<Pair<Long, Long>> transferSections = new ArrayList<>(); Pair<Long, Long> lastSection = null; for (CompressionMetadata.Chunk chunk : chunks) { if (lastSection != null) { if (chunk.offset == lastSection.right) { lastSection = Pair.create(lastSection.left, chunk.offset + chunk.length + 4); } else { transferSections.add(lastSection); lastSection = Pair.create(chunk.offset, chunk.offset + chunk.length + 4); } } else { lastSection = Pair.create(chunk.offset, chunk.offset + chunk.length + 4); } } if (lastSection != null) transferSections.add(lastSection); return transferSections; }', 'ground_truth': 'private List<SSTableReader.PartitionPositionBounds> getTransferSections(CompressionMetadata.Chunk[] chunks) { List<SSTableReader.PartitionPositionBounds> transferSections = new ArrayList<>(); SSTableReader.PartitionPositionBounds lastSection = null; for (CompressionMetadata.Chunk chunk : chunks) { if (lastSection != null) { if (chunk.offset == lastSection.upperPosition) { lastSection = new SSTableReader.PartitionPositionBounds(lastSection.lowerPosition, chunk.offset + chunk.length + 4); } else { transferSections.add(lastSection); lastSection = new SSTableReader.PartitionPositionBounds(chunk.offset, chunk.offset + chunk.length + 4); } } else { lastSection = new SSTableReader.PartitionPositionBounds(chunk.offset, chunk.offset + chunk.length + 4); } } if (lastSection != null) transferSections.add(lastSection); return transferSections; }', 'output': 'private List<Pair<Long, Long>> getTransferSections(List<CompressionMetadata.Chunk> chunks) { List<Pair<Long, Long>> transferSections = new ArrayList<>(); Pair<Long, Long> lastSection = null; for (CompressionMetadata.Chunk chunk : chunks) { if (lastSection != null) { if (chunk.offset == lastSection.right) { lastSection = Pair.create(lastSection.left, chunk.offset + chunk.length + 4); } else { transferSections.add(lastSection); lastSection = Pair.create(chunk.offset, chunk.offset + chunk.length + 4); } } else { lastSection = Pair.create(chunk.offset, chunk.offset + chunk.length + 4); } } if (lastSection != null) transferSections.add(lastSection); return transferSections; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void cleanupCompactionFinalized() throws NoSuchRepairSessionException { XXX repairID = registerSession(cfs, true, true); LocalSessionAccessor.prepareUnsafe(repairID, COORDINATOR, PARTICIPANTS); SSTableReader sstable = makeSSTable(true); mutateRepaired(sstable, repairID, false); csm.handleNotification(new SSTableAddedNotification(Collections.singleton(sstable), null), cfs.getTracker()); LocalSessionAccessor.finalizeUnsafe(repairID); Assert.assertTrue(hasPendingStrategiesFor(repairID)); Assert.assertFalse(hasTransientStrategiesFor(repairID)); Assert.assertTrue(pendingContains(sstable)); Assert.assertTrue(sstable.isPendingRepair()); Assert.assertFalse(sstable.isRepaired()); cfs.getCompactionStrategyManager().enable(); AbstractCompactionTask compactionTask = csm.getNextBackgroundTask(FBUtilities.nowInSeconds()); Assert.assertNotNull(compactionTask); Assert.assertSame(PendingRepairManager.RepairFinishedCompactionTask.class, compactionTask.getClass()); compactionTask.execute(ActiveCompactionsTracker.NOOP); Assert.assertTrue(repairedContains(sstable)); Assert.assertFalse(unrepairedContains(sstable)); Assert.assertFalse(pendingContains(sstable)); Assert.assertFalse(hasPendingStrategiesFor(repairID)); Assert.assertFalse(hasTransientStrategiesFor(repairID)); long expectedRepairedAt = ActiveRepairService.instance.getParentRepairSession(repairID).repairedAt; Assert.assertFalse(sstable.isPendingRepair()); Assert.assertTrue(sstable.isRepaired()); Assert.assertEquals(expectedRepairedAt, sstable.getSSTableMetadata().repairedAt); }', 'ground_truth': 'public void cleanupCompactionFinalized() throws NoSuchRepairSessionException { TimeUUID repairID = registerSession(cfs, true, true); LocalSessionAccessor.prepareUnsafe(repairID, COORDINATOR, PARTICIPANTS); SSTableReader sstable = makeSSTable(true); mutateRepaired(sstable, repairID, false); csm.handleNotification(new SSTableAddedNotification(Collections.singleton(sstable), null), cfs.getTracker()); LocalSessionAccessor.finalizeUnsafe(repairID); Assert.assertTrue(hasPendingStrategiesFor(repairID)); Assert.assertFalse(hasTransientStrategiesFor(repairID)); Assert.assertTrue(pendingContains(sstable)); Assert.assertTrue(sstable.isPendingRepair()); Assert.assertFalse(sstable.isRepaired()); cfs.getCompactionStrategyManager().enable(); AbstractCompactionTask compactionTask = csm.getNextBackgroundTask(FBUtilities.nowInSeconds()); Assert.assertNotNull(compactionTask); Assert.assertSame(PendingRepairManager.RepairFinishedCompactionTask.class, compactionTask.getClass()); compactionTask.execute(ActiveCompactionsTracker.NOOP); Assert.assertTrue(repairedContains(sstable)); Assert.assertFalse(unrepairedContains(sstable)); Assert.assertFalse(pendingContains(sstable)); Assert.assertFalse(hasPendingStrategiesFor(repairID)); Assert.assertFalse(hasTransientStrategiesFor(repairID)); long expectedRepairedAt = ActiveRepairService.instance.getParentRepairSession(repairID).repairedAt; Assert.assertFalse(sstable.isPendingRepair()); Assert.assertTrue(sstable.isRepaired()); Assert.assertEquals(expectedRepairedAt, sstable.getSSTableMetadata().repairedAt); }', 'output': 'public void cleanupCompactionFinalized() throws NoSuchRepairSessionException { int repairID = registerSession(cfs, true, true); LocalSessionAccessor.prepareUnsafe(repairID, COORDINATOR, PARTICIPANTS); SSTableReader sstable = makeSSTable(true); mutateRepaired(sstable, repairID, false); csm.handleNotification(new SSTableAddedNotification(Collections.singleton(sstable), null), cfs.getTracker()); LocalSessionAccessor.finalizeUnsafe(repairID); Assert.assertTrue(hasPendingStrategiesFor(repairID)); Assert.assertFalse(hasTransientStrategiesFor(repairID)); Assert.assertTrue(pendingContains(sstable)); Assert.assertTrue(sstable.isPendingRepair()); Assert.assertFalse(sstable.isRepaired()); cfs.getCompactionStrategyManager().enable(); AbstractCompactionTask compactionTask = csm.getNextBackgroundTask(FBUtilities.nowInSeconds()); Assert.assertNotNull(compactionTask); Assert.assertSame(PendingRepairManager.RepairFinishedCompactionTask.class, compactionTask.getClass()); compactionTask.execute(ActiveCompactionsTracker.NOOP); Assert.assertTrue(repairedContains(sstable)); Assert.assertFalse(unrepairedContains(sstable)); Assert.assertFalse(pendingContains(sstable)); Assert.assertFalse(hasPendingStrategiesFor(repairID)); Assert.assertFalse(hasTransientStrategiesFor(repairID)); long expectedRepairedAt = ActiveRepairService.instance.getParentRepairSession(repairID).repairedAt; Assert.assertFalse(sstable.isPendingRepair()); Assert.assertTrue(sstable.isRepaired()); Assert.assertEquals(expectedRepairedAt, sstable.getSSTableMetadata().repairedAt); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected String add_escapes(String str) { XXX retval = new StringBuffer(); char ch; for (int i = 0; i < str.length(); i++) { switch (str.charAt(i)) { case 0 : continue; case \'\\b\': retval.append("\\\\b"); continue; case \'\\t\': retval.append("\\\\t"); continue; case \'\\n\': retval.append("\\\\n"); continue; case \'\\f\': retval.append("\\\\f"); continue; case \'\\r\': retval.append("\\\\r"); continue; case \'\\"\': retval.append("\\\\\\""); continue; case \'\\\'\': retval.append("\\\\\\\'"); continue; case \'\\\\\': retval.append("\\\\\\\\"); continue; default: if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) { String s = "0000" + Integer.toString(ch, 16); retval.append("\\\\u" + s.substring(s.length() - 4, s.length())); } else { retval.append(ch); } continue; } } return retval.toString(); }', 'ground_truth': 'protected String add_escapes(String str) { StringBuilder retval = new StringBuilder(); char ch; for (int i = 0; i < str.length(); i++) { switch (str.charAt(i)) { case 0 : continue; case \'\\b\': retval.append("\\\\b"); continue; case \'\\t\': retval.append("\\\\t"); continue; case \'\\n\': retval.append("\\\\n"); continue; case \'\\f\': retval.append("\\\\f"); continue; case \'\\r\': retval.append("\\\\r"); continue; case \'\\"\': retval.append("\\\\\\""); continue; case \'\\\'\': retval.append("\\\\\\\'"); continue; case \'\\\\\': retval.append("\\\\\\\\"); continue; default: if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) { String s = "0000" + Integer.toString(ch, 16); retval.append("\\\\u" + s.substring(s.length() - 4, s.length())); } else { retval.append(ch); } continue; } } return retval.toString(); }', 'output': 'protected String add_escapes(String str) { StringBuilder retval = new StringBuilder(); char ch; for (int i = 0; i < str.length(); i++) { switch (str.charAt(i)) { case 0 : continue; case \'\\b\': retval.append("\\\\b"); continue; case \'\\t\': retval.append("\\\\t"); continue; case \'\\n\': retval.append("\\\\n"); continue; case \'\\f\': retval.append("\\\\f"); continue; case \'\\r\': retval.append("\\\\r"); continue; case \'\\"\': retval.append("\\\\\\""); continue; case \'\\\'\': retval.append("\\\\\\\'"); continue; case \'\\\\\': retval.append("\\\\\\\\"); continue; default: if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) { String s = "0000" + Integer.toString(ch, 16); retval.append("\\\\u" + s.substring(s.length() - 4, s.length())); } else { retval.append(ch); } continue; } } return retval.toString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void whenDomainObjectAndObjectTypeSetToTrue() { facetFactory.process(new ProcessObjectSpecIdContext(CustomerWithDomainObjectAndObjectTypeSet.class, facetHolder)); final Facet facet = facetHolder.getFacet(ObjectSpecIdFacet.class); Assert.assertNotNull(facet); Assert.assertTrue(facet instanceof ObjectSpecIdFacetForDomainObjectAnnotation); final XXX facetForDomainObjectAnnotation = (ObjectSpecIdFacetForDomainObjectAnnotation) facet; assertThat(facetForDomainObjectAnnotation.value(), is("CUS")); expectNoMethodsRemoved(); }', 'ground_truth': 'public void whenDomainObjectAndObjectTypeSetToTrue() { facetFactory.process(new ProcessObjectTypeContext(CustomerWithDomainObjectAndObjectTypeSet.class, facetHolder)); final Facet facet = facetHolder.getFacet(ObjectTypeFacet.class); Assert.assertNotNull(facet); Assert.assertTrue(facet instanceof ObjectTypeFacetForDomainObjectAnnotation); final ObjectTypeFacetForDomainObjectAnnotation facetForDomainObjectAnnotation = (ObjectTypeFacetForDomainObjectAnnotation) facet; assertThat(facetForDomainObjectAnnotation.value(), is("CUS")); expectNoMethodsRemoved(); }', 'output': 'public void whenDomainObjectAndObjectTypeSetToTrue() { facetFactory.process(new ProcessObjectSpecIdContext(CustomerWithDomainObjectAndObjectTypeSet.class, facetHolder)); final Facet facet = facetHolder.getFacet(ObjectSpecIdFacet.class); Assert.assertNotNull(facet); Assert.assertTrue(facet instanceof ObjectSpecIdFacetForDomainObjectAnnotation); final ObjectSpecIdFacetForDomainObjectAnnotation facetForDomainObjectAnnotation = (ObjectSpecIdFacetForDomainObjectAnnotation) facet; assertThat(facetForDomainObjectAnnotation.value(), is("CUS")); expectNoMethodsRemoved(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static String XMLEncNQG(String s) { int ln = s.length(); for (int i = 0; i < ln; i++) { char c = s.charAt(i); if (c == \'<\' || (c == \'>\' && i > 1 && s.charAt(i - 1) == \']\' && s.charAt(i - 2) == \']\') || c == \'&\') { XXX b = new StringBuffer(s.substring(0, i)); switch (c) { case \'<\': b.append("&lt;"); break; case \'>\': b.append("&gt;"); break; case \'&\': b.append("&amp;"); break; default: throw new BugException(); } i++; int next = i; while (i < ln) { c = s.charAt(i); if (c == \'<\' || (c == \'>\' && i > 1 && s.charAt(i - 1) == \']\' && s.charAt(i - 2) == \']\') || c == \'&\') { b.append(s.substring(next, i)); switch (c) { case \'<\': b.append("&lt;"); break; case \'>\': b.append("&gt;"); break; case \'&\': b.append("&amp;"); break; default: throw new BugException(); } next = i + 1; } i++; } if (next < ln) { b.append(s.substring(next)); } s = b.toString(); break; } } return s; }', 'ground_truth': 'public static String XMLEncNQG(String s) { int ln = s.length(); for (int i = 0; i < ln; i++) { char c = s.charAt(i); if (c == \'<\' || (c == \'>\' && i > 1 && s.charAt(i - 1) == \']\' && s.charAt(i - 2) == \']\') || c == \'&\') { StringBuilder b = new StringBuilder(s.substring(0, i)); switch (c) { case \'<\': b.append("&lt;"); break; case \'>\': b.append("&gt;"); break; case \'&\': b.append("&amp;"); break; default: throw new BugException(); } i++; int next = i; while (i < ln) { c = s.charAt(i); if (c == \'<\' || (c == \'>\' && i > 1 && s.charAt(i - 1) == \']\' && s.charAt(i - 2) == \']\') || c == \'&\') { b.append(s.substring(next, i)); switch (c) { case \'<\': b.append("&lt;"); break; case \'>\': b.append("&gt;"); break; case \'&\': b.append("&amp;"); break; default: throw new BugException(); } next = i + 1; } i++; } if (next < ln) { b.append(s.substring(next)); } s = b.toString(); break; } } return s; }', 'output': 'public static String XMLEncNQG(String s) { int ln = s.length(); for (int i = 0; i < ln; i++) { char c = s.charAt(i); if (c == \'<\' || (c == \'>\' && i > 1 && s.charAt(i - 1) == \']\' && s.charAt(i - 2) == \']\') || c == \'&\') { StringBuffer b = new StringBuffer(s.substring(0, i)); switch (c) { case \'<\': b.append("&lt;"); break; case \'>\': b.append("&gt;"); break; case \'&\': b.append("&amp;"); break; default: throw new BugException(); } i++; int next = i; while (i < ln) { c = s.charAt(i); if (c == \'<\' || (c == \'>\' && i > 1 && s.charAt(i - 1) == \']\' && s.charAt(i - 2) == \']\') || c == \'&\') { b.append(s.substring(next, i)); switch (c) { case \'<\': b.append("&lt;"); break; case \'>\': b.append("&gt;"); break; case \'&\': b.append("&amp;"); break; default: throw new BugException(); } next = i + 1; } i++; } if (next < ln) { b.append(s.substring(next)); } s = b.toString(); break; } } return s; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private ClusteringIndexNamesFilter reduceFilter(ClusteringIndexNamesFilter filter, Partition result, long sstableTimestamp) { if (result == null) return filter; SearchIterator<Clustering, Row> searchIter = result.searchIterator(columnFilter(), false); XXX columns = columnFilter().fetchedColumns(); NavigableSet<Clustering> clusterings = filter.requestedRows(); boolean removeStatic = false; if (!columns.statics.isEmpty()) { Row staticRow = searchIter.next(Clustering.STATIC_CLUSTERING); removeStatic = staticRow != null && canRemoveRow(staticRow, columns.statics, sstableTimestamp); } NavigableSet<Clustering> toRemove = null; for (Clustering clustering : clusterings) { if (!searchIter.hasNext()) break; Row row = searchIter.next(clustering); if (row == null || !canRemoveRow(row, columns.regulars, sstableTimestamp)) continue; if (toRemove == null) toRemove = new TreeSet<>(result.metadata().comparator); toRemove.add(clustering); } if (!removeStatic && toRemove == null) return filter; boolean hasNoMoreStatic = columns.statics.isEmpty() || removeStatic; boolean hasNoMoreClusterings = clusterings.isEmpty() || (toRemove != null && toRemove.size() == clusterings.size()); if (hasNoMoreStatic && hasNoMoreClusterings) return null; if (toRemove != null) { BTreeSet.Builder<Clustering> newClusterings = BTreeSet.builder(result.metadata().comparator); newClusterings.addAll(Sets.difference(clusterings, toRemove)); clusterings = newClusterings.build(); } return new ClusteringIndexNamesFilter(clusterings, filter.isReversed()); }', 'ground_truth': 'private ClusteringIndexNamesFilter reduceFilter(ClusteringIndexNamesFilter filter, Partition result, long sstableTimestamp) { if (result == null) return filter; SearchIterator<Clustering, Row> searchIter = result.searchIterator(columnFilter(), false); RegularAndStaticColumns columns = columnFilter().fetchedColumns(); NavigableSet<Clustering> clusterings = filter.requestedRows(); boolean removeStatic = false; if (!columns.statics.isEmpty()) { Row staticRow = searchIter.next(Clustering.STATIC_CLUSTERING); removeStatic = staticRow != null && canRemoveRow(staticRow, columns.statics, sstableTimestamp); } NavigableSet<Clustering> toRemove = null; for (Clustering clustering : clusterings) { if (!searchIter.hasNext()) break; Row row = searchIter.next(clustering); if (row == null || !canRemoveRow(row, columns.regulars, sstableTimestamp)) continue; if (toRemove == null) toRemove = new TreeSet<>(result.metadata().comparator); toRemove.add(clustering); } if (!removeStatic && toRemove == null) return filter; boolean hasNoMoreStatic = columns.statics.isEmpty() || removeStatic; boolean hasNoMoreClusterings = clusterings.isEmpty() || (toRemove != null && toRemove.size() == clusterings.size()); if (hasNoMoreStatic && hasNoMoreClusterings) return null; if (toRemove != null) { BTreeSet.Builder<Clustering> newClusterings = BTreeSet.builder(result.metadata().comparator); newClusterings.addAll(Sets.difference(clusterings, toRemove)); clusterings = newClusterings.build(); } return new ClusteringIndexNamesFilter(clusterings, filter.isReversed()); }', 'output': 'private ClusteringIndexNamesFilter reduceFilter(ClusteringIndexNamesFilter filter, Partition result, long sstableTimestamp) { if (result == null) return filter; SearchIterator<Clustering, Row> searchIter = result.searchIterator(columnFilter(), false); List<Clustering> columns = columnFilter().fetchedColumns(); NavigableSet<Clustering> clusterings = filter.requestedRows(); boolean removeStatic = false; if (!columns.statics.isEmpty()) { Row staticRow = searchIter.next(Clustering.STATIC_CLUSTERING); removeStatic = staticRow != null && canRemoveRow(staticRow, columns.statics, sstableTimestamp); } NavigableSet<Clustering> toRemove = null; for (Clustering clustering : clusterings) { if (!searchIter.hasNext()) break; Row row = searchIter.next(clustering); if (row == null || !canRemoveRow(row, columns.regulars, sstableTimestamp)) continue; if (toRemove == null) toRemove = new TreeSet<>(result.metadata().comparator); toRemove.add(clustering); } if (!removeStatic && toRemove == null) return filter; boolean hasNoMoreStatic = columns.statics.isEmpty() || removeStatic; boolean hasNoMoreClusterings = clusterings.isEmpty() || (toRemove != null && toRemove.size() == clusterings.size()); if (hasNoMoreStatic && hasNoMoreClusterings) return null; if (toRemove != null) { BTreeSet.Builder<Clustering> newClusterings = BTreeSet.builder(result.metadata().comparator); newClusterings.addAll(Sets.difference(clusterings, toRemove)); clusterings = newClusterings.build(); } return new ClusteringIndexNamesFilter(clusterings, filter.isReversed()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static boolean areRowTypesEqual( RelDataType rowType1, RelDataType rowType2, boolean compareNames) { if (rowType1 == rowType2) { return true; } if (compareNames) { return false; } int n = rowType1.getFieldCount(); if (rowType2.getFieldCount() != n) { return false; } RelDataTypeField [] f1 = rowType1.getFields(); RelDataTypeField [] f2 = rowType2.getFields(); for (int i = 0; i < n; ++i) { if (!f1[i].getType().equals(f2[i].getType())) { return false; } } return true; }', 'ground_truth': 'public static boolean areRowTypesEqual( RelDataType rowType1, RelDataType rowType2, boolean compareNames) { if (rowType1 == rowType2) { return true; } if (compareNames) { return false; } int n = rowType1.getFieldCount(); if (rowType2.getFieldCount() != n) { return false; } List<RelDataTypeField> f1 = rowType1.getFieldList(); List<RelDataTypeField> f2 = rowType2.getFieldList(); for (int i = 0; i < n; ++i) { if (!f1.get(i).getType().equals(f2.get(i).getType())) { return false; } } return true; }', 'output': 'public static boolean areRowTypesEqual( RelDataType rowType1, RelDataType rowType2, boolean compareNames) { if (rowType1 == rowType2) { return true; } if (compareNames) { return false; } int n = rowType1.getFieldCount(); if (rowType2.getFieldCount() != n) { return false; } RelDataTypeField [] f1 = rowType1.getFields(); RelDataTypeField [] f2 = rowType2.getFields(); for (int i = 0; i < n; ++i) { if (!f1[i].getType().equals(f2[i].getType())) { return false; } } return true; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Pair<List<SSTableReader>, Map<UUID, LifecycleTransaction>> getCompactingAndNonCompactingSSTables() { List<SSTableReader> allCompacting = new ArrayList<>(); Map<UUID, LifecycleTransaction> allNonCompacting = new HashMap<>(); for (Keyspace ks : Keyspace.all()) { for (ColumnFamilyStore cfStore: ks.getColumnFamilyStores()) { Set<SSTableReader> nonCompacting, allSSTables; LifecycleTransaction txn = null; do { View view = cfStore.getTracker().getView(); allSSTables = ImmutableSet.copyOf(view.select(SSTableSet.CANONICAL)); nonCompacting = ImmutableSet.copyOf(view.getUncompacting(allSSTables)); } while (null == (txn = cfStore.getTracker().tryModify(nonCompacting, OperationType.UNKNOWN))); allNonCompacting.put(cfStore.metadata.cfId, txn); allCompacting.addAll(Sets.difference(allSSTables, nonCompacting)); } } return Pair.create(allCompacting, allNonCompacting); }', 'ground_truth': 'private Pair<List<SSTableReader>, Map<TableId, LifecycleTransaction>> getCompactingAndNonCompactingSSTables() { List<SSTableReader> allCompacting = new ArrayList<>(); Map<TableId, LifecycleTransaction> allNonCompacting = new HashMap<>(); for (Keyspace ks : Keyspace.all()) { for (ColumnFamilyStore cfStore: ks.getColumnFamilyStores()) { Set<SSTableReader> nonCompacting, allSSTables; LifecycleTransaction txn = null; do { View view = cfStore.getTracker().getView(); allSSTables = ImmutableSet.copyOf(view.select(SSTableSet.CANONICAL)); nonCompacting = ImmutableSet.copyOf(view.getUncompacting(allSSTables)); } while (null == (txn = cfStore.getTracker().tryModify(nonCompacting, OperationType.UNKNOWN))); allNonCompacting.put(cfStore.metadata.id, txn); allCompacting.addAll(Sets.difference(allSSTables, nonCompacting)); } } return Pair.create(allCompacting, allNonCompacting); }', 'output': 'private Pair<List<SSTableReader>, Map<String, LifecycleTransaction>> getCompactingAndNonCompactingSSTables() { List<SSTableReader> allCompacting = new ArrayList<>(); Map<String, LifecycleTransaction> allNonCompacting = new HashMap<>(); for (Keyspace ks : Keyspace.all()) { for (ColumnFamilyStore cfStore: ks.getColumnFamilyStores()) { Set<SSTableReader> nonCompacting, allSSTables; LifecycleTransaction txn = null; do { View view = cfStore.getTracker().getView(); allSSTables = ImmutableSet.copyOf(view.select(SSTableSet.CANONICAL)); nonCompacting = ImmutableSet.copyOf(view.getUncompacting(allSSTables)); } while (null == (txn = cfStore.getTracker().tryModify(nonCompacting, OperationType.UNKNOWN))); allNonCompacting.put(cfStore.metadata.cfId, txn); allCompacting.addAll(Sets.difference(allSSTables, nonCompacting)); } } return Pair.create(allCompacting, allNonCompacting); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Mutation without(UUID cfId) { Mutation mutation = new Mutation(keyspaceName, key); for (Map.Entry<UUID, ColumnFamily> entry : modifications.entrySet()) if (!entry.getKey().equals(cfId)) mutation.add(entry.getValue()); return mutation; }', 'ground_truth': 'public Mutation without(UUID cfId) { Mutation mutation = new Mutation(keyspaceName, key); for (Map.Entry<UUID, PartitionUpdate> entry : modifications.entrySet()) if (!entry.getKey().equals(cfId)) mutation.add(entry.getValue()); return mutation; }', 'output': 'public Mutation without(String cfName) { Mutation mutation = new Mutation(keyspaceName, key); for (Map.Entry<String, ColumnFamily> entry : modifications.entrySet()) if (!entry.getKey().equals(cfName)) mutation.add(entry.getValue()); return mutation; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Request findRequestWithMessageId(Long msgId) { GridCacheProjection<Object, Request> cache = ignite.cache(null).projection(Object.class, Request.class); GridCacheQuery<Map.Entry<Object, Request>> qry = cache.queries().createSqlQuery( Request.class, "messageId = ?"); try { Map.Entry<Object, Request> entry = F.first(qry.projection(ignite.cluster().forLocal()).execute(msgId).get()); if (entry == null) return null; return entry.getValue(); } catch (IgniteCheckedException e) { e.printStackTrace(); return null; } }', 'ground_truth': 'private Request findRequestWithMessageId(Long msgId) { CacheProjection<Object, Request> cache = ignite.cache(null).projection(Object.class, Request.class); CacheQuery<Map.Entry<Object, Request>> qry = cache.queries().createSqlQuery( Request.class, "messageId = ?"); try { Map.Entry<Object, Request> entry = F.first(qry.projection(ignite.cluster().forLocal()).execute(msgId).get()); if (entry == null) return null; return entry.getValue(); } catch (IgniteCheckedException e) { e.printStackTrace(); return null; } }', 'output': 'private Request findRequestWithMessageId(Long msgId) { GridCacheProjection<Object, Request> cache = ignite.cache(null).projection(Object.class, Request.class); GridCacheQuery<Map.Entry<Object, Request>> qry = cache.queries().createSqlQuery( Request.class, "messageId = ?"); try { Map.Entry<Object, Request> entry = F.first(qry.projection(ignite.cluster().forLocal()).execute(msgId).get()); if (entry == null) return null; return entry.getValue(); } catch (IgniteCheckedException e) { e.printStackTrace(); return null; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void writeToXML(PrintWriter pw) /*throws IOException */ { XXX it = new TreeSet(keySet()).iterator(); XMLMapAttr.writeHeading(pw); while (it.hasNext()) { String file = (String) it.next(); XMLMapAttr attr = (XMLMapAttr) get(file); if ((attr != null) && !attr.isEmpty()) { attr.write(pw, file, "    "); } } XMLMapAttr.writeEnding(pw); }', 'ground_truth': 'public void writeToXML(PrintWriter pw) /*throws IOException */ { Iterator<String> it = new TreeSet(keySet()).iterator(); XMLMapAttr.writeHeading(pw); while (it.hasNext()) { String file = it.next(); XMLMapAttr attr = (XMLMapAttr) get(file); if ((attr != null) && !attr.isEmpty()) { attr.write(pw, file, "    "); } } XMLMapAttr.writeEnding(pw); }', 'output': 'public void writeToXML(PrintWriter pw) throws IOException { Iterator<String> it = new TreeSet(keySet()).iterator(); XMLMapAttr.writeHeading(pw); while (it.hasNext()) { String file = (String) it.next(); XMLMapAttr attr = (XMLMapAttr) get(file); if ((attr != null) && !attr.isEmpty()) { attr.write(pw, file, "    "); } } XMLMapAttr.writeEnding(pw); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private DynamicDestinations<T, ?> resolveDynamicDestinations() { DynamicDestinations<T, ?> dynamicDestinations = getDynamicDestinations(); if (dynamicDestinations == null) { FilenamePolicy usedFilenamePolicy = getFilenamePolicy(); if (usedFilenamePolicy == null) { usedFilenamePolicy = DefaultFilenamePolicy.fromStandardParameters( getFilenamePrefix(), getShardTemplate(), getFilenameSuffix(), getWindowedWrites()); } dynamicDestinations = DynamicFileDestinations.constant(usedFilenamePolicy); } return dynamicDestinations; }', 'ground_truth': 'private DynamicDestinations<UserT, ?, String> resolveDynamicDestinations() { DynamicDestinations<UserT, ?, String> dynamicDestinations = getDynamicDestinations(); if (dynamicDestinations == null) { if (getDestinationFunction() != null) { dynamicDestinations = DynamicFileDestinations.toDefaultPolicies( getDestinationFunction(), getEmptyDestination(), getFormatFunction()); } else { FilenamePolicy usedFilenamePolicy = getFilenamePolicy(); if (usedFilenamePolicy == null) { usedFilenamePolicy = DefaultFilenamePolicy.fromStandardParameters( getFilenamePrefix(), getShardTemplate(), getFilenameSuffix(), getWindowedWrites()); } dynamicDestinations = DynamicFileDestinations.constant(usedFilenamePolicy, getFormatFunction()); } } return dynamicDestinations; }', 'output': 'private DynamicDestinations<T> resolveDynamicDestinations() { DynamicDestinations<T> dynamicDestinations = getDynamicDestinations(); if (dynamicDestinations == null) { FilenamePolicy usedFilenamePolicy = getFilenamePolicy(); if (usedFilenamePolicy == null) { usedFilenamePolicy = DefaultFilenamePolicy.fromStandardParameters( getFilenamePrefix(), getShardTemplate(), getFilenameSuffix(), getWindowedWrites()); } dynamicDestinations = DynamicFileDestinations.constant(usedFilenamePolicy); } return dynamicDestinations; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void visitArrayExpression(ArrayExpression expression) { XXX type = expression.getType().getComponentType(); String typeName = BytecodeHelper.getClassInternalName(type); Expression sizeExpression = expression.getSizeExpression(); int size=0; if (sizeExpression != null) { visitAndAutoboxBoolean(sizeExpression); asIntMethod.call(cv); } else { size = expression.getExpressions().size(); helper.pushConstant(size); } int storeIns=AASTORE; if (type.isPrimitiveType()) { int primType=0; if (type==Type.boolean_TYPE) { primType = T_BOOLEAN; storeIns = BASTORE; } else if (type==Type.char_TYPE) { primType = T_CHAR; storeIns = CASTORE; } else if (type==Type.float_TYPE) { primType = T_FLOAT; storeIns = FASTORE; } else if (type==Type.double_TYPE) { primType = T_DOUBLE; storeIns = DASTORE; } else if (type==Type.byte_TYPE) { primType = T_BYTE; storeIns = BASTORE; } else if (type==Type.short_TYPE) { primType = T_SHORT; storeIns = SASTORE; } else if (type==Type.int_TYPE) { primType = T_INT; storeIns=IASTORE; } else if (type==Type.long_TYPE) { primType = T_LONG; storeIns = LASTORE; } cv.visitIntInsn(NEWARRAY, primType); } else { cv.visitTypeInsn(ANEWARRAY, typeName); } for (int i = 0; i < size; i++) { cv.visitInsn(DUP); helper.pushConstant(i); Expression elementExpression = expression.getExpression(i); if (elementExpression == null) { ConstantExpression.NULL.visit(this); } else { if (!type.equals(elementExpression.getType())) { visitCastExpression(new CastExpression(type, elementExpression, true)); } else { visitAndAutoboxBoolean(elementExpression); } } cv.visitInsn(storeIns); } if (type.isPrimitiveType()) { int par = defineVariable("par",Type.OBJECT_TYPE).getIndex(); cv.visitVarInsn(ASTORE, par); cv.visitVarInsn(ALOAD, par); } }', 'ground_truth': 'public void visitArrayExpression(ArrayExpression expression) { ClassNode type = expression.getType().getComponentType(); String typeName = BytecodeHelper.getClassInternalName(type); Expression sizeExpression = expression.getSizeExpression(); int size=0; if (sizeExpression != null) { visitAndAutoboxBoolean(sizeExpression); asIntMethod.call(cv); } else { size = expression.getExpressions().size(); helper.pushConstant(size); } int storeIns=AASTORE; if (ClassHelper.isPrimitiveType(type)) { int primType=0; if (type==ClassHelper.boolean_TYPE) { primType = T_BOOLEAN; storeIns = BASTORE; } else if (type==ClassHelper.char_TYPE) { primType = T_CHAR; storeIns = CASTORE; } else if (type==ClassHelper.float_TYPE) { primType = T_FLOAT; storeIns = FASTORE; } else if (type==ClassHelper.double_TYPE) { primType = T_DOUBLE; storeIns = DASTORE; } else if (type==ClassHelper.byte_TYPE) { primType = T_BYTE; storeIns = BASTORE; } else if (type==ClassHelper.short_TYPE) { primType = T_SHORT; storeIns = SASTORE; } else if (type==ClassHelper.int_TYPE) { primType = T_INT; storeIns=IASTORE; } else if (type==ClassHelper.long_TYPE) { primType = T_LONG; storeIns = LASTORE; } cv.visitIntInsn(NEWARRAY, primType); } else { cv.visitTypeInsn(ANEWARRAY, typeName); } for (int i = 0; i < size; i++) { cv.visitInsn(DUP); helper.pushConstant(i); Expression elementExpression = expression.getExpression(i); if (elementExpression == null) { ConstantExpression.NULL.visit(this); } else { if (!type.equals(elementExpression.getType())) { visitCastExpression(new CastExpression(type, elementExpression, true)); } else { visitAndAutoboxBoolean(elementExpression); } } cv.visitInsn(storeIns); } if (ClassHelper.isPrimitiveType(type)) { int par = defineVariable("par",ClassHelper.OBJECT_TYPE).getIndex(); cv.visitVarInsn(ASTORE, par); cv.visitVarInsn(ALOAD, par); } }', 'output': 'public void visitArrayExpression(ArrayExpression expression) { Class<?> type = expression.getType().getComponentType(); String typeName = BytecodeHelper.getClassInternalName(type); Expression sizeExpression = expression.getSizeExpression(); int size=0; if (sizeExpression != null) { visitAndAutoboxBoolean(sizeExpression); asIntMethod.call(cv); } else { size = expression.getExpressions().size(); helper.pushConstant(size); } int storeIns=AASTORE; if (type.isPrimitiveType()) { int primType=0; if (type==Type.boolean_TYPE) { primType = T_BOOLEAN; storeIns = BASTORE; } else if (type==Type.char_TYPE) { primType = T_CHAR; storeIns = CASTORE; } else if (type==Type.float_TYPE) { primType = T_FLOAT; storeIns = FASTORE; } else if (type==Type.double_TYPE) { primType = T_DOUBLE; storeIns = DASTORE; } else if (type==Type.byte_TYPE) { primType = T_BYTE; storeIns = BASTORE; } else if (type==Type.short_TYPE) { primType = T_SHORT; storeIns = SASTORE; } else if (type==Type.int_TYPE) { primType = T_INT; storeIns=IASTORE; } else if (type==Type.long_TYPE) { primType = T_LONG; storeIns = LASTORE; } cv.visitIntInsn(NEWARRAY, primType); } else { cv.visitTypeInsn(ANEWARRAY, typeName); } for (int i = 0; i < size; i++) { cv.visitInsn(DUP); helper.pushConstant(i); Expression elementExpression = expression.getExpression(i); if (elementExpression == null) { ConstantExpression.NULL.visit(this); } else { if (!type.equals(elementExpression.getType())) { visitCastExpression(new CastExpression(type, elementExpression, true)); } else { visitAndAutoboxBoolean(elementExpression); } }'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSynchronousExecute() throws Exception { IgniteCompute comp = ignite.compute().enableAsync(); assertNull(comp.execute(GridTestTask.class,  "testArg")); XXX fut = comp.future(); assert fut != null; info("Task result: " + fut.get()); }', 'ground_truth': 'public void testSynchronousExecute() throws Exception { IgniteCompute comp = ignite.compute().enableAsync(); assertNull(comp.execute(GridTestTask.class,  "testArg")); ComputeTaskFuture<?> fut = comp.future(); assert fut != null; info("Task result: " + fut.get()); }', 'output': 'public void testSynchronousExecute() throws Exception { IgniteCompute comp = igniteCompute(); assertNull(comp.execute(GridTestTask.class, "testArg")); IgniteFuture<?> fut = comp.future(); assert fut != null; info("Task result: " + fut.get()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public synchronized Set<SubResourceDefinition> getSubResourceDefinitions(String viewName) { Set<SubResourceDefinition> subResourceDefinitions = subResourceDefinitionsMap.get(viewName); if (subResourceDefinitions == null) { subResourceDefinitions = new HashSet<SubResourceDefinition>(); XXX definition = getDefinition(viewName); if (definition != null) { for (Resource.Type type : definition.getViewResourceTypes()) { subResourceDefinitions.add(new SubResourceDefinition(type)); } } subResourceDefinitionsMap.put(viewName, subResourceDefinitions); } return subResourceDefinitions; }', 'ground_truth': 'public synchronized Set<SubResourceDefinition> getSubResourceDefinitions(String viewName) { Set<SubResourceDefinition> subResourceDefinitions = subResourceDefinitionsMap.get(viewName); if (subResourceDefinitions == null) { subResourceDefinitions = new HashSet<SubResourceDefinition>(); ViewEntity definition = getDefinition(viewName); if (definition != null) { for (Resource.Type type : definition.getViewResourceTypes()) { subResourceDefinitions.add(new SubResourceDefinition(type)); } } subResourceDefinitionsMap.put(viewName, subResourceDefinitions); } return subResourceDefinitions; }', 'output': 'public synchronized Set<SubResourceDefinition> getSubResourceDefinitions(String viewName) { Set<SubResourceDefinition> subResourceDefinitions = subResourceDefinitionsMap.get(viewName); if (subResourceDefinitions == null) { subResourceDefinitions = new HashSet<SubResourceDefinition>(); Definition definition = getDefinition(viewName); if (definition != null) { for (Resource.Type type : definition.getViewResourceTypes()) { subResourceDefinitions.add(new SubResourceDefinition(type)); } } subResourceDefinitionsMap.put(viewName, subResourceDefinitions); } return subResourceDefinitions; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void getResourcesTest(Authentication authentication) throws Exception { Injector injector = createInjector(); Users users = injector.getInstance(Users.class); if ("admin".equals(authentication.getName())) { XXX allUsers = Arrays.asList( createMockUser("User1"), createMockUser("User10"), createMockUser("User100"), createMockUser("admin") ); expect(users.getAllUsers()).andReturn(allUsers).atLeastOnce(); } else { expect(users.getAnyUser("User1")).andReturn(createMockUser("User1")).atLeastOnce(); } replayAll(); AmbariMetaInfo ambariMetaInfo = injector.getInstance(AmbariMetaInfo.class); ambariMetaInfo.init(); SecurityContextHolder.getContext().setAuthentication(authentication); AmbariManagementController managementController = injector.getInstance(AmbariManagementController.class); ResourceProvider provider = getResourceProvider(managementController); Set<String> propertyIds = new HashSet<>(); propertyIds.add(UserResourceProvider.USER_USERNAME_PROPERTY_ID); propertyIds.add(UserResourceProvider.USER_PASSWORD_PROPERTY_ID); Request request = PropertyHelper.getReadRequest(propertyIds); Set<Resource> resources = provider.getResources(request, null); if ("admin".equals(authentication.getName())) { List<String> expectedList = Arrays.asList("User1", "User10", "User100", "admin"); Assert.assertEquals(4, resources.size()); for (Resource resource : resources) { String userName = (String) resource.getPropertyValue(UserResourceProvider.USER_USERNAME_PROPERTY_ID); Assert.assertTrue(expectedList.contains(userName)); } } else { Assert.assertEquals(1, resources.size()); for (Resource resource : resources) { Assert.assertEquals("User1", resource.getPropertyValue(UserResourceProvider.USER_USERNAME_PROPERTY_ID)); } } verifyAll(); }', 'ground_truth': 'private void getResourcesTest(Authentication authentication) throws Exception { Injector injector = createInjector(); Users users = injector.getInstance(Users.class); if ("admin".equals(authentication.getName())) { UserEntity userEntity1 = createMockUserEntity("User1"); UserEntity userEntity10 = createMockUserEntity("User10"); UserEntity userEntity100 = createMockUserEntity("User100"); UserEntity userEntityAdmin = createMockUserEntity("admin"); List<UserEntity> allUsers = Arrays.asList(userEntity1, userEntity10, userEntity100, userEntityAdmin); expect(users.getAllUserEntities()).andReturn(allUsers).once(); expect(users.hasAdminPrivilege(userEntity1)).andReturn(false).once(); expect(users.hasAdminPrivilege(userEntity10)).andReturn(false).once(); expect(users.hasAdminPrivilege(userEntity100)).andReturn(false).once(); expect(users.hasAdminPrivilege(userEntityAdmin)).andReturn(true).once(); } else { UserEntity userEntity = createMockUserEntity("User1"); expect(users.getUserEntity("User1")).andReturn(userEntity).once(); expect(users.hasAdminPrivilege(userEntity)).andReturn(false).once(); } replayAll(); AmbariMetaInfo ambariMetaInfo = injector.getInstance(AmbariMetaInfo.class); ambariMetaInfo.init(); SecurityContextHolder.getContext().setAuthentication(authentication); ResourceProvider provider = getResourceProvider(injector); Set<String> propertyIds = new HashSet<>(); propertyIds.add(UserResourceProvider.USER_USERNAME_PROPERTY_ID); propertyIds.add(UserResourceProvider.USER_PASSWORD_PROPERTY_ID); Request request = PropertyHelper.getReadRequest(propertyIds); Set<Resource> resources = provider.getResources(request, null); if ("admin".equals(authentication.getName())) { List<String> expectedList = Arrays.asList("User1", "User10", "User100", "admin"); Assert.assertEquals(4, resources.size()); for (Resource resource : resources) { String userName = (String) resource.getPropertyValue(UserResourceProvider.USER_USERNAME_PROPERTY_ID); Assert.assertTrue(expectedList.contains(userName)); } } else { Assert.assertEquals(1, resources.size()); for (Resource resource : resources) { Assert.assertEquals("User1", resource.getPropertyValue(UserResourceProvider.USER_USERNAME_PROPERTY_ID)); } } verifyAll(); }', 'output': 'private void getResourcesTest(Authentication authentication) throws Exception { Injector injector = createInjector(); Users users = injector.getInstance(Users.class); if ("admin".equals(authentication.getName())) { List<MockUser> allUsers = Arrays.asList( createMockUser("User1"), createMockUser("User10"), createMockUser("User100"), createMockUser("admin") ); expect(users.getAllUsers()).andReturn(allUsers).atLeastOnce(); } else { expect(users.getAnyUser("User1")).andReturn(createMockUser("User1")).atLeastOnce(); } replayAll(); AmbariMetaInfo ambariMetaInfo = injector.getInstance(AmbariMetaInfo.class); ambariMetaInfo.init(); SecurityContextHolder.getContext().setAuthentication(authentication); AmbariManagementController managementController = injector.getInstance(AmbariManagementController.class); ResourceProvider provider = getResourceProvider(managementController); Set<String> propertyIds = new HashSet<>(); propertyIds.add(UserResourceProvider.USER_USERNAME_PROPERTY_ID); propertyIds.add(UserResourceProvider.USER_PASSWORD_PROPERTY_ID); Request request = PropertyHelper.getReadRequest(propertyIds); Set<Resource> resources = provider.getResources(request, null); if ("admin".equals(authentication.getName())) { List<String> expectedList = Arrays.asList("User1", "User10", "User100", "admin"); Assert.assertEquals(4, resources.size()); for (Resource resource : resources) { String userName = (String) resource.getPropertyValue(UserResourceProvider.USER_USERNAME_PROPERTY_ID); Assert.assertTrue(expectedList.contains(userName)); } } else { Assert.assertEquals(1, resources.size()); for (Resource resource : resources) { Assert.assertEquals("User1", resource.getPropertyValue(UserResourceProvider.USER_USERNAME_PROPERTY_ID)); } } verifyAll(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static RexNode createInputRef( RelNode rel, int i) { final RelDataTypeField [] fields = rel.getRowType().getFields(); if (i < 0) { i = fields.length + i; } return rel.getCluster().getRexBuilder().makeInputRef( fields[i].getType(), i); }', 'ground_truth': 'public static RexNode createInputRef( RelNode rel, int i) { final List<RelDataTypeField> fields = rel.getRowType().getFieldList(); if (i < 0) { i = fields.size() + i; } return rel.getCluster().getRexBuilder().makeInputRef( fields.get(i).getType(), i); }', 'output': 'public static RexNode createInputRef( RelNode rel, int i) { final List<RelDataTypeField> fields = rel.getRowType().getFields(); if (i < 0) { i = fields.size() + i; } return rel.getCluster().getRexBuilder().makeInputRef( fields.get(i).getType(), i); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void createSqlFunctions() throws SQLException, IgniteCheckedException { Class<?>[] idxCustomFuncClss = cfg.getIndexCustomFunctionClasses(); if (F.isEmpty(idxCustomFuncClss)) return; for (Class<?> cls : idxCustomFuncClss) { for (Method m : cls.getDeclaredMethods()) { XXX ann = m.getAnnotation(GridCacheQuerySqlFunction.class); if (ann != null) { int modifiers = m.getModifiers(); if (!Modifier.isStatic(modifiers) || !Modifier.isPublic(modifiers)) throw new IgniteCheckedException("Method " + m.getName() + " must be public static."); String alias = ann.alias().isEmpty() ? m.getName() : ann.alias(); String clause = "CREATE ALIAS " + alias + (ann.deterministic() ? " DETERMINISTIC FOR \\"" : " FOR \\"") + cls.getName() + \'.\' + m.getName() + \'"\'; Collection<String> schemas = new ArrayList<>(schemaNames); if (!schemaNames.contains(schema(null))) schemas.add(schema(null)); for (String schema : schemas) { Connection c = connectionForThread(schema); Statement s = c.createStatement(); s.execute(clause); s.close(); } } } } }', 'ground_truth': 'private void createSqlFunctions() throws SQLException, IgniteCheckedException { Class<?>[] idxCustomFuncClss = cfg.getIndexCustomFunctionClasses(); if (F.isEmpty(idxCustomFuncClss)) return; for (Class<?> cls : idxCustomFuncClss) { for (Method m : cls.getDeclaredMethods()) { CacheQuerySqlFunction ann = m.getAnnotation(CacheQuerySqlFunction.class); if (ann != null) { int modifiers = m.getModifiers(); if (!Modifier.isStatic(modifiers) || !Modifier.isPublic(modifiers)) throw new IgniteCheckedException("Method " + m.getName() + " must be public static."); String alias = ann.alias().isEmpty() ? m.getName() : ann.alias(); String clause = "CREATE ALIAS " + alias + (ann.deterministic() ? " DETERMINISTIC FOR \\"" : " FOR \\"") + cls.getName() + \'.\' + m.getName() + \'"\'; Collection<String> schemas = new ArrayList<>(schemaNames); if (!schemaNames.contains(schema(null))) schemas.add(schema(null)); for (String schema : schemas) { Connection c = connectionForThread(schema); Statement s = c.createStatement(); s.execute(clause); s.close(); } } } } }', 'output': 'private void createSqlFunctions() throws SQLException, IgniteCheckedException { Class<?>[] idxCustomFuncClss = cfg.getIndexCustomFunctionClasses(); if (F.isEmpty(idxCustomFuncClss)) return; for (Class<?> cls : idxCustomFuncClss) { for (Method m : cls.getDeclaredMethods()) { GridCacheQuerySqlFunction ann = m.getAnnotation(GridCacheQuerySqlFunction.class); if (ann != null) { int modifiers = m.getModifiers(); if (!Modifier.isStatic(modifiers) || !Modifier.isPublic(modifiers)) throw new IgniteCheckedException("Method " + m.getName() + " must be public static."); String alias = ann.alias().isEmpty() ? m.getName() : ann.alias(); String clause = "CREATE ALIAS " + alias + (ann.deterministic() ? " DETERMINISTIC FOR \\"" : " FOR \\"") + cls.getName() + \'.\' + m.getName() + \'"\'; Collection<String> schemas = new ArrayList<>(schemaNames); if (!schemaNames.contains(schema(null))) schemas.add(schema(null)); for (String schema : schemas) { Connection c = connectionForThread(schema); Statement s = c.createStatement(); s.execute(clause); s.close(); } } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void testAddMultinode(final boolean collocated) throws Exception { final String queueName = UUID.randomUUID().toString(); info("Queue name: " + queueName + ", collocated: " + collocated); try { Collection<IgniteFuture> futs = new ArrayList<>(); final int THREADS_PER_NODE = 3; final int ITEMS_PER_THREAD = 1000; for (int i = 0; i < GRID_CNT; i++) { final int idx = i; futs.add(GridTestUtils.runMultiThreadedAsync(new Callable<Void>() { @Override public Void call() throws Exception { GridCache cache = grid(idx).cache(null); XXX queue = cache.dataStructures().queue(queueName, 0, collocated, true); for (int i = 0; i < ITEMS_PER_THREAD; i++) assertTrue(queue.add(i)); return null; } }, THREADS_PER_NODE, "testPutMultiNode")); } for (IgniteFuture fut : futs) fut.get(); GridCache cache = grid(0).cache(null); XXX queue = cache.dataStructures().queue(queueName, 0, collocated, true); assertEquals(THREADS_PER_NODE * ITEMS_PER_THREAD * GRID_CNT, queue.size()); int[] items = new int[ITEMS_PER_THREAD]; Integer item; while ((item = queue.poll()) != null) items[item]++; for (int i = 0; i < ITEMS_PER_THREAD; i++) assertEquals(THREADS_PER_NODE * GRID_CNT, items[i]); } finally { grid(0).cache(null).dataStructures().removeQueue(queueName); } }', 'ground_truth': 'private void testAddMultinode(final boolean collocated) throws Exception { final String queueName = UUID.randomUUID().toString(); info("Queue name: " + queueName + ", collocated: " + collocated); try { Collection<IgniteFuture> futs = new ArrayList<>(); final int THREADS_PER_NODE = 3; final int ITEMS_PER_THREAD = 1000; for (int i = 0; i < GRID_CNT; i++) { final int idx = i; futs.add(GridTestUtils.runMultiThreadedAsync(new Callable<Void>() { @Override public Void call() throws Exception { GridCache cache = grid(idx).cache(null); IgniteQueue<Integer> queue = cache.dataStructures().queue(queueName, 0, collocated, true); for (int i = 0; i < ITEMS_PER_THREAD; i++) assertTrue(queue.add(i)); return null; } }, THREADS_PER_NODE, "testPutMultiNode")); } for (IgniteFuture fut : futs) fut.get(); GridCache cache = grid(0).cache(null); IgniteQueue<Integer> queue = cache.dataStructures().queue(queueName, 0, collocated, true); assertEquals(THREADS_PER_NODE * ITEMS_PER_THREAD * GRID_CNT, queue.size()); int[] items = new int[ITEMS_PER_THREAD]; Integer item; while ((item = queue.poll()) != null) items[item]++; for (int i = 0; i < ITEMS_PER_THREAD; i++) assertEquals(THREADS_PER_NODE * GRID_CNT, items[i]); } finally { grid(0).cache(null).dataStructures().removeQueue(queueName); } }', 'output': 'private void testAddMultinode(final boolean collocated) throws Exception { final String queueName = UUID.randomUUID().toString(); info("Queue name: " + queueName + ", collocated: " + collocated); try { Collection<IgniteFuture> futs = new ArrayList<>(); final int THREADS_PER_NODE = 3; final int ITEMS_PER_THREAD = 1000; for (int i = 0; i < GRID_CNT; i++) { final int idx = i; futs.add(GridTestUtils.runMultiThreadedAsync(new Callable<Void>() { @Override public Void call() throws Exception { GridCache cache = grid(idx).cache(null); DataStructures dataStructures = cache.dataStructures(); Queue queue = dataStructures.queue(queueName, 0, collocated, true); for (int i = 0; i < ITEMS_PER_THREAD; i++) assertTrue(queue.add(i)); return null; } }, THREADS_PER_NODE, "testPutMultiNode")); } for (IgniteFuture fut : futs) fut.get(); GridCache cache = grid(0).cache(null); DataStructures dataStructures = cache.dataStructures(); Queue queue = dataStructures.queue(queueName, 0, collocated, true); assertEquals(THREADS_PER_NODE * ITEMS_PER_THREAD * GRID_CNT, queue.size()); int[] items = new int[ITEMS_PER_THREAD]; Integer item; while ((item = queue.poll()) != null) items[item]++; for (int i = 0; i < ITEMS_PER_THREAD; i++) assertEquals(THREADS_PER_NODE * GRID_CNT, items[i]); } finally { grid(0).cache(null).dataStructures().removeQueue(queueName); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void visitArrayExpression(ArrayExpression expression) { ClassNode elementType = expression.getElementType(); String arrayTypeName = BytecodeHelper.getClassInternalName(elementType); XXX sizeExpression = expression.getSizeExpression(); int size = 0; int dimensions = 0; if (sizeExpression != null) { for (Iterator iter = sizeExpression.iterator(); iter.hasNext();) { Expression element = (Expression) iter.next(); if (element == ConstantExpression.EMPTY_EXPRESSION) break; dimensions++; visitAndAutoboxBoolean(element); helper.unbox(int.class); } } else { size = expression.getExpressions().size(); helper.pushConstant(size); } int storeIns = AASTORE; if (sizeExpression != null) { arrayTypeName = BytecodeHelper.getTypeDescription(expression.getType()); mv.visitMultiANewArrayInsn(arrayTypeName, dimensions); } else if (ClassHelper.isPrimitiveType(elementType)) { int primType = 0; if (elementType == ClassHelper.boolean_TYPE) { primType = T_BOOLEAN; storeIns = BASTORE; } else if (elementType == ClassHelper.char_TYPE) { primType = T_CHAR; storeIns = CASTORE; } else if (elementType == ClassHelper.float_TYPE) { primType = T_FLOAT; storeIns = FASTORE; } else if (elementType == ClassHelper.double_TYPE) { primType = T_DOUBLE; storeIns = DASTORE; } else if (elementType == ClassHelper.byte_TYPE) { primType = T_BYTE; storeIns = BASTORE; } else if (elementType == ClassHelper.short_TYPE) { primType = T_SHORT; storeIns = SASTORE; } else if (elementType == ClassHelper.int_TYPE) { primType = T_INT; storeIns = IASTORE; } else if (elementType == ClassHelper.long_TYPE) { primType = T_LONG; storeIns = LASTORE; } mv.visitIntInsn(NEWARRAY, primType); } else { mv.visitTypeInsn(ANEWARRAY, arrayTypeName); } for (int i = 0; i < size; i++) { mv.visitInsn(DUP); helper.pushConstant(i); Expression elementExpression = expression.getExpression(i); if (elementExpression == null) { ConstantExpression.NULL.visit(this); } else { if (!elementType.equals(elementExpression.getType())) { visitCastExpression(new CastExpression(elementType, elementExpression, true)); } else { visitAndAutoboxBoolean(elementExpression); } } mv.visitInsn(storeIns); } if (sizeExpression == null && ClassHelper.isPrimitiveType(elementType)) { int par = compileStack.defineTemporaryVariable("par", true); mv.visitVarInsn(ALOAD, par); } }', 'ground_truth': 'public void visitArrayExpression(ArrayExpression expression) { ClassNode elementType = expression.getElementType(); String arrayTypeName = BytecodeHelper.getClassInternalName(elementType); List<Expression> sizeExpression = expression.getSizeExpression(); int size = 0; int dimensions = 0; if (sizeExpression != null) { for (Expression element : sizeExpression) { if (element == ConstantExpression.EMPTY_EXPRESSION) break; dimensions++; visitAndAutoboxBoolean(element); helper.unbox(int.class); } } else { size = expression.getExpressions().size(); helper.pushConstant(size); } int storeIns = AASTORE; if (sizeExpression != null) { arrayTypeName = BytecodeHelper.getTypeDescription(expression.getType()); mv.visitMultiANewArrayInsn(arrayTypeName, dimensions); } else if (ClassHelper.isPrimitiveType(elementType)) { int primType = 0; if (elementType == ClassHelper.boolean_TYPE) { primType = T_BOOLEAN; storeIns = BASTORE; } else if (elementType == ClassHelper.char_TYPE) { primType = T_CHAR; storeIns = CASTORE; } else if (elementType == ClassHelper.float_TYPE) { primType = T_FLOAT; storeIns = FASTORE; } else if (elementType == ClassHelper.double_TYPE) { primType = T_DOUBLE; storeIns = DASTORE; } else if (elementType == ClassHelper.byte_TYPE) { primType = T_BYTE; storeIns = BASTORE; } else if (elementType == ClassHelper.short_TYPE) { primType = T_SHORT; storeIns = SASTORE; } else if (elementType == ClassHelper.int_TYPE) { primType = T_INT; storeIns = IASTORE; } else if (elementType == ClassHelper.long_TYPE) { primType = T_LONG; storeIns = LASTORE; } mv.visitIntInsn(NEWARRAY, primType); } else { mv.visitTypeInsn(ANEWARRAY, arrayTypeName); } for (int i = 0; i < size; i++) { mv.visitInsn(DUP); helper.pushConstant(i); Expression elementExpression = expression.getExpression(i); if (elementExpression == null) { ConstantExpression.NULL.visit(this); } else { if (!elementType.equals(elementExpression.getType())) { visitCastExpression(new CastExpression(elementType, elementExpression, true)); } else { visitAndAutoboxBoolean(elementExpression); } } mv.visitInsn(storeIns); } if (sizeExpression == null && ClassHelper.isPrimitiveType(elementType)) { int par = compileStack.defineTemporaryVariable("par", true); mv.visitVarInsn(ALOAD, par); } }', 'output': 'public void visitArrayExpression(ArrayExpression expression) { ClassNode elementType = expression.getElementType(); String arrayTypeName = BytecodeHelper.getClassInternalName(elementType); Expression sizeExpression = expression.getSizeExpression(); int size = 0; int dimensions = 0; if (sizeExpression != null) { for (Iterator iter = sizeExpression.iterator(); iter.hasNext();) { Expression element = (Expression) iter.next(); if (element == ConstantExpression.EMPTY_EXPRESSION) break; dimensions++; visitAndAutoboxBoolean(element); helper.unbox(int.class); } } else { size = expression.getExpressions().size(); helper.pushConstant(size); } int storeIns = AASTORE; if (sizeExpression != null) { arrayTypeName = BytecodeHelper.getTypeDescription(expression.getType()); mv.visitMultiANewArrayInsn(arrayTypeName, dimensions); } else if (ClassHelper.isPrimitiveType(elementType)) { int primType = 0; if (elementType == ClassHelper.boolean_TYPE) { primType = T_BOOLEAN; storeIns = BASTORE; } else if (elementType == ClassHelper.char_TYPE) { primType = T_CHAR; storeIns = CASTORE; } else if (elementType == ClassHelper.float_TYPE) { primType = T_FLOAT; storeIns = FASTORE; } else if (elementType == ClassHelper.double_TYPE) { primType = T_DOUBLE; storeIns = DASTORE; } else if (elementType == ClassHelper.byte_TYPE) { primType = T_BYTE; storeIns = BASTORE; } else if (elementType == ClassHelper.short_TYPE) { primType = T_SHORT; storeIns = SASTORE; } else if (elementType == ClassHelper.int_TYPE) { primType = T_INT; storeIns = IASTORE; } else if (elementType == ClassHelper.long_TYPE) { primType = T_LONG; storeIns = LASTORE; } mv.visitIntInsn(NEWARRAY, primType); } else { mv.visitTypeInsn('}
{'instruction': 'Please refactor the following code:', 'input': 'public Tuple getNext() throws IOException { try { if (!reader.nextKeyValue()) return null; XXX key = (String)reader.getCurrentKey(); SortedMap<byte[],IColumn> cf = (SortedMap<byte[],IColumn>)reader.getCurrentValue(); assert key != null && cf != null; Tuple tuple = TupleFactory.getInstance().newTuple(2); ArrayList<Tuple> columns = new ArrayList<Tuple>(); tuple.set(0, new DataByteArray(key)); for (Map.Entry<byte[], IColumn> entry : cf.entrySet()) columns.add(columnToTuple(entry.getKey(), entry.getValue())); tuple.set(1, new DefaultDataBag(columns)); return tuple; } catch (InterruptedException e) { throw new IOException(e.getMessage()); } }', 'ground_truth': 'public Tuple getNext() throws IOException { try { if (!reader.nextKeyValue()) return null; byte[] key = (byte[])reader.getCurrentKey(); SortedMap<byte[],IColumn> cf = (SortedMap<byte[],IColumn>)reader.getCurrentValue(); assert key != null && cf != null; Tuple tuple = TupleFactory.getInstance().newTuple(2); ArrayList<Tuple> columns = new ArrayList<Tuple>(); tuple.set(0, new DataByteArray(key)); for (Map.Entry<byte[], IColumn> entry : cf.entrySet()) columns.add(columnToTuple(entry.getKey(), entry.getValue())); tuple.set(1, new DefaultDataBag(columns)); return tuple; } catch (InterruptedException e) { throw new IOException(e.getMessage()); } }', 'output': 'public Tuple getNext() throws IOException { try { if (!reader.nextKeyValue()) return null; String key = (String)reader.getCurrentKey(); SortedMap<byte[],IColumn> cf = (SortedMap<byte[],IColumn>)reader.getCurrentValue(); assert key != null && cf != null; Tuple tuple = TupleFactory.getInstance().newTuple(2); ArrayList<Tuple> columns = new ArrayList<Tuple>(); tuple.set(0, new DataByteArray(key)); for (Map.Entry<byte[], IColumn> entry : cf.entrySet()) columns.add(columnToTuple(entry.getKey(), entry.getValue())); tuple.set(1, new DefaultDataBag(columns)); return tuple; } catch (InterruptedException e) { throw new IOException(e.getMessage()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkMode(String pathStr, IgniteFsMode expMode) throws Exception { assert ggfs != null; XXX path = new IgniteFsPath(pathStr); IgfsModeResolver rslvr = ggfs.modeResolver(); IgniteFsMode mode = rslvr.resolveMode(path); assertEquals(expMode, mode); }', 'ground_truth': 'private void checkMode(String pathStr, IgniteFsMode expMode) throws Exception { assert ggfs != null; IgfsPath path = new IgfsPath(pathStr); IgfsModeResolver rslvr = ggfs.modeResolver(); IgniteFsMode mode = rslvr.resolveMode(path); assertEquals(expMode, mode); }', 'output': 'private void checkMode(String pathStr, IgniteFsMode expMode) throws Exception { assert ggfs != null; IgfsPath path = new IgfsPath(pathStr); IgfsModeResolver rslvr = ggfs.modeResolver(); IgniteFsMode mode = rslvr.resolveMode(path); assertEquals(expMode, mode); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testWriteThrough() throws Exception { XXX grid = grid(0); TestStore firstStore = (TestStore)grid(0).configuration().getCacheConfiguration()[1].getCacheStoreFactory().create(); Collection<String> evts = firstStore.events(); try (IgniteTx tx = grid.transactions().txStart()) { GridCache<Object, Object> cacheA = grid.cache("cacheA"); GridCache<Object, Object> cacheB = grid.cache("cacheB"); cacheA.put("1", "1"); cacheA.put("2", "2"); cacheB.put("1", "1"); cacheB.put("2", "2"); cacheA.remove("3"); cacheA.remove("4"); cacheB.remove("3"); cacheB.remove("4"); cacheA.put("5", "5"); cacheA.remove("6"); cacheB.put("7", "7"); tx.commit(); } assertEqualsCollections(F.asList( "writeAll cacheA 2", "writeAll cacheB 2", "deleteAll cacheA 2", "deleteAll cacheB 2", "write cacheA", "delete cacheA", "write cacheB", "txEnd true" ), evts); }', 'ground_truth': 'public void testWriteThrough() throws Exception { IgniteEx grid = grid(0); TestStore firstStore = (TestStore)grid(0).configuration().getCacheConfiguration()[1].getCacheStoreFactory().create(); Collection<String> evts = firstStore.events(); try (IgniteTx tx = grid.transactions().txStart()) { GridCache<Object, Object> cacheA = grid.cache("cacheA"); GridCache<Object, Object> cacheB = grid.cache("cacheB"); cacheA.put("1", "1"); cacheA.put("2", "2"); cacheB.put("1", "1"); cacheB.put("2", "2"); cacheA.remove("3"); cacheA.remove("4"); cacheB.remove("3"); cacheB.remove("4"); cacheA.put("5", "5"); cacheA.remove("6"); cacheB.put("7", "7"); tx.commit(); } assertEqualsCollections(F.asList( "writeAll cacheA 2", "writeAll cacheB 2", "deleteAll cacheA 2", "deleteAll cacheB 2", "write cacheA", "delete cacheA", "write cacheB", "txEnd true" ), evts); }', 'output': 'public void testWriteThrough() throws Exception { IgniteEx grid = grid(0); TestStore firstStore = (TestStore)grid(0).configuration().getCacheConfiguration()[1].getCacheStoreFactory().create(); Collection<String> evts = firstStore.events(); try (IgniteTx tx = grid.transactions().txStart()) { GridCache<Object, Object> cacheA = grid.cache("cacheA"); GridCache<Object, Object> cacheB = grid.cache("cacheB"); cacheA.put("1", "1"); cacheA.put("2", "2"); cacheB.put("1", "1"); cacheB.put("2", "2"); cacheA.remove("3"); cacheA.remove("4"); cacheB.remove("3"); cacheB.remove("4"); cacheA.put("5", "5"); cacheA.remove("6"); cacheB.put("7", "7"); tx.commit(); } assertEqualsCollections(F.asList( "writeAll cacheA 2", "writeAll cacheB 2", "deleteAll cacheA 2", "deleteAll cacheB 2", "write cacheA", "delete cacheA", "write cacheB", "txEnd true" ), evts); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testInstallAutoload() throws Exception { MockModuleInstaller installer = new MockModuleInstaller(); MockEvents ev = new MockEvents(); ModuleManager mgr = new ModuleManager(installer, ev); mgr.mutexPrivileged().enterWriteAccess(); try { Module m2 = mgr.create(new File(jars, "depends-on-simple-module.jar"), null, false, false, false); Module m1 = mgr.create(new File(jars, "simple-module.jar"), null, false, true, false); try { mgr.simulateEnable(new HashSet<Module>(Arrays.asList(m1, m2))); assertTrue("Should not permit you to simulate enablement of an autoload", false); } catch (IllegalArgumentException iae) { } assertEquals(Collections.EMPTY_SET, m1.getProblems()); assertEquals(Collections.EMPTY_SET, m2.getProblems()); XXX toEnable = mgr.simulateEnable(Collections.singleton(m2)); assertEquals("correct result of simulateEnable", Arrays.asList(m1, m2), toEnable); mgr.enable(Collections.singleton(m2)); assertEquals(Arrays.asList( "prepare", "prepare", "load" ), installer.actions); assertEquals(Arrays.asList( m1, m2, Arrays.asList(m1, m2) ), installer.args); Class<?> somethingelse = Class.forName("org.bar.SomethingElse", true, m2.getClassLoader()); Method somemethod = somethingelse.getMethod("message"); assertEquals("hello", somemethod.invoke(somethingelse.newInstance())); assertEquals("correct result of simulateDisable", Arrays.asList(m2, m1), mgr.simulateDisable(Collections.singleton(m2))); installer.clear(); mgr.disable(Collections.singleton(m2)); assertEquals(Arrays.asList( "unload", "dispose", "dispose" ), installer.actions); assertEquals(Arrays.asList( Arrays.asList(m2, m1), m2, m1 ), installer.args); } finally { mgr.mutexPrivileged().exitWriteAccess(); } }', 'ground_truth': 'public void testInstallAutoload() throws Exception { MockModuleInstaller installer = new MockModuleInstaller(); MockEvents ev = new MockEvents(); ModuleManager mgr = new ModuleManager(installer, ev); mgr.mutexPrivileged().enterWriteAccess(); try { Module m2 = mgr.create(new File(jars, "depends-on-simple-module.jar"), null, false, false, false); Module m1 = mgr.create(new File(jars, "simple-module.jar"), null, false, true, false); try { mgr.simulateEnable(new HashSet<Module>(Arrays.asList(m1, m2))); assertTrue("Should not permit you to simulate enablement of an autoload", false); } catch (IllegalArgumentException iae) { } assertEquals(Collections.EMPTY_SET, m1.getProblems()); assertEquals(Collections.EMPTY_SET, m2.getProblems()); List<Module> toEnable = mgr.simulateEnable(Collections.singleton(m2)); assertEquals("correct result of simulateEnable", Arrays.asList(m1, m2), toEnable); mgr.enable(Collections.singleton(m2)); assertEquals(Arrays.asList( "prepare", "prepare", "load" ), installer.actions); assertEquals(Arrays.asList( m1, m2, Arrays.asList(m1, m2) ), installer.args); Class<?> somethingelse = Class.forName("org.bar.SomethingElse", true, m2.getClassLoader()); Method somemethod = somethingelse.getMethod("message"); assertEquals("hello", somemethod.invoke(somethingelse.newInstance())); assertEquals("correct result of simulateDisable", Arrays.asList(m2, m1), mgr.simulateDisable(Collections.singleton(m2))); installer.clear(); mgr.disable(Collections.singleton(m2)); assertEquals(Arrays.asList( "unload", "dispose", "dispose" ), installer.actions); assertEquals(Arrays.asList( Arrays.asList(m2, m1), m2, m1 ), installer.args); } finally { mgr.mutexPrivileged().exitWriteAccess(); } }', 'output': 'public void testInstallAutoload() throws Exception { MockModuleInstaller installer = new MockModuleInstaller(); MockEvents ev = new MockEvents(); ModuleManager mgr = new ModuleManager(installer, ev); mgr.mutexPrivileged().enterWriteAccess(); try { Module m2 = mgr.create(new File(jars, "depends-on-simple-module.jar"), null, false, false, false); Module m1 = mgr.create(new File(jars, "simple-module.jar"), null, false, true, false); try { mgr.simulateEnable(new HashSet<Module>(Arrays.asList(m1, m2))); assertTrue("Should not permit you to simulate enablement of an autoload", false); } catch (IllegalArgumentException iae) { } assertEquals(Collections.EMPTY_SET, m1.getProblems()); assertEquals(Collections.EMPTY_SET, m2.getProblems()); List<Module> toEnable = mgr.simulateEnable(Collections.singleton(m2)); assertEquals("correct result of simulateEnable", Arrays.asList(m1, m2), toEnable); mgr.enable(Collections.singleton(m2)); assertEquals(Arrays.asList( "prepare", "prepare", "load" ), installer.actions); assertEquals(Arrays.asList( m1, m2, Arrays.asList(m1, m2) ), installer.args); Class<?> somethingelse = Class.forName("org.bar.SomethingElse", true, m2.getClassLoader()); Method somemethod = somethingelse.getMethod("message"); assertEquals("hello", somemethod.invoke(somethingelse.newInstance())); assertEquals("correct result of simulateDisable", Arrays.asList(m2, m1), mgr.simulateDisable(Collections.singleton(m2))); installer.clear(); mgr.disable(Collections.singleton(m2)); assertEquals(Arrays.asList( "unload", "dispose", "dispose" ), installer.actions); assertEquals(Arrays.asList( Arrays.asList(m2, m1), m2, m1 ), installer.args); } finally { mgr.mutexPriv'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testOutputStreamDeferredException() throws Exception { final byte[] data = "test".getBytes(); try { switchHandlerErrorFlag(true); GridGgfsHadoop client = new GridGgfsHadoopOutProc("127.0.0.1", 10500, getTestGridName(0), "ggfs", LOG); client.handshake(null); XXX path = new GridGgfsPath("/test1.file"); GridGgfsHadoopStreamDelegate delegate = client.create(path, true, false, 1, 1024, null); final GridGgfsHadoopOutputStream ggfsOut = new GridGgfsHadoopOutputStream(delegate, LOG, GridGgfsLogger.disabledLogger(), 0); ggfsOut.write(data); U.sleep(500); GridTestUtils.assertThrows(null, new Callable<Object>() { @Override public Object call() throws Exception { ggfsOut.write(data); return null; } }, IOException.class, "Failed to write data to server (test)."); } finally { switchHandlerErrorFlag(false); } }', 'ground_truth': 'public void testOutputStreamDeferredException() throws Exception { final byte[] data = "test".getBytes(); try { switchHandlerErrorFlag(true); GridGgfsHadoop client = new GridGgfsHadoopOutProc("127.0.0.1", 10500, getTestGridName(0), "ggfs", LOG); client.handshake(null); IgniteFsPath path = new IgniteFsPath("/test1.file"); GridGgfsHadoopStreamDelegate delegate = client.create(path, true, false, 1, 1024, null); final GridGgfsHadoopOutputStream ggfsOut = new GridGgfsHadoopOutputStream(delegate, LOG, GridGgfsLogger.disabledLogger(), 0); ggfsOut.write(data); U.sleep(500); GridTestUtils.assertThrows(null, new Callable<Object>() { @Override public Object call() throws Exception { ggfsOut.write(data); return null; } }, IOException.class, "Failed to write data to server (test)."); } finally { switchHandlerErrorFlag(false); } }', 'output': 'public void testOutputStreamDeferredException() throws Exception { final byte[] data = "test".getBytes(); try { switchHandlerErrorFlag(true); GridGgfsHadoop client = new GridGgfsHadoopOutProc("127.0.0.1", 10500, getTestGridName(0), "ggfs", LOG); client.handshake(null); GridGgfsPath path = new GridGgfsPath("/test1.file"); GridGgfsHadoopStreamDelegate delegate = client.create(path, true, false, 1, 1024, null); final GridGgfsHadoopOutputStream ggfsOut = new GridGgfsHadoopOutputStream(delegate, LOG, GridGgfsLogger.disabledLogger(), 0); ggfsOut.write(data); U.sleep(500); GridTestUtils.assertThrows(null, new Callable<Object>() { @Override public Object call() throws Exception { ggfsOut.write(data); return null; } }, IOException.class, "Failed to write data to server (test)."); } finally { switchHandlerErrorFlag(false); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void setExposeFields(boolean exposeFields) { checkModifiable(); if (classIntrospector.getExposeFields() != exposeFields) { XXX pa = classIntrospector.getPropertyAssignments(); pa.setExposeFields(exposeFields); replaceClassIntrospector(pa); } }', 'ground_truth': 'public void setExposeFields(boolean exposeFields) { checkModifiable(); if (classIntrospector.getExposeFields() != exposeFields) { ClassIntrospectorBuilder pa = classIntrospector.getPropertyAssignments(); pa.setExposeFields(exposeFields); replaceClassIntrospector(pa); } }', 'output': 'public void setExposeFields(boolean exposeFields) { checkModifiable(); if (classIntrospector.getExposeFields() != exposeFields) { PropertyAssignments pa = classIntrospector.getPropertyAssignments(); pa.setExposeFields(exposeFields); replaceClassIntrospector(pa); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void main(String[] args) throws Exception { ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext("org/apache/ignite/loadtests/capacity/spring-capacity-cache.xml"); IgniteConfiguration cfg = (IgniteConfiguration)ctx.getBean("grid.cfg"); try (Ignite g = G.start(cfg)) { GridCache<Integer, Integer> c = g.cache(null); long init = mem.getHeapMemoryUsage().getUsed(); printHeap(init); int cnt = 0; for (; cnt < 3000000; cnt++) { c.put(cnt, cnt); if (cnt % 10000 == 0) { X.println("Stored count: " + cnt); printHeap(init); if (cnt > 2100000 &&  cnt % 100000 == 0) System.gc(); } } System.gc(); Thread.sleep(1000); printHeap(init); MemoryUsage heap = mem.getHeapMemoryUsage(); long used = heap.getUsed() - init; long entrySize = cnt > 0 ? used / cnt : 0; X.println("Average entry size: " + entrySize); } }', 'ground_truth': 'public static void main(String[] args) throws Exception { ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext("org/apache/ignite/loadtests/capacity/spring-capacity-cache.xml"); IgniteConfiguration cfg = (IgniteConfiguration)ctx.getBean("grid.cfg"); try (Ignite g = G.start(cfg)) { IgniteCache<Integer, Integer> c = g.jcache(null); long init = mem.getHeapMemoryUsage().getUsed(); printHeap(init); int cnt = 0; for (; cnt < 3000000; cnt++) { c.put(cnt, cnt); if (cnt % 10000 == 0) { X.println("Stored count: " + cnt); printHeap(init); if (cnt > 2100000 &&  cnt % 100000 == 0) System.gc(); } } System.gc(); Thread.sleep(1000); printHeap(init); MemoryUsage heap = mem.getHeapMemoryUsage(); long used = heap.getUsed() - init; long entrySize = cnt > 0 ? used / cnt : 0; X.println("Average entry size: " + entrySize); } }', 'output': 'public static void main(String[] args) throws Exception { ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext("org/apache/ignite/loadtests/capacity/spring-capacity-cache.xml"); IgniteConfiguration cfg = (IgniteConfiguration)ctx.getBean("grid.cfg"); try (Ignite g = G.start(cfg)) { Cache<Integer, Integer> c = g.cache(null); long init = mem.getHeapMemoryUsage().getUsed(); printHeap(init); int cnt = 0; for (; cnt < 3000000; cnt++) { c.put(cnt, cnt); if (cnt % 10000 == 0) { X.println("Stored count: " + cnt); printHeap(init); if (cnt > 2100000 &&  cnt % 100000 == 0) System.gc(); } } System.gc(); Thread.sleep(1000); printHeap(init); MemoryUsage heap = mem.getHeapMemoryUsage(); long used = heap.getUsed() - init; long entrySize = cnt > 0 ? used / cnt : 0; X.println("Average entry size: " + entrySize); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void processStopRequest(UUID nodeId, GridContinuousMessage req) { assert nodeId != null; assert req != null; UUID routineId = req.routineId(); unregisterRemote(routineId); try { sendWithRetries(nodeId, new GridContinuousMessage(MSG_STOP_ACK, routineId, null), null); } catch (XXX ignored) { if (log.isDebugEnabled()) log.debug("Failed to send stop acknowledgement to node (is node alive?): " + nodeId); } catch (GridException e) { U.error(log, "Failed to send stop acknowledgement to node: " + nodeId, e); } }', 'ground_truth': 'private void processStopRequest(UUID nodeId, GridContinuousMessage req) { assert nodeId != null; assert req != null; UUID routineId = req.routineId(); unregisterRemote(routineId); try { sendWithRetries(nodeId, new GridContinuousMessage(MSG_STOP_ACK, routineId, null), null); } catch (ClusterTopologyException ignored) { if (log.isDebugEnabled()) log.debug("Failed to send stop acknowledgement to node (is node alive?): " + nodeId); } catch (GridException e) { U.error(log, "Failed to send stop acknowledgement to node: " + nodeId, e); } }', 'output': 'private void processStopRequest(UUID nodeId, ContinuousMessage req) { assert nodeId != null; assert req != null; UUID routineId = req.routineId(); unregisterRemote(routineId); try { sendWithRetries(nodeId, new ContinuousMessage(MSG_STOP_ACK, routineId, null), null); } catch (Throwable ignored) { if (log.isDebugEnabled()) log.debug("Failed to send stop acknowledgement to node (is node alive?): " + nodeId); } catch (GridException e) { U.error(log, "Failed to send stop acknowledgement to node: " + nodeId, e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAlterWithId() throws Throwable { createTable("CREATE TABLE %s (a int, b int, c int, PRIMARY KEY(a, b))"); XXX id = currentTableMetadata().cfId; execute(String.format("ALTER TABLE %%s WITH ID = %s", id)); }', 'ground_truth': 'public void testAlterWithId() throws Throwable { createTable("CREATE TABLE %s (a int, b int, c int, PRIMARY KEY(a, b))"); TableId id = currentTableMetadata().id; execute(String.format("ALTER TABLE %%s WITH ID = %s", id)); }', 'output': 'public void testAlterWithId() throws Throwable { createTable("CREATE TABLE %s (a int, b int, c int, PRIMARY KEY(a, b))"); String id = currentTableMetadata().cfId; execute(String.format("ALTER TABLE %%s WITH ID = %s", id)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkRemove() throws Exception { log.info("Check remove."); Ignite ignite0 = grid(0); GridCache<Integer, Integer> cache0 = ignite0.cache(null); Integer primaryKey = key(ignite0, PRIMARY); log.info("Put from primary."); cache0.put(primaryKey, primaryKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, primaryKey, false); log.info("Remove from primary."); cache0.remove(primaryKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, null, false); if (backups > 0) { Integer backupKey = key(ignite0, BACKUP); log.info("Put from backup."); cache0.put(backupKey, backupKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), backupKey, backupKey, false); log.info("Remove from backup."); cache0.remove(backupKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), backupKey, null, false); } }', 'ground_truth': 'private void checkRemove() throws Exception { log.info("Check remove."); Ignite ignite0 = grid(0); IgniteCache<Integer, Integer> cache0 = ignite0.jcache(null); Integer primaryKey = key(ignite0, PRIMARY); log.info("Put from primary."); cache0.put(primaryKey, primaryKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, primaryKey, false); log.info("Remove from primary."); cache0.remove(primaryKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, null, false); if (backups > 0) { Integer backupKey = key(ignite0, BACKUP); log.info("Put from backup."); cache0.put(backupKey, backupKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), backupKey, backupKey, false); log.info("Remove from backup."); cache0.remove(backupKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), backupKey, null, false); } }', 'output': 'private void checkRemove() throws Exception { log.info("Check remove."); Ignite ignite0 = grid(0); Cache<Integer, Integer> cache0 = ignite0.cache(null); Integer primaryKey = key(ignite0, PRIMARY); log.info("Put from primary."); cache0.put(primaryKey, primaryKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, primaryKey, false); log.info("Remove from primary."); cache0.remove(primaryKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, null, false); if (backups > 0) { Integer backupKey = key(ignite0, BACKUP); log.info("Put from backup."); cache0.put(backupKey, backupKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), backupKey, backupKey, false); log.info("Remove from backup."); cache0.remove(backupKey); for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), backupKey, null, false); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testReadEmptyFile() throws Exception { TextReader<String> textReader = new TextReader<>(tmpFolder.newFile().getPath(), true, null, null, new WholeLineVerifyingCoder(), TextIO.CompressionType.UNCOMPRESSED); try (XXX iterator = textReader.iterator()) { assertFalse(iterator.hasNext()); } }', 'ground_truth': 'public void testReadEmptyFile() throws Exception { TextReader<String> textReader = new TextReader<>(tmpFolder.newFile().getPath(), true, null, null, new WholeLineVerifyingCoder(), TextIO.CompressionType.UNCOMPRESSED); try (NativeReader.NativeReaderIterator<String> iterator = textReader.iterator()) { assertFalse(iterator.start()); } }', 'output': 'public void testReadEmptyFile() throws Exception { TextReader<String> textReader = new TextReader<>(tmpFolder.newFile().getPath(), true, null, null, new WholeLineVerifyingCoder(), TextIO.CompressionType.UNCOMPRESSED); try (Iterator<String> iterator = textReader.iterator()) { assertFalse(iterator.hasNext()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void internal_insert(ByteBuffer key, ColumnParent column_parent, Column column, ConsistencyLevel consistency_level) throws RequestValidationException, UnavailableException, TimedOutException { ThriftClientState cState = state(); String keyspace = cState.getKeyspace(); cState.hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.MODIFY); CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_parent.column_family, false); ThriftValidation.validateKey(metadata, key); ThriftValidation.validateColumnParent(metadata, column_parent); if (metadata.cfType == ColumnFamilyType.Super && column_parent.super_column == null) { throw new org.apache.cassandra.exceptions.InvalidRequestException("missing mandatory super column name for super CF " + column_parent.column_family); } ThriftValidation.validateColumnNames(metadata, column_parent, Arrays.asList(column.name)); ThriftValidation.validateColumnData(metadata, key, column_parent.super_column, column); org.apache.cassandra.db.Mutation mutation; try { CellName name = metadata.isSuper() ? metadata.comparator.makeCellName(column_parent.super_column, column.name) : metadata.comparator.cellFromByteBuffer(column.name); ColumnFamily cf = ArrayBackedSortedColumns.factory.create(cState.getKeyspace(), column_parent.column_family); cf.addColumn(name, column.value, column.timestamp, column.ttl); mutation = new org.apache.cassandra.db.Mutation(cState.getKeyspace(), key, cf); } catch (XXX e) { throw new org.apache.cassandra.exceptions.InvalidRequestException(e.getMessage()); } doInsert(consistency_level, Arrays.asList(mutation)); }', 'ground_truth': 'private void internal_insert(ByteBuffer key, ColumnParent column_parent, Column column, ConsistencyLevel consistency_level) throws RequestValidationException, UnavailableException, TimedOutException { ThriftClientState cState = state(); String keyspace = cState.getKeyspace(); cState.hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.MODIFY); CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_parent.column_family, false); ThriftValidation.validateKey(metadata, key); ThriftValidation.validateColumnParent(metadata, column_parent); if (metadata.isSuper() && column_parent.super_column == null) { throw new org.apache.cassandra.exceptions.InvalidRequestException("missing mandatory super column name for super CF " + column_parent.column_family); } ThriftValidation.validateColumnNames(metadata, column_parent, Arrays.asList(column.name)); ThriftValidation.validateColumnData(metadata, column_parent.super_column, column); org.apache.cassandra.db.Mutation mutation; try { LegacyLayout.LegacyCellName name = LegacyLayout.decodeCellName(metadata, column_parent.super_column, column.name); DecoratedKey dk = StorageService.getPartitioner().decorateKey(key); PartitionUpdate update = new PartitionUpdate(metadata, dk, PartitionColumns.of(name.column), 1); Row.Writer writer = name.column.isStatic() ? update.staticWriter() : update.writer(); name.clustering.writeTo(writer); CellPath path = name.collectionElement == null ? null : CellPath.create(name.collectionElement); writer.writeCell(name.column, false, column.value, SimpleLivenessInfo.forUpdate(column.timestamp, column.ttl, FBUtilities.nowInSeconds(), metadata), path); writer.endOfRow(); mutation = new org.apache.cassandra.db.Mutation(update); } catch (MarshalException|UnknownColumnException e) { throw new org.apache.cassandra.exceptions.InvalidRequestException(e.getMessage()); } doInsert(consistency_level, Arrays.asList(mutation)); }', 'output': 'private void internal_insert(ByteBuffer key, ColumnParent column_parent, Column column, ConsistencyLevel consistency_level) throws RequestValidationException, UnavailableException, TimedOutException { ThriftClientState cState = state(); String keyspace = cState.getKeyspace(); cState.hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.MODIFY); CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_parent.column_family, false); ThriftValidation.validateKey(metadata, key); ThriftValidation.validateColumnParent(metadata, column_parent); if (metadata.cfType == ColumnFamilyType.Super && column_parent.super_column == null) { throw new org.apache.cassandra.exceptions.InvalidRequestException("missing mandatory super column name for super CF " + column_parent.column_family); } ThriftValidation.validateColumnNames(metadata, column_parent, Arrays.asList(column.name)); ThriftValidation.validateColumnData(metadata, key, column_parent.super_column, column); org.apache.cassandra.db.Mutation mutation; try { CellName name = metadata.isSuper() ? metadata.comparator.makeCellName(column_parent.super_column, column.name) : metadata.comparator.cellFromByteBuffer(column.name); ColumnFamily cf = ArrayBackedSortedColumns.factory.create(cState.getKeyspace(), column_parent.column_family); cf.addColumn(name, column.value, column.timestamp, column.ttl); mutation = new org.apache.cassandra.db.Mutation(cState.getKeyspace(), key, cf); } catch (Throwable e) { throw new org.apache.cassandra.exceptions.InvalidRequestException(e.getMessage()); } doInsert(consistency_level, Arrays.asList(mutation)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testParse_NullBody() { RequestBodyParser parser = new JsonPropertyParser(); Set<Map<PropertyId, Object>> setProps = parser.parse(null); assertNotNull(setProps); assertEquals(0, setProps.size()); }', 'ground_truth': 'public void testParse_NullBody() { RequestBodyParser parser = new JsonPropertyParser(); Set<Map<String, Object>> setProps = parser.parse(null); assertNotNull(setProps); assertEquals(0, setProps.size()); }', 'output': 'public void testParse_NullBody() { RequestBodyParser parser = new JsonPropertyParser(); Set<Map<String, Object>> setProps = parser.parse(null); assertNotNull(setProps); assertEquals(0, setProps.size()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testDhtUpdateRequestLost() throws Exception { Ignite ignite = grid(0); GridCache<Object, Object> cache = ignite.cache(null); int key = keyForTest(); cache.put(key, 0); TestCommunicationSpi commSpi = (TestCommunicationSpi)grid(1).configuration().getCommunicationSpi(); commSpi.skipDhtRequest = true; XXX fut = cache.putAsync(key, 1); Map<UUID, GridCommunicationClient> clients = U.field(commSpi, "clients"); GridTcpNioCommunicationClient client = (GridTcpNioCommunicationClient)clients.get(grid(2).localNode().id()); client.session().close().get(); try { fut.get(); fail(); } catch (IgniteCheckedException e) { assertTrue("Invalid exception thrown: " + e, X.hasCause(e, CacheAtomicUpdateTimeoutException.class) || X.hasSuppressed(e, CacheAtomicUpdateTimeoutException.class)); } }', 'ground_truth': 'public void testDhtUpdateRequestLost() throws Exception { Ignite ignite = grid(0); GridCache<Object, Object> cache = ignite.cache(null); int key = keyForTest(); cache.put(key, 0); TestCommunicationSpi commSpi = (TestCommunicationSpi)grid(1).configuration().getCommunicationSpi(); commSpi.skipDhtRequest = true; IgniteInternalFuture<Object> fut = cache.putAsync(key, 1); Map<UUID, GridCommunicationClient> clients = U.field(commSpi, "clients"); GridTcpNioCommunicationClient client = (GridTcpNioCommunicationClient)clients.get(grid(2).localNode().id()); client.session().close().get(); try { fut.get(); fail(); } catch (IgniteCheckedException e) { assertTrue("Invalid exception thrown: " + e, X.hasCause(e, CacheAtomicUpdateTimeoutException.class) || X.hasSuppressed(e, CacheAtomicUpdateTimeoutException.class)); } }', 'output': 'public void testDhtUpdateRequestLost() throws Exception { Ignite ignite = grid(0); GridCache<Object, Object> cache = ignite.cache(null); int key = keyForTest(); cache.put(key, 0); TestCommunicationSpi commSpi = (TestCommunicationSpi)grid(1).configuration().getCommunicationSpi(); commSpi.skipDhtRequest = true; IgniteInternalFuture<?> fut = cache.putAsync(key, 1); Map<UUID, GridCommunicationClient> clients = U.field(commSpi, "clients"); GridTcpNioCommunicationClient client = (GridTcpNioCommunicationClient)clients.get(grid(2).localNode().id()); client.session().close().get(); try { fut.get(); fail(); } catch (IgniteCheckedException e) { assertTrue("Invalid exception thrown: " + e, X.hasCause(e, CacheAtomicUpdateTimeoutException.class) || X.hasSuppressed(e, CacheAtomicUpdateTimeoutException.class)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void main(String[] args) throws GridClientException, IgniteCheckedException { GridFileLock fileLock = GridLoadTestUtils.fileLock(); fileLock.lock(); try { System.out.printf("%8s, %12s, %12s, %12s\\n", "Threads", "It./s.", "It./s.*th.", "Iters."); if (args.length == 0) { for (int i = 1; i <= 16; i *= 2) { XXX benchmark = new GridClientCacheBenchmark(i, 10000); benchmark.run(false); System.gc(); } for (int i = 1; i <= 64; i *= 2) { XXX benchmark = new GridClientCacheBenchmark(i, 10000); benchmark.run(true); System.gc(); } } else { int nThreads = Integer.parseInt(args[0]); String outputFileName = (args.length >= 2 ? args[1] : null); XXX benchmark = null; for (int i = 0; i < 2; i++) { benchmark = new GridClientCacheBenchmark(nThreads, 10000); benchmark.run(true); } if (outputFileName != null) { X.println("Writing test results to a file: " + outputFileName); assert benchmark != null; try { GridLoadTestUtils.appendLineToFile( outputFileName, "%s,%d", GridLoadTestUtils.DATE_TIME_FORMAT.format(new Date()), Math.round(benchmark.getItersPerSec())); } catch (IOException e) { X.error("Failed to output to a file", e); } } } } finally { fileLock.close(); } }', 'ground_truth': 'public static void main(String[] args) throws GridClientException, IgniteCheckedException { GridFileLock fileLock = GridLoadTestUtils.fileLock(); fileLock.lock(); try { System.out.printf("%8s, %12s, %12s, %12s\\n", "Threads", "It./s.", "It./s.*th.", "Iters."); if (args.length == 0) { for (int i = 1; i <= 16; i *= 2) { ClientCacheBenchmark benchmark = new ClientCacheBenchmark(i, 10000); benchmark.run(false); System.gc(); } for (int i = 1; i <= 64; i *= 2) { ClientCacheBenchmark benchmark = new ClientCacheBenchmark(i, 10000); benchmark.run(true); System.gc(); } } else { int nThreads = Integer.parseInt(args[0]); String outputFileName = (args.length >= 2 ? args[1] : null); ClientCacheBenchmark benchmark = null; for (int i = 0; i < 2; i++) { benchmark = new ClientCacheBenchmark(nThreads, 10000); benchmark.run(true); } if (outputFileName != null) { X.println("Writing test results to a file: " + outputFileName); assert benchmark != null; try { GridLoadTestUtils.appendLineToFile( outputFileName, "%s,%d", GridLoadTestUtils.DATE_TIME_FORMAT.format(new Date()), Math.round(benchmark.getItersPerSec())); } catch (IOException e) { X.error("Failed to output to a file", e); } } } } finally { fileLock.close(); } }', 'output': 'public static void main(String[] args) throws GridClientException, IgniteCheckedException { GridFileLock fileLock = GridLoadTestUtils.fileLock(); fileLock.lock(); try { System.out.printf("%8s, %12s, %12s, %12s\\n", "Threads", "It./s.", "It./s.*th.", "Iters."); if (args.length == 0) { for (int i = 1; i <= 16; i *= 2) { GridClientCacheBenchmark benchmark = new GridClientCacheBenchmark(i, 10000); benchmark.run(false); System.gc(); } for (int i = 1; i <= 64; i *= 2) { GridClientCacheBenchmark benchmark = new GridClientCacheBenchmark(i, 10000); benchmark.run(true); System.gc(); } } else { int nThreads = Integer.parseInt(args[0]); String outputFileName = (args.length >= 2 ? args[1] : null); GridClientCacheBenchmark benchmark = null; for (int i = 0; i < 2; i++) { benchmark = new GridClientCacheBenchmark(nThreads, 10000); benchmark.run(true); } if (outputFileName != null) { X.println("Writing test results to a file: " + outputFileName); assert benchmark != null; try { GridLoadTestUtils.appendLineToFile( outputFileName, "%s,%d", GridLoadTestUtils.DATE_TIME_FORMAT.format(new Date()), Math.round(benchmark.getItersPerSec())); } catch (IOException e) { X.error("Failed to output to a file", e); } } } } finally { fileLock.close(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testUndeclaredSideOutputs() throws Exception { TestDoFn fn = new TestDoFn(Arrays.asList("declared", "undecl1", "undecl2", "undecl3")); XXX fnInfo = new DoFnInfo(fn, WindowingStrategy.globalDefault()); CounterSet counters = new CounterSet(); NormalParDoFn normalParDoFn = NormalParDoFn.of( PipelineOptionsFactory.create(), fnInfo, NullSideInputReader.empty(), Arrays.asList("output", "declared"), "doFn", "doFn", DataflowExecutionContext.withoutSideInputs(), counters.getAddCounterMutator()); normalParDoFn.startBundle(new TestReceiver(), new TestReceiver()); normalParDoFn.processElement(WindowedValue.valueInGlobalWindow(5)); normalParDoFn.finishBundle(); assertEquals( new CounterSet( Counter.longs(getElementCounterName("implicit-undecl1"), SUM).resetToValue(3L), Counter.longs(getObjectCounterName("implicit-undecl1"), SUM).resetToValue(3L), Counter.longs(getElementCounterName("implicit-undecl2"), SUM).resetToValue(3L), Counter.longs(getObjectCounterName("implicit-undecl2"), SUM).resetToValue(3L), Counter.longs(getElementCounterName("implicit-undecl3"), SUM).resetToValue(3L), Counter.longs(getObjectCounterName("implicit-undecl3"), SUM).resetToValue(3L)), counters); }', 'ground_truth': 'public void testUndeclaredSideOutputs() throws Exception { TestDoFn fn = new TestDoFn(Arrays.asList("declared", "undecl1", "undecl2", "undecl3")); DoFnInfo<?, ?> fnInfo = new DoFnInfo<>(fn, WindowingStrategy.globalDefault()); CounterSet counters = new CounterSet(); NormalParDoFn normalParDoFn = NormalParDoFn.of( PipelineOptionsFactory.create(), fnInfo, NullSideInputReader.empty(), Arrays.asList("output", "declared"), "doFn", "doFn", DataflowExecutionContext.withoutSideInputs(), counters.getAddCounterMutator()); normalParDoFn.startBundle(new TestReceiver(), new TestReceiver()); normalParDoFn.processElement(WindowedValue.valueInGlobalWindow(5)); normalParDoFn.finishBundle(); assertEquals( new CounterSet( Counter.longs(getElementCounterName("implicit-undecl1"), SUM).resetToValue(3L), Counter.longs(getObjectCounterName("implicit-undecl1"), SUM).resetToValue(3L), Counter.longs(getElementCounterName("implicit-undecl2"), SUM).resetToValue(3L), Counter.longs(getObjectCounterName("implicit-undecl2"), SUM).resetToValue(3L), Counter.longs(getElementCounterName("implicit-undecl3"), SUM).resetToValue(3L), Counter.longs(getObjectCounterName("implicit-undecl3"), SUM).resetToValue(3L)), counters); }', 'output': 'public void testUndeclaredSideOutputs() throws Exception { TestDoFn fn = new TestDoFn(Arrays.asList("declared", "undecl1", "undecl2", "undecl3")); DoFnInfo fnInfo = new DoFnInfo(fn, WindowingStrategy.globalDefault()); CounterSet counters = new CounterSet(); NormalParDoFn normalParDoFn = NormalParDoFn.of( PipelineOptionsFactory.create(), fnInfo, NullSideInputReader.empty(), Arrays.asList("output", "declared"), "doFn", "doFn", DataflowExecutionContext.withoutSideInputs(), counters.getAddCounterMutator()); normalParDoFn.startBundle(new TestReceiver(), new TestReceiver()); normalParDoFn.processElement(WindowedValue.valueInGlobalWindow(5)); normalParDoFn.finishBundle(); assertEquals( new CounterSet( Counter.longs(getElementCounterName("implicit-undecl1"), SUM).resetToValue(3L), Counter.longs(getObjectCounterName("implicit-undecl1"), SUM).resetToValue(3L), Counter.longs(getElementCounterName("implicit-undecl2"), SUM).resetToValue(3L), Counter.longs(getObjectCounterName("implicit-undecl2"), SUM).resetToValue(3L), Counter.longs(getElementCounterName("implicit-undecl3"), SUM).resetToValue(3L), Counter.longs(getObjectCounterName("implicit-undecl3"), SUM).resetToValue(3L)), counters); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGetEarliestWatermarkHoldWithEarliestInNewTable() { BoundedWindow first = new BoundedWindow() { @Override public Instant maxTimestamp() { return new Instant(2048L); } }; BoundedWindow second = new BoundedWindow() { @Override public Instant maxTimestamp() { return new Instant(689743L); } }; CopyOnAccessInMemoryStateInternals<String> underlying = CopyOnAccessInMemoryStateInternals.withUnderlying("foo", null); StateTag<Object, WatermarkHoldState> firstHoldAddress = StateTags.watermarkStateInternal("foo", TimestampCombiner.EARLIEST); WatermarkHoldState firstHold = underlying.state(StateNamespaces.window(null, first), firstHoldAddress); firstHold.add(new Instant(224L)); CopyOnAccessInMemoryStateInternals<String> internals = CopyOnAccessInMemoryStateInternals.withUnderlying("foo", underlying.commit()); StateTag<Object, WatermarkHoldState> secondHoldAddress = StateTags.watermarkStateInternal("foo", TimestampCombiner.EARLIEST); WatermarkHoldState secondHold = internals.state(StateNamespaces.window(null, second), secondHoldAddress); secondHold.add(new Instant(24L)); internals.commit(); assertThat(internals.getEarliestWatermarkHold(), equalTo(new Instant(24L))); }', 'ground_truth': 'public void testGetEarliestWatermarkHoldWithEarliestInNewTable() { BoundedWindow first = new BoundedWindow() { @Override public Instant maxTimestamp() { return new Instant(2048L); } }; BoundedWindow second = new BoundedWindow() { @Override public Instant maxTimestamp() { return new Instant(689743L); } }; CopyOnAccessInMemoryStateInternals<String>underlying = CopyOnAccessInMemoryStateInternals.withUnderlying("foo", null); StateTag<WatermarkHoldState> firstHoldAddress = StateTags.watermarkStateInternal("foo", TimestampCombiner.EARLIEST); WatermarkHoldState firstHold = underlying.state(StateNamespaces.window(null, first), firstHoldAddress); firstHold.add(new Instant(224L)); CopyOnAccessInMemoryStateInternals<String>internals = CopyOnAccessInMemoryStateInternals.withUnderlying("foo", underlying.commit()); StateTag<WatermarkHoldState> secondHoldAddress = StateTags.watermarkStateInternal("foo", TimestampCombiner.EARLIEST); WatermarkHoldState secondHold = internals.state(StateNamespaces.window(null, second), secondHoldAddress); secondHold.add(new Instant(24L)); internals.commit(); assertThat(internals.getEarliestWatermarkHold(), equalTo(new Instant(24L))); }', 'output': 'public void testGetEarliestWatermarkHoldWithEarliestInNewTable() { BoundedWindow first = new BoundedWindow() { @Override public Instant maxTimestamp() { return new Instant(2048L); } }; BoundedWindow second = new BoundedWindow() { @Override public Instant maxTimestamp() { return new Instant(689743L); } }; CopyOnAccessInMemoryStateInternals<String> underlying = CopyOnAccessInMemoryStateInternals.withUnderlying("foo", null); StateTag<Object, WatermarkHoldState> firstHoldAddress = StateTags.watermarkStateInternal("foo", TimestampCombiner.EARLIEST); WatermarkHoldState firstHold = underlying.state(StateNamespaces.window(null, first), firstHoldAddress); firstHold.add(new Instant(224L)); CopyOnAccessInMemoryStateInternals<String> internals = CopyOnAccessInMemoryStateInternals.withUnderlying("foo", underlying.commit()); StateTag<Object, WatermarkHoldState> secondHoldAddress = StateTags.watermarkStateInternal("foo", TimestampCombiner.EARLIEST); WatermarkHoldState secondHold = internals.state(StateNamespaces.window(null, second), secondHoldAddress); secondHold.add(new Instant(24L)); internals.commit(); assertThat(internals.getEarliestWatermarkHold(), equalTo(new Instant(24L))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCheckpointsAfterDuration() throws Exception { int max = Integer.MAX_VALUE; Duration maxBundleDuration = Duration.standardSeconds(1); DoFn<Integer, String> fn = new CounterFn(Integer.MAX_VALUE); Instant base = Instant.now(); int baseIndex = 42; ProcessFnTester<Integer, String, OffsetRange, Long, OffsetRangeTracker> tester = new ProcessFnTester<>( base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(OffsetRange.class), max, maxBundleDuration); List<String> elements; tester.startElement(baseIndex, new OffsetRange(0, Long.MAX_VALUE)); elements = tester.takeOutputElements(); assertFalse(elements.isEmpty()); assertThat( Instant.now().getMillis() - base.getMillis(), greaterThanOrEqualTo(maxBundleDuration.getMillis())); }', 'ground_truth': 'public void testCheckpointsAfterDuration() throws Exception { int max = Integer.MAX_VALUE; Duration maxBundleDuration = Duration.standardSeconds(1); DoFn<Integer, String> fn = new CounterFn(Integer.MAX_VALUE); Instant base = Instant.now(); int baseIndex = 42; ProcessFnTester<Integer, String, OffsetRange, Long> tester = new ProcessFnTester<>( base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(OffsetRange.class), max, maxBundleDuration); List<String> elements; tester.startElement(baseIndex, new OffsetRange(0, Long.MAX_VALUE)); elements = tester.takeOutputElements(); assertFalse(elements.isEmpty()); assertThat( Instant.now().getMillis() - base.getMillis(), greaterThanOrEqualTo(maxBundleDuration.getMillis())); }', 'output': 'public void testCheckpointsAfterDuration() throws Exception { int max = Integer.MAX_VALUE; Duration maxBundleDuration = Duration.standardSeconds(1); DoFn<Integer, String> fn = new CounterFn(Integer.MAX_VALUE); Instant base = Instant.now(); int baseIndex = 42; ProcessFnTester<Integer, String, OffsetRange, Long, OffsetRangeTracker> tester = new ProcessFnTester<>( base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(OffsetRange.class), max, maxBundleDuration); List<String> elements; tester.startElement(baseIndex, new OffsetRange(0, Long.MAX_VALUE)); elements = tester.takeOutputElements(); assertFalse(elements.isEmpty()); assertThat( Instant.now().getMillis() - base.getMillis(), greaterThanOrEqualTo(maxBundleDuration.getMillis())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private String makeSelect(int pivot) throws SQLException { String keyspace = determineCurrentKeyspace(cql, connection.currentKeyspace); String columnFamily = determineCurrentColumnFamily(cql); if (columnFamily == null) throw new SQLTransientException(NO_CF); ParameterIterator params = new ParameterIterator(); String left = cql.substring(0, pivot); AbstractTerm leftType = connection.decoder.getComparator(keyspace, columnFamily); if (leftType == null) throw new SQLDataException(String.format(NO_COMPARATOR, keyspace, columnFamily)); left = applySimpleBindings(left, leftType, params); String right = cql.substring(pivot); XXX keyVald = connection.decoder.getKeyValidator(keyspace, columnFamily); if (keyVald == null) throw new SQLDataException(String.format(NO_VALIDATOR, keyspace, columnFamily)); right = applySimpleBindings(right, keyVald, params); return left + right; }', 'ground_truth': 'private String makeSelect(int pivot) throws SQLException { String keyspace = determineCurrentKeyspace(cql, connection.currentKeyspace); String columnFamily = determineCurrentColumnFamily(cql); if (columnFamily == null) throw new SQLTransientException(NO_CF); ParameterIterator params = new ParameterIterator(); String left = cql.substring(0, pivot); AbstractJdbcType leftType = connection.decoder.getComparator(keyspace, columnFamily); if (leftType == null) throw new SQLDataException(String.format(NO_COMPARATOR, keyspace, columnFamily)); left = applySimpleBindings(left, leftType, params); String right = cql.substring(pivot); AbstractJdbcType keyVald = connection.decoder.getKeyValidator(keyspace, columnFamily); if (keyVald == null) throw new SQLDataException(String.format(NO_VALIDATOR, keyspace, columnFamily)); right = applySimpleBindings(right, keyVald, params); return left + right; }', 'output': 'private String makeSelect(int pivot) throws SQLException { String keyspace = determineCurrentKeyspace(cql, connection.currentKeyspace); String columnFamily = determineCurrentColumnFamily(cql); if (columnFamily == null) throw new SQLTransientException(NO_CF); ParameterIterator params = new ParameterIterator(); String left = cql.substring(0, pivot); AbstractTerm leftType = connection.decoder.getComparator(keyspace, columnFamily); if (leftType == null) throw new SQLDataException(String.format(NO_COMPARATOR, keyspace, columnFamily)); left = applySimpleBindings(left, leftType, params); String right = cql.substring(pivot); String keyValidator = connection.decoder.getKeyValidator(keyspace, columnFamily); if (keyValidator == null) throw new SQLDataException(String.format(NO_VALIDATOR, keyspace, columnFamily)); right = applySimpleBindings(right, keyValidator, params); return left + right; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testFileBasedWriterWithWritableByteChannelFactory() throws Exception { final String testUid = "testId"; ResourceId root = getBaseOutputDirectory(); FileBasedWriteOperation<String> writeOp = new SimpleSink(root, "file", "-SS-of-NN", "txt", new DrunkWritableByteChannelFactory()) .createWriteOperation(); final XXX writer = writeOp.createWriter(); final ResourceId expectedFile = writeOp.tempDirectory.get().resolve(testUid, StandardResolveOptions.RESOLVE_FILE); final List<String> expected = new ArrayList<>(); expected.add("header"); expected.add("header"); expected.add("a"); expected.add("a"); expected.add("b"); expected.add("b"); expected.add("footer"); expected.add("footer"); writer.openUnwindowed(testUid, -1, -1); writer.write("a"); writer.write("b"); final FileResult result = writer.close(); assertEquals(expectedFile, result.getFilename()); assertFileContains(expected, expectedFile); }', 'ground_truth': 'public void testFileBasedWriterWithWritableByteChannelFactory() throws Exception { final String testUid = "testId"; ResourceId root = getBaseOutputDirectory(); WriteOperation<String> writeOp = new SimpleSink(root, "file", "-SS-of-NN", "txt", new DrunkWritableByteChannelFactory()) .createWriteOperation(); final Writer<String> writer = writeOp.createWriter(); final ResourceId expectedFile = writeOp.tempDirectory.get().resolve(testUid, StandardResolveOptions.RESOLVE_FILE); final List<String> expected = new ArrayList<>(); expected.add("header"); expected.add("header"); expected.add("a"); expected.add("a"); expected.add("b"); expected.add("b"); expected.add("footer"); expected.add("footer"); writer.openUnwindowed(testUid, -1, -1); writer.write("a"); writer.write("b"); final FileResult result = writer.close(); assertEquals(expectedFile, result.getFilename()); assertFileContains(expected, expectedFile); }', 'output': 'public void testFileBasedWriterWithWritableByteChannelFactory() throws Exception { final String testUid = "testId"; ResourceId root = getBaseOutputDirectory(); FileBasedWriteOperation<String> writeOp = new SimpleSink(root, "file", "-SS-of-NN", "txt", new DrunkWritableByteChannelFactory()) .createWriteOperation(); final WritableByteChannel writer = writeOp.createWriter(); final ResourceId expectedFile = writeOp.tempDirectory.get().resolve(testUid, StandardResolveOptions.RESOLVE_FILE); final List<String> expected = new ArrayList<>(); expected.add("header"); expected.add("header"); expected.add("a"); expected.add("a"); expected.add("b"); expected.add("b"); expected.add("footer"); expected.add("footer"); writer.openUnwindowed(testUid, -1, -1); writer.write("a"); writer.write("b"); final FileResult result = writer.close(); assertEquals(expectedFile, result.getFilename()); assertFileContains(expected, expectedFile); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<InetAddress> getEndpoints(String keyspace, String cf, String key) { return ssProxy.getNaturalEndpoints(keyspace, cf, key); } public EndpointSnitchInfoMBean getEndpointSnitchInfoProxy()\n    { { try { return JMX.newMBeanProxy(mbeanServerConn, new ObjectName("org.apache.cassandra.db:type=EndpointSnitchInfo"), EndpointSnitchInfoMBean.class); } catch (MalformedObjectNameException e) { throw new RuntimeException(e); } } public String getEndpoint()\n    { { String stringToken = ssProxy.getToken(); Map<Token, String> tokenToEndpoint = ssProxy.getTokenToEndpointMap(); for (Map.Entry<Token, String> pair : tokenToEndpoint.entrySet()) { if (pair.getKey().toString().equals(stringToken)) { return pair.getValue(); } } throw new AssertionError("Could not find myself in the endpoint list, something is very wrong!"); }', 'ground_truth': 'public String getEndpoint() { String stringToken = ssProxy.getToken(); Map<String, String> tokenToEndpoint = ssProxy.getTokenToEndpointMap(); for (Map.Entry<String, String> pair : tokenToEndpoint.entrySet()) { if (pair.getKey().toString().equals(stringToken)) { return pair.getValue(); } } throw new AssertionError("Could not find myself in the endpoint list, something is very wrong!"); }', 'output': 'public List<InetAddress> getEndpoints(String keyspace, String cf, String key) { return ssProxy.getNaturalEndpoints(keyspace, cf, key); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void toSchemaNoColumnsNoTriggers(RowMutation rm, long timestamp) { ColumnFamily cf = rm.addOrGet(SchemaColumnFamiliesCf); int ldt = (int) (System.currentTimeMillis() / 1000); cf.addColumn(Column.create("", timestamp, cfName, "")); cf.addColumn(Column.create(cfType.toString(), timestamp, cfName, "type")); if (isSuper()) { CompositeType ct = (CompositeType)comparator; cf.addColumn(Column.create(ct.types.get(0).toString(), timestamp, cfName, "comparator")); cf.addColumn(Column.create(ct.types.get(1).toString(), timestamp, cfName, "subcomparator")); } else { cf.addColumn(Column.create(comparator.toString(), timestamp, cfName, "comparator")); } cf.addColumn(comment == null ? DeletedColumn.create(ldt, timestamp, cfName, "comment") : Column.create(comment, timestamp, cfName, "comment")); cf.addColumn(Column.create(readRepairChance, timestamp, cfName, "read_repair_chance")); cf.addColumn(Column.create(dcLocalReadRepairChance, timestamp, cfName, "local_read_repair_chance")); cf.addColumn(Column.create(replicateOnWrite, timestamp, cfName, "replicate_on_write")); cf.addColumn(Column.create(populateIoCacheOnFlush, timestamp, cfName, "populate_io_cache_on_flush")); cf.addColumn(Column.create(gcGraceSeconds, timestamp, cfName, "gc_grace_seconds")); cf.addColumn(Column.create(defaultValidator.toString(), timestamp, cfName, "default_validator")); cf.addColumn(Column.create(keyValidator.toString(), timestamp, cfName, "key_validator")); cf.addColumn(Column.create(minCompactionThreshold, timestamp, cfName, "min_compaction_threshold")); cf.addColumn(Column.create(maxCompactionThreshold, timestamp, cfName, "max_compaction_threshold")); cf.addColumn(bloomFilterFpChance == null ? DeletedColumn.create(ldt, timestamp, cfName, "bloomFilterFpChance") : Column.create(bloomFilterFpChance, timestamp, cfName, "bloom_filter_fp_chance")); cf.addColumn(Column.create(memtableFlushPeriod, timestamp, cfName, "memtable_flush_period_in_ms")); cf.addColumn(Column.create(caching.toString(), timestamp, cfName, "caching")); cf.addColumn(Column.create(defaultTimeToLive, timestamp, cfName, "default_time_to_live")); cf.addColumn(Column.create(compactionStrategyClass.getName(), timestamp, cfName, "compaction_strategy_class")); cf.addColumn(Column.create(json(compressionParameters.asThriftOptions()), timestamp, cfName, "compression_parameters")); cf.addColumn(Column.create(json(compactionStrategyOptions), timestamp, cfName, "compaction_strategy_options")); cf.addColumn(Column.create(indexInterval, timestamp, cfName, "index_interval")); cf.addColumn(Column.create(speculativeRetry.toString(), timestamp, cfName, "speculative_retry")); for (Map.Entry<ByteBuffer, Long> entry : droppedColumns.entrySet()) cf.addColumn(new Column(makeDroppedColumnName(entry.getKey()), LongType.instance.decompose(entry.getValue()), timestamp)); cf.addColumn(Column.create(aliasesToJson(partitionKeyColumns), timestamp, cfName, "key_aliases")); cf.addColumn(Column.create(aliasesToJson(clusteringKeyColumns), timestamp, cfName, "column_aliases")); cf.addColumn(compactValueColumn == null ? DeletedColumn.create(ldt, timestamp, cfName, "value_alias") : Column.create(compactValueColumn.name, timestamp, cfName, "value_alias")); }', 'ground_truth': 'private void toSchemaNoColumnsNoTriggers(RowMutation rm, long timestamp) { ColumnFamily cf = rm.addOrGet(SchemaColumnFamiliesCf); int ldt = (int) (System.currentTimeMillis() / 1000); cf.addColumn(Column.create("", timestamp, cfName, "")); cf.addColumn(Column.create(cfType.toString(), timestamp, cfName, "type")); if (isSuper()) { CompositeType ct = (CompositeType)comparator; cf.addColumn(Column.create(ct.types.get(0).toString(), timestamp, cfName, "comparator")); cf.addColumn(Column.create(ct.types.get(1).toString(), timestamp, cfName, "subcomparator")); } else { cf.addColumn(Column.create(comparator.toString(), timestamp, cfName, "comparator")); } cf.addColumn(comment == null ? DeletedColumn.create(ldt, timestamp, cfName, "comment") : Column.create(comment, timestamp, cfName, "comment")); cf.addColumn(Column.create(readRepairChance, timestamp, cfName, "read_repair_chance")); cf.addColumn(Column.create(dcLocalReadRepairChance, timestamp, cfName, "local_read_repair_chance")); cf.addColumn(Column.create(replicateOnWrite, timestamp, cfName, "replicate_on_write")); cf.addColumn(Column.create(populateIoCacheOnFlush, timestamp, cfName, "populate_io_cache_on_flush")); cf.addColumn(Column.create(gcGraceSeconds, timestamp, cfName, "gc_grace_seconds")); cf.addColumn(Column.create(defaultValidator.toString(), timestamp, cfName, "default_validator")); cf.addColumn(Column.create(keyValidator.toString(), timestamp, cfName, "key_validator")); cf.addColumn(Column.create(minCompactionThreshold, timestamp, cfName, "min_compaction_threshold")); cf.addColumn(Column.create(maxCompactionThreshold, timestamp, cfName, "max_compaction_threshold")); cf.addColumn(bloomFilterFpChance == null ? DeletedColumn.create(ldt, timestamp, cfName, "bloomFilterFpChance") : Column.create(bloomFilterFpChance, timestamp, cfName, "bloom_filter_fp_chance")); cf.addColumn(Column.create(memtableFlushPeriod, timestamp, cfName, "memtable_flush_period_in_ms")); cf.addColumn(Column.create(caching.toString(), timestamp, cfName, "caching")); cf.addColumn(Column.create(defaultTimeToLive, timestamp, cfName, "default_time_to_live")); cf.addColumn(Column.create(compactionStrategyClass.getName(), timestamp, cfName, "compaction_strategy_class")); cf.addColumn(Column.create(json(compressionParameters.asThriftOptions()), timestamp, cfName, "compression_parameters")); cf.addColumn(Column.create(json(compactionStrategyOptions), timestamp, cfName, "compaction_strategy_options")); cf.addColumn(Column.create(indexInterval, timestamp, cfName, "index_interval")); cf.addColumn(Column.create(speculativeRetry.toString(), timestamp, cfName, "speculative_retry")); for (Map.Entry<ColumnIdentifier, Long> entry : droppedColumns.entrySet()) cf.addColumn(new Column(makeDroppedColumnName(entry.getKey()), LongType.instance.decompose(entry.getValue()), timestamp)); cf.addColumn(Column.create(aliasesToJson(partitionKeyColumns), timestamp, cfName, "key_aliases")); cf.addColumn(Column.create(aliasesToJson(clusteringColumns), timestamp, cfName, "column_aliases")); cf.addColumn(compactValueColumn == null ? DeletedColumn.create(ldt, timestamp, cfName, "value_alias") : Column.create(compactValueColumn.name.bytes, timestamp, cfName, "value_alias")); }', 'output': 'private void toSchemaNoColumnsNoTriggers(RowMutation rm, long timestamp) { ColumnFamily cf = rm.addOrGet(SchemaColumnFamiliesCf); int ldt = (int) (System.currentTimeMillis() / 1000); cf.addColumn(Column.create("", timestamp, cfName, "")); cf.addColumn(Column.create(cfType.toString(), timestamp, cfName, "type")); if (isSuper()) { CompositeType ct = (CompositeType)comparator; cf.addColumn(Column.create(ct.types.get(0).toString(), timestamp, cfName, "comparator")); cf.addColumn(Column.create(ct.types.get(1).toString(), timestamp, cfName, "subcomparator")); } else { cf.addColumn(Column.create(comparator.toString(), timestamp, cfName, "comparator")); } cf.addColumn(comment == null ? DeletedColumn.create(ldt, timestamp, cfName, "comment") : Column.create(comment, timestamp, cfName, "comment")); cf.addColumn(Column.create(readRepairChance, timestamp, cfName, "read_repair_chance")); cf.addColumn(Column.create(dcLocalReadRepairChance, timestamp, cfName, "local_read_repair_chance")); cf.addColumn(Column.create(replicateOnWrite, timestamp, cfName, "replicate_on_write")); cf.addColumn(Column.create(populateIoCacheOnFlush, timestamp, cfName, "populate_io_cache_on_flush")); cf.addColumn(Column.create(gcGraceSeconds, timestamp, cfName, "gc_grace_seconds")); cf.addColumn(Column.create(defaultValidator.toString(), timestamp, cfName, "default_validator")); cf.addColumn(Column.create(keyValidator.toString(), timestamp, cfName, "key_validator")); cf.addColumn(Column.create(minCompactionThreshold, timestamp, cfName, "min_compaction_threshold")); cf.addColumn(Column'}
{'instruction': 'Please refactor the following code:', 'input': 'private void call4(AtomicInteger cnt) throws Exception { Collection<String> args = F.asList("a", "b", "c"); IgniteCompute comp = compute(prj).withAsync(); comp.apply(clrJob, args); XXX fut = comp.future(); waitForExecution(fut); cnt.set(0); compute(prj).apply(clrJob, args); waitForValue(cnt, args.size()); }', 'ground_truth': 'private void call4(AtomicInteger cnt) throws Exception { Collection<String> args = F.asList("a", "b", "c"); IgniteCompute comp = compute(prj).withAsync(); comp.apply(clrJob, args); IgniteInternalFuture fut = comp.future(); waitForExecution(fut); cnt.set(0); compute(prj).apply(clrJob, args); waitForValue(cnt, args.size()); }', 'output': 'private void call4(AtomicInteger cnt) throws Exception { Collection<String> args = F.asList("a", "b", "c"); IgniteCompute comp = compute(prj).withAsync(); comp.apply(clrJob, args); IgniteFuture<?> fut = comp.future(); waitForExecution(fut); cnt.set(0); compute(prj).apply(clrJob, args); waitForValue(cnt, args.size()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testIncompatibleCaches1() throws Exception { XXX grid = grid(0); try (IgniteTx ignored = grid.transactions().txStart()) { GridCache<Object, Object> cacheA = grid.cache("cacheA"); GridCache<Object, Object> cacheC = grid.cache("cacheC"); cacheA.put("1", "2"); cacheC.put("1", "2"); fail("Must not allow to enlist caches with different stores to one transaction"); } catch (IgniteCheckedException e) { assertTrue(e.getMessage().startsWith("Failed to enlist new cache to existing transaction")); } }', 'ground_truth': 'public void testIncompatibleCaches1() throws Exception { IgniteEx grid = grid(0); try (IgniteTx ignored = grid.transactions().txStart()) { GridCache<Object, Object> cacheA = grid.cache("cacheA"); GridCache<Object, Object> cacheC = grid.cache("cacheC"); cacheA.put("1", "2"); cacheC.put("1", "2"); fail("Must not allow to enlist caches with different stores to one transaction"); } catch (IgniteCheckedException e) { assertTrue(e.getMessage().startsWith("Failed to enlist new cache to existing transaction")); } }', 'output': 'public void testIncompatibleCaches1() throws Exception { IgniteEx grid = grid(0); try (IgniteTx ignored = grid.transactions().txStart()) { Cache<Object, Object> cacheA = grid.jcache("cacheA"); Cache<Object, Object> cacheC = grid.jcache("cacheC"); cacheA.put("1", "2"); cacheC.put("1", "2"); fail("Must not allow to enlist caches with different stores to one transaction"); } catch (IgniteCheckedException e) { assertTrue(e.getMessage().startsWith("Failed to enlist new cache to existing transaction")); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public void applyDelta(PageMemory pageMem, long pageAddr) throws IgniteCheckedException { assert payload != null; XXX io = DataPageIO.VERSIONS.forPage(pageAddr); io.updateRow(pageAddr, itemId, pageMem.pageSize(), payload, null, 0); } /** {@inheritDoc} */ @Override public RecordType type() { return RecordType.DATA_PAGE_UPDATE_RECORD; } /** {@inheritDoc} */ @Override public String toString() { return S.toString(DataPageUpdateRecord.class, this, "super", super.toString()); } } }', 'ground_truth': '@Override public void applyDelta(PageMemory pageMem, long pageAddr) throws IgniteCheckedException { assert payload != null; AbstractDataPageIO io = PageIO.getPageIO(pageAddr); io.updateRow(pageAddr, itemId, pageMem.pageSize(), payload, null, 0); }', 'output': '@Override public void applyDelta(PageMemory pageMem, long pageAddr) throws IgniteCheckedException { assert payload != null; DataPageIO io = DataPageIO.VERSIONS.forPage(pageAddr); io.updateRow(pageAddr, itemId, pageMem.pageSize(), payload, null, 0); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void updatePartitionSingleMap(GridDhtPartitionsSingleMessage msg) { for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) { Integer cacheId = entry.getKey(); GridCacheContext cacheCtx = cctx.cacheContext(cacheId); GridDhtPartitionTopology top = cacheCtx != null ? cacheCtx.topology() : cctx.exchange().clientTopology(cacheId, this); top.update(exchId, entry.getValue(), msg.partitionUpdateCounters(cacheId)); } }', 'ground_truth': 'private void updatePartitionSingleMap(GridDhtPartitionsSingleMessage msg) { for (Map.Entry<Integer, GridDhtPartitionMap2> entry : msg.partitions().entrySet()) { Integer cacheId = entry.getKey(); GridCacheContext cacheCtx = cctx.cacheContext(cacheId); GridDhtPartitionTopology top = cacheCtx != null ? cacheCtx.topology() : cctx.exchange().clientTopology(cacheId, this); top.update(exchId, entry.getValue(), msg.partitionUpdateCounters(cacheId)); } }', 'output': 'private void updatePartitionSingleMap(GridDhtPartitionsSingleMessage msg) { for (Map.Entry<UUID, GridDhtPartitionMap> entry : msg.partitions().entrySet()) { UUID cacheId = entry.getKey(); GridCacheContext cacheCtx = cctx.cacheContext(cacheId); GridDhtPartitionTopology top = cacheCtx != null ? cacheCtx.topology() : cctx.exchange().clientTopology(cacheId, this); top.update(exchId, entry.getValue(), msg.partitionUpdateCounters(cacheId)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testContainsAllWithLargeNumberOfColumns() { List<String> names = new ArrayList<>(); for (int i = 0; i < 50; i++) names.add("clustering_" + i); XXX defs = new ArrayList<>(); addClustering(names, defs); Columns columns = Columns.from(new HashSet<>(defs)); defs = new ArrayList<>(); addClustering(names.subList(0, 8), defs); Columns subset = Columns.from(new HashSet<>(defs)); Assert.assertTrue(columns.containsAll(subset)); }', 'ground_truth': 'public void testContainsAllWithLargeNumberOfColumns() { List<String> names = new ArrayList<>(); for (int i = 0; i < 50; i++) names.add("clustering_" + i); List<ColumnMetadata> defs = new ArrayList<>(); addClustering(names, defs); Columns columns = Columns.from(new HashSet<>(defs)); defs = new ArrayList<>(); addClustering(names.subList(0, 8), defs); Columns subset = Columns.from(new HashSet<>(defs)); Assert.assertTrue(columns.containsAll(subset)); }', 'output': 'public void testContainsAllWithLargeNumberOfColumns() { List<String> names = new ArrayList<>(); for (int i = 0; i < 50; i++) names.add("clustering_" + i); TableDefs defs = new ArrayList<>(); addClustering(names, defs); Columns columns = Columns.from(new HashSet<>(defs)); defs = new ArrayList<>(); addClustering(names.subList(0, 8), defs); Columns subset = Columns.from(new HashSet<>(defs)); Assert.assertTrue(columns.containsAll(subset)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testTrainWithTheLinearlySeparableCase() { Map<Integer, double[]> data = new HashMap<>(); ThreadLocalRandom rndX = ThreadLocalRandom.current(); ThreadLocalRandom rndY = ThreadLocalRandom.current(); for (int i = 0; i < AMOUNT_OF_OBSERVATIONS; i++) { double x = rndX.nextDouble(-1000, 1000); double y = rndY.nextDouble(-1000, 1000); double[] vec = new double[AMOUNT_OF_FEATURES + 1]; vec[0] = y - x > 0 ? 1 : -1; vec[1] = x; vec[2] = y; data.put(i, vec); } SVMLinearMultiClassClassificationTrainer<Integer, double[]> trainer = new SVMLinearMultiClassClassificationTrainer<Integer, double[]>() .withLambda(0.3) .withAmountOfLocIterations(100) .withAmountOfIterations(20); SVMLinearMultiClassClassificationModel mdl = trainer.fit( new LocalDatasetBuilder<>(data, 10), (k, v) -> Arrays.copyOfRange(v, 1, v.length), (k, v) -> v[0], AMOUNT_OF_FEATURES); TestUtils.assertEquals(-1, mdl.apply(new DenseLocalOnHeapVector(new double[]{100, 10})), PRECISION); TestUtils.assertEquals(1, mdl.apply(new DenseLocalOnHeapVector(new double[]{10, 100})), PRECISION); }', 'ground_truth': 'public void testTrainWithTheLinearlySeparableCase() { Map<Integer, double[]> data = new HashMap<>(); ThreadLocalRandom rndX = ThreadLocalRandom.current(); ThreadLocalRandom rndY = ThreadLocalRandom.current(); for (int i = 0; i < AMOUNT_OF_OBSERVATIONS; i++) { double x = rndX.nextDouble(-1000, 1000); double y = rndY.nextDouble(-1000, 1000); double[] vec = new double[AMOUNT_OF_FEATURES + 1]; vec[0] = y - x > 0 ? 1 : -1; vec[1] = x; vec[2] = y; data.put(i, vec); } SVMLinearMultiClassClassificationTrainer trainer = new SVMLinearMultiClassClassificationTrainer() .withLambda(0.3) .withAmountOfLocIterations(100) .withAmountOfIterations(20); SVMLinearMultiClassClassificationModel mdl = trainer.fit( new LocalDatasetBuilder<>(data, 10), (k, v) -> Arrays.copyOfRange(v, 1, v.length), (k, v) -> v[0] ); TestUtils.assertEquals(-1, mdl.apply(new DenseLocalOnHeapVector(new double[]{100, 10})), PRECISION); TestUtils.assertEquals(1, mdl.apply(new DenseLocalOnHeapVector(new double[]{10, 100})), PRECISION); }', 'output': 'public void testTrainWithTheLinearlySeparableCase() { Map<Integer, double[]> data = new HashMap<>(); ThreadLocalRandom rndX = ThreadLocalRandom.current(); ThreadLocalRandom rndY = ThreadLocalRandom.current(); for (int i = 0; i < AMOUNT_OF_OBSERVATIONS; i++) { double x = rndX.nextDouble(-1000, 1000); double y = rndY.nextDouble(-1000, 1000); double[] vec = new double[AMOUNT_OF_FEATURES + 1]; vec[0] = y - x > 0 ? 1 : -1; vec[1] = x; vec[2] = y; data.put(i, vec); } SVMLinearMultiClassClassificationTrainer<Integer, double[]> trainer = new SVMLinearMultiClassClassificationTrainer<Integer, double[]>() .withLambda(0.3) .withAmountOfLocIterations(100) .withAmountOfIterations(20); SVMLinearMultiClassClassificationModel mdl = trainer.fit( new LocalDatasetBuilder<>(data, 10), (k, v) -> Arrays.copyOfRange(v, 1, v.length), (k, v) -> v[0], AMOUNT_OF_FEATURES); TestUtils.assertEquals(-1, mdl.apply(new DenseLocalOnHeapVector(new double[]{100, 10})), PRECISION); TestUtils.assertEquals(1, mdl.apply(new DenseLocalOnHeapVector(new double[]{10, 100})), PRECISION); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static int compareNullLast( @Nullable final String sequence1, @Nullable final String sequence2, final String separator) { if(_Strings.isEmpty(separator)) throw new IllegalArgumentException("a non empty separator is required"); if (sequence1 == null && sequence2 == null) { return 0; } if (sequence1 == null && sequence2 != null) { return +1; } if (sequence1 != null && sequence2 == null) { return -1; } final StringTokenizer components1 = tokenizerFor(sequence1, separator); final StringTokenizer components2 = tokenizerFor(sequence2, separator); final int length1 = components1.countTokens(); final int length2 = components2.countTokens(); if (length1 == 0 && length2 == 0) { return 0; } int n = 0; while (true) { final int length = n + 1; if (length1 < length && length2 >= length) { return -1; } if (length2 < length && length1 >= length) { return +1; } if (length1 < length && length2 < length) { return 0; } final String token1 = components1.nextToken(); final XXX token2 = components2.nextToken(); int componentCompare = 0; try { final int c1 = Integer.parseInt(token1); final int c2 = Integer.parseInt(token2); componentCompare = Integer.compare(c1, c2); } catch (final NumberFormatException nfe) { componentCompare = token1.compareTo(token2); } if (componentCompare != 0) { return componentCompare; } n++; } }', 'ground_truth': 'public static int compareNullLast( @Nullable final String sequence1, @Nullable final String sequence2, final String separator) { if(_Strings.isEmpty(separator)) throw new IllegalArgumentException("a non empty separator is required"); if (sequence1 == null && sequence2 == null) { return 0; } if (sequence1 == null && sequence2 != null) { return +1; } if (sequence1 != null && sequence2 == null) { return -1; } final StringTokenizer components1 = tokenizerFor(sequence1, separator); final StringTokenizer components2 = tokenizerFor(sequence2, separator); final int length1 = components1.countTokens(); final int length2 = components2.countTokens(); if (length1 == 0 && length2 == 0) { return 0; } int n = 0; while (true) { final int length = n + 1; if (length1 < length && length2 >= length) { return -1; } if (length2 < length && length1 >= length) { return +1; } if (length1 < length && length2 < length) { return 0; } val token1 = components1.nextToken(); val token2 = components2.nextToken(); int componentCompare = 0; val int1 = _Ints.parseInt(token1, 10); val int2 = _Ints.parseInt(token2, 10); if(int1.isPresent() && int2.isPresent()) { componentCompare = Integer.compare(int1.getAsInt(), int2.getAsInt()); } else { componentCompare = token1.compareTo(token2); } if (componentCompare != 0) { return componentCompare; } n++; } }', 'output': 'public static int compareNullLast( @Nullable final String sequence1, @Nullable final String sequence2, final char separator) { if(_Strings.isEmpty(separator)) throw new IllegalArgumentException("a non empty separator is required"); if (sequence1 == null && sequence2 == null) { return 0; } if (sequence1 == null && sequence2 != null) { return +1; } if (sequence1 != null && sequence2 == null) { return -1; } final StringTokenizer components1 = tokenizerFor(sequence1, separator); final StringTokenizer components2 = tokenizerFor(sequence2, separator); final int length1 = components1.countTokens(); final int length2 = components2.countTokens(); if (length1 == 0 && length2 == 0) { return 0; } int n = 0; while (true) { final int length = n + 1; if (length1 < length && length2 >= length) { return -1; } if (length2 < length && length1 >= length) { return +1; } if (length1 < length && length2 < length) { return 0; } final String token1 = components1.nextToken(); final String token2 = components2.nextToken(); int componentCompare = 0; try { final int c1 = Integer.parseInt(token1); final int c2 = Integer.parseInt(token2); componentCompare = Integer.compare(c1, c2); } catch (final NumberFormatException nfe) { componentCompare = token1.compareTo(token2); } if (componentCompare != 0) { return componentCompare; } n++; } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void fillGroups() { List<DocumentGroupImpl> groups = GroupsManager.getDefault().getGroups(); XXX model = new DefaultListModel(); for( DocumentGroupImpl group : groups ) { model.addElement( group ); } listGroups.setModel( model ); }', 'ground_truth': 'private void fillGroups() { List<DocumentGroupImpl> groups = GroupsManager.getDefault().getGroups(); DefaultListModel<DocumentGroupImpl> model = new DefaultListModel<>(); for (DocumentGroupImpl group : groups) { model.addElement( group ); } listGroups.setModel( model ); }', 'output': 'private void fillGroups() { List<DocumentGroupImpl> groups = GroupsManager.getDefault().getGroups(); ListModel<DocumentGroupImpl> model = new ListModel<>(); for( DocumentGroupImpl group : groups ) { model.addElement( group ); } listGroups.setModel( model ); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Expression transformExpression(ExpressionTransformer transformer) { List exprList = transformExpressions(expressions, transformer); XXX sizes = null; if (sizeExpression!=null) sizes = transformExpressions(sizeExpression,transformer); Expression ret = new ArrayExpression(elementType, exprList, sizes); ret.setSourcePosition(this); return ret; }', 'ground_truth': 'public Expression transformExpression(ExpressionTransformer transformer) { List<Expression> exprList = transformExpressions(expressions, transformer); List<Expression> sizes = null; if (sizeExpression!=null) sizes = transformExpressions(sizeExpression,transformer); Expression ret = new ArrayExpression(elementType, exprList, sizes); ret.setSourcePosition(this); return ret; }', 'output': 'public Expression transformExpression(ExpressionTransformer transformer) { List<Expression> exprList = transformExpressions(expressions, transformer); int[] sizes = null; if (sizeExpression != null) sizes = transformExpressions(sizeExpression, transformer); Expression ret = new ArrayExpression(elementType, exprList, sizes); ret.setSourcePosition(this); return ret; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testBuildXmlSink() { XXX sink = XmlFileBasedSink.write() .toFilenamePrefix(testFilePrefix) .ofRecordClass(testClass) .withRootElement(testRootElement); assertEquals(testClass, sink.classToBind); assertEquals(testRootElement, sink.rootElementName); assertEquals(testFilePrefix, sink.baseOutputFilename); } public void testBuildXmlSinkDirect() { XXX sink = XmlFileBasedSink.writeOf(Bird.class, testRootElement, testFilePrefix); assertEquals(testClass, sink.classToBind); assertEquals(testRootElement, sink.rootElementName); assertEquals(testFilePrefix, sink.baseOutputFilename); }', 'ground_truth': 'public void testBuildXmlSink() { XmlSink.Bound<Bird> sink = XmlSink.write() .toFilenamePrefix(testFilePrefix) .ofRecordClass(testClass) .withRootElement(testRootElement); assertEquals(testClass, sink.classToBind); assertEquals(testRootElement, sink.rootElementName); assertEquals(testFilePrefix, sink.baseOutputFilename); }', 'output': 'public void testBuildXmlSink() { Sink<?> sink = XmlFileBasedSink.write() .toFilenamePrefix(testFilePrefix) .ofRecordClass(testClass) .withRootElement(testRootElement); assertEquals(testClass, sink.classToBind); assertEquals(testRootElement, sink.rootElementName); assertEquals(testFilePrefix, sink.baseOutputFilename); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void remoteDCTest() throws Exception { Map<InetAddressAndPort, Mutation> repairs = new HashMap<>(); repairs.put(target1, mutation(cell1)); InetAddressAndPort remote1 = InetAddressAndPort.getByName("10.0.0.1"); InetAddressAndPort remote2 = InetAddressAndPort.getByName("10.0.0.2"); repairs.put(remote1, mutation(cell1)); XXX participants = Lists.newArrayList(target1, target2, remote1, remote2); InstrumentedReadRepairHandler handler = createRepairHandler(repairs, 2, participants); handler.sendInitialRepairs(); Assert.assertEquals(2, handler.mutationsSent.size()); Assert.assertTrue(handler.mutationsSent.containsKey(target1)); Assert.assertTrue(handler.mutationsSent.containsKey(remote1)); Assert.assertEquals(1, handler.waitingOn()); Assert.assertFalse(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); handler.ack(remote1); Assert.assertEquals(1, handler.waitingOn()); Assert.assertFalse(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); handler.ack(target1); Assert.assertEquals(0, handler.waitingOn()); Assert.assertTrue(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); }', 'ground_truth': 'public void remoteDCTest() throws Exception { Map<Replica, Mutation> repairs = new HashMap<>(); repairs.put(replica1, mutation(cell1)); Replica remote1 = ReplicaUtils.full(InetAddressAndPort.getByName("10.0.0.1")); Replica remote2 = ReplicaUtils.full(InetAddressAndPort.getByName("10.0.0.2")); repairs.put(remote1, mutation(cell1)); EndpointsForRange participants = EndpointsForRange.of(replica1, replica2, remote1, remote2); ReplicaLayout.ForRange replicaLayout = new ReplicaLayout.ForRange(ks, ConsistencyLevel.LOCAL_QUORUM, ReplicaUtils.FULL_BOUNDS, participants, participants); InstrumentedReadRepairHandler handler = createRepairHandler(repairs, 2, replicaLayout); handler.sendInitialRepairs(); Assert.assertEquals(2, handler.mutationsSent.size()); Assert.assertTrue(handler.mutationsSent.containsKey(replica1.endpoint())); Assert.assertTrue(handler.mutationsSent.containsKey(remote1.endpoint())); Assert.assertEquals(1, handler.waitingOn()); Assert.assertFalse(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); handler.ack(remote1.endpoint()); Assert.assertEquals(1, handler.waitingOn()); Assert.assertFalse(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); handler.ack(replica1.endpoint()); Assert.assertEquals(0, handler.waitingOn()); Assert.assertTrue(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); }', 'output': 'public void remoteDCTest() throws Exception { Map<InetAddressAndPort, Mutation> repairs = new HashMap<>(); repairs.put(target1, mutation(cell1)); InetAddressAndPort remote1 = InetAddressAndPort.getByName("10.0.0.1"); InetAddressAndPort remote2 = InetAddressAndPort.getByName("10.0.0.2"); repairs.put(remote1, mutation(cell1)); List<InetAddressAndPort> participants = Lists.newArrayList(target1, target2, remote1, remote2); InstrumentedReadRepairHandler handler = createRepairHandler(repairs, 2, participants); handler.sendInitialRepairs(); Assert.assertEquals(2, handler.mutationsSent.size()); Assert.assertTrue(handler.mutationsSent.containsKey(target1)); Assert.assertTrue(handler.mutationsSent.containsKey(remote1)); Assert.assertEquals(1, handler.waitingOn()); Assert.assertFalse(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); handler.ack(remote1); Assert.assertEquals(1, handler.waitingOn()); Assert.assertFalse(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); handler.ack(target1); Assert.assertEquals(0, handler.waitingOn()); Assert.assertTrue(handler.awaitRepairs(0, TimeUnit.NANOSECONDS)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void processStartRequest(UUID nodeId, GridContinuousMessage req) { assert nodeId != null; assert req != null; UUID routineId = req.routineId(); StartRequestData data = req.data(); GridContinuousHandler hnd = data.hnd; GridException err = null; try { if (ctx.config().isPeerClassLoadingEnabled()) { String clsName = data.clsName; if (clsName != null) { GridDeploymentInfo depInfo = data.depInfo; GridDeployment dep = ctx.deploy().getGlobalDeployment(depInfo.deployMode(), clsName, clsName, depInfo.userVersion(), nodeId, depInfo.classLoaderId(), depInfo.participants(), null); if (dep == null) throw new GridDeploymentException("Failed to obtain deployment for class: " + clsName); data.p2pUnmarshal(marsh, dep.classLoader()); } hnd.p2pUnmarshal(nodeId, ctx); } } catch (GridException e) { err = e; U.error(log, "Failed to register handler [nodeId=" + nodeId + ", routineId=" + routineId + \']\', e); } boolean registered = false; if (err == null) { try { XXX prjPred = data.prjPred; if (prjPred == null || prjPred.apply(ctx.discovery().node(ctx.localNodeId()))) { registered = registerHandler(nodeId, routineId, hnd, data.bufSize, data.interval, data.autoUnsubscribe, false); } } catch (GridException e) { err = e; U.error(log, "Failed to register handler [nodeId=" + nodeId + ", routineId=" + routineId + \']\', e); } } try { sendWithRetries(nodeId, new GridContinuousMessage(MSG_START_ACK, routineId, err), null); } catch (GridTopologyException ignored) { if (log.isDebugEnabled()) log.debug("Failed to send start acknowledgement to node (is node alive?): " + nodeId); } catch (GridException e) { U.error(log, "Failed to send start acknowledgement to node: " + nodeId, e); } if (registered) hnd.onListenerRegistered(routineId, ctx); }', 'ground_truth': 'private void processStartRequest(UUID nodeId, GridContinuousMessage req) { assert nodeId != null; assert req != null; UUID routineId = req.routineId(); StartRequestData data = req.data(); GridContinuousHandler hnd = data.hnd; GridException err = null; try { if (ctx.config().isPeerClassLoadingEnabled()) { String clsName = data.clsName; if (clsName != null) { GridDeploymentInfo depInfo = data.depInfo; GridDeployment dep = ctx.deploy().getGlobalDeployment(depInfo.deployMode(), clsName, clsName, depInfo.userVersion(), nodeId, depInfo.classLoaderId(), depInfo.participants(), null); if (dep == null) throw new GridDeploymentException("Failed to obtain deployment for class: " + clsName); data.p2pUnmarshal(marsh, dep.classLoader()); } hnd.p2pUnmarshal(nodeId, ctx); } } catch (GridException e) { err = e; U.error(log, "Failed to register handler [nodeId=" + nodeId + ", routineId=" + routineId + \']\', e); } boolean registered = false; if (err == null) { try { IgnitePredicate<ClusterNode> prjPred = data.prjPred; if (prjPred == null || prjPred.apply(ctx.discovery().node(ctx.localNodeId()))) { registered = registerHandler(nodeId, routineId, hnd, data.bufSize, data.interval, data.autoUnsubscribe, false); } } catch (GridException e) { err = e; U.error(log, "Failed to register handler [nodeId=" + nodeId + ", routineId=" + routineId + \']\', e); } } try { sendWithRetries(nodeId, new GridContinuousMessage(MSG_START_ACK, routineId, err), null); } catch (GridTopologyException ignored) { if (log.isDebugEnabled()) log.debug("Failed to send start acknowledgement to node (is node alive?): " + nodeId); } catch (GridException e) { U.error(log, "Failed to send start acknowledgement to node: " + nodeId, e); } if (registered) hnd.onListenerRegistered(routineId, ctx); }', 'output': 'private void processStartRequest(UUID nodeId, GridContinuousMessage req) { assert nodeId != null; assert req != null; UUID routineId = req.routineId(); StartRequestData data = req.data(); GridContinuousHandler hnd = data.hnd; GridException err = null; try { if (ctx.config().isPeerClassLoadingEnabled()) { String clsName = data.clsName; if (clsName != null) { GridDeploymentInfo depInfo = data.depInfo; GridDeployment dep = ctx.deploy().getGlobalDeployment(depInfo.deployMode(), clsName, clsName, depInfo.userVersion(), nodeId, depInfo.classLoaderId(), depInfo.participants(), null); if (dep == null) throw new GridDeploymentException("Failed to obtain deployment for class: " + clsName); data.p2pUnmarshal(marsh, dep.classLoader()); } hnd.p2pUnmarshal(nodeId, ctx); } } catch (GridException e) { err = e; U.error(log, "Failed to register handler [nodeId=" + nodeId + ", routineId=" + routineId + \']\', e); } boolean registered = false; if (err == null) { try { XXX prjPred = data.prjPred; if (prjPred == null || prjPred.apply(ctx.discovery().node(ctx.localNodeId()))) { registered = registerHandler(nodeId, routineId, hnd, data.bufSize, data.interval, data.autoUnsubscribe, false); } } catch (GridException e) { err = e; U.error(log, "Failed to register handler [nodeId=" + nodeId + ", routineId=" + routineId + \']\', e); } } try { sendWithRetries(nodeId, new GridContinuousMessage(MSG_START_ACK, routineId, err), null); } catch (GridTopologyException ignored) { if (log.isDebugEnabled()) log.debug("Failed to send start acknowledgement to node (is node alive?): " + nodeId); } catch (GridException e) { U.error(log, "Failed to send start acknowledgement to node: " + nodeId, e); }'}
{'instruction': 'Please refactor the following code:', 'input': 'public static XXX suite() { XXX suite = new TestSuite("Ignite Persistent Store Test Suite (with page compression)."); suite.addTest(new JUnit4TestAdapter(CompressionConfigurationTest.class)); suite.addTest(new JUnit4TestAdapter(CompressionProcessorTest.class)); suite.addTest(new JUnit4TestAdapter(FileSystemUtilsTest.class)); suite.addTest(new JUnit4TestAdapter(DiskPageCompressionIntegrationTest.class)); suite.addTest(new JUnit4TestAdapter(DiskPageCompressionIntegrationAsyncTest.class)); enableCompressionByDefault(); IgnitePdsTestSuite.addRealPageStoreTests(suite, null); return suite; }', 'ground_truth': 'public static List<Class<?>> suite() { List<Class<?>> suite = new ArrayList<>(); suite.add(CompressionConfigurationTest.class); suite.add(CompressionProcessorTest.class); suite.add(FileSystemUtilsTest.class); suite.add(DiskPageCompressionIntegrationTest.class); suite.add(DiskPageCompressionIntegrationAsyncTest.class); enableCompressionByDefault(); IgnitePdsTestSuite.addRealPageStoreTests(suite, null); return suite; }', 'output': 'public static TestSuite suite() { TestSuite suite = new TestSuite("Ignite Persistent Store Test Suite (with page compression)."); suite.addTest(new JUnit4TestAdapter(CompressionConfigurationTest.class)); suite.addTest(new JUnit4TestAdapter(CompressionProcessorTest.class)); suite.addTest(new JUnit4TestAdapter(FileSystemUtilsTest.class)); suite.addTest(new JUnit4TestAdapter(DiskPageCompressionIntegrationTest.class)); suite.addTest(new JUnit4TestAdapter(DiskPageCompressionIntegrationAsyncTest.class)); enableCompressionByDefault(); IgnitePdsTestSuite.addRealPageStoreTests(suite, null); return suite; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private List<GridEvent> pullEvents(long since, int evtCnt) throws Exception { XXX filter = new CustomEventFilter(GridAllEventsTestTask.class.getName(), since); for (int i = 0; i < 3; i++) { List<GridEvent> evts = new ArrayList<>(ignite.events().localQuery((filter))); info("Filtered events [size=" + evts.size() + ", evts=" + evts + \']\'); if (evtCnt != evts.size() && i < 2) { U.warn(log, "Invalid event count (will retry in 1000 ms) [actual=" + evts.size() + ", expected=" + evtCnt + ", evts=" + evts + \']\'); U.sleep(1000); continue; } assert evtCnt <= evts.size() : "Invalid event count [actual=" + evts.size() + ", expected=" + evtCnt + ", evts=" + evts + \']\'; return evts; } assert false; return null; }', 'ground_truth': 'private List<GridEvent> pullEvents(long since, int evtCnt) throws Exception { IgnitePredicate<GridEvent> filter = new CustomEventFilter(GridAllEventsTestTask.class.getName(), since); for (int i = 0; i < 3; i++) { List<GridEvent> evts = new ArrayList<>(ignite.events().localQuery((filter))); info("Filtered events [size=" + evts.size() + ", evts=" + evts + \']\'); if (evtCnt != evts.size() && i < 2) { U.warn(log, "Invalid event count (will retry in 1000 ms) [actual=" + evts.size() + ", expected=" + evtCnt + ", evts=" + evts + \']\'); U.sleep(1000); continue; } assert evtCnt <= evts.size() : "Invalid event count [actual=" + evts.size() + ", expected=" + evtCnt + ", evts=" + evts + \']\'; return evts; } assert false; return null; }', 'output': 'private List<GridEvent> pullEvents(long since, int evtCnt) throws Exception { CustomEventFilter filter = new CustomEventFilter(GridAllEventsTestTask.class.getName(), since); for (int i = 0; i < 3; i++) { List<GridEvent> evts = ignite.events().localQuery(filter); info("Filtered events [size=" + evts.size() + ", evts=" + evts + \']\'); if (evtCnt != evts.size() && i < 2) { U.warn(log, "Invalid event count (will retry in 1000 ms) [actual=" + evts.size() + ", expected=" + evtCnt + ", evts=" + evts + \']\'); U.sleep(1000); continue; } assert evtCnt <= evts.size() : "Invalid event count [actual=" + evts.size() + ", expected=" + evtCnt + ", evts=" + evts + \']\'; return evts; } assert false; return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPutGetMultithreadBounded() throws Exception { String queueName = UUID.randomUUID().toString(); final XXX queue = grid().cache(null).dataStructures() .queue(queueName, QUEUE_CAPACITY, false, true); multithreaded(new Callable<String>() { @Override public String call() throws Exception { String thName = Thread.currentThread().getName(); for (int i = 0; i < QUEUE_CAPACITY * 5; i++) { queue.put(thName); queue.peek(); queue.take(); } return ""; } }, THREAD_NUM); assert queue.isEmpty() : queue.size(); }', 'ground_truth': 'public void testPutGetMultithreadBounded() throws Exception { String queueName = UUID.randomUUID().toString(); final CacheQueue<String> queue = grid().cache(null).dataStructures() .queue(queueName, QUEUE_CAPACITY, false, true); multithreaded(new Callable<String>() { @Override public String call() throws Exception { String thName = Thread.currentThread().getName(); for (int i = 0; i < QUEUE_CAPACITY * 5; i++) { queue.put(thName); queue.peek(); queue.take(); } return ""; } }, THREAD_NUM); assert queue.isEmpty() : queue.size(); }', 'output': 'public void testPutGetMultithreadBounded() throws Exception { String queueName = UUID.randomUUID().toString(); final IgniteCache<String, String> queue = grid().cache(null).dataStructures() .queue(queueName, QUEUE_CAPACITY, false, true); multithreaded(new Callable<String>() { @Override public String call() throws Exception { String thName = Thread.currentThread().getName(); for (int i = 0; i < QUEUE_CAPACITY * 5; i++) { queue.put(thName); queue.peek(); queue.take(); } return ""; } }, THREAD_NUM); assert queue.isEmpty() : queue.size(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMultipleAnnotationsJob() throws Exception { try { grid(0).compute().call(new MultipleAnnotationsJob(0)); fail(); } catch (XXX e) { info("Caught expected exception: " + e); } }', 'ground_truth': 'public void testMultipleAnnotationsJob() throws Exception { try { grid(0).compute().call(new MultipleAnnotationsJob(0)); fail(); } catch (IgniteException e) { info("Caught expected exception: " + e); } }', 'output': 'public void testMultipleAnnotationsJob() throws Exception { try { grid(0).compute().call(new MultipleAnnotationsJob(0)); fail(); } catch (IgniteException e) { info("Caught expected exception: " + e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkTransactionConfiguration(ClusterNode rmt) throws IgniteCheckedException { TransactionsConfiguration txCfg = rmt.attribute(ATTR_TX_CONFIG); if (txCfg != null) { XXX locTxCfg = ctx.config().getTransactionsConfiguration(); if (locTxCfg.isTxSerializableEnabled() != txCfg.isTxSerializableEnabled()) throw new IgniteCheckedException("Serializable transactions enabled mismatch " + "(fix txSerializableEnabled property or set -D" + IGNITE_SKIP_CONFIGURATION_CONSISTENCY_CHECK + "=true " + "system property) [rmtNodeId=" + rmt.id() + ", locTxSerializableEnabled=" + locTxCfg.isTxSerializableEnabled() + ", rmtTxSerializableEnabled=" + txCfg.isTxSerializableEnabled() + \']\'); } }', 'ground_truth': 'private void checkTransactionConfiguration(ClusterNode rmt) throws IgniteCheckedException { TransactionConfiguration txCfg = rmt.attribute(ATTR_TX_CONFIG); if (txCfg != null) { TransactionConfiguration locTxCfg = ctx.config().getTransactionsConfiguration(); if (locTxCfg.isTxSerializableEnabled() != txCfg.isTxSerializableEnabled()) throw new IgniteCheckedException("Serializable transactions enabled mismatch " + "(fix txSerializableEnabled property or set -D" + IGNITE_SKIP_CONFIGURATION_CONSISTENCY_CHECK + "=true " + "system property) [rmtNodeId=" + rmt.id() + ", locTxSerializableEnabled=" + locTxCfg.isTxSerializableEnabled() + ", rmtTxSerializableEnabled=" + txCfg.isTxSerializableEnabled() + \']\'); } }', 'output': 'private void checkTransactionConfiguration(ClusterNode rmt) throws IgniteCheckedException { TransactionsConfiguration txCfg = rmt.attribute(ATTR_TX_CONFIG); if (txCfg != null) { TransactionsConfiguration locTxCfg = ctx.configuration().getTransactionsConfiguration(); if (locTxCfg.isTxSerializableEnabled() != txCfg.isTxSerializableEnabled()) throw new IgniteCheckedException("Serializable transactions enabled mismatch " + "(fix txSerializableEnabled property or set -D" + IGNITE_SKIP_CONFIGURATION_CONSISTENCY_CHECK + "=true " + "system property) [rmtNodeId=" + rmt.id() + ", locTxSerializableEnabled=" + locTxCfg.isTxSerializableEnabled() + ", rmtTxSerializableEnabled=" + txCfg.isTxSerializableEnabled() + \']\'); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private GridMergeTable createTable(Connection conn, GridCacheSqlQuery qry) { try { try (PreparedStatement s = conn.prepareStatement( "CREATE LOCAL TEMPORARY TABLE " + qry.alias() + " ENGINE \\"" + GridMergeTable.Engine.class.getName() + "\\" " + " AS SELECT * FROM (" + qry.query() + ") WHERE FALSE")) { h2.bindParameters(s, F.asList(qry.parameters())); s.execute(); } return GridMergeTable.Engine.getCreated(); } catch (XXX e) { throw new IgniteException(e); } }', 'ground_truth': 'private GridMergeTable createTable(Connection conn, GridCacheSqlQuery qry) throws IgniteCheckedException { try { try (PreparedStatement s = conn.prepareStatement( "CREATE LOCAL TEMPORARY TABLE " + qry.alias() + " ENGINE \\"" + GridMergeTable.Engine.class.getName() + "\\" " + " AS SELECT * FROM (" + qry.query() + ") WHERE FALSE")) { h2.bindParameters(s, F.asList(qry.parameters())); s.execute(); } return GridMergeTable.Engine.getCreated(); } catch (SQLException e) { U.closeQuiet(conn); throw new IgniteCheckedException(e); } }', 'output': 'private GridMergeTable createTable(Connection conn, GridCacheSqlQuery qry) { try { try (PreparedStatement s = conn.prepareStatement( "CREATE LOCAL TEMPORARY TABLE " + qry.alias() + " ENGINE \\"" + GridMergeTable.Engine.class.getName() + "\\" " + " AS SELECT * FROM (" + qry.query() + ") WHERE FALSE")) { h2.bindParameters(s, F.asList(qry.parameters())); s.execute(); } return GridMergeTable.Engine.getCreated(); } catch (SQLException e) { throw new IgniteException(e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void replaceStar(ColumnProvider tableReader) { if (_select.hasAsteriskQualifier()) { ArrayList columns = new ArrayList(); List<Table> tables = _from.getTables(); for (int i=0; i<tables.size(); i++) { TableNode tbl = (TableNode) tables.get(i); String fullTableName = tbl.getFullTableName(); XXX columnNames = new ArrayList(); tableReader.getColumnNames(fullTableName, columnNames); String corrName=tbl.getCorrName(); String tableName=tbl.getTableName(); String schemaName=tbl.getSchemaName(); for (int j=0; j<columnNames.size(); j++) { String columnName = (String) columnNames.get(j); columns.add(new ColumnNode(tableName, columnName, corrName, schemaName)); } } _select.setColumnList(columns); } }', 'ground_truth': 'public void replaceStar(ColumnProvider tableReader) { if (_select.hasAsteriskQualifier()) { List<ColumnNode> columns = new ArrayList<>(); List<Table> tables = _from.getTables(); for (int i=0; i<tables.size(); i++) { TableNode tbl = (TableNode) tables.get(i); String fullTableName = tbl.getFullTableName(); List<String> columnNames = new ArrayList<>(); tableReader.getColumnNames(fullTableName, columnNames); String corrName=tbl.getCorrName(); String tableName=tbl.getTableName(); String schemaName=tbl.getSchemaName(); for (int j=0; j<columnNames.size(); j++) { String columnName = (String) columnNames.get(j); columns.add(new ColumnNode(tableName, columnName, corrName, schemaName)); } } _select.setColumnList(columns); } }', 'output': 'public void replaceStar(ColumnProvider tableReader) { if (_select.hasAsteriskQualifier()) { ArrayList columns = new ArrayList(); List<Table> tables = _from.getTables(); for (int i=0; i<tables.size(); i++) { TableNode tbl = (TableNode) tables.get(i); String fullTableName = tbl.getFullTableName(); List<String> columnNames = new ArrayList(); tableReader.getColumnNames(fullTableName, columnNames); String corrName=tbl.getCorrName(); String tableName=tbl.getTableName(); String schemaName=tbl.getSchemaName(); for (int j=0; j<columnNames.size(); j++) { String columnName = (String) columnNames.get(j); columns.add(new ColumnNode(tableName, columnName, corrName, schemaName)); } } _select.setColumnList(columns); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testLinkedListOverride() { TestObjectContainer obj = new TestObjectContainer(); XXX mutObj = wrap(obj); List<Object> list = Lists.newLinkedList(Arrays.asList(mutObj, "a", Lists.newLinkedList(Arrays.asList(1, 2)))); mutObj.setField("foo", list); TestObjectContainer res = mutObj.build().deserialize(); list.set(0, res); assertNotSame(list, res.foo); assertEquals(list, res.foo); }', 'ground_truth': 'public void testLinkedListOverride() { TestObjectContainer obj = new TestObjectContainer(); BinaryObjectBuilderImpl mutObj = wrap(obj); List<Object> list = Lists.newLinkedList(Arrays.asList(mutObj, "a", Lists.newLinkedList(Arrays.asList(1, 2)))); mutObj.setField("foo", list); TestObjectContainer res = mutObj.build().deserialize(); list.set(0, res); assertNotSame(list, res.foo); assertEquals(list, res.foo); }', 'output': 'public void testLinkedListOverride() { TestObjectContainer obj = new TestObjectContainer(); ObjectContainer mutObj = wrap(obj); List<Object> list = Lists.newLinkedList(Arrays.asList(mutObj, "a", Lists.newLinkedList(Arrays.asList(1, 2)))); mutObj.setField("foo", list); TestObjectContainer res = mutObj.build().deserialize(); list.set(0, res); assertNotSame(list, res.foo); assertEquals(list, res.foo); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testBoundsAsClusteringWithSeveralMultiColumnRestrictions() { TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC); ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); ByteBuffer value4 = ByteBufferUtil.bytes(4); ByteBuffer value5 = ByteBufferUtil.bytes(5); Restriction multiEq = newMultiEq(tableMetadata, 0, value1, value2); Restriction multiSlice = newMultiSlice(tableMetadata, 2, Bound.START, false, value3, value4); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiSlice); XXX bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3, value4); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value2); multiEq = newMultiEq(tableMetadata, 0, value1, value2); Restriction multiIN = newMultiIN(tableMetadata, 2, asList(value3, value4), asList(value4, value5)); restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiIN); bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertStartBound(get(bounds, 0), true, value1, value2, value3, value4); assertStartBound(get(bounds, 1), true, value1, value2, value4, value5); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value2, value3, value4); assertEndBound(get(bounds, 1), true, value1, value2, value4, value5); multiEq = newMultiEq(tableMetadata, 0, value1, value2); Restriction multiEq2 = newMultiEq(tableMetadata, 2, value3, value4); restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiEq2); bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), true, value1, value2, value3, value4); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value2, value3, value4); }', 'ground_truth': 'public void testBoundsAsClusteringWithSeveralMultiColumnRestrictions() { TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC); ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); ByteBuffer value4 = ByteBufferUtil.bytes(4); ByteBuffer value5 = ByteBufferUtil.bytes(5); Restriction multiEq = newMultiEq(tableMetadata, 0, value1, value2); Restriction multiSlice = newMultiSlice(tableMetadata, 2, Bound.START, false, value3, value4); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiSlice); SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3, value4); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value2); multiEq = newMultiEq(tableMetadata, 0, value1, value2); Restriction multiIN = newMultiIN(tableMetadata, 2, asList(value3, value4), asList(value4, value5)); restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiIN); bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertStartBound(get(bounds, 0), true, value1, value2, value3, value4); assertStartBound(get(bounds, 1), true, value1, value2, value4, value5); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(2, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value2, value3, value4); assertEndBound(get(bounds, 1), true, value1, value2, value4, value5); multiEq = newMultiEq(tableMetadata, 0, value1, value2); Restriction multiEq2 = newMultiEq(tableMetadata, 2, value3, value4); restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiEq2); bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), true, value1, value2, value3, value4); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value2, value3, value4); }', 'output': 'public void testBoundsAsClusteringWithSeveralMultiColumnRestrictions() { TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC); ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); ByteBuffer value4 = ByteBufferUtil.bytes(4); ByteBuffer value5 = ByteBufferUtil.bytes(5); Restriction multiEq = newMultiEq(tableMetadata, 0, value1, value2); Restriction multiSlice = newMultiSlice(tableMetadata, 2, Bound.START, false, value3, value4); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiSlice); List<ClusteringBound> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3, value4); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value2); multiEq = newMultiEq(tableMetadata, 0, value1, value2); Restriction multiIN = newMultiIN(tableMetadata, 2, asList(value3, value4), asList(value4, value5)); restrictions = new ClusteringColumnRestrictions(tableMetadata); restrictions = restrictions.mergeWith(multiEq).mergeWith(multiIN); List<ClusteringBound> bounds2 = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(2, bounds2.size()); assertStartBound(get(bounds2, 0), true, value1, value2, value3, value4); assertStartBound(get(bounds2, 1), true, value1, value2, value4, value5); bounds2 = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(2, bounds2.size()); assertEndBound(get('}
{'instruction': 'Please refactor the following code:', 'input': 'public void testJoinManyToOneMapping() { leftListOfKv.add(KV.of("Key2", 4L)); leftListOfKv.add(KV.of("Key2", 6L)); PCollection<KV<String, Long>> leftCollection = p.apply(Create.of(leftListOfKv)); listRightOfKv.add(KV.of("Key2", "bar")); PCollection<KV<String, String>> rightCollection = p.apply(Create.of(listRightOfKv)); PCollection<KV<Long, String>> output = Join.leftOuterJoin( leftCollection, rightCollection, ""); expectedResult.add(KV.of(4L, "bar")); expectedResult.add(KV.of(6L, "bar")); DataflowAssert.that(output).containsInAnyOrder(expectedResult); p.run(); }', 'ground_truth': 'public void testJoinManyToOneMapping() { leftListOfKv.add(KV.of("Key2", 4L)); leftListOfKv.add(KV.of("Key2", 6L)); PCollection<KV<String, Long>> leftCollection = p.apply(Create.of(leftListOfKv)); listRightOfKv.add(KV.of("Key2", "bar")); PCollection<KV<String, String>> rightCollection = p.apply(Create.of(listRightOfKv)); PCollection<KV<String, KV<Long, String>>> output = Join.leftOuterJoin( leftCollection, rightCollection, ""); expectedResult.add(KV.of("Key2", KV.of(4L, "bar"))); expectedResult.add(KV.of("Key2", KV.of(6L, "bar"))); DataflowAssert.that(output).containsInAnyOrder(expectedResult); p.run(); }', 'output': 'public void testJoinManyToOneMapping() { leftListOfKv.add(KV.of("Key2", 4L)); leftListOfKv.add(KV.of("Key2", 6L)); PCollection<KV<String, Long>> leftCollection = p.apply(Create.of(leftListOfKv)); listRightOfKv.add(KV.of("Key2", "bar")); PCollection<KV<String, String>> rightCollection = p.apply(Create.of(listRightOfKv)); PCollection<KV<Long, String>> output = Join.leftOuterJoin( leftCollection, rightCollection, ""); expectedResult.add(KV.of(4L, "bar")); expectedResult.add(KV.of(6L, "bar")); DataflowAssert.that(output).containsInAnyOrder(expectedResult); p.run(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCombiningIsEmpty() throws Exception { CombiningValueState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR); assertThat(value.isEmpty().read(), Matchers.is(true)); XXX readFuture = value.isEmpty(); value.add(5); assertThat(readFuture.read(), Matchers.is(false)); value.clear(); assertThat(readFuture.read(), Matchers.is(true)); }', 'ground_truth': 'public void testCombiningIsEmpty() throws Exception { CombiningState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR); assertThat(value.isEmpty().read(), Matchers.is(true)); ReadableState<Boolean> readFuture = value.isEmpty(); value.add(5); assertThat(readFuture.read(), Matchers.is(false)); value.clear(); assertThat(readFuture.read(), Matchers.is(true)); }', 'output': 'public void testCombiningIsEmpty() throws Exception { CombiningValueState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR); assertThat(value.isEmpty().read(), Matchers.is(true)); Future<Boolean> readFuture = value.isEmpty(); value.add(5); assertThat(readFuture.read(), Matchers.is(false)); value.clear(); assertThat(readFuture.read(), Matchers.is(true)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public final List<? extends FileBasedSource<T>> split( long desiredBundleSizeBytes, PipelineOptions options) throws Exception { if (mode == Mode.FILEPATTERN) { long startTime = System.currentTimeMillis(); List<ListenableFuture<List<? extends FileBasedSource<T>>>> futures = new ArrayList<>(); ListeningExecutorService service = MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(THREAD_POOL_SIZE)); try { checkState(fileOrPatternSpec.isAccessible(), "Bundle splitting should only happen at execution time."); XXX expandedFiles = FileBasedSource.expandFilePattern(fileOrPatternSpec.get()); checkArgument(!expandedFiles.isEmpty(), "Unable to find any files matching %s", fileOrPatternSpec.get()); for (final String file : expandedFiles) { futures.add(createFutureForFileSplit(file, desiredBundleSizeBytes, options, service)); } List<? extends FileBasedSource<T>> splitResults = ImmutableList.copyOf(Iterables.concat(Futures.allAsList(futures).get())); LOG.info( "Splitting filepattern {} into bundles of size {} took {} ms " + "and produced {} files and {} bundles", fileOrPatternSpec, desiredBundleSizeBytes, System.currentTimeMillis() - startTime, expandedFiles.size(), splitResults.size()); return splitResults; } finally { service.shutdown(); } } else { if (isSplittable()) { List<FileBasedSource<T>> splitResults = new ArrayList<>(); for (OffsetBasedSource<T> split : super.split(desiredBundleSizeBytes, options)) { splitResults.add((FileBasedSource<T>) split); } return splitResults; } else { LOG.debug("The source for file {} is not split into sub-range based sources since " + "the file is not seekable", fileOrPatternSpec); return ImmutableList.of(this); } } }', 'ground_truth': 'public final List<? extends FileBasedSource<T>> split( long desiredBundleSizeBytes, PipelineOptions options) throws Exception { if (mode == Mode.FILEPATTERN) { long startTime = System.currentTimeMillis(); checkState(fileOrPatternSpec.isAccessible(), "Bundle splitting should only happen at execution time."); List<Metadata> expandedFiles = FileBasedSource.expandFilePattern(fileOrPatternSpec.get()); checkArgument(!expandedFiles.isEmpty(), "Unable to find any files matching %s", fileOrPatternSpec.get()); List<FileBasedSource<T>> splitResults = new ArrayList<>(expandedFiles.size()); for (Metadata metadata : expandedFiles) { FileBasedSource<T> split = createForSubrangeOfFile(metadata, 0, metadata.sizeBytes()); verify(split.getMode() == Mode.SINGLE_FILE_OR_SUBRANGE, "%s.createForSubrangeOfFile must return a source in mode %s", split, Mode.SINGLE_FILE_OR_SUBRANGE); splitResults.addAll(split.split(desiredBundleSizeBytes, options)); } LOG.info( "Splitting filepattern {} into bundles of size {} took {} ms " + "and produced {} files and {} bundles", fileOrPatternSpec, desiredBundleSizeBytes, System.currentTimeMillis() - startTime, expandedFiles.size(), splitResults.size()); return splitResults; } else { if (isSplittable()) { @SuppressWarnings("unchecked") List<FileBasedSource<T>> splits = (List<FileBasedSource<T>>) super.split(desiredBundleSizeBytes, options); return splits; } else { LOG.debug("The source for file {} is not split into sub-range based sources since " + "the file is not seekable", fileOrPatternSpec); return ImmutableList.of(this); } } }', 'output': 'public final List<? extends FileBasedSource<?>> split( long desiredBundleSizeBytes, PipelineOptions options) throws Exception { if (mode == Mode.FILEPATTERN) { long startTime = System.currentTimeMillis(); List<ListenableFuture<List<? extends FileBasedSource<?>>>> futures = new ArrayList<>(); ListeningExecutorService service = MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(THREAD_POOL_SIZE)); try { checkState(fileOrPatternSpec.isAccessible(), "Bundle splitting should only happen at execution time."); XXX expandedFiles = FileBasedSource.expandFilePattern(fileOrPatternSpec.get()); checkArgument(!expandedFiles.isEmpty(), "Unable to find any files matching %s", fileOrPatternSpec.get()); for (final String file : expandedFiles) { futures.add(createFutureForFileSplit(file, desiredBundleSizeBytes, options, service)); } List<? extends FileBasedSource<?>> splitResults = ImmutableList.copyOf(Iterables.concat(Futures.allAsList(futures).get())); LOG.info( "Splitting filepattern {} into bundles of size {} took {} ms " + "and produced {} files and {} bundles", fileOrPatternSpec, desiredBundleSizeBytes, System.currentTimeMillis() - startTime, expandedFiles.size(), splitResults.size()); return splitResults; } finally { service.shutdown(); } } else { if (isSplittable()) { List<FileBasedSource<?>> splitResults = new ArrayList<>(); for (OffsetBasedSource<?> split : super.split(desiredBundleSizeBytes, options)) { splitResults.add((FileBasedSource<?>) split); } return splitResults; } else { LOG.debug("The source for file {} is not split into sub-range based sources since " + "the file is not seekable", fileOrPatternSpec); return ImmutableList.of(this); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<DataSink<?>> translateInto(Flow flow) { DAG<Operator<?, ?>> dag = flowToDag(flow); SparkExecutorContext executorContext = new SparkExecutorContext(sparkEnv, dag); dag.traverse().map(Node::get).forEach(op -> { Translation tx = translations.get(op.getClass()); if (tx == null) { throw new UnsupportedOperationException( "Operator " + op.getClass().getSimpleName() + " not supported"); } Preconditions.checkState( tx.accept == null || Boolean.TRUE.equals(tx.accept.apply(op))); JavaRDD<?> out = tx.translator.translate(op, executorContext); executorContext.setOutput(op, out); }); final List<DataSink<?>> sinks = new ArrayList<>(); dag.getLeafs() .stream() .map(Node::get) .filter(op -> op.output().getOutputSink() != null) .forEach(op -> { final DataSink<?> sink = op.output().getOutputSink(); sinks.add(sink); XXX sparkOutput = Objects.requireNonNull((JavaRDD) executorContext.getOutput(op)); JavaPairRDD<NullWritable, Object> unwrapped = sparkOutput.mapToPair(el -> new Tuple2<>(NullWritable.get(), el.getElement())); try { Configuration conf = DataSinkOutputFormat.configure(new Configuration(), sink); conf.set(JobContext.OUTPUT_FORMAT_CLASS_ATTR, DataSinkOutputFormat.class.getName()); unwrapped.saveAsNewAPIHadoopDataset(conf); } catch (IOException e) { throw new RuntimeException(); } }); return sinks; }', 'ground_truth': 'public List<DataSink<?>> translateInto(Flow flow) { DAG<Operator<?, ?>> dag = flowToDag(flow); SparkExecutorContext executorContext = new SparkExecutorContext(sparkEnv, dag); dag.traverse().map(Node::get).forEach(op -> { Translation tx = translations.get(op.getClass()); if (tx == null) { throw new UnsupportedOperationException( "Operator " + op.getClass().getSimpleName() + " not supported"); } Preconditions.checkState( tx.accept == null || Boolean.TRUE.equals(tx.accept.apply(op))); JavaRDD<?> out = tx.translator.translate(op, executorContext); executorContext.setOutput(op, out); }); final List<DataSink<?>> sinks = new ArrayList<>(); dag.getLeafs() .stream() .map(Node::get) .filter(op -> op.output().getOutputSink() != null) .forEach(op -> { final DataSink<?> sink = op.output().getOutputSink(); sinks.add(sink); JavaRDD<SparkElement> sparkOutput = Objects.requireNonNull((JavaRDD) executorContext.getOutput(op)); JavaPairRDD<NullWritable, Object> unwrapped = sparkOutput.mapToPair(el -> new Tuple2<>(NullWritable.get(), el.getElement())); try { Configuration conf = DataSinkOutputFormat.configure(new Configuration(), sink); conf.set(JobContext.OUTPUT_FORMAT_CLASS_ATTR, DataSinkOutputFormat.class.getName()); unwrapped.saveAsNewAPIHadoopDataset(conf); } catch (IOException e) { throw new RuntimeException(); } }); return sinks; }', 'output': 'public List<DataSink<?>> translateInto(Flow flow) { DAG<Operator<?, ?>> dag = flowToDag(flow); SparkExecutorContext executorContext = new SparkExecutorContext(sparkEnv, dag); dag.traverse().map(Node::get).forEach(op -> { Translation tx = translations.get(op.getClass()); if (tx == null) { throw new UnsupportedOperationException( "Operator " + op.getClass().getSimpleName() + " not supported"); } Preconditions.checkState( tx.accept == null || Boolean.TRUE.equals(tx.accept.apply(op))); JavaRDD<?> out = tx.translator.translate(op, executorContext); executorContext.setOutput(op, out); }); final List<DataSink<?>> sinks = new ArrayList<>(); dag.getLeafs() .stream() .map(Node::get) .filter(op -> op.output().getOutputSink() != null) .forEach(op -> { final DataSink<?> sink = op.output().getOutputSink(); sinks.add(sink); JavaPairRDD<NullWritable, Object> unwrapped = sparkOutput.mapToPair(el -> new Tuple2<>(NullWritable.get(), el.getElement())); try { Configuration conf = DataSinkOutputFormat.configure(new Configuration(), sink); conf.set(JobContext.OUTPUT_FORMAT_CLASS_ATTR, DataSinkOutputFormat.class.getName()); unwrapped.saveAsNewAPIHadoopDataset(conf); } catch (IOException e) { throw new RuntimeException(); } }); return sinks; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCommitWithOverwrittenUnderlying() { CopyOnAccessInMemoryStateInternals<String> underlying = CopyOnAccessInMemoryStateInternals.withUnderlying(key, null); CopyOnAccessInMemoryStateInternals<String> internals = CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying); StateNamespace namespace = new StateNamespaceForTest("foo"); StateTag<Object, BagState<String>> bagTag = StateTags.bag("foo", StringUtf8Coder.of()); BagState<String> stringBag = underlying.state(namespace, bagTag); assertThat(stringBag.read(), emptyIterable()); stringBag.add("bar"); stringBag.add("baz"); BagState<String> internalsState = internals.state(namespace, bagTag); internalsState.add("eggs"); internalsState.add("ham"); internalsState.add("0x00ff00"); internalsState.add("&"); internals.commit(); BagState<String> reReadInternalState = internals.state(namespace, bagTag); assertThat( reReadInternalState.read(), containsInAnyOrder("bar", "baz", "0x00ff00", "eggs", "&", "ham")); BagState<String> reReadUnderlyingState = underlying.state(namespace, bagTag); assertThat(reReadUnderlyingState.read(), containsInAnyOrder("bar", "baz")); }', 'ground_truth': 'public void testCommitWithOverwrittenUnderlying() { CopyOnAccessInMemoryStateInternals<String>underlying = CopyOnAccessInMemoryStateInternals.withUnderlying(key, null); CopyOnAccessInMemoryStateInternals<String>internals = CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying); StateNamespace namespace = new StateNamespaceForTest("foo"); StateTag<BagState<String>> bagTag = StateTags.bag("foo", StringUtf8Coder.of()); BagState<String> stringBag = underlying.state(namespace, bagTag); assertThat(stringBag.read(), emptyIterable()); stringBag.add("bar"); stringBag.add("baz"); BagState<String> internalsState = internals.state(namespace, bagTag); internalsState.add("eggs"); internalsState.add("ham"); internalsState.add("0x00ff00"); internalsState.add("&"); internals.commit(); BagState<String> reReadInternalState = internals.state(namespace, bagTag); assertThat( reReadInternalState.read(), containsInAnyOrder("bar", "baz", "0x00ff00", "eggs", "&", "ham")); BagState<String> reReadUnderlyingState = underlying.state(namespace, bagTag); assertThat(reReadUnderlyingState.read(), containsInAnyOrder("bar", "baz")); }', 'output': 'public void testCommitWithOverwrittenUnderlying() { CopyOnAccessInMemoryStateInternals<String> underlying = CopyOnAccessInMemoryStateInternals.withUnderlying(key, null); CopyOnAccessInMemoryStateInternals<String> internals = CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying); StateNamespace namespace = new StateNamespaceForTest("foo"); StateTag<Object, BagState<String>> bagTag = StateTags.bag("foo", StringUtf8Coder.of()); BagState<String> stringBag = underlying.state(namespace, bagTag); assertThat(stringBag.read(), emptyIterable()); stringBag.add("bar"); stringBag.add("baz"); BagState<String> internalsState = internals.state(namespace, bagTag); internalsState.add("eggs"); internalsState.add("ham"); internalsState.add("0x00ff00"); internalsState.add("&"); internals.commit(); BagState<String> reReadInternalState = internals.state(namespace, bagTag); assertThat( reReadInternalState.read(), containsInAnyOrder("bar", "baz", "0x00ff00", "eggs", "&", "ham")); BagState<String> reReadUnderlyingState = underlying.state(namespace, bagTag); assertThat(reReadUnderlyingState.read(), containsInAnyOrder("bar", "baz")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void close() throws Exception { for (PreparingJob preparingJob : ImmutableList.copyOf(unpreparedJobs.values())) { try { preparingJob.close(); } catch (Exception e) { LOG.warn("Exception while closing preparing job {}", preparingJob); } } while (!artifactStagingServices.isEmpty()) { XXX artifactStagingService = artifactStagingServices.remove(); try { artifactStagingService.close(); } catch (Exception e) { LOG.error( "Unable to close staging sevice started on %s", artifactStagingService.getApiServiceDescriptor().getUrl(), e); } } }', 'ground_truth': 'public void close() throws Exception { for (PreparingJob preparingJob : ImmutableList.copyOf(unpreparedJobs.values())) { try { preparingJob.close(); } catch (Exception e) { LOG.warn("Exception while closing preparing job {}", preparingJob); } } while (!artifactStagingServices.isEmpty()) { GrpcFnServer<BeamFileSystemArtifactStagingService> artifactStagingService = artifactStagingServices.remove(); try { artifactStagingService.close(); } catch (Exception e) { LOG.error( "Unable to close staging sevice started on %s", artifactStagingService.getApiServiceDescriptor().getUrl(), e); } } }', 'output': 'public void close() throws Exception { for (PreparingJob preparingJob : ImmutableList.copyOf(unpreparedJobs.values())) { try { preparingJob.close(); } catch (Exception e) { LOG.warn("Exception while closing preparing job {}", preparingJob); } } while (!artifactStagingServices.isEmpty()) { ArtifactStagingService artifactStagingService = artifactStagingServices.remove(); try { artifactStagingService.close(); } catch (Exception e) { LOG.error( "Unable to close staging sevice started on %s", artifactStagingService.getApiServiceDescriptor().getUrl(), e); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean isIndexPrimitiveValue(String spaceName) { XXX cfg = cacheQueryConfiguration(spaceName); return cfg != null && cfg.isIndexPrimitiveValue(); }', 'ground_truth': 'public boolean isIndexPrimitiveValue(String spaceName) { CacheQueryConfiguration cfg = cacheQueryConfiguration(spaceName); return cfg != null && cfg.isIndexPrimitiveValue(); }', 'output': 'public boolean isIndexPrimitiveValue(String spaceName, String indexName) { CacheQueryConfiguration cfg = cacheQueryConfiguration(spaceName, indexName); return cfg != null && cfg.isIndexPrimitiveValue(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void freeDirectBuffers() { ConcurrentHashMap8<Long, Thread> buffers = managedBuffers; if (buffers == null) return; managedBuffers = null; if (log.isDebugEnabled()) log.debug("Direct IO buffers to be freed: " + buffers.size()); for (Map.Entry<Long, Thread> next : buffers.entrySet()) { Thread th = next.getValue(); Long addr = next.getKey(); if (log.isDebugEnabled()) log.debug(String.format("Free Direct IO buffer [address=%d; Thread=%s; alive=%s]", addr, th != null ? th.getName() : "", th != null && th.isAlive())); AlignedBuffers.free(addr); } buffers.clear(); }', 'ground_truth': 'private void freeDirectBuffers() { ConcurrentHashMap<Long, Thread> buffers = managedBuffers; if (buffers == null) return; managedBuffers = null; if (log.isDebugEnabled()) log.debug("Direct IO buffers to be freed: " + buffers.size()); for (Map.Entry<Long, Thread> next : buffers.entrySet()) { Thread th = next.getValue(); Long addr = next.getKey(); if (log.isDebugEnabled()) log.debug(String.format("Free Direct IO buffer [address=%d; Thread=%s; alive=%s]", addr, th != null ? th.getName() : "", th != null && th.isAlive())); AlignedBuffers.free(addr); } buffers.clear(); }', 'output': 'private void freeDirectBuffers() { Map<Long, Thread> buffers = managedBuffers; if (buffers == null) return; managedBuffers = null; if (log.isDebugEnabled()) log.debug("Direct IO buffers to be freed: " + buffers.size()); for (Map.Entry<Long, Thread> next : buffers.entrySet()) { Thread th = next.getValue(); Long addr = next.getKey(); if (log.isDebugEnabled()) log.debug(String.format("Free Direct IO buffer [address=%d; Thread=%s; alive=%s]", addr, th != null ? th.getName() : "", th != null && th.isAlive())); AlignedBuffers.free(addr); } buffers.clear(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSideInputNotReady() throws Exception { PCollectionView<String> view = createView(); when(stepContext.getSideInputNotifications()) .thenReturn(Arrays.<Windmill.GlobalDataId>asList()); when(stepContext.issueSideInputFetch( eq(view), any(BoundedWindow.class), eq(SideInputState.UNKNOWN))) .thenReturn(false); XXX outputManager = new DoFnRunner.ListOutputManager(); StreamingSideInputDoFnRunner<String, String, IntervalWindow> runner = createRunner(outputManager, Arrays.asList(view)); runner.startBundle(); runner.processElement(createDatum("e", 0)); runner.finishBundle(); assertTrue(outputManager.getOutput(mainOutputTag).isEmpty()); IntervalWindow window = new IntervalWindow(new Instant(0), new Instant(10)); ValueState<Map<IntervalWindow, Set<GlobalDataRequest>>> blockedMapState = state.state(StateNamespaces.global(), StreamingSideInputDoFnRunner.blockedMapAddr(WINDOW_FN)); assertEquals( blockedMapState.get().read(), Collections.singletonMap( window, Collections.singleton(Windmill.GlobalDataRequest.newBuilder() .setDataId(Windmill.GlobalDataId.newBuilder() .setTag(view.getTagInternal().getId()) .setVersion(ByteString.copyFrom(CoderUtils.encodeToByteArray( IntervalWindow.getCoder(), window))) .build()) .setExistenceWatermarkDeadline(9000) .build()))); assertThat(runner.elementBag(createWindow(0)).get().read(), Matchers.contains(createDatum("e", 0))); assertEquals(runner.watermarkHold(createWindow(0)).get().read(), new Instant(0)); }', 'ground_truth': 'public void testSideInputNotReady() throws Exception { PCollectionView<String> view = createView(); when(stepContext.getSideInputNotifications()) .thenReturn(Arrays.<Windmill.GlobalDataId>asList()); when(stepContext.issueSideInputFetch( eq(view), any(BoundedWindow.class), eq(SideInputState.UNKNOWN))) .thenReturn(false); DoFnRunnerBase.ListOutputManager outputManager = new DoFnRunnerBase.ListOutputManager(); StreamingSideInputDoFnRunner<String, String, IntervalWindow> runner = createRunner(outputManager, Arrays.asList(view)); runner.startBundle(); runner.processElement(createDatum("e", 0)); runner.finishBundle(); assertTrue(outputManager.getOutput(mainOutputTag).isEmpty()); IntervalWindow window = new IntervalWindow(new Instant(0), new Instant(10)); ValueState<Map<IntervalWindow, Set<GlobalDataRequest>>> blockedMapState = state.state(StateNamespaces.global(), StreamingSideInputDoFnRunner.blockedMapAddr(WINDOW_FN)); assertEquals( blockedMapState.get().read(), Collections.singletonMap( window, Collections.singleton(Windmill.GlobalDataRequest.newBuilder() .setDataId(Windmill.GlobalDataId.newBuilder() .setTag(view.getTagInternal().getId()) .setVersion(ByteString.copyFrom(CoderUtils.encodeToByteArray( IntervalWindow.getCoder(), window))) .build()) .setExistenceWatermarkDeadline(9000) .build()))); assertThat(runner.elementBag(createWindow(0)).get().read(), Matchers.contains(createDatum("e", 0))); assertEquals(runner.watermarkHold(createWindow(0)).get().read(), new Instant(0)); }', 'output': 'public void testSideInputNotReady() throws Exception { PCollectionView<String> view = createView(); when(stepContext.getSideInputNotifications()) .thenReturn(Arrays.<Windmill.GlobalDataId>asList()); when(stepContext.issueSideInputFetch(eq(view), any(BoundedWindow.class), eq(SideInputState.UNKNOWN))) .thenReturn(false); ListOutputManager outputManager = new ListOutputManager(); StreamingSideInputDoFnRunner<String, String, IntervalWindow> runner = createRunner(outputManager, Arrays.asList(view)); runner.startBundle(); runner.processElement(createDatum("e", 0)); runner.finishBundle(); assertTrue(outputManager.getOutput(mainOutputTag).isEmpty()); IntervalWindow window = new IntervalWindow(new Instant(0), new Instant(10)); ValueState<Map<IntervalWindow, Set<GlobalDataRequest>>> blockedMapState = state.state(StateNamespaces.global(), StreamingSideInputDoFnRunner.blockedMapAddr(WINDOW_FN)); assertEquals( blockedMapState.get().read(), Collections.singletonMap( window, Collections.singleton(Windmill.GlobalDataRequest.newBuilder() .setDataId(Windmill.GlobalDataId.newBuilder() .setTag(view.getTagInternal().getId()) .setVersion(ByteString.copyFrom(CoderUtils.encodeToByteArray( IntervalWindow.getCoder(), window))) .build()) .setExistenceWatermarkDeadline(9000) .build()))); assertThat(runner.elementBag(createWindow(0)).get().read(), Matchers.contains(createDatum("e", 0))); assertEquals(runner.watermarkHold(createWindow(0)).get().read(), new Instant(0)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void writeFieldsCollection(ObjectOutput out) throws IOException { assert fields; out.writeInt(data != null ? data.size() : -1); if (data == null) return; for (Object o : data) { List<GridIndexingEntity<?>> list = (List<GridIndexingEntity<?>>)o; out.writeInt(list.size()); for (XXX idxEnt : list) { try { out.writeObject(idxEnt.value()); } catch (IgniteSpiException e) { throw new IOException("Failed to write indexing entity: " + idxEnt, e); } } } }', 'ground_truth': 'private void writeFieldsCollection(ObjectOutput out) throws IOException { assert fields; out.writeInt(data != null ? data.size() : -1); if (data == null) return; for (Object o : data) { List<IndexingEntity<?>> list = (List<IndexingEntity<?>>)o; out.writeInt(list.size()); for (IndexingEntity<?> idxEnt : list) { try { out.writeObject(idxEnt.value()); } catch (IgniteSpiException e) { throw new IOException("Failed to write indexing entity: " + idxEnt, e); } } } }', 'output': 'private void writeFieldsCollection(ObjectOutputStream out) throws IOException { assert fields; out.writeInt(data != null ? data.size() : -1); if (data == null) return; for (Object o : data) { List<GridIndexingEntity<?>> list = (List<GridIndexingEntity<?>>)o; out.writeInt(list.size()); for (GridIndexingEntity<?> idxEnt : list) { try { out.writeObject(idxEnt.value()); } catch (IgniteSpiException e) { throw new IOException("Failed to write indexing entity: " + idxEnt, e); } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testLogWrite() throws Exception { XXX log = GridGgfsHadoopLogger.logger(ENDPOINT, GGFS_NAME, LOG_DIR, 10); log.logCreate(1, PATH, PRIMARY, true, 2, new Integer(3).shortValue(), 4L); log.logAppend(2, PATH, PRIMARY, 8); log.logCloseOut(2, 9L, 10L, 11); log.close(); checkLog( new SB().a(U.jvmPid() + d() + TYPE_OPEN_OUT + d() + PATH_STR_ESCAPED + d() + PRIMARY + d() + 1 + d() + 2 + d(2) + 0 + d() + 1 + d() + 3 + d() + 4 + d(10)).toString(), new SB().a(U.jvmPid() + d() + TYPE_OPEN_OUT + d() + PATH_STR_ESCAPED + d() + PRIMARY + d() + 2 + d() + 8 + d(2) + 1 + d(13)).toString(), new SB().a(U.jvmPid() + d() + TYPE_CLOSE_OUT + d(3) + 2 + d(11) + 9 + d() + 10 + d() + 11 + d(3)) .toString() ); }', 'ground_truth': 'public void testLogWrite() throws Exception { GridGgfsLogger log = GridGgfsLogger.logger(ENDPOINT, GGFS_NAME, LOG_DIR, 10); log.logCreate(1, PATH, PRIMARY, true, 2, new Integer(3).shortValue(), 4L); log.logAppend(2, PATH, PRIMARY, 8); log.logCloseOut(2, 9L, 10L, 11); log.close(); checkLog( new SB().a(U.jvmPid() + d() + TYPE_OPEN_OUT + d() + PATH_STR_ESCAPED + d() + PRIMARY + d() + 1 + d() + 2 + d(2) + 0 + d() + 1 + d() + 3 + d() + 4 + d(10)).toString(), new SB().a(U.jvmPid() + d() + TYPE_OPEN_OUT + d() + PATH_STR_ESCAPED + d() + PRIMARY + d() + 2 + d() + 8 + d(2) + 1 + d(13)).toString(), new SB().a(U.jvmPid() + d() + TYPE_CLOSE_OUT + d(3) + 2 + d(11) + 9 + d() + 10 + d() + 11 + d(3)) .toString() ); }', 'output': 'public void testLogWrite() throws Exception { IgniteLogger log = GridGgfsHadoopLogger.logger(ENDPOINT, GGFS_NAME, LOG_DIR, 10); log.logCreate(1, PATH, PRIMARY, true, 2, new Integer(3).shortValue(), 4L); log.logAppend(2, PATH, PRIMARY, 8); log.logCloseOut(2, 9L, 10L, 11); log.close(); checkLog( new SB().a(U.jvmPid() + d() + TYPE_OPEN_OUT + d() + PATH_STR_ESCAPED + d() + PRIMARY + d() + 1 + d() + 2 + d(2) + 0 + d() + 1 + d() + 3 + d() + 4 + d(10)).toString(), new SB().a(U.jvmPid() + d() + TYPE_OPEN_OUT + d() + PATH_STR_ESCAPED + d() + PRIMARY + d() + 2 + d() + 8 + d(2) + 1 + d(13)).toString(), new SB().a(U.jvmPid() + d() + TYPE_CLOSE_OUT + d(3) + 2 + d(11) + 9 + d() + 10 + d() + 11 + d(3)) .toString() ); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static String jQuoteNoXSS(Object obj) { return jQuoteNoXSS(obj != null ? obj.toString() : null); } public static String jQuoteNoXSS(String s) { if (s == null) { return "null"; } int ln = s.length(); XXX b = new StringBuffer(ln + 4); b.append(\'"\'); for (int i = 0; i < ln; i++) { char c = s.charAt(i); if (c == \'"\') { b.append("\\\\\\""); } else if (c == \'\\\\\') { b.append("\\\\\\\\"); } else if (c == \'<\') { b.append("\\\\u003C"); } else if (c < 0x20) { if (c == \'\\n\') { b.append("\\\\n"); } else if (c == \'\\r\') { b.append("\\\\r"); } else if (c == \'\\f\') { b.append("\\\\f"); } else if (c == \'\\b\') { b.append("\\\\b"); } else if (c == \'\\t\') { b.append("\\\\t"); } else { b.append("\\\\u00"); int x = c / 0x10; b.append((char) (x < 0xA ? x + \'0\' : x - 0xA + \'A\')); x = c & 0xF; b.append((char) (x < 0xA ? x + \'0\' : x - 0xA + \'A\')); } } else { b.append(c); } } b.append(\'"\'); return b.toString(); }', 'ground_truth': 'public static String jQuoteNoXSS(String s) { if (s == null) { return "null"; } int ln = s.length(); StringBuilder b = new StringBuilder(ln + 4); b.append(\'"\'); for (int i = 0; i < ln; i++) { char c = s.charAt(i); if (c == \'"\') { b.append("\\\\\\""); } else if (c == \'\\\\\') { b.append("\\\\\\\\"); } else if (c == \'<\') { b.append("\\\\u003C"); } else if (c < 0x20) { if (c == \'\\n\') { b.append("\\\\n"); } else if (c == \'\\r\') { b.append("\\\\r"); } else if (c == \'\\f\') { b.append("\\\\f"); } else if (c == \'\\b\') { b.append("\\\\b"); } else if (c == \'\\t\') { b.append("\\\\t"); } else { b.append("\\\\u00"); int x = c / 0x10; b.append((char) (x < 0xA ? x + \'0\' : x - 0xA + \'A\')); x = c & 0xF; b.append((char) (x < 0xA ? x + \'0\' : x - 0xA + \'A\')); } } else { b.append(c); } } b.append(\'"\'); return b.toString(); }', 'output': 'public static String jQuoteNoXSS(Object obj) { return jQuoteNoXSS(obj != null ? obj.toString() : null); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void collectFires(Collection<Object> evAndListeners) { XXX prev = IN.get(); if (this == prev) { return; } try { IN.set(this); collImpl(evAndListeners); } finally { IN.set(prev); } } protected void collectFires(Collection<Object> evAndListeners) { R<?> r = result.get(); if (r != null) { r.collectFires(evAndListeners); } else { removeListeners(); } }', 'ground_truth': 'protected void collectFires(Collection<Object> evAndListeners) { R<?> prev = IN.get(); if (this == prev) { return; } try { IN.set(this); collImpl(evAndListeners); } finally { IN.set(prev); } }', 'output': 'protected void collectFires(Collection<Object> evAndListeners) { Object prev = IN.get(); if (this == prev) { return; } try { IN.set(this); collImpl(evAndListeners); } finally { IN.set(prev); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPutAllManyKeysTwoReaders() throws Exception { aff.backups(1); grids = 5; aff.partitions(grids); startGrids(); try { GridCache<Object, Object> prj0 = grid(0).cache(null); GridCache<Object, Object> prj1 = grid(1).cache(null); GridCache<Object, Object> prj2 = grid(2).cache(null); Map<Integer, Integer> putMap = new HashMap<>(); int size = 100; for (int i = 0; i < size; i++) putMap.put(i, i); prj0.putAll(putMap); for (int i = 0; i < size; i++) putMap.put(i, i * i); prj1.putAll(putMap); for (int i = 0; i < size; i++) putMap.put(i, i * i * i); prj2.putAll(putMap); for (int i = 0; i < size; i++) { assertEquals(i * i * i, prj0.get(i)); assertEquals(i * i * i, prj1.get(i)); assertEquals(i * i * i, prj2.get(i)); } } finally { stopAllGrids(); } }', 'ground_truth': 'public void testPutAllManyKeysTwoReaders() throws Exception { aff.backups(1); grids = 5; aff.partitions(grids); startGrids(); try { Cache<Object, Object> prj0 = grid(0).cache(null); Cache<Object, Object> prj1 = grid(1).cache(null); Cache<Object, Object> prj2 = grid(2).cache(null); Map<Integer, Integer> putMap = new HashMap<>(); int size = 100; for (int i = 0; i < size; i++) putMap.put(i, i); prj0.putAll(putMap); for (int i = 0; i < size; i++) putMap.put(i, i * i); prj1.putAll(putMap); for (int i = 0; i < size; i++) putMap.put(i, i * i * i); prj2.putAll(putMap); for (int i = 0; i < size; i++) { assertEquals(i * i * i, prj0.get(i)); assertEquals(i * i * i, prj1.get(i)); assertEquals(i * i * i, prj2.get(i)); } } finally { stopAllGrids(); } }', 'output': 'public void testPutAllManyKeysTwoReaders() throws Exception { aff.backups(1); grids = 5; aff.partitions(grids); startGrids(); try { Cache<Object, Object> prj0 = grid(0).cache(null); Cache<Object, Object> prj1 = grid(1).cache(null); Cache<Object, Object> prj2 = grid(2).cache(null); Map<Integer, Integer> putMap = new HashMap<>(); int size = 100; for (int i = 0; i < size; i++) putMap.put(i, i); prj0.putAll(putMap); for (int i = 0; i < size; i++) putMap.put(i, i * i); prj1.putAll(putMap); for (int i = 0; i < size; i++) putMap.put(i, i * i); prj2.putAll(putMap); for (int i = 0; i < size; i++) { assertEquals(i * i * i, prj0.get(i)); assertEquals(i * i * i, prj1.get(i)); assertEquals(i * i * i, prj2.get(i)); } } finally { stopAllGrids(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void registerMetricsMBean( MemoryMetricsImpl memMetrics, MemoryPolicyConfiguration memPlcCfg, IgniteConfiguration cfg ) { try { U.registerMBean( cfg.getMBeanServer(), cfg.getIgniteInstanceName(), "MemoryMetrics", memPlcCfg.getName(), new MemoryMetricsMXBeanImpl(memMetrics, memPlcCfg), MemoryMetricsMXBean.class); } catch (XXX e) { U.error(log, "Failed to register MBean for MemoryMetrics with name: \'" + memMetrics.getName() + "\'", e); } }', 'ground_truth': 'private void registerMetricsMBean( MemoryMetricsImpl memMetrics, MemoryPolicyConfiguration memPlcCfg, IgniteConfiguration cfg ) { assert !U.IGNITE_MBEANS_DISABLED; try { U.registerMBean( cfg.getMBeanServer(), cfg.getIgniteInstanceName(), "MemoryMetrics", memPlcCfg.getName(), new MemoryMetricsMXBeanImpl(memMetrics, memPlcCfg), MemoryMetricsMXBean.class); } catch (Throwable e) { U.error(log, "Failed to register MBean for MemoryMetrics with name: \'" + memMetrics.getName() + "\'", e); } }', 'output': 'private void registerMetricsMBean( MemoryMetricsImpl memMetrics, MemoryPolicyConfiguration memPlcCfg, IgniteConfiguration cfg ) { try { U.registerMBean( cfg.getMBeanServer(), cfg.getIgniteInstanceName(), "MemoryMetrics", memPlcCfg.getName(), new MemoryMetricsMXBeanImpl(memMetrics, memPlcCfg), MemoryMetricsMXBean.class); } catch (Throwable e) { U.error(log, "Failed to register MBean for MemoryMetrics with name: \'" + memMetrics.getName() + "\'", e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected String dump(boolean canonical) { XXX sb = new StringBuffer(); if (canonical) sb.append(\'<\'); sb.append(getNodeTypeSymbol()); if (targetNode != null) { sb.append(\' \'); sb.append(targetNode.getCanonicalForm()); } if (namespaces != null) { sb.append(" using "); sb.append(namespaces.getCanonicalForm()); } if (canonical) sb.append("/>"); return sb.toString(); }', 'ground_truth': 'protected String dump(boolean canonical) { StringBuilder sb = new StringBuilder(); if (canonical) sb.append(\'<\'); sb.append(getNodeTypeSymbol()); if (targetNode != null) { sb.append(\' \'); sb.append(targetNode.getCanonicalForm()); } if (namespaces != null) { sb.append(" using "); sb.append(namespaces.getCanonicalForm()); } if (canonical) sb.append("/>"); return sb.toString(); }', 'output': 'protected String dump(boolean canonical) { StringBuilder sb = new StringBuilder(); if (canonical) sb.append(\'<\'); sb.append(getNodeTypeSymbol()); if (targetNode != null) { sb.append(\' \'); sb.append(targetNode.getCanonicalForm()); } if (namespaces != null) { sb.append(" using "); sb.append(namespaces.getCanonicalForm()); } if (canonical) sb.append("/>"); return sb.toString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void filterOperator() { XXX nums = Dataset.of(pipeline.apply(Create.of(asList(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)))); Dataset<Integer> divisibleBythree = Filter.named("divisibleByFive").of(nums).by(e -> e % 3 == 0).output(); PAssert.that(divisibleBythree.getPCollection()).containsInAnyOrder(0, 3, 6, 9); pipeline.run(); }', 'ground_truth': 'public void filterOperator() { PCollection<Integer> nums = pipeline.apply(Create.of(asList(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))); PCollection<Integer> divisibleBythree = Filter.named("divisibleByFive").of(nums).by(e -> e % 3 == 0).output(); PAssert.that(divisibleBythree).containsInAnyOrder(0, 3, 6, 9); pipeline.run(); }', 'output': 'public void filterOperator() { PCollection<Integer> nums = pipeline.apply(Create.of(asList(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))); PCollection<Integer> divisibleBythree = Filter.named("divisibleByFive").of(nums).by(e -> e % 3 == 0).output(); PAssert.that(divisibleBythree).containsInAnyOrder(0, 3, 6, 9); pipeline.run(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void checkCache(Object[] keys) throws Exception { preloadMode = SYNC; lifecycleBean = lifecycleBean(keys); for (int i = 0; i < gridCnt; i++) { startGrid(i); info("Checking \'" + (i + 1) + "\' nodes..."); for (int j = 0; j < G.allGrids().size(); j++) { GridCache<Object, MyValue> c1 = grid(j).cache("one"); GridCache<Object, MyValue> c2 = grid(j).cache("two"); int k = 0; for (Object key : keys) { assertNotNull(c1.get(key)); if (k++ % 2 == 0) assertNotNull("Value is null for key: " + key, c2.get(key)); } } } }', 'ground_truth': 'public void checkCache(Object[] keys) throws Exception { preloadMode = SYNC; lifecycleBean = lifecycleBean(keys); for (int i = 0; i < gridCnt; i++) { startGrid(i); info("Checking \'" + (i + 1) + "\' nodes..."); for (int j = 0; j < G.allGrids().size(); j++) { Cache<Object, MyValue> c1 = grid(j).cache("one"); Cache<Object, MyValue> c2 = grid(j).cache("two"); int k = 0; for (Object key : keys) { assertNotNull(c1.get(key)); if (k++ % 2 == 0) assertNotNull("Value is null for key: " + key, c2.get(key)); } } } }', 'output': 'public void checkCache(Object[] keys) throws Exception { preloadMode = SYNC; lifecycleBean = lifecycleBean(keys); for (int i = 0; i < gridCnt; i++) { startGrid(i); info("Checking \'" + (i + 1) + "\' nodes..."); for (int j = 0; j < G.allGrids().size(); j++) { Cache<Object, MyValue> c1 = grid(j).cache("one"); Cache<Object, MyValue> c2 = grid(j).cache("two"); int k = 0; for (Object key : keys) { assertNotNull(c1.get(key)); if (k++ % 2 == 0) assertNotNull("Value is null for key: " + key, c2.get(key)); } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSerializationTopicCreatedByStrinAndLong() throws Exception { for (XXX marsh : getMarshallers()) { info("Test GridTopic externalization [marshaller=" + marsh + \']\'); for (GridTopic topic : GridTopic.values()) { Externalizable msgOut = (Externalizable)topic.topic(A_STRING, A_LONG); assertEquals(msgOut, GridTestIoUtils.externalize(msgOut, marsh)); } } }', 'ground_truth': 'public void testSerializationTopicCreatedByStrinAndLong() throws Exception { for (Marshaller marsh : getMarshallers()) { info("Test GridTopic externalization [marshaller=" + marsh + \']\'); for (GridTopic topic : GridTopic.values()) { Externalizable msgOut = (Externalizable)topic.topic(A_STRING, A_LONG); assertEquals(msgOut, GridTestIoUtils.externalize(msgOut, marsh)); } } }', 'output': 'public void testSerializationTopicCreatedByStrinAndLong() throws Exception { for (IgniteMarshaller marsh : getMarshallers()) { info("Test GridTopic externalization [marshaller=" + marsh + \']\'); for (GridTopic topic : GridTopic.values()) { Externalizable msgOut = (Externalizable)topic.topic(A_STRING, A_LONG); assertEquals(msgOut, GridTestIoUtils.externalize(msgOut, marsh)); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean isFolder(String name) { XXX ref = findReference(name); if ((ref != null) && (ref instanceof FileObjRef)) { return ((FileObjRef) ref).isFolder(); } return false; }', 'ground_truth': 'private boolean isFolder(String name) { Reference<? extends FileObject> ref = findReference(name); if ((ref != null) && (ref instanceof FileObjRef)) { return ((FileObjRef) ref).isFolder(); } return false; }', 'output': 'private boolean isFolder(String name) { Object ref = findReference(name); if ((ref != null) && (ref instanceof FileObjRef)) { return ((FileObjRef) ref).isFolder(); } return false; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Map<GridComputeJob, ClusterNode> absMap(GridClosureCallMode mode, Collection<? extends Runnable> jobs, Collection<ClusterNode> nodes, GridComputeLoadBalancer lb) throws GridException { assert mode != null; assert jobs != null; assert nodes != null; assert lb != null; if (!F.isEmpty(jobs) && !F.isEmpty(nodes)) { Map<GridComputeJob, ClusterNode> map = new HashMap<>(jobs.size(), 1); JobMapper mapper = new JobMapper(map); switch (mode) { case BROADCAST: { for (ClusterNode n : nodes) for (Runnable r : jobs) mapper.map(job(r), n); break; } case BALANCE: { for (Runnable r : jobs) { XXX job = job(r); mapper.map(job, lb.getBalancedNode(job, null)); } break; } } return map; } else return Collections.emptyMap(); }', 'ground_truth': 'private Map<ComputeJob, ClusterNode> absMap(GridClosureCallMode mode, Collection<? extends Runnable> jobs, Collection<ClusterNode> nodes, GridComputeLoadBalancer lb) throws GridException { assert mode != null; assert jobs != null; assert nodes != null; assert lb != null; if (!F.isEmpty(jobs) && !F.isEmpty(nodes)) { Map<ComputeJob, ClusterNode> map = new HashMap<>(jobs.size(), 1); JobMapper mapper = new JobMapper(map); switch (mode) { case BROADCAST: { for (ClusterNode n : nodes) for (Runnable r : jobs) mapper.map(job(r), n); break; } case BALANCE: { for (Runnable r : jobs) { ComputeJob job = job(r); mapper.map(job, lb.getBalancedNode(job, null)); } break; } } return map; } else return Collections.emptyMap(); }', 'output': 'private Map<GridComputeJob, ClusterNode> absMap(GridClosureCallMode mode, Collection<? extends Runnable> jobs, Collection<ClusterNode> nodes, GridComputeLoadBalancer lb) throws GridException { assert mode != null; assert jobs != null; assert nodes != null; assert lb != null; if (!F.isEmpty(jobs) && !F.isEmpty(nodes)) { Map<GridComputeJob, ClusterNode> map = new HashMap<>(jobs.size(), 1); JobMapper mapper = new JobMapper(map); switch (mode) { case BROADCAST: { for (ClusterNode n : nodes) for (Runnable r : jobs) mapper.map(job(r), n); break; } case BALANCE: { for (Runnable r : jobs) { GridComputeJob job = job(r); mapper.map(job, lb.getBalancedNode(job, null)); } break; } } return map; } else return Collections.emptyMap(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public IColumnIterator next() { if (iterator == null) iterator = exhausted ? Arrays.asList(new IColumnIterator[0]).iterator() : new KeyScanningIterator(); return iterator.next(); } public IColumnIterator next()\n        { { try { if (row != null) file.seek(finishedAt); assert !file.isEOF(); DecoratedKey key = StorageService.getPartitioner().convertFromDiskFormat(FBUtilities.readShortByteArray(file)); XXX dataSize = file.readInt(); dataStart = file.getFilePointer(); finishedAt = dataStart + dataSize; if (filter == null) { row = new SSTableIdentityIterator(sstable, file, key, dataStart, dataSize); ((SSTableIdentityIterator)row).reset(); return row; } else { return row = filter.getSSTableColumnIterator(sstable, file, key, dataStart); } } catch (IOException e) { throw new RuntimeException(e); } }', 'ground_truth': 'public IColumnIterator next() { try { if (row != null) file.seek(finishedAt); assert !file.isEOF(); DecoratedKey key = StorageService.getPartitioner().convertFromDiskFormat(FBUtilities.readShortByteArray(file)); long dataSize = SSTableReader.readRowSize(file, sstable.getDescriptor()); dataStart = file.getFilePointer(); finishedAt = dataStart + dataSize; if (filter == null) { row = new SSTableIdentityIterator(sstable, file, key, dataStart, dataSize); ((SSTableIdentityIterator)row).reset(); return row; } else { return row = filter.getSSTableColumnIterator(sstable, file, key, dataStart); } } catch (IOException e) { throw new RuntimeException(e); } }', 'output': 'public IColumnIterator next() throws IOException { if (iterator == null) iterator = exhausted ? Arrays.asList(new IColumnIterator[0]).iterator() : new KeyScanningIterator(); return iterator.next(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public List<Pair<Long,Long>> getPositionsForRanges(Collection<Range<Token>> ranges) { List<Pair<Long,Long>> positions = new ArrayList<>(); for (Range<Token> range : Range.normalize(ranges)) { assert !range.isWrapAround() || range.right.isMinimum(); AbstractBounds<RowPosition> bounds = Range.makeRowRange(range); RowPosition leftBound = bounds.left.compareTo(first) > 0 ? bounds.left : first.getToken().minKeyBound(); XXX rightBound = bounds.right.isMinimum() ? last.getToken().maxKeyBound() : bounds.right; if (leftBound.compareTo(last) > 0 || rightBound.compareTo(first) < 0) continue; long left = getPosition(leftBound, Operator.GT).position; long right = (rightBound.compareTo(last) > 0) ? uncompressedLength() : getPosition(rightBound, Operator.GT).position; if (left == right) continue; assert left < right : String.format("Range=%s openReason=%s first=%s last=%s left=%d right=%d", range, openReason, first, last, left, right); positions.add(Pair.create(left, right)); } return positions; }', 'ground_truth': 'public List<Pair<Long,Long>> getPositionsForRanges(Collection<Range<Token>> ranges) { List<Pair<Long,Long>> positions = new ArrayList<>(); for (Range<Token> range : Range.normalize(ranges)) { assert !range.isWrapAround() || range.right.isMinimum(); AbstractBounds<PartitionPosition> bounds = Range.makeRowRange(range); PartitionPosition leftBound = bounds.left.compareTo(first) > 0 ? bounds.left : first.getToken().minKeyBound(); PartitionPosition rightBound = bounds.right.isMinimum() ? last.getToken().maxKeyBound() : bounds.right; if (leftBound.compareTo(last) > 0 || rightBound.compareTo(first) < 0) continue; long left = getPosition(leftBound, Operator.GT).position; long right = (rightBound.compareTo(last) > 0) ? uncompressedLength() : getPosition(rightBound, Operator.GT).position; if (left == right) continue; assert left < right : String.format("Range=%s openReason=%s first=%s last=%s left=%d right=%d", range, openReason, first, last, left, right); positions.add(Pair.create(left, right)); } return positions; }', 'output': 'public List<Pair<Long,Long>> getPositionsForRanges(Collection<Range<Token>> ranges) { List<Pair<Long,Long>> positions = new ArrayList<>(); for (Range<Token> range : Range.normalize(ranges)) { assert !range.isWrapAround() || range.right.isMinimum(); AbstractBounds<RowPosition> bounds = Range.makeRowRange(range); RowPosition leftBound = bounds.left.compareTo(first) > 0 ? bounds.left : first.getToken().minKeyBound(); RowPosition rightBound = bounds.right.isMinimum() ? last.getToken().maxKeyBound() : bounds.right; if (leftBound.compareTo(last) > 0 || rightBound.compareTo(first) < 0) continue; long left = getPosition(leftBound, Operator.GT).position; long right = (rightBound.compareTo(last) > 0) ? uncompressedLength() : getPosition(rightBound, Operator.GT).position; if (left == right) continue; assert left < right : String.format("Range=%s openReason=%s first=%s last=%s left=%d right=%d", range, openReason, first, last, left, right); positions.add(Pair.create(left, right)); } return positions; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static IgniteConfiguration configuration() throws IgniteCheckedException { IgniteConfiguration cfg = new IgniteConfiguration(); cfg.setLocalHost("127.0.0.1"); cfg.setDeploymentMode(SHARED); cfg.setPeerClassLoadingEnabled(true); IgniteOptimizedMarshaller marsh = new IgniteOptimizedMarshaller(); marsh.setRequireSerializable(false); cfg.setMarshaller(marsh); CacheConfiguration cacheCfg = new CacheConfiguration(); cacheCfg.setAtomicityMode(TRANSACTIONAL); cacheCfg.setWriteSynchronizationMode(FULL_SYNC); cacheCfg.setPreloadMode(SYNC); cacheCfg.setAtomicityMode(TRANSACTIONAL); XXX qryCfg = new GridCacheQueryConfiguration(); qryCfg.setIndexPrimitiveKey(true); qryCfg.setIndexFixedTyping(false); cacheCfg.setQueryConfiguration(qryCfg); cfg.setCacheConfiguration(cacheCfg); TcpDiscoverySpi discoSpi = new TcpDiscoverySpi(); TcpDiscoveryVmIpFinder ipFinder = new TcpDiscoveryVmIpFinder(); ipFinder.setAddresses(Collections.singletonList("127.0.0.1:47500..47509")); discoSpi.setIpFinder(ipFinder); cfg.setDiscoverySpi(discoSpi); return cfg; }', 'ground_truth': 'public static IgniteConfiguration configuration() throws IgniteCheckedException { IgniteConfiguration cfg = new IgniteConfiguration(); cfg.setLocalHost("127.0.0.1"); cfg.setDeploymentMode(SHARED); cfg.setPeerClassLoadingEnabled(true); IgniteOptimizedMarshaller marsh = new IgniteOptimizedMarshaller(); marsh.setRequireSerializable(false); cfg.setMarshaller(marsh); CacheConfiguration cacheCfg = new CacheConfiguration(); cacheCfg.setAtomicityMode(TRANSACTIONAL); cacheCfg.setWriteSynchronizationMode(FULL_SYNC); cacheCfg.setPreloadMode(SYNC); cacheCfg.setAtomicityMode(TRANSACTIONAL); CacheQueryConfiguration qryCfg = new CacheQueryConfiguration(); qryCfg.setIndexPrimitiveKey(true); qryCfg.setIndexFixedTyping(false); cacheCfg.setQueryConfiguration(qryCfg); cfg.setCacheConfiguration(cacheCfg); TcpDiscoverySpi discoSpi = new TcpDiscoverySpi(); TcpDiscoveryVmIpFinder ipFinder = new TcpDiscoveryVmIpFinder(); ipFinder.setAddresses(Collections.singletonList("127.0.0.1:47500..47509")); discoSpi.setIpFinder(ipFinder); cfg.setDiscoverySpi(discoSpi); return cfg; }', 'output': 'public static IgniteConfiguration configuration() throws IgniteCheckedException { IgniteConfiguration cfg = new IgniteConfiguration(); cfg.setLocalHost("127.0.0.1"); cfg.setDeploymentMode(SHARED); cfg.setPeerClassLoadingEnabled(true); IgniteOptimizedMarshaller marsh = new IgniteOptimizedMarshaller(); marsh.setRequireSerializable(false); cfg.setMarshaller(marsh); CacheConfiguration cacheCfg = new CacheConfiguration(); cacheCfg.setAtomicityMode(TRANSACTIONAL); cacheCfg.setWriteSynchronizationMode(FULL_SYNC); cacheCfg.setPreloadMode(SYNC); cacheCfg.setAtomicityMode(TRANSACTIONAL); QueryConfiguration qryCfg = new QueryConfiguration(); qryCfg.setIndexPrimitiveKey(true); qryCfg.setIndexFixedTyping(false); cacheCfg.setQueryConfiguration(qryCfg); cfg.setCacheConfiguration(cacheCfg); TcpDiscoverySpi discoSpi = new TcpDiscoverySpi(); TcpDiscoveryVmIpFinder ipFinder = new TcpDiscoveryVmIpFinder(); ipFinder.setAddresses(Collections.singletonList("127.0.0.1:47500..47509")); discoSpi.setIpFinder(ipFinder); cfg.setDiscoverySpi(discoSpi); return cfg; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testNonEmptyFiles() throws Exception { File file1 = temporaryFolder.newFile("file1.txt"); Writer writer = Files.newWriter(file1, Charsets.UTF_8); for (int i = 0; i < 100; i++) { writer.write("123123123\\n"); } writer.flush(); writer.close(); temporaryFolder.newFolder("testfolder"); temporaryFolder.newFolder("testfolder2"); file1 = temporaryFolder.newFile("testfolder/test2.txt"); writer = Files.newWriter(file1, Charsets.UTF_8); for (int i = 0; i < 50; i++) { writer.write("123123123\\n"); } writer.flush(); writer.close(); TextRowCountEstimator textRowCountEstimator = TextRowCountEstimator.builder().setFilePattern(temporaryFolder.getRoot() + "/**").build(); XXX rows = textRowCountEstimator.estimateRowCount(PipelineOptionsFactory.create()); Assert.assertNotNull(rows); Assert.assertEquals(150L, rows.longValue()); }', 'ground_truth': 'public void testNonEmptyFiles() throws Exception { File file1 = temporaryFolder.newFile("file1.txt"); Writer writer = Files.newWriter(file1, Charsets.UTF_8); for (int i = 0; i < 100; i++) { writer.write("123123123\\n"); } writer.flush(); writer.close(); temporaryFolder.newFolder("testfolder"); temporaryFolder.newFolder("testfolder2"); file1 = temporaryFolder.newFile("testfolder/test2.txt"); writer = Files.newWriter(file1, Charsets.UTF_8); for (int i = 0; i < 50; i++) { writer.write("123123123\\n"); } writer.flush(); writer.close(); TextRowCountEstimator textRowCountEstimator = TextRowCountEstimator.builder().setFilePattern(temporaryFolder.getRoot() + "/**").build(); Double rows = textRowCountEstimator.estimateRowCount(PipelineOptionsFactory.create()); Assert.assertNotNull(rows); Assert.assertEquals(150d, rows, 0.01); }', 'output': 'public void testNonEmptyFiles() throws Exception { File file1 = temporaryFolder.newFile("file1.txt"); Writer writer = Files.newWriter(file1, Charsets.UTF_8); for (int i = 0; i < 100; i++) { writer.write("123123123\\n"); } writer.flush(); writer.close(); temporaryFolder.newFolder("testfolder"); temporaryFolder.newFolder("testfolder2"); file1 = temporaryFolder.newFile("testfolder/test2.txt"); writer = Files.newWriter(file1, Charsets.UTF_8); for (int i = 0; i < 50; i++) { writer.write("123123123\\n"); } writer.flush(); writer.close(); TextRowCountEstimator textRowCountEstimator = TextRowCountEstimator.builder().setFilePattern(temporaryFolder.getRoot() + "/**").build(); long rows = textRowCountEstimator.estimateRowCount(PipelineOptionsFactory.create()); Assert.assertNotNull(rows); Assert.assertEquals(150L, rows); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override protected GridConfiguration getConfiguration() throws Exception { XXX cfg = super.getConfiguration(); GridCacheConfiguration cacheCfg = defaultCacheConfiguration(); cacheCfg.setCacheMode(PARTITIONED); cacheCfg.setBackups(1); cacheCfg.setAtomicityMode(TRANSACTIONAL); cacheCfg.setDistributionMode(NEAR_PARTITIONED); cfg.setCacheConfiguration(cacheCfg); return cfg; } } }', 'ground_truth': '@Override protected IgniteConfiguration getConfiguration() throws Exception { IgniteConfiguration cfg = super.getConfiguration(); GridCacheConfiguration cacheCfg = defaultCacheConfiguration(); cacheCfg.setCacheMode(PARTITIONED); cacheCfg.setBackups(1); cacheCfg.setAtomicityMode(TRANSACTIONAL); cacheCfg.setDistributionMode(NEAR_PARTITIONED); cfg.setCacheConfiguration(cacheCfg); return cfg; }', 'output': '@Override protected IgniteConfiguration getConfiguration() throws Exception { IgniteConfiguration cfg = super.getConfiguration(); GridCacheConfiguration cacheCfg = defaultCacheConfiguration(); cacheCfg.setCacheMode(PARTITIONED); cacheCfg.setBackups(1); cacheCfg.setAtomicityMode(TRANSACTIONAL); cacheCfg.setDistributionMode(NEAR_PARTITIONED); cfg.setCacheConfiguration(cacheCfg); return cfg; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMapPartitionsToNodeCollection() throws Exception { Collection<Integer> parts = new LinkedList<>(); for (int p = 0; p < affinity().partitions(); p++) parts.add(p); Map<Integer, ClusterNode> map = cache().affinity().mapPartitionsToNodes(parts); GridCacheAffinityFunctionContext ctx = new GridCacheAffinityFunctionContextImpl(new ArrayList<>(grid(0).nodes()), null, null, 1, 1); XXX aff = affinity(); List<List<ClusterNode>> assignment = aff.assignPartitions(ctx); for (Map.Entry<Integer, ClusterNode> e : map.entrySet()) assert F.eqNodes(F.first(nodes(assignment, aff, e.getKey())), e.getValue()); }', 'ground_truth': 'public void testMapPartitionsToNodeCollection() throws Exception { Collection<Integer> parts = new LinkedList<>(); for (int p = 0; p < affinity().partitions(); p++) parts.add(p); Map<Integer, ClusterNode> map = cache().affinity().mapPartitionsToNodes(parts); CacheAffinityFunctionContext ctx = new GridCacheAffinityFunctionContextImpl(new ArrayList<>(grid(0).nodes()), null, null, 1, 1); CacheAffinityFunction aff = affinity(); List<List<ClusterNode>> assignment = aff.assignPartitions(ctx); for (Map.Entry<Integer, ClusterNode> e : map.entrySet()) assert F.eqNodes(F.first(nodes(assignment, aff, e.getKey())), e.getValue()); }', 'output': 'public void testMapPartitionsToNodeCollection() throws Exception { Collection<Integer> parts = new LinkedList<>(); for (int p = 0; p < affinity().partitions(); p++) parts.add(p); Map<Integer, ClusterNode> map = cache().affinity().mapPartitionsToNodes(parts); GridCacheAffinityFunctionContext ctx = new GridCacheAffinityFunctionContextImpl(new ArrayList<>(grid(0).nodes()), null, null, 1, 1); AffinityFunction aff = affinity(); List<List<ClusterNode>> assignment = aff.assignPartitions(ctx); for (Map.Entry<Integer, ClusterNode> e : map.entrySet()) assert F.eqNodes(F.first(nodes(assignment, aff, e.getKey())), e.getValue()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testQuery() throws Exception { GridCache<Integer, ObjectValue> cache = ignite.cache(null); int cnt = 10; for (int i = 0; i < cnt; i++) cache.putx(i, new ObjectValue("test" + i, i)); for (int i = 0; i < cnt; i++) { assertNotNull(cache.peek(i)); cache.evict(i); } GridCacheQuery<Map.Entry<Integer, ObjectValue>> qry = cache.queries().createSqlQuery(ObjectValue.class, "intVal >= ? order by intVal"); qry.enableDedup(true); assertEquals(0, qry.execute(0).get().size()); }', 'ground_truth': 'public void testQuery() throws Exception { Cache<Integer, ObjectValue> cache = ignite.cache(null); int cnt = 10; for (int i = 0; i < cnt; i++) cache.putx(i, new ObjectValue("test" + i, i)); for (int i = 0; i < cnt; i++) { assertNotNull(cache.peek(i)); cache.evict(i); } CacheQuery<Map.Entry<Integer, ObjectValue>> qry = cache.queries().createSqlQuery(ObjectValue.class, "intVal >= ? order by intVal"); qry.enableDedup(true); assertEquals(0, qry.execute(0).get().size()); }', 'output': 'public void testQuery() throws Exception { Cache<Integer, ObjectValue> cache = ignite.cache(null); int cnt = 10; for (int i = 0; i < cnt; i++) cache.putx(i, new ObjectValue("test" + i, i)); for (int i = 0; i < cnt; i++) { assertNotNull(cache.peek(i)); cache.evict(i); } CacheQuery<Map.Entry<Integer, ObjectValue>> qry = cache.queries().createSqlQuery(ObjectValue.class, "intVal >= ? order by intVal"); qry.enableDedup(true); assertEquals(0, qry.execute(0).get().size()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void whenValidAnnotatedDomainObjectAndDomainObjectLayout() throws Exception { @org.apache.isis.applib.annotation.DomainObject @org.apache.isis.applib.annotation.DomainObjectLayout class ValidAnnotatedDomainObjectAndDomainObjectLayout { } final XXX validationFailures = processThenValidate(ValidAnnotatedDomainObjectAndDomainObjectLayout.class); assertThat(validationFailures.getNumberOfFailures(), is(0)); }', 'ground_truth': 'public void whenValidAnnotatedDomainObjectAndDomainObjectLayout() throws Exception { @org.apache.isis.applib.annotation.DomainObject @org.apache.isis.applib.annotation.DomainObjectLayout class ValidAnnotatedDomainObjectAndDomainObjectLayout { } val validationFailures = processThenValidate(ValidAnnotatedDomainObjectAndDomainObjectLayout.class); assertThat(validationFailures.getNumberOfFailures(), is(0)); }', 'output': 'public void whenValidAnnotatedDomainObjectAndDomainObjectLayout() throws Exception { @org.apache.isis.applib.annotation.DomainObject @org.apache.isis.applib.annotation.DomainObjectLayout class ValidAnnotatedDomainObjectAndDomainObjectLayout { } final List<ValidationFailure> validationFailures = processThenValidate(ValidAnnotatedDomainObjectAndDomainObjectLayout.class); assertThat(validationFailures.size(), is(0)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void addNewTable() throws ConfigurationException { final String ksName = KEYSPACE1; final String tableName = "anewtable"; XXX original = Schema.instance.getKSMetaData(ksName); CFMetaData cfm = addTestTable(original.name, tableName, "A New Table"); assertFalse(Schema.instance.getKSMetaData(ksName).tables.get(cfm.cfName).isPresent()); MigrationManager.announceNewColumnFamily(cfm); assertTrue(Schema.instance.getKSMetaData(ksName).tables.get(cfm.cfName).isPresent()); assertEquals(cfm, Schema.instance.getKSMetaData(ksName).tables.get(cfm.cfName).get()); QueryProcessor.executeInternal(String.format("INSERT INTO %s.%s (key, col, val) VALUES (?, ?, ?)", ksName, tableName), "key0", "col0", "val0"); ColumnFamilyStore cfs = Keyspace.open(ksName).getColumnFamilyStore(tableName); assertNotNull(cfs); cfs.forceBlockingFlush(); UntypedResultSet rows = QueryProcessor.executeInternal(String.format("SELECT * FROM %s.%s", ksName, tableName)); assertRows(rows, row("key0", "col0", "val0")); }', 'ground_truth': 'public void addNewTable() throws ConfigurationException { final String ksName = KEYSPACE1; final String tableName = "anewtable"; KeyspaceMetadata original = Schema.instance.getKSMetaData(ksName); CFMetaData cfm = addTestTable(original.name, tableName, "A New Table"); assertFalse(Schema.instance.getKSMetaData(ksName).tables.get(cfm.cfName).isPresent()); MigrationManager.announceNewColumnFamily(cfm); assertTrue(Schema.instance.getKSMetaData(ksName).tables.get(cfm.cfName).isPresent()); assertEquals(cfm, Schema.instance.getKSMetaData(ksName).tables.get(cfm.cfName).get()); QueryProcessor.executeInternal(String.format("INSERT INTO %s.%s (key, col, val) VALUES (?, ?, ?)", ksName, tableName), "key0", "col0", "val0"); ColumnFamilyStore cfs = Keyspace.open(ksName).getColumnFamilyStore(tableName); assertNotNull(cfs); cfs.forceBlockingFlush(); UntypedResultSet rows = QueryProcessor.executeInternal(String.format("SELECT * FROM %s.%s", ksName, tableName)); assertRows(rows, row("key0", "col0", "val0")); }', 'output': 'public void addNewTable() throws ConfigurationException { final String ksName = KEYSPACE1; final String tableName = "anewtable"; MetaData original = Schema.instance.getKSMetaData(ksName); CFMetaData cfm = addTestTable(original.name, tableName, "A New Table"); assertFalse(Schema.instance.getKSMetaData(ksName).tables.get(cfm.cfName).isPresent()); MigrationManager.announceNewColumnFamily(cfm); assertTrue(Schema.instance.getKSMetaData(ksName).tables.get(cfm.cfName).isPresent()); assertEquals(cfm, Schema.instance.getKSMetaData(ksName).tables.get(cfm.cfName).get()); QueryProcessor.executeInternal(String.format("INSERT INTO %s.%s (key, col, val) VALUES (?, ?, ?)", ksName, tableName), "key0", "col0", "val0"); ColumnFamilyStore cfs = Keyspace.open(ksName).getColumnFamilyStore(tableName); assertNotNull(cfs); cfs.forceBlockingFlush(); UntypedResultSet rows = QueryProcessor.executeInternal(String.format("SELECT * FROM %s.%s", ksName, tableName)); assertRows(rows, row("key0", "col0", "val0")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Key retrieveKey(String keyAlias) throws IOException { try { return cache.get(keyAlias); } catch (XXX e) { if (e.getCause() instanceof IOException) throw (IOException)e.getCause(); throw new IOException("failed to load key from cache: " + keyAlias, e); } }', 'ground_truth': 'private Key retrieveKey(String keyAlias) throws IOException { try { return cache.get(keyAlias); } catch (CompletionException e) { if (e.getCause() instanceof IOException) throw (IOException)e.getCause(); throw new IOException("failed to load key from cache: " + keyAlias, e); } }', 'output': 'private Key retrieveKey(String keyAlias) throws IOException { try { return cache.get(keyAlias); } catch (CacheException e) { if (e.getCause() instanceof IOException) throw (IOException)e.getCause(); throw new IOException("failed to load key from cache: " + keyAlias, e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void runTestRoundTrip(Iterable<String> elems, int numShards, String suffix, CompressionType writeCompressionType, CompressionType readCompressionType) throws IOException { String outputName = "file"; Path baseDir = Files.createTempDirectory(tempFolder, "test-rt"); String baseFilename = baseDir.resolve(outputName).toString(); TFRecordIO.Write.Bound write = TFRecordIO.Write.to(baseFilename) .withNumShards(numShards) .withSuffix(suffix) .withCompressionType(writeCompressionType); p.apply(Create.of(elems).withCoder(StringUtf8Coder.of())) .apply(ParDo.of(new StringToByteArray())) .apply(write); p.run(); XXX read = TFRecordIO.Read.from(baseFilename + "*") .withCompressionType(readCompressionType); PCollection<String> output = p2.apply(read).apply(ParDo.of(new ByteArrayToString())); PAssert.that(output).containsInAnyOrder(elems); p2.run(); }', 'ground_truth': 'private void runTestRoundTrip(Iterable<String> elems, int numShards, String suffix, CompressionType writeCompressionType, CompressionType readCompressionType) throws IOException { String outputName = "file"; Path baseDir = Files.createTempDirectory(tempFolder, "test-rt"); String baseFilename = baseDir.resolve(outputName).toString(); TFRecordIO.Write.Bound write = TFRecordIO.Write.to(baseFilename) .withNumShards(numShards) .withSuffix(suffix) .withCompressionType(writeCompressionType); p.apply(Create.of(elems).withCoder(StringUtf8Coder.of())) .apply(ParDo.of(new StringToByteArray())) .apply(write); p.run(); TFRecordIO.Read read = TFRecordIO.read().from(baseFilename + "*") .withCompressionType(readCompressionType); PCollection<String> output = p2.apply(read).apply(ParDo.of(new ByteArrayToString())); PAssert.that(output).containsInAnyOrder(elems); p2.run(); }', 'output': 'private void runTestRoundTrip(Iterable<String> elems, int numShards, String suffix, CompressionType writeCompressionType, CompressionType readCompressionType) throws IOException { String outputName = "file"; Path baseDir = Files.createTempDirectory(tempFolder, "test-rt"); String baseFilename = baseDir.resolve(outputName).toString(); TFRecordIO.Write.Bound write = TFRecordIO.Write.to(baseFilename) .withNumShards(numShards) .withSuffix(suffix) .withCompressionType(writeCompressionType); p.apply(Create.of(elems).withCoder(StringUtf8Coder.of())) .apply(ParDo.of(new StringToByteArray())) .apply(write); p.run(); TFRecordIO.Read.Bound read = TFRecordIO.Read.from(baseFilename + "*") .withCompressionType(readCompressionType); PCollection<String> output = p2.apply(read).apply(ParDo.of(new ByteArrayToString())); PAssert.that(output).containsInAnyOrder(elems); p2.run(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public final Map<? extends GridComputeJob, ClusterNode> map(List<ClusterNode> subgrid, T arg) throws GridException { assert subgrid != null; assert !subgrid.isEmpty(); Collection<? extends GridComputeJob> jobs = split(subgrid.size(), arg); if (F.isEmpty(jobs)) throw new GridException("Split returned no jobs."); Map<GridComputeJob, ClusterNode> map = U.newHashMap(jobs.size()); for (XXX job : jobs) { ClusterNode old = map.put(job, balancer.getBalancedNode(job, null)); if (old != null) throw new GridException("Failed to map task (same job instance is being mapped more than once) " + "[job=" + job + ", task=" + this + \']\'); } return map; } } }', 'ground_truth': '@Override public final Map<? extends ComputeJob, ClusterNode> map(List<ClusterNode> subgrid, T arg) throws GridException { assert subgrid != null; assert !subgrid.isEmpty(); Collection<? extends ComputeJob> jobs = split(subgrid.size(), arg); if (F.isEmpty(jobs)) throw new GridException("Split returned no jobs."); Map<ComputeJob, ClusterNode> map = U.newHashMap(jobs.size()); for (ComputeJob job : jobs) { ClusterNode old = map.put(job, balancer.getBalancedNode(job, null)); if (old != null) throw new GridException("Failed to map task (same job instance is being mapped more than once) " + "[job=" + job + ", task=" + this + \']\'); } return map; }', 'output': '@Override public final Map<? extends ComputeJob, ClusterNode> map(List<ClusterNode> subgrid, T arg) throws GridException { assert subgrid != null; assert !subgrid.isEmpty(); Collection<? extends ComputeJob> jobs = split(subgrid.size(), arg); if (F.isEmpty(jobs)) throw new GridException("Split returned no jobs."); Map<ComputeJob, ClusterNode> map = U.newHashMap(jobs.size()); for (ComputeJob job : jobs) { ClusterNode old = map.put(job, balancer.getBalancedNode(job, null)); if (old != null) throw new GridException("Failed to map task (same job instance is being mapped more than once) " + "[job=" + job + ", task=" + this + \']\'); } return map; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testRemoveFromExistingObjectAfterGet() { TestObjectAllTypes obj = new TestObjectAllTypes(); obj.setDefaultData(); obj.enumArr = null; XXX builder = builder(toPortable(obj)); builder.getField("i_"); builder.removeField("str"); assertNull(builder.build().<TestObjectAllTypes>deserialize().str); }', 'ground_truth': 'public void testRemoveFromExistingObjectAfterGet() { TestObjectAllTypes obj = new TestObjectAllTypes(); obj.setDefaultData(); obj.enumArr = null; BinaryObjectBuilderImpl builder = builder(toPortable(obj)); builder.getField("i_"); builder.removeField("str"); assertNull(builder.build().<TestObjectAllTypes>deserialize().str); }', 'output': 'public void testRemoveFromExistingObjectAfterGet() { TestObjectAllTypes obj = new TestObjectAllTypes(); obj.setDefaultData(); obj.enumArr = null; PortableBuilder builder = builder(toPortable(obj)); builder.getField("i_"); builder.removeField("str"); assertNull(builder.build().<TestObjectAllTypes>deserialize().str); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean equals(Object obj) { if (this == obj) { return true; } if (obj == null) { return false; } if (getClass() != obj.getClass()) { return false; } XXX other = (BeamSQLRow) obj; return toString().equals(other.toString()); }', 'ground_truth': 'public boolean equals(Object obj) { if (this == obj) { return true; } if (obj == null) { return false; } if (getClass() != obj.getClass()) { return false; } BeamSqlRow other = (BeamSqlRow) obj; return toString().equals(other.toString()); }', 'output': 'public boolean equals(Object obj) { if (this == obj) { return true; } if (obj == null) { return false; } if (getClass() != obj.getClass()) { return false; } BeamSQLRow other = (BeamSQLRow) obj; return toString().equals(other.toString()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean killJob(GridHadoopJobId jobId) throws IgniteCheckedException { if (!busyLock.tryReadLock()) return false; try { GridHadoopJobMetadata meta = jobMetaCache().get(jobId); if (meta != null && meta.phase() != PHASE_COMPLETE && meta.phase() != PHASE_CANCELLING) { GridHadoopTaskCancelledException err = new GridHadoopTaskCancelledException("Job cancelled."); jobMetaCache().invoke(jobId, new CancelJobProcessor(null, err)); } } finally { busyLock.readUnlock(); } XXX fut = finishFuture(jobId); if (fut != null) { try { fut.get(); } catch (Throwable e) { if (e.getCause() instanceof GridHadoopTaskCancelledException) return true; } } return false; }', 'ground_truth': 'public boolean killJob(GridHadoopJobId jobId) throws IgniteCheckedException { if (!busyLock.tryReadLock()) return false; try { GridHadoopJobMetadata meta = jobMetaCache().get(jobId); if (meta != null && meta.phase() != PHASE_COMPLETE && meta.phase() != PHASE_CANCELLING) { GridHadoopTaskCancelledException err = new GridHadoopTaskCancelledException("Job cancelled."); jobMetaCache().invoke(jobId, new CancelJobProcessor(null, err)); } } finally { busyLock.readUnlock(); } IgniteInternalFuture<?> fut = finishFuture(jobId); if (fut != null) { try { fut.get(); } catch (Throwable e) { if (e.getCause() instanceof GridHadoopTaskCancelledException) return true; } } return false; }', 'output': 'public boolean killJob(IgniteUuid jobId) throws IgniteCheckedException { if (!busyLock.tryReadLock()) return false; try { GridHadoopJobMetadata meta = jobMetaCache().get(jobId); if (meta != null && meta.phase() != PHASE_COMPLETE && meta.phase() != PHASE_CANCELLING) { GridHadoopTaskCancelledException err = new GridHadoopTaskCancelledException("Job cancelled."); jobMetaCache().invoke(jobId, new CancelJobProcessor(null, err)); } } finally { busyLock.readUnlock(); } IgniteFuture<?> fut = finishFuture(jobId); if (fut != null) { try { fut.get(); } catch (Throwable e) { if (e.getCause() instanceof GridHadoopTaskCancelledException) return true; } } return false; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testZeroReplicationFactor() throws Exception { if (mode == PRIMARY) { Path ggfsHome = new Path(primaryFsUri); Path file = new Path(ggfsHome, "someFile"); try (FSDataOutputStream out = fs.create(file, EnumSet.noneOf(CreateFlag.class), Options.CreateOpts.perms(FsPermission.getDefault()), Options.CreateOpts.repFac((short)1))) { out.write(new byte[1024 * 1024]); } IgniteFs igniteFs = grid(0).fileSystem("ggfs"); XXX filePath = new IgniteFsPath("/someFile"); IgniteFsFile fileInfo = igniteFs.info(filePath); Collection<IgniteFsBlockLocation> locations = igniteFs.affinity(filePath, 0, fileInfo.length()); assertEquals(1, locations.size()); IgniteFsBlockLocation location = F.first(locations); assertEquals(1, location.nodeIds().size()); } }', 'ground_truth': 'public void testZeroReplicationFactor() throws Exception { if (mode == PRIMARY) { Path ggfsHome = new Path(primaryFsUri); Path file = new Path(ggfsHome, "someFile"); try (FSDataOutputStream out = fs.create(file, EnumSet.noneOf(CreateFlag.class), Options.CreateOpts.perms(FsPermission.getDefault()), Options.CreateOpts.repFac((short)1))) { out.write(new byte[1024 * 1024]); } IgniteFs igniteFs = grid(0).fileSystem("ggfs"); IgfsPath filePath = new IgfsPath("/someFile"); IgniteFsFile fileInfo = igniteFs.info(filePath); Collection<IgniteFsBlockLocation> locations = igniteFs.affinity(filePath, 0, fileInfo.length()); assertEquals(1, locations.size()); IgniteFsBlockLocation location = F.first(locations); assertEquals(1, location.nodeIds().size()); } }', 'output': 'public void testZeroReplicationFactor() throws Exception { if (mode == PRIMARY) { Path ggfsHome = new Path(primaryFsUri); Path file = new Path(ggfsHome, "someFile"); try (FSDataOutputStream out = fs.create(file, EnumSet.noneOf(CreateFlag.class), Options.CreateOpts.perms(FsPermission.getDefault()), Options.CreateOpts.repFac((short)1))) { out.write(new byte[1024 * 1024]); } IgniteFs igniteFs = grid(0).fileSystem("ggfs"); IgniteFsPath filePath = new IgniteFsPath("/someFile"); IgniteFsFile fileInfo = igniteFs.info(filePath); Collection<IgniteFsBlockLocation> locations = igniteFs.affinity(filePath, 0, fileInfo.length()); assertEquals(1, locations.size()); IgniteFsBlockLocation location = F.first(locations); assertEquals(1, location.nodeIds().size()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCloneFlag() throws Exception { CacheProjection<String, TestCloneable> prj = cacheCloneable().flagsOn(CLONE); final TestCloneable val = new TestCloneable("val"); prj.put("key", val); checkClone(prj.get("key"), val); checkClone(prj.getAsync("key").get(), val); Map<String, TestCloneable> map = prj.getAll(F.asList("key")); assertEquals(1, map.size()); checkClone(map.get("key"), val); map = prj.getAllAsync(F.asList("key")).get(); assertEquals(1, map.size()); checkClone(map.get("key"), val); checkClone(prj.peek("key"), val); Collection<TestCloneable> vals = prj.values(); assert vals != null; assertEquals(1, vals.size()); checkClone(vals.iterator().next(), val); Set<CacheEntry<String, TestCloneable>> entries = prj.entrySet(); assertEquals(1, entries.size()); checkClone(entries.iterator().next().getValue(), val); CacheEntry<String, TestCloneable> entry = prj.entry("key"); assert entry != null; checkClone(entry.peek(), val); }', 'ground_truth': 'public void testCloneFlag() throws Exception { CacheProjection<String, TestCloneable> prj = cacheCloneable().flagsOn(CLONE); final TestCloneable val = new TestCloneable("val"); prj.put("key", val); checkClone(prj.get("key"), val); checkClone(prj.getAsync("key").get(), val); Map<String, TestCloneable> map = prj.getAll(F.asList("key")); assertEquals(1, map.size()); checkClone(map.get("key"), val); map = prj.getAllAsync(F.asList("key")).get(); assertEquals(1, map.size()); checkClone(map.get("key"), val); checkClone(prj.peek("key"), val); Collection<TestCloneable> vals = prj.values(); assert vals != null; assertEquals(1, vals.size()); checkClone(vals.iterator().next(), val); Set<Entry<String, TestCloneable>> entries = prj.entrySet(); assertEquals(1, entries.size()); checkClone(entries.iterator().next().getValue(), val); Entry<String, TestCloneable> entry = prj.entry("key"); assert entry != null; checkClone(entry.peek(), val); }', 'output': 'public void testCloneFlag() throws Exception { CacheProjection<String, TestCloneable> prj = cacheCloneable().flagsOn(CLONE); final TestCloneable val = new TestCloneable("val"); prj.put("key", val); checkClone(prj.get("key"), val); checkClone(prj.getAsync("key").get(), val); Map<String, TestCloneable> map = prj.getAll(F.asList("key")); assertEquals(1, map.size()); checkClone(map.get("key"), val); map = prj.getAllAsync(F.asList("key")).get(); assertEquals(1, map.size()); checkClone(map.get("key"), val); checkClone(prj.peek("key"), val); Collection<TestCloneable> vals = prj.values(); assert vals != null; assertEquals(1, vals.size()); checkClone(vals.iterator().next(), val); Set<CacheEntry<String, TestCloneable>> entries = prj.entrySet(); assertEquals(1, entries.size()); checkClone(entries.iterator().next().getValue(), val); CacheEntry<String, TestCloneable> entry = prj.entry("key"); assert entry != null; checkClone(entry.peek(), val); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void testPollFailover(GridCacheQueue<Integer> queue, final List<Integer> killIdxs) throws Exception { assert !killIdxs.isEmpty(); final int ITEMS = atomicityMode() == ATOMIC && !queue.collocated() ? 10_000 : 3000; for (int i = 0; i < ITEMS; i++) { assertTrue(queue.add(i)); if ((i + 1) % 500 == 0) log.info("Added " + (i + 1) + " items."); } log.info("Added all items."); final AtomicBoolean stop = new AtomicBoolean(); final AtomicInteger stopCnt = new AtomicInteger(); XXX fut = startNodeKiller(stop, stopCnt, killIdxs); int err = 0; try { int pollNum = ITEMS; int exp = 0; for (int i = 0; i < pollNum; i++) { Integer e = queue.poll(); if (atomicityMode() == ATOMIC) { if (e == null || e != exp) { log.info("Unexpected data [expected=" + i + ", actual=" + e + \']\'); err++; pollNum--; exp = e != null ? (e + 1) : (exp + 1); } else exp++; } else assertEquals((Integer)i, e); if ((i + 1) % 500 == 0) log.info("Polled " + (i + 1) + " items."); } } finally { stop.set(true); } fut.get(); if (atomicityMode() == ATOMIC) assertTrue("Too many errors for atomic cache: " + err, err <= stopCnt.get()); assertNull(queue.poll()); assertEquals(0, queue.size()); }', 'ground_truth': 'private void testPollFailover(GridCacheQueue<Integer> queue, final List<Integer> killIdxs) throws Exception { assert !killIdxs.isEmpty(); final int ITEMS = atomicityMode() == ATOMIC && !queue.collocated() ? 10_000 : 3000; for (int i = 0; i < ITEMS; i++) { assertTrue(queue.add(i)); if ((i + 1) % 500 == 0) log.info("Added " + (i + 1) + " items."); } log.info("Added all items."); final AtomicBoolean stop = new AtomicBoolean(); final AtomicInteger stopCnt = new AtomicInteger(); IgniteFuture<?> fut = startNodeKiller(stop, stopCnt, killIdxs); int err = 0; try { int pollNum = ITEMS; int exp = 0; for (int i = 0; i < pollNum; i++) { Integer e = queue.poll(); if (atomicityMode() == ATOMIC) { if (e == null || e != exp) { log.info("Unexpected data [expected=" + i + ", actual=" + e + \']\'); err++; pollNum--; exp = e != null ? (e + 1) : (exp + 1); } else exp++; } else assertEquals((Integer)i, e); if ((i + 1) % 500 == 0) log.info("Polled " + (i + 1) + " items."); } } finally { stop.set(true); } fut.get(); if (atomicityMode() == ATOMIC) assertTrue("Too many errors for atomic cache: " + err, err <= stopCnt.get()); assertNull(queue.poll()); assertEquals(0, queue.size()); }', 'output': 'private void testPollFailover(GridCacheQueue<Integer> queue, final List<Integer> killIdxs) throws Exception { assert !killIdxs.isEmpty(); final int ITEMS = atomicityMode() == ATOMIC && !queue.collocated() ? 10_000 : 3000; for (int i = 0; i < ITEMS; i++) { assertTrue(queue.add(i)); if ((i + 1) % 500 == 0) log.info("Added " + (i + 1) + " items."); } log.info("Added all items."); final AtomicBoolean stop = new AtomicBoolean(); final AtomicInteger stopCnt = new AtomicInteger(); IgniteFuture<?> fut = startNodeKiller(stop, stopCnt, killIdxs); int err = 0; try { int pollNum = ITEMS; int exp = 0; for (int i = 0; i < pollNum; i++) { Integer e = queue.poll(); if (atomicityMode() == ATOMIC) { if (e == null || e != exp) { log.info("Unexpected data [expected=" + i + ", actual=" + e + \']\'); err++; pollNum--; exp = e != null ? (e + 1) : (exp + 1); } else exp++; } else assertEquals((Integer)i, e); if ((i + 1) % 500 == 0) log.info("Polled " + (i + 1) + " items."); } } finally { stop.set(true); } fut.get(); if (atomicityMode() == ATOMIC) assertTrue("Too many errors for atomic cache: " + err, err <= stopCnt.get()); assertNull(queue.poll()); assertEquals(0, queue.size()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testWithoutCacheUseNoopSwapSapce() throws Exception { try { Ignite ignite = startGrid(1); XXX spi = ignite.configuration().getSwapSpaceSpi(); assertNotNull(spi); assertTrue(spi instanceof GridNoopSwapSpaceSpi); } finally { stopAllGrids(); } }', 'ground_truth': 'public void testWithoutCacheUseNoopSwapSapce() throws Exception { try { Ignite ignite = startGrid(1); SwapSpaceSpi spi = ignite.configuration().getSwapSpaceSpi(); assertNotNull(spi); assertTrue(spi instanceof NoopSwapSpaceSpi); } finally { stopAllGrids(); } }', 'output': 'public void testWithoutCacheUseNoopSwapSapce() throws Exception { try { Ignite ignite = startGrid(1); SwapSpaceSpi spi = ignite.configuration().getSwapSpaceSpi(); assertNotNull(spi); assertTrue(spi instanceof GridNoopSwapSpaceSpi); } finally { stopAllGrids(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testDone() { XXX p1 = new PropertyIdImpl("prop1", "cat1", false); Resource resource = new ResourceImpl(Resource.Type.Cluster); resource.setProperty(p1, "foo"); PredicateBuilder pb = new PredicateBuilder(); Predicate predicate = pb.property(p1).equals("foo").toPredicate(); try { pb.property(p1).equals("foo").toPredicate(); Assert.fail("Expected IllegalStateException."); } catch (IllegalStateException e) { } Assert.assertSame(predicate, pb.toPredicate()); }', 'ground_truth': 'public void testDone() { String p1 = PropertyHelper.getPropertyId("cat1", "prop1"); Resource resource = new ResourceImpl(Resource.Type.Cluster); resource.setProperty(p1, "foo"); PredicateBuilder pb = new PredicateBuilder(); Predicate predicate = pb.property(p1).equals("foo").toPredicate(); try { pb.property(p1).equals("foo").toPredicate(); Assert.fail("Expected IllegalStateException."); } catch (IllegalStateException e) { } Assert.assertSame(predicate, pb.toPredicate()); }', 'output': 'public void testDone() { PropertyIdImpl p1 = new PropertyIdImpl("prop1", "cat1", false); Resource resource = new ResourceImpl(Resource.Type.Cluster); resource.setProperty(p1, "foo"); PredicateBuilder pb = new PredicateBuilder(); Predicate predicate = pb.property(p1).equals("foo").toPredicate(); try { pb.property(p1).equals("foo").toPredicate(); Assert.fail("Expected IllegalStateException."); } catch (IllegalStateException e) { } Assert.assertSame(predicate, pb.toPredicate()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGrowthTrackerPollAfterCheckpointWithoutNewOutputsNoWatermark() { Instant now = Instant.now(); GrowthTracker<String, Integer> tracker = newTracker(); tracker.addNewAsPending( PollResult.incomplete( Arrays.asList( TimestampedValue.of("d", now.plus(standardSeconds(4))), TimestampedValue.of("c", now.plus(standardSeconds(3))), TimestampedValue.of("a", now.plus(standardSeconds(1))), TimestampedValue.of("b", now.plus(standardSeconds(2)))))); assertEquals("a", tracker.tryClaimNextPending().getValue()); assertEquals("b", tracker.tryClaimNextPending().getValue()); assertEquals("c", tracker.tryClaimNextPending().getValue()); assertEquals("d", tracker.tryClaimNextPending().getValue()); assertEquals(now.plus(standardSeconds(1)), tracker.getWatermark()); GrowthState<String, Integer> checkpoint = tracker.checkpoint(); GrowthTracker<String, Integer> residualTracker = newTracker(checkpoint); residualTracker.addNewAsPending( PollResult.incomplete( Arrays.asList( TimestampedValue.of("c", now.plus(standardSeconds(3))), TimestampedValue.of("d", now.plus(standardSeconds(4))), TimestampedValue.of("a", now.plus(standardSeconds(1))), TimestampedValue.of("b", now.plus(standardSeconds(2)))))); assertEquals(now.plus(standardSeconds(1)), residualTracker.getWatermark()); }', 'ground_truth': 'public void testGrowthTrackerPollAfterCheckpointWithoutNewOutputsNoWatermark() { Instant now = Instant.now(); GrowthTracker<String, String, Integer> tracker = newTracker(); tracker.addNewAsPending( PollResult.incomplete( Arrays.asList( TimestampedValue.of("d", now.plus(standardSeconds(4))), TimestampedValue.of("c", now.plus(standardSeconds(3))), TimestampedValue.of("a", now.plus(standardSeconds(1))), TimestampedValue.of("b", now.plus(standardSeconds(2)))))); assertEquals("a", tracker.tryClaimNextPending().getValue()); assertEquals("b", tracker.tryClaimNextPending().getValue()); assertEquals("c", tracker.tryClaimNextPending().getValue()); assertEquals("d", tracker.tryClaimNextPending().getValue()); assertEquals(now.plus(standardSeconds(1)), tracker.getWatermark()); GrowthState<String, String, Integer> checkpoint = tracker.checkpoint(); GrowthTracker<String, String, Integer> residualTracker = newTracker(checkpoint); residualTracker.addNewAsPending( PollResult.incomplete( Arrays.asList( TimestampedValue.of("c", now.plus(standardSeconds(3))), TimestampedValue.of("d", now.plus(standardSeconds(4))), TimestampedValue.of("a", now.plus(standardSeconds(1))), TimestampedValue.of("b", now.plus(standardSeconds(2)))))); assertEquals(now.plus(standardSeconds(1)), residualTracker.getWatermark()); }', 'output': 'public void testGrowthTrackerPollAfterCheckpointWithoutNewOutputsNoWatermark() { Instant now = Instant.now(); GrowthTracker<String, Integer> tracker = newTracker(); tracker.addNewAsPending( PollResult.incomplete( Arrays.asList( TimestampedValue.of("d", now.plus(standardSeconds(4))), TimestampedValue.of("c", now.plus(standardSeconds(3))), TimestampedValue.of("a", now.plus(standardSeconds(1))), TimestampedValue.of("b", now.plus(standardSeconds(2)))))); assertEquals("a", tracker.tryClaimNextPending().getValue()); assertEquals("b", tracker.tryClaimNextPending().getValue()); assertEquals("c", tracker.tryClaimNextPending().getValue()); assertEquals("d", tracker.tryClaimNextPending().getValue()); assertEquals(now.plus(standardSeconds(1)), tracker.getWatermark()); GrowthState<String, Integer> checkpoint = tracker.checkpoint(); GrowthTracker<String, Integer> residualTracker = newTracker(checkpoint); residualTracker.addNewAsPending( PollResult.incomplete( Arrays.asList( TimestampedValue.of("c", now.plus(standardSeconds(3))), TimestampedValue.of("d", now.plus(standardSeconds(4))), TimestampedValue.of("a", now.plus(standardSeconds(1))), TimestampedValue.of("b", now.plus(standardSeconds(2)))))); assertEquals(now.plus(standardSeconds(1)), residualTracker.getWatermark()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testWaitForEvent() throws Exception { Ignite ignite = grid(); final int usrType = Integer.MAX_VALUE - 1; IgniteInternalFuture<IgniteEvent> fut = waitForLocalEvent(ignite.events(), new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent e) { return e.type() == usrType; } }, usrType); try { fut.get(500); fail("GridFutureTimeoutException must have been thrown."); } catch (XXX e) { info("Caught expected exception: " + e); } ignite.events().recordLocal(new IgniteEventAdapter(null, "Test message.", usrType) { }); assert fut.get() != null; assertEquals(usrType, fut.get().type()); } public void testWaitForEventContinuationTimeout() throws Exception { Ignite ignite = grid(); try { int usrType = Integer.MAX_VALUE - 1; waitForLocalEvent(ignite.events(), F.<IgniteEvent>alwaysTrue(), usrType).get(1000); fail("GridFutureTimeoutException must have been thrown."); } catch (XXX e) { info("Caught expected exception: " + e); } }', 'ground_truth': 'public void testWaitForEvent() throws Exception { Ignite ignite = grid(); final int usrType = Integer.MAX_VALUE - 1; IgniteInternalFuture<IgniteEvent> fut = waitForLocalEvent(ignite.events(), new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent e) { return e.type() == usrType; } }, usrType); try { fut.get(500); fail("GridFutureTimeoutException must have been thrown."); } catch (IgniteFutureTimeoutCheckedException e) { info("Caught expected exception: " + e); } ignite.events().recordLocal(new IgniteEventAdapter(null, "Test message.", usrType) { }); assert fut.get() != null; assertEquals(usrType, fut.get().type()); }', 'output': 'public void testWaitForEvent() throws Exception { Ignite ignite = grid(); final int usrType = Integer.MAX_VALUE - 1; IgniteInternalFuture<IgniteEvent> fut = waitForLocalEvent(ignite.events(), new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent e) { return e.type() == usrType; } }, usrType); try { fut.get(500); fail("GridFutureTimeoutException must have been thrown."); } catch (IgniteTimeoutCheckedException e) { info("Caught expected exception: " + e); } ignite.events().recordLocal(new IgniteEventAdapter(null, "Test message.", usrType) { }); assert fut.get() != null; assertEquals(usrType, fut.get().type()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMergeRangeTombstones() throws InterruptedException { PartitionUpdate update1 = new PartitionUpdate(metadata, dk, metadata.regularAndStaticColumns(), 1); writeRangeTombstone(update1, "1", "11", 123, 123); writeRangeTombstone(update1, "2", "22", 123, 123); writeRangeTombstone(update1, "3", "31", 123, 123); writeRangeTombstone(update1, "4", "41", 123, 123); XXX update2 = new PartitionUpdate(metadata, dk, metadata.regularAndStaticColumns(), 1); writeRangeTombstone(update2, "1", "11", 123, 123); writeRangeTombstone(update2, "111", "112", 1230, 123); writeRangeTombstone(update2, "2", "24", 123, 123); writeRangeTombstone(update2, "3", "31", 1230, 123); writeRangeTombstone(update2, "4", "41", 123, 1230); writeRangeTombstone(update2, "5", "51", 123, 1230); try (UnfilteredRowIterator merged = UnfilteredRowIterators.merge(ImmutableList.of(update1.unfilteredIterator(), update2.unfilteredIterator()), nowInSeconds)) { Object[][] expected = new Object[][]{ { "1", "11", 123l, 123 }, { "111", "112", 1230l, 123 }, { "2", "24", 123l, 123 }, { "3", "31", 1230l, 123 }, { "4", "41", 123l, 1230 }, { "5", "51", 123l, 1230 } }; int i = 0; while (merged.hasNext()) { RangeTombstoneBoundMarker openMarker = (RangeTombstoneBoundMarker)merged.next(); ClusteringBound openBound = openMarker.clustering(); DeletionTime openDeletion = new DeletionTime(openMarker.deletionTime().markedForDeleteAt(), openMarker.deletionTime().localDeletionTime()); RangeTombstoneBoundMarker closeMarker = (RangeTombstoneBoundMarker)merged.next(); ClusteringBound closeBound = closeMarker.clustering(); DeletionTime closeDeletion = new DeletionTime(closeMarker.deletionTime().markedForDeleteAt(), closeMarker.deletionTime().localDeletionTime()); assertEquals(openDeletion, closeDeletion); assertRangeTombstoneMarkers(openBound, closeBound, openDeletion, expected[i++]); } } }', 'ground_truth': 'public void testMergeRangeTombstones() throws InterruptedException { PartitionUpdate.Builder update1 = new PartitionUpdate.Builder(metadata, dk, metadata.regularAndStaticColumns(), 1); writeRangeTombstone(update1, "1", "11", 123, 123); writeRangeTombstone(update1, "2", "22", 123, 123); writeRangeTombstone(update1, "3", "31", 123, 123); writeRangeTombstone(update1, "4", "41", 123, 123); PartitionUpdate.Builder update2 = new PartitionUpdate.Builder(metadata, dk, metadata.regularAndStaticColumns(), 1); writeRangeTombstone(update2, "1", "11", 123, 123); writeRangeTombstone(update2, "111", "112", 1230, 123); writeRangeTombstone(update2, "2", "24", 123, 123); writeRangeTombstone(update2, "3", "31", 1230, 123); writeRangeTombstone(update2, "4", "41", 123, 1230); writeRangeTombstone(update2, "5", "51", 123, 1230); try (UnfilteredRowIterator merged = UnfilteredRowIterators.merge(ImmutableList.of(update1.build().unfilteredIterator(), update2.build().unfilteredIterator()), nowInSeconds)) { Object[][] expected = new Object[][]{ { "1", "11", 123l, 123 }, { "111", "112", 1230l, 123 }, { "2", "24", 123l, 123 }, { "3", "31", 1230l, 123 }, { "4", "41", 123l, 1230 }, { "5", "51", 123l, 1230 } }; int i = 0; while (merged.hasNext()) { RangeTombstoneBoundMarker openMarker = (RangeTombstoneBoundMarker)merged.next(); ClusteringBound openBound = openMarker.clustering(); DeletionTime openDeletion = new DeletionTime(openMarker.deletionTime().markedForDeleteAt(), openMarker.deletionTime().localDeletionTime()); RangeTombstoneBoundMarker closeMarker = (RangeTombstoneBoundMarker)merged.next(); ClusteringBound closeBound = closeMarker.clustering(); DeletionTime closeDeletion = new DeletionTime(closeMarker.deletionTime().markedForDeleteAt(), closeMarker.deletionTime().localDeletionTime()); assertEquals(openDeletion, closeDeletion); assertRangeTombstoneMarkers(openBound, closeBound, openDeletion, expected[i++]); } } }', 'output': 'public void testMergeRangeTombstones() throws InterruptedException { PartitionUpdate update1 = new PartitionUpdate(metadata, dk, metadata.regularAndStaticColumns(), 1); writeRangeTombstone(update1, "1", "11", 123, 123); writeRangeTombstone(update1, "2", "22", 123, 123); writeRangeTombstone(update1, "3", "31", 123, 123); writeRangeTombstone(update1, "4", "41", 123, 123); update2 = new PartitionUpdate(metadata, dk, metadata.regularAndStaticColumns(), 1); writeRangeTombstone(update2, "1", "11", 123, 123); writeRangeTombstone(update2, "111", "112", 1230, 123); writeRangeTombstone(update2, "2", "24", 123, 123); writeRangeTombstone(update2, "3", "31", 1230, 123); writeRangeTombstone(update2, "4", "41", 123, 1230); writeRangeTombstone(update2, "5", "51", 123, 1230); try (UnfilteredRowIterator merged = UnfilteredRowIterators.merge(ImmutableList.of(update1.unfilteredIterator(), update2.unfilteredIterator()), nowInSeconds)) { Object[][] expected = new Object[][]{ { "1", "11", 123l, 123 }, { "111", "112", 1230l, 123 }, { "2", "24", 123l, 123 }, { "3", "31", 1230l, 123 }, { "4", "41", 123l, 1230 }, { "5", "51", 123l, 1230 } }; int i ='}
{'instruction': 'Please refactor the following code:', 'input': 'public void removesOnExceptionInProcessElement() throws Exception { ParDoEvaluator<Object, Object> underlying = mock(ParDoEvaluator.class); doThrow(Exception.class).when(underlying).processElement(any(WindowedValue.class)); DoFn<?, ?> original = lifecycleManager.get(); assertThat(original, not(nullValue())); TransformEvaluator<Object> evaluator = DoFnLifecycleManagerRemovingTransformEvaluator.wrapping(underlying, lifecycleManager); try { evaluator.processElement(WindowedValue.valueInGlobalWindow(new Object())); } catch (Exception e) { assertThat(lifecycleManager.get(), not(Matchers.<DoFn<?, ?>>theInstance(original))); return; } fail("Expected underlying evaluator to throw on method call"); }', 'ground_truth': 'public void removesOnExceptionInProcessElement() throws Exception { ParDoEvaluator<Object> underlying = mock(ParDoEvaluator.class); doThrow(Exception.class).when(underlying).processElement(any(WindowedValue.class)); DoFn<?, ?> original = lifecycleManager.get(); assertThat(original, not(nullValue())); TransformEvaluator<Object> evaluator = DoFnLifecycleManagerRemovingTransformEvaluator.wrapping(underlying, lifecycleManager); try { evaluator.processElement(WindowedValue.valueInGlobalWindow(new Object())); } catch (Exception e) { assertThat(lifecycleManager.get(), not(Matchers.<DoFn<?, ?>>theInstance(original))); return; } fail("Expected underlying evaluator to throw on method call"); }', 'output': 'public void removesOnExceptionInProcessElement() throws Exception { ParDoEvaluator<Object, Object> underlying = mock(ParDoEvaluator.class); doThrow(Exception.class).when(underlying).processElement(any(WindowedValue.class)); DoFn<?, ?> original = lifecycleManager.get(); assertThat(original, not(nullValue())); TransformEvaluator<Object> evaluator = DoFnLifecycleManagerRemovingTransformEvaluator.wrapping(underlying, lifecycleManager); try { evaluator.processElement(WindowedValue.valueInGlobalWindow(new Object())); } catch (Exception e) { assertThat(lifecycleManager.get(), not(Matchers.<DoFn<?, ?>>theInstance(original))); return; } fail("Expected underlying evaluator to throw on method call"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void initKeys() throws TemplateModelException { if (keys == null) { XXX keySet = new HashSet(); SimpleSequence keySeq = new SimpleSequence((ObjectWrapper) null); for (TemplateHashModelEx hash : hashes) { addKeys(keySet, keySeq, hash); } size = keySet.size(); keys = new CollectionAndSequence(keySeq); } }', 'ground_truth': 'private void initKeys() throws TemplateModelException { if (keys == null) { Set<String> keySet = new HashSet<String>(); SimpleSequence keySeq = new SimpleSequence((ObjectWrapper) null); for (TemplateHashModelEx hash : hashes) { addKeys(keySet, keySeq, hash); } size = keySet.size(); keys = new CollectionAndSequence(keySeq); } }', 'output': 'private void initKeys() throws TemplateModelException { if (keys == null) { Set<String> keySet = new HashSet<>(); SimpleSequence keySeq = new SimpleSequence((ObjectWrapper) null); for (TemplateHashModelEx hash : hashes) { addKeys(keySet, keySeq, hash); } size = keySet.size(); keys = new CollectionAndSequence(keySeq); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testExecution() throws Exception { String name = "oneMoreGrid"; Collection<IgniteBiTuple<Ignite, IgnitePredicate<IgniteEvent>>> lsnrs = new LinkedList<>(); try { final AtomicInteger cnt = new AtomicInteger(); Ignite g = startGrid(name); IgnitePredicate<IgniteEvent> lsnr; if (!Ignite.class.isAssignableFrom(projection().getClass())) { g.events().localListen(lsnr = new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent evt) { assert evt.type() == EVT_JOB_STARTED; assert false; return true; } }, EVT_JOB_STARTED); lsnrs.add(F.t(g, lsnr)); } for (ClusterNode node : prj.nodes()) { g = G.ignite(node.id()); g.events().localListen(lsnr = new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent evt) { assert evt.type() == EVT_JOB_STARTED; synchronized (mux) { cnt.incrementAndGet(); mux.notifyAll(); } return true; } }, EVT_JOB_STARTED); lsnrs.add(F.t(g, lsnr)); } run1(cnt); run2(cnt); call1(cnt); call2(cnt); call3(cnt); call4(cnt); call5(cnt); forkjoin1(cnt); forkjoin2(cnt); exec1(cnt); exec2(cnt); executorService(cnt); checkActiveFutures(); } finally { for (IgniteBiTuple<Ignite, IgnitePredicate<IgniteEvent>> t : lsnrs) t.get1().events().stopLocalListen(t.get2(), EVT_JOB_STARTED); stopGrid(name); } }', 'ground_truth': 'public void testExecution() throws Exception { String name = "oneMoreGrid"; Collection<IgniteBiTuple<Ignite, IgnitePredicate<Event>>> lsnrs = new LinkedList<>(); try { final AtomicInteger cnt = new AtomicInteger(); Ignite g = startGrid(name); IgnitePredicate<Event> lsnr; if (!Ignite.class.isAssignableFrom(projection().getClass())) { g.events().localListen(lsnr = new IgnitePredicate<Event>() { @Override public boolean apply(Event evt) { assert evt.type() == EVT_JOB_STARTED; assert false; return true; } }, EVT_JOB_STARTED); lsnrs.add(F.t(g, lsnr)); } for (ClusterNode node : prj.nodes()) { g = G.ignite(node.id()); g.events().localListen(lsnr = new IgnitePredicate<Event>() { @Override public boolean apply(Event evt) { assert evt.type() == EVT_JOB_STARTED; synchronized (mux) { cnt.incrementAndGet(); mux.notifyAll(); } return true; } }, EVT_JOB_STARTED); lsnrs.add(F.t(g, lsnr)); } run1(cnt); run2(cnt); call1(cnt); call2(cnt); call3(cnt); call4(cnt); call5(cnt); forkjoin1(cnt); forkjoin2(cnt); exec1(cnt); exec2(cnt); executorService(cnt); checkActiveFutures(); } finally { for (IgniteBiTuple<Ignite, IgnitePredicate<Event>> t : lsnrs) t.get1().events().stopLocalListen(t.get2(), EVT_JOB_STARTED); stopGrid(name); } }', 'output': 'public void testExecution() throws Exception { String name = "oneMoreGrid"; Collection<IgniteBiTuple<Ignite, IgnitePredicate<IgniteEvent>>> lsnrs = new LinkedList<>(); try { final AtomicInteger cnt = new AtomicInteger(); Ignite g = startGrid(name); IgnitePredicate<IgniteEvent> lsnr; if (!Ignite.class.isAssignableFrom(projection().getClass())) { g.events().localListen(lsnr = new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent evt) { assert evt.type() == EVT_JOB_STARTED; assert false; return true; } }, EVT_JOB_STARTED); lsnrs.add(F.t(g, lsnr)); } for (ClusterNode node : prj.nodes()) { g = G.ignite(node.id()); g.events().localListen(lsnr = new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent evt) { assert evt.type() == EVT_JOB_STARTED; synchronized (mux) { cnt.incrementAndGet(); mux.notifyAll(); } return true; } }, EVT_JOB_STARTED); lsnrs.add(F.t(g, lsnr)); } run1(cnt); run2(cnt); call1(cnt); call2(cnt); call3(cnt); call4(cnt); call5(cnt); forkjoin1(cnt); forkjoin2(cnt); exec1(cnt); exec2(cnt); executorService(cnt); checkActiveFutures(); } finally { for (IgniteBiTuple<Ignite, IgnitePredicate<IgniteEvent>> t : lsnrs) t.get1().events().stopLocalListen(t.get2(), EVT_JOB_STARTED); stopGrid(name); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String getString(ByteBuffer bytes) { try { return BooleanTerm.instance.getString(bytes); } catch (XXX e) { throw new MarshalException(e.getMessage()); } }', 'ground_truth': 'public String getString(ByteBuffer bytes) { try { return JdbcBoolean.instance.getString(bytes); } catch (org.apache.cassandra.cql.jdbc.MarshalException e) { throw new MarshalException(e.getMessage()); } }', 'output': 'public String getString(ByteBuffer bytes) { try { return BooleanTerm.instance.getString(bytes); } catch (IOException e) { throw new MarshalException(e.getMessage()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public StateRequestHandler getStateRequestHandler( ExecutableStage executableStage, RuntimeContext runtimeContext) { XXX sideInputHandlerFactory = FlinkBatchSideInputHandlerFactory.forStage(executableStage, runtimeContext); try { return StateRequestHandlers.forMultimapSideInputHandlerFactory( ProcessBundleDescriptors.getMultimapSideInputs(executableStage), sideInputHandlerFactory); } catch (IOException e) { throw new RuntimeException(e); } }', 'ground_truth': 'public StateRequestHandler getStateRequestHandler( ExecutableStage executableStage, RuntimeContext runtimeContext) { SideInputHandlerFactory sideInputHandlerFactory = FlinkBatchSideInputHandlerFactory.forStage(executableStage, runtimeContext); try { return StateRequestHandlers.forSideInputHandlerFactory( ProcessBundleDescriptors.getSideInputs(executableStage), sideInputHandlerFactory); } catch (IOException e) { throw new RuntimeException(e); } }', 'output': 'public StateRequestHandler getStateRequestHandler( ExecutableStage executableStage, RuntimeContext runtimeContext) { SideInputHandlerFactory sideInputHandlerFactory = FlinkBatchSideInputHandlerFactory.forStage(executableStage, runtimeContext); try { return StateRequestHandlers.forMultimapSideInputHandlerFactory( ProcessBundleDescriptors.getMultimapSideInputs(executableStage), sideInputHandlerFactory); } catch (IOException e) { throw new RuntimeException(e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private GridNodeMetrics createMetrics() { XXX metrics = new GridDiscoveryMetricsAdapter(); metrics.setAvailableProcessors(1); metrics.setAverageActiveJobs(2); metrics.setAverageCancelledJobs(3); metrics.setAverageJobExecuteTime(4); metrics.setAverageJobWaitTime(5); metrics.setAverageRejectedJobs(6); metrics.setAverageWaitingJobs(7); metrics.setCurrentActiveJobs(8); metrics.setCurrentCancelledJobs(9); metrics.setCurrentIdleTime(10); metrics.setCurrentIdleTime(11); metrics.setCurrentJobExecuteTime(12); metrics.setCurrentJobWaitTime(13); metrics.setCurrentRejectedJobs(14); metrics.setCurrentWaitingJobs(15); metrics.setCurrentDaemonThreadCount(16); metrics.setHeapMemoryCommitted(17); metrics.setHeapMemoryInitialized(18); metrics.setHeapMemoryMaximum(19); metrics.setHeapMemoryUsed(20); metrics.setLastUpdateTime(21); metrics.setMaximumActiveJobs(22); metrics.setMaximumCancelledJobs(23); metrics.setMaximumJobExecuteTime(24); metrics.setMaximumJobWaitTime(25); metrics.setMaximumRejectedJobs(26); metrics.setMaximumWaitingJobs(27); metrics.setNonHeapMemoryCommitted(28); metrics.setNonHeapMemoryInitialized(29); metrics.setNonHeapMemoryMaximum(30); metrics.setNonHeapMemoryUsed(31); metrics.setMaximumThreadCount(32); metrics.setStartTime(33); metrics.setCurrentCpuLoad(34); metrics.setCurrentThreadCount(35); metrics.setTotalCancelledJobs(36); metrics.setTotalExecutedJobs(37); metrics.setTotalIdleTime(38); metrics.setTotalRejectedJobs(39); metrics.setTotalStartedThreadCount(40); metrics.setUpTime(41); metrics.setSentMessagesCount(42); metrics.setSentBytesCount(43); metrics.setReceivedMessagesCount(44); metrics.setReceivedBytesCount(45); metrics.setOutboundMessagesQueueSize(46); return metrics; }', 'ground_truth': 'private ClusterNodeMetrics createMetrics() { ClusterDiscoveryMetricsAdapter metrics = new ClusterDiscoveryMetricsAdapter(); metrics.setAvailableProcessors(1); metrics.setAverageActiveJobs(2); metrics.setAverageCancelledJobs(3); metrics.setAverageJobExecuteTime(4); metrics.setAverageJobWaitTime(5); metrics.setAverageRejectedJobs(6); metrics.setAverageWaitingJobs(7); metrics.setCurrentActiveJobs(8); metrics.setCurrentCancelledJobs(9); metrics.setCurrentIdleTime(10); metrics.setCurrentIdleTime(11); metrics.setCurrentJobExecuteTime(12); metrics.setCurrentJobWaitTime(13); metrics.setCurrentRejectedJobs(14); metrics.setCurrentWaitingJobs(15); metrics.setCurrentDaemonThreadCount(16); metrics.setHeapMemoryCommitted(17); metrics.setHeapMemoryInitialized(18); metrics.setHeapMemoryMaximum(19); metrics.setHeapMemoryUsed(20); metrics.setLastUpdateTime(21); metrics.setMaximumActiveJobs(22); metrics.setMaximumCancelledJobs(23); metrics.setMaximumJobExecuteTime(24); metrics.setMaximumJobWaitTime(25); metrics.setMaximumRejectedJobs(26); metrics.setMaximumWaitingJobs(27); metrics.setNonHeapMemoryCommitted(28); metrics.setNonHeapMemoryInitialized(29); metrics.setNonHeapMemoryMaximum(30); metrics.setNonHeapMemoryUsed(31); metrics.setMaximumThreadCount(32); metrics.setStartTime(33); metrics.setCurrentCpuLoad(34); metrics.setCurrentThreadCount(35); metrics.setTotalCancelledJobs(36); metrics.setTotalExecutedJobs(37); metrics.setTotalIdleTime(38); metrics.setTotalRejectedJobs(39); metrics.setTotalStartedThreadCount(40); metrics.setUpTime(41); metrics.setSentMessagesCount(42); metrics.setSentBytesCount(43); metrics.setReceivedMessagesCount(44); metrics.setReceivedBytesCount(45); metrics.setOutboundMessagesQueueSize(46); return metrics; }', 'output': 'private GridNodeMetrics createMetrics() { GridDiscoveryMetricsAdapter metrics = new GridDiscoveryMetricsAdapter(); metrics.setAvailableProcessors(1); metrics.setAverageActiveJobs(2); metrics.setAverageCancelledJobs(3); metrics.setAverageJobExecuteTime(4); metrics.setAverageJobWaitTime(5); metrics.setAverageRejectedJobs(6); metrics.setAverageWaitingJobs(7); metrics.setCurrentActiveJobs(8); metrics.setCurrentCancelledJobs(9); metrics.setCurrentIdleTime(10); metrics.setCurrentIdleTime(11); metrics.setCurrentJobExecuteTime(12); metrics.setCurrentJobWaitTime(13); metrics.setCurrentRejectedJobs(14); metrics.setCurrentWaitingJobs(15); metrics.setCurrentDaemonThreadCount(16); metrics.setHeapMemoryCommitted(17); metrics.setHeapMemoryInitialized(18); metrics.setHeapMemoryMaximum(19); metrics.setHeapMemoryUsed(20); metrics.setLastUpdateTime(21); metrics.setMaximumActiveJobs(22); metrics.setMaximumCancelledJobs(23); metrics.setMaximumJobExecuteTime(24); metrics.setMaximumJobWaitTime(25); metrics.setMaximumRejectedJobs(26); metrics.setMaximumWaitingJobs(27); metrics.setNonHeapMemoryCommitted(28); metrics.setNonHeapMemoryInitialized(29); metrics.setNonHeapMemoryMaximum(30); metrics.setNonHeapMemoryUsed(31); metrics.setMaximumThreadCount(32); metrics.setStartTime(33); metrics.setCurrentCpuLoad(34); metrics.setCurrentThreadCount(35); metrics.setTotalCancelledJobs(36); metrics.setTotalExecutedJobs(37); metrics.setTotalIdleTime(38); metrics.setTotalRejectedJobs(39); metrics.setTotalStartedThreadCount(40); metrics.setUpTime(41); metrics.setSentMessagesCount'}
{'instruction': 'Please refactor the following code:', 'input': 'private void startMemoryPolicies() { for (XXX memPlc : memPlcMap.values()) { memPlc.pageMemory().start(); memPlc.evictionTracker().start(); } }', 'ground_truth': 'private void startMemoryPolicies() { for (DataRegion memPlc : dataRegionMap.values()) { memPlc.pageMemory().start(); memPlc.evictionTracker().start(); } }', 'output': 'private void startMemoryPolicies() { for (final MemoryPolicy memPlc : memPlcMap.values()) { memPlc.pageMemory().start(); memPlc.evictionTracker().start(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testOnProjection() throws Exception { GridCacheProjection<Integer, FactPurchase> prj = ignite.<Integer, FactPurchase>cache("partitioned").projection( new IgnitePredicate<GridCacheEntry<Integer, FactPurchase>>() { @Override public boolean apply(GridCacheEntry<Integer, FactPurchase> e) { return e.getKey() > 12; } }); List<Map.Entry<Integer, FactPurchase>> res = body(prj); check(res); }', 'ground_truth': 'public void testOnProjection() throws Exception { CacheProjection<Integer, FactPurchase> prj = ignite.<Integer, FactPurchase>cache("partitioned").projection( new IgnitePredicate<CacheEntry<Integer, FactPurchase>>() { @Override public boolean apply(CacheEntry<Integer, FactPurchase> e) { return e.getKey() > 12; } }); List<Map.Entry<Integer, FactPurchase>> res = body(prj); check(res); }', 'output': 'public void testOnProjection() throws Exception { CacheProjection<Integer, FactPurchase> prj = ignite.<Integer, FactPurchase>cache("partitioned").projection( new IgnitePredicate<CacheEntry<Integer, FactPurchase>>() { @Override public boolean apply(CacheEntry<Integer, FactPurchase> e) { return e.getKey() > 12; } }); List<Map.Entry<Integer, FactPurchase>> res = body(prj); check(res); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private CacheStoreHolder initDir(File cacheWorkDir, int grpId, int partitions, AllocatedPageTracker allocatedTracker, boolean encrypted) throws IgniteCheckedException { try { boolean dirExisted = checkAndInitCacheWorkDir(cacheWorkDir); File idxFile = new File(cacheWorkDir, INDEX_FILE_NAME); if (dirExisted && !idxFile.exists()) grpsWithoutIdx.add(grpId); FileIOFactory pageStoreFileIoFactory = this.pageStoreFileIoFactory; FileIOFactory pageStoreV1FileIoFactory = this.pageStoreV1FileIoFactory; if (encrypted) { pageStoreFileIoFactory = new EncryptedFileIOFactory( this.pageStoreFileIoFactory, grpId, pageSize(), cctx.kernalContext().encryption(), cctx.gridConfig().getEncryptionSpi()); pageStoreV1FileIoFactory = new EncryptedFileIOFactory( this.pageStoreV1FileIoFactory, grpId, pageSize(), cctx.kernalContext().encryption(), cctx.gridConfig().getEncryptionSpi()); } FileVersionCheckingFactory pageStoreFactory = new FileVersionCheckingFactory( pageStoreFileIoFactory, pageStoreV1FileIoFactory, igniteCfg.getDataStorageConfiguration()); if (encrypted) { int headerSize = pageStoreFactory.headerSize(pageStoreFactory.latestVersion()); ((EncryptedFileIOFactory)pageStoreFileIoFactory).headerSize(headerSize); ((EncryptedFileIOFactory)pageStoreV1FileIoFactory).headerSize(headerSize); } PageStore idxStore = pageStoreFactory.createPageStore( PageMemory.FLAG_IDX, idxFile, allocatedTracker); PageStore[] partStores = new PageStore[partitions]; for (int partId = 0; partId < partStores.length; partId++) { PageStore partStore = pageStoreFactory.createPageStore( PageMemory.FLAG_DATA, getPartitionFile(cacheWorkDir, partId), allocatedTracker); partStores[partId] = partStore; } return new CacheStoreHolder(idxStore, partStores); } catch (XXX e) { cctx.kernalContext().failure().process(new FailureContext(FailureType.CRITICAL_ERROR, e)); throw e; } }', 'ground_truth': 'private CacheStoreHolder initDir(File cacheWorkDir, int grpId, int partitions, AllocatedPageTracker allocatedTracker, boolean encrypted) throws IgniteCheckedException { try { boolean dirExisted = checkAndInitCacheWorkDir(cacheWorkDir); File idxFile = new File(cacheWorkDir, INDEX_FILE_NAME); if (dirExisted && !idxFile.exists()) grpsWithoutIdx.add(grpId); FileIOFactory pageStoreFileIoFactory = this.pageStoreFileIoFactory; FileIOFactory pageStoreV1FileIoFactory = this.pageStoreV1FileIoFactory; if (encrypted) { pageStoreFileIoFactory = new EncryptedFileIOFactory( this.pageStoreFileIoFactory, grpId, pageSize(), cctx.kernalContext().encryption(), cctx.gridConfig().getEncryptionSpi()); pageStoreV1FileIoFactory = new EncryptedFileIOFactory( this.pageStoreV1FileIoFactory, grpId, pageSize(), cctx.kernalContext().encryption(), cctx.gridConfig().getEncryptionSpi()); } FileVersionCheckingFactory pageStoreFactory = new FileVersionCheckingFactory( pageStoreFileIoFactory, pageStoreV1FileIoFactory, igniteCfg.getDataStorageConfiguration()); if (encrypted) { int headerSize = pageStoreFactory.headerSize(pageStoreFactory.latestVersion()); ((EncryptedFileIOFactory)pageStoreFileIoFactory).headerSize(headerSize); ((EncryptedFileIOFactory)pageStoreV1FileIoFactory).headerSize(headerSize); } PageStore idxStore = pageStoreFactory.createPageStore( PageMemory.FLAG_IDX, idxFile, allocatedTracker); PageStore[] partStores = new PageStore[partitions]; for (int partId = 0; partId < partStores.length; partId++) { PageStore partStore = pageStoreFactory.createPageStore( PageMemory.FLAG_DATA, getPartitionFile(cacheWorkDir, partId), allocatedTracker); partStores[partId] = partStore; } return new CacheStoreHolder(idxStore, partStores); } catch (IgniteCheckedException e) { if (X.hasCause(e, StorageException.class, IOException.class)) cctx.kernalContext().failure().process(new FailureContext(FailureType.CRITICAL_ERROR, e)); throw e; } }', 'output': 'private CacheStoreHolder initDir(File cacheWorkDir, int grpId, int partitions, AllocatedPageTracker allocatedTracker, boolean encrypted) throws IgniteCheckedException { try { boolean dirExisted = checkAndInitCacheWorkDir(cacheWorkDir); File idxFile = new File(cacheWorkDir, INDEX_FILE_NAME); if (dirExisted && !idxFile.exists()) grpsWithoutIdx.add(grpId); FileIOFactory pageStoreFileIoFactory = this.pageStoreFileIoFactory; FileIOFactory pageStoreV1FileIoFactory = this.pageStoreV1FileIoFactory; if (encrypted) { pageStoreFileIoFactory = new EncryptedFileIOFactory( this.pageStoreFileIoFactory, grpId, pageSize(), cctx.kernalContext().encryption(), cctx.gridConfig().getEncryptionSpi()); pageStoreV1FileIoFactory = new EncryptedFileIOFactory( this.pageStoreV1FileIoFactory, grpId, pageSize(), cctx.kernalContext().encryption(), cctx.gridConfig().getEncryptionSpi()); } FileVersionCheckingFactory pageStoreFactory = new FileVersionCheckingFactory( pageStoreFileIoFactory, pageStoreV1FileIoFactory, igniteCfg.getDataStorageConfiguration()); if (encrypted) { int headerSize = pageStoreFactory.headerSize(pageStoreFactory.latestVersion()); ((EncryptedFileIOFactory)pageStoreFileIoFactory).headerSize(headerSize); ((EncryptedFileIOFactory)pageStoreV1FileIoFactory).headerSize(headerSize); } PageStore idxStore = pageStoreFactory.createPageStore( PageMemory.FLAG_IDX, idxFile, allocatedTracker); PageStore[] partStores = new PageStore[partitions]; for (int partId = 0; partId < partStores.length; partId++) { PageStore partStore = pageStoreFactory.createPageStore( PageMemory.FLAG_DATA, getPartitionFile(cacheWorkDir, partId), allocatedTracker); partStores[partId] = partStore; } return new CacheStoreHolder(idxStore, partStores); } catch (IgniteCheckedException e) { cctx.kernalContext().failure().process('}
{'instruction': 'Please refactor the following code:', 'input': 'public Can<ManagedObject> getChoices( final PendingParameterModel pendingArgs /*not used on properties*/) { final PropertyMemento propertyMemento = getPropertyMemento(); final OneToOneAssociation property = propertyMemento .getProperty(getSpecificationLoader()); XXX parentAdapter = getParentUiModel().load(); final Can<ManagedObject> choices = property.getChoices( parentAdapter, InteractionInitiatedBy.USER); return choices; }', 'ground_truth': 'public Can<ManagedObject> getChoices( final PendingParameterModel pendingArgs /*not used on properties*/) { val parentAdapter = getParentUiModel().load(); return getMetaModel().getChoices( parentAdapter, InteractionInitiatedBy.USER); }', 'output': 'public Can<ManagedObject> getChoices( final PendingParameterModel pendingArgs /*not used on properties*/) { final PropertyMemento propertyMemento = getPropertyMemento(); final OneToOneAssociation property = propertyMemento .getProperty(getSpecificationLoader()); ManagedObject parentAdapter = getParentUiModel().load(); final Can<ManagedObject> choices = property.getChoices( parentAdapter, InteractionInitiatedBy.USER); return choices; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGetProperties() { XXX propertyId = new PropertyIdImpl("foo", "category1", false); GreaterEqualsPredicate predicate = new GreaterEqualsPredicate(propertyId, 10); Set<PropertyId> ids = predicate.getPropertyIds(); Assert.assertEquals(1, ids.size()); Assert.assertTrue(ids.contains(propertyId)); }', 'ground_truth': 'public void testGetProperties() { String propertyId = PropertyHelper.getPropertyId("category1", "foo"); GreaterEqualsPredicate predicate = new GreaterEqualsPredicate<Integer>(propertyId, 10); Set<String> ids = predicate.getPropertyIds(); Assert.assertEquals(1, ids.size()); Assert.assertTrue(ids.contains(propertyId)); }', 'output': 'public void testGetProperties() { PropertyIdImpl propertyId = new PropertyIdImpl("foo", "category1", false); GreaterEqualsPredicate predicate = new GreaterEqualsPredicate(propertyId, 10); Set<PropertyId> ids = predicate.getPropertyIds(); Assert.assertEquals(1, ids.size()); Assert.assertTrue(ids.contains(propertyId)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testTryFlush() throws Exception { mode = LOCAL; useCache = true; try { Ignite g = startGrid(); GridCache<Integer, Integer> c = g.cache(null); GridDataLoader<Integer, Integer> ldr = g.dataLoader(null); ldr.perNodeBufferSize(10); for (int i = 0; i < 9; i++) ldr.addData(i, i); assertTrue(c.isEmpty()); ldr.tryFlush(); Thread.sleep(100); assertEquals(9, c.size()); ldr.close(false); } finally { stopAllGrids(); } }', 'ground_truth': 'public void testTryFlush() throws Exception { mode = LOCAL; useCache = true; try { Ignite g = startGrid(); GridCache<Integer, Integer> c = g.cache(null); IgniteDataLoader<Integer, Integer> ldr = g.dataLoader(null); ldr.perNodeBufferSize(10); for (int i = 0; i < 9; i++) ldr.addData(i, i); assertTrue(c.isEmpty()); ldr.tryFlush(); Thread.sleep(100); assertEquals(9, c.size()); ldr.close(false); } finally { stopAllGrids(); } }', 'output': 'public void testTryFlush() throws Exception { mode = LOCAL; useCache = true; try { Ignite g = startGrid(); GridCache<Integer, Integer> c = g.cache(null); GridDataLoader<Integer, Integer> ldr = g.dataLoader(null); ldr.perNodeBufferSize(10); for (int i = 0; i < 9; i++) ldr.addData(i, i); assertTrue(c.isEmpty()); ldr.tryFlush(); Thread.sleep(100); assertEquals(9, c.size()); ldr.close(false); } finally { stopAllGrids(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Set<Resource> getResources(Request request, Predicate predicate) throws AmbariException { ClusterRequest clusterRequest = getRequest(getProperties(predicate)); XXX requestedIds   = PropertyHelper.getRequestPropertyIds(getPropertyIds(), request, predicate); Set<ClusterResponse> responses = getManagementController().getClusters(Collections.singleton(clusterRequest)); Set<Resource> resources = new HashSet<Resource>(); for (ClusterResponse response : responses) { Resource resource = new ResourceImpl(Resource.Type.Cluster); setResourceProperty(resource, CLUSTER_ID_PROPERTY_ID, response.getClusterId(), requestedIds); setResourceProperty(resource, CLUSTER_NAME_PROPERTY_ID, response.getClusterName(), requestedIds); resource.setProperty(CLUSTER_VERSION_PROPERTY_ID, response.getDesiredStackVersion()); resources.add(resource); } return resources; }', 'ground_truth': 'public Set<Resource> getResources(Request request, Predicate predicate) throws AmbariException, UnsupportedPropertyException { ClusterRequest clusterRequest = getRequest(getProperties(predicate)); Set<String> requestedIds   = PropertyHelper.getRequestPropertyIds(getPropertyIds(), request, predicate); Set<ClusterResponse> responses = getManagementController().getClusters(Collections.singleton(clusterRequest)); Set<Resource> resources = new HashSet<Resource>(); if (LOG.isDebugEnabled()) { LOG.debug("Found clusters matching getClusters request" + ", clusterResponseCount=" + responses.size()); } for (ClusterResponse response : responses) { Resource resource = new ResourceImpl(Resource.Type.Cluster); setResourceProperty(resource, CLUSTER_ID_PROPERTY_ID, response.getClusterId(), requestedIds); setResourceProperty(resource, CLUSTER_NAME_PROPERTY_ID, response.getClusterName(), requestedIds); resource.setProperty(CLUSTER_VERSION_PROPERTY_ID, response.getDesiredStackVersion()); if (LOG.isDebugEnabled()) { LOG.debug("Adding ClusterResponse to resource" + ", clusterResponse=" + response.toString()); } resources.add(resource); } return resources; }', 'output': 'public Set<Resource> getResources(Request request, Predicate predicate) throws AmbariException { ClusterRequest clusterRequest = getRequest(getProperties(predicate)); List<String> requestedIds = PropertyHelper.getRequestPropertyIds(getPropertyIds(), request, predicate); Set<ClusterResponse> responses = getManagementController().getClusters(Collections.singleton(clusterRequest)); Set<Resource> resources = new HashSet<Resource>(); for (ClusterResponse response : responses) { Resource resource = new ResourceImpl(Resource.Type.Cluster); setResourceProperty(resource, CLUSTER_ID_PROPERTY_ID, response.getClusterId(), requestedIds); setResourceProperty(resource, CLUSTER_NAME_PROPERTY_ID, response.getClusterName(), requestedIds); resource.setProperty(CLUSTER_VERSION_PROPERTY_ID, response.getDesiredStackVersion()); resources.add(resource); } return resources; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private GridBiTuple<GridNode, GridNode> getNodes(Integer key) { GridCacheAffinity<Integer> aff = grid(0).<Integer, Object>cache(null).affinity(); int part = aff.partition(key); GridNode primary = aff.mapPartitionToNode(part); assert primary != null; Collection<GridNode> nodes = new ArrayList<>(grid(0).nodes()); nodes.remove(primary); XXX other = F.first(nodes); assert other != null; assert !F.eqNodes(primary, other); return F.t(primary, other); }', 'ground_truth': 'private GridBiTuple<ClusterNode, ClusterNode> getNodes(Integer key) { GridCacheAffinity<Integer> aff = grid(0).<Integer, Object>cache(null).affinity(); int part = aff.partition(key); ClusterNode primary = aff.mapPartitionToNode(part); assert primary != null; Collection<ClusterNode> nodes = new ArrayList<>(grid(0).nodes()); nodes.remove(primary); ClusterNode other = F.first(nodes); assert other != null; assert !F.eqNodes(primary, other); return F.t(primary, other); }', 'output': 'private GridBiTuple<ClusterNode, ClusterNode> getNodes(Integer key) { CacheAffinity<Integer> aff = grid(0).<Integer, Object>cache(null).affinity(); int part = aff.partition(key); ClusterNode primary = aff.mapPartitionToNode(part); assert primary != null; Collection<ClusterNode> nodes = new ArrayList<>(grid(0).nodes()); nodes.remove(primary); ClusterNode other = F.first(nodes); assert other != null; assert !F.eqNodes(primary, other); return F.t(primary, other); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private SSTableReader writeSortedContents(List<DecoratedKey> sortedKeys) throws IOException { logger.info("Writing " + this); String path = cfs.getFlushPath(); SSTableWriter writer = new SSTableWriter(path, sortedKeys.size(), cfs.metadata, cfs.partitioner); for (DecoratedKey key : sortedKeys) { XXX bytes = columnFamilies.get(key); assert bytes.length > 0; writer.append(key, bytes); } SSTableReader sstable = writer.closeAndOpenReader(); logger.info("Completed flushing " + writer.getFilename()); return sstable; }', 'ground_truth': 'private SSTableReader writeSortedContents(List<DecoratedKey> sortedKeys) throws IOException { logger.info("Writing " + this); String path = cfs.getFlushPath(); SSTableWriter writer = new SSTableWriter(path, sortedKeys.size(), cfs.metadata, cfs.partitioner); for (DecoratedKey key : sortedKeys) { ByteBuffer bytes = columnFamilies.get(key); assert bytes.remaining() > 0; writer.append(key, bytes); } SSTableReader sstable = writer.closeAndOpenReader(); logger.info("Completed flushing " + writer.getFilename()); return sstable; }', 'output': 'private SSTableReader writeSortedContents(List<DecoratedKey> sortedKeys) throws IOException { logger.info("Writing " + this); String path = cfs.getFlushPath(); SSTableWriter writer = new SSTableWriter(path, sortedKeys.size(), cfs.metadata, cfs.partitioner); for (DecoratedKey key : sortedKeys) { ByteBuffer bytes = columnFamilies.get(key); assert bytes != null; writer.append(key, bytes); } SSTableReader sstable = writer.closeAndOpenReader(); logger.info("Completed flushing " + writer.getFilename()); return sstable; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void queryBirthdate(Table table) throws CharacterCodingException { IndexExpression expr = new IndexExpression(ByteBufferUtil.bytes("birthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(1L)); List<IndexExpression> clause = Arrays.asList(expr); XXX filter = new IdentityQueryFilter(); IPartitioner p = StorageService.getPartitioner(); List<Row> rows = table.getColumnFamilyStore("Indexed2").search(clause, Util.range("", ""), 100, filter); assert rows.size() == 1 : StringUtils.join(rows, ","); assertEquals("k1", ByteBufferUtil.string(rows.get(0).key.key)); }', 'ground_truth': 'private void queryBirthdate(Table table) throws CharacterCodingException { IndexExpression expr = new IndexExpression(ByteBufferUtil.bytes("birthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(1L)); List<IndexExpression> clause = Arrays.asList(expr); IDiskAtomFilter filter = new IdentityQueryFilter(); IPartitioner p = StorageService.getPartitioner(); List<Row> rows = table.getColumnFamilyStore("Indexed2").search(clause, Util.range("", ""), 100, filter); assert rows.size() == 1 : StringUtils.join(rows, ","); assertEquals("k1", ByteBufferUtil.string(rows.get(0).key.key)); }', 'output': 'private void queryBirthdate(Table table) throws CharacterCodingException { IndexExpression expr = new IndexExpression(ByteBufferUtil.bytes("birthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(1L)); List<IndexExpression> clause = Arrays.asList(expr); QueryFilter filter = new IdentityQueryFilter(); IPartitioner p = StorageService.getPartitioner(); List<Row> rows = table.getColumnFamilyStore("Indexed2").search(clause, Util.range("", ""), 100, filter); assert rows.size() == 1 : StringUtils.join(rows, ","); assertEquals("k1", ByteBufferUtil.string(rows.get(0).key.key)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPutAllPutAll() throws Exception { for (int i = 0; i < gridCount(); i++) info(">>>>> Grid" + i + ": " + grid(i).localNode().id()); Map<Integer, Integer> putMap = new LinkedHashMap<>(); int size = 100; for (int i = 0; i < size; i++) putMap.put(i, i); GridCache<Object, Object> prj0 = grid(0).cache(null); GridCache<Object, Object> prj1 = grid(1).cache(null); prj0.putAll(putMap); for (int i = 0; i < size; i++) { assertEquals(i, prj0.get(i)); assertEquals(i, prj1.get(i)); } for (int i = 0; i < size; i++) putMap.put(i, i * i); info(">>> Before second put."); prj1.putAll(putMap); info(">>> After second put."); for (int i = 0; i < size; i++) { assertEquals(i * i, prj0.get(i)); assertEquals(i * i, prj1.get(i)); } }', 'ground_truth': 'public void testPutAllPutAll() throws Exception { for (int i = 0; i < gridCount(); i++) info(">>>>> Grid" + i + ": " + grid(i).localNode().id()); Map<Integer, Integer> putMap = new LinkedHashMap<>(); int size = 100; for (int i = 0; i < size; i++) putMap.put(i, i); Cache<Object, Object> prj0 = grid(0).cache(null); Cache<Object, Object> prj1 = grid(1).cache(null); prj0.putAll(putMap); for (int i = 0; i < size; i++) { assertEquals(i, prj0.get(i)); assertEquals(i, prj1.get(i)); } for (int i = 0; i < size; i++) putMap.put(i, i * i); info(">>> Before second put."); prj1.putAll(putMap); info(">>> After second put."); for (int i = 0; i < size; i++) { assertEquals(i * i, prj0.get(i)); assertEquals(i * i, prj1.get(i)); } }', 'output': 'public void testPutAllPutAll() throws Exception { for (int i = 0; i < gridCount(); i++) info(">>>>> Grid" + i + ": " + grid(i).localNode().id()); Map<Integer, Integer> putMap = new LinkedHashMap<>(); int size = 100; for (int i = 0; i < size; i++) putMap.put(i, i); GridCache<Object, Object> prj0 = grid(0).cache(null); GridCache<Object, Object> prj1 = grid(1).cache(null); prj0.putAll(putMap); for (int i = 0; i < size; i++) { assertEquals(i, prj0.get(i)); assertEquals(i, prj1.get(i)); } for (int i = 0; i < size; i++) putMap.put(i, i * i); info(">>> Before second put."); prj1.putAll(putMap); info(">>> After second put."); for (int i = 0; i < size; i++) { assertEquals(i * i, prj0.get(i)); assertEquals(i * i, prj1.get(i)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPopulateResources_singleProperty() throws Exception { TestStreamProvider  streamProvider = new TestStreamProvider(); TestJMXHostProvider hostProvider = new TestJMXHostProvider(false); XXX metricsHostProvider = new TestMetricsHostProvider(); JMXPropertyProvider propertyProvider = new JMXPropertyProvider( PropertyHelper.getJMXPropertyIds(Resource.Type.HostComponent), streamProvider, hostProvider, metricsHostProvider, PropertyHelper.getPropertyId("HostRoles", "cluster_name"), PropertyHelper.getPropertyId("HostRoles", "host_name"), PropertyHelper.getPropertyId("HostRoles", "component_name"), PropertyHelper.getPropertyId("HostRoles", "state")); Resource resource = new ResourceImpl(Resource.Type.HostComponent); resource.setProperty(HOST_COMPONENT_HOST_NAME_PROPERTY_ID, "domu-12-31-39-0e-34-e1.compute-1.internal"); resource.setProperty(HOST_COMPONENT_COMPONENT_NAME_PROPERTY_ID, "NAMENODE"); resource.setProperty(HOST_COMPONENT_STATE_PROPERTY_ID, "STARTED"); Map<String, TemporalInfo> temporalInfoMap = new HashMap<String, TemporalInfo>(); Request  request = PropertyHelper.getReadRequest(Collections.singleton("metrics/rpc/ReceivedBytes"), temporalInfoMap); Assert.assertEquals(1, propertyProvider.populateResources(Collections.singleton(resource), request, null).size()); Assert.assertEquals(propertyProvider.getSpec("http", "domu-12-31-39-0e-34-e1.compute-1.internal", "50070", "/jmx"), streamProvider.getLastSpec()); Assert.assertEquals(13670605,  resource.getPropertyValue("metrics/rpc/ReceivedBytes")); Assert.assertNull(resource.getPropertyValue("metrics/dfs/namenode/CreateFileOps")); }', 'ground_truth': 'public void testPopulateResources_singleProperty() throws Exception { TestStreamProvider  streamProvider = new TestStreamProvider(); TestJMXHostProvider hostProvider = new TestJMXHostProvider(false); TestMetricHostProvider metricsHostProvider = new TestMetricHostProvider(); JMXPropertyProvider propertyProvider = new JMXPropertyProvider( PropertyHelper.getJMXPropertyIds(Resource.Type.HostComponent), streamProvider, hostProvider, metricsHostProvider, PropertyHelper.getPropertyId("HostRoles", "cluster_name"), PropertyHelper.getPropertyId("HostRoles", "host_name"), PropertyHelper.getPropertyId("HostRoles", "component_name"), PropertyHelper.getPropertyId("HostRoles", "state")); Resource resource = new ResourceImpl(Resource.Type.HostComponent); resource.setProperty(HOST_COMPONENT_HOST_NAME_PROPERTY_ID, "domu-12-31-39-0e-34-e1.compute-1.internal"); resource.setProperty(HOST_COMPONENT_COMPONENT_NAME_PROPERTY_ID, "NAMENODE"); resource.setProperty(HOST_COMPONENT_STATE_PROPERTY_ID, "STARTED"); Map<String, TemporalInfo> temporalInfoMap = new HashMap<String, TemporalInfo>(); Request  request = PropertyHelper.getReadRequest(Collections.singleton("metrics/rpc/ReceivedBytes"), temporalInfoMap); Assert.assertEquals(1, propertyProvider.populateResources(Collections.singleton(resource), request, null).size()); Assert.assertEquals(propertyProvider.getSpec("http", "domu-12-31-39-0e-34-e1.compute-1.internal", "50070", "/jmx"), streamProvider.getLastSpec()); Assert.assertEquals(13670605,  resource.getPropertyValue("metrics/rpc/ReceivedBytes")); Assert.assertNull(resource.getPropertyValue("metrics/dfs/namenode/CreateFileOps")); }', 'output': 'public void testPopulateResources_singleProperty() throws Exception { TestStreamProvider  streamProvider = new TestStreamProvider(); TestJMXHostProvider hostProvider = new TestJMXHostProvider(false); TestMetricsHostProvider metricsHostProvider = new TestMetricsHostProvider(); JMXPropertyProvider propertyProvider = new JMXPropertyProvider( PropertyHelper.getJMXPropertyIds(Resource.Type.HostComponent), streamProvider, hostProvider, metricsHostProvider, PropertyHelper.getPropertyId("HostRoles", "cluster_name"), PropertyHelper.getPropertyId("HostRoles", "host_name"), PropertyHelper.getPropertyId("HostRoles", "component_name"), PropertyHelper.getPropertyId("HostRoles", "state")); Resource resource = new ResourceImpl(Resource.Type.HostComponent); resource.setProperty(HOST_COMPONENT_HOST_NAME_PROPERTY_ID, "domu-12-31-39-0e-34-e1.compute-1.internal"); resource.setProperty(HOST_COMPONENT_COMPONENT_NAME_PROPERTY_ID, "NAMENODE"); resource.setProperty(HOST_COMPONENT_STATE_PROPERTY_ID, "STARTED"); Map<String, TemporalInfo> temporalInfoMap = new HashMap<String, TemporalInfo>(); Request  request = PropertyHelper.getReadRequest(Collections.singleton("metrics/rpc/ReceivedBytes"), temporalInfoMap); Assert.assertEquals(1, propertyProvider.populateResources(Collections.singleton(resource), request, null).size()); Assert.assertEquals(propertyProvider.getSpec("http", "domu-12-31-39-0e-34-e1.compute-1.internal", "50070", "/jmx"), streamProvider.getLastSpec()); Assert.assertEquals(13670605,  resource.getPropertyValue("metrics/rpc/ReceivedBytes")); Assert.assertNull(resource.getPropertyValue("metrics/dfs/namenode/CreateFileOps")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private GridCacheQueryInfo localQueryInfo() throws IgniteCheckedException { GridCacheQueryBean qry = query(); IgnitePredicate<CacheEntry<Object, Object>> prjPred = qry.query().projectionFilter() == null ? F.<CacheEntry<Object, Object>>alwaysTrue() : qry.query().projectionFilter(); IgniteMarshaller marsh = cctx.marshaller(); IgniteReducer<Object, Object> rdc = qry.reducer() != null ? marsh.<IgniteReducer<Object, Object>>unmarshal(marsh.marshal(qry.reducer()), null) : null; IgniteClosure<Object, Object> trans = qry.transform() != null ? marsh.<IgniteClosure<Object, Object>>unmarshal(marsh.marshal(qry.transform()), null) : null; return new GridCacheQueryInfo( true, prjPred, trans, rdc, qry.query(), GridCacheLocalQueryFuture.this, ctx.localNodeId(), cctx.io().nextIoId(), qry.query().includeMetadata(), true, qry.arguments() ); }', 'ground_truth': 'private GridCacheQueryInfo localQueryInfo() throws IgniteCheckedException { GridCacheQueryBean qry = query(); IgnitePredicate<Entry<Object, Object>> prjPred = qry.query().projectionFilter() == null ? F.<Entry<Object, Object>>alwaysTrue() : qry.query().projectionFilter(); IgniteMarshaller marsh = cctx.marshaller(); IgniteReducer<Object, Object> rdc = qry.reducer() != null ? marsh.<IgniteReducer<Object, Object>>unmarshal(marsh.marshal(qry.reducer()), null) : null; IgniteClosure<Object, Object> trans = qry.transform() != null ? marsh.<IgniteClosure<Object, Object>>unmarshal(marsh.marshal(qry.transform()), null) : null; return new GridCacheQueryInfo( true, prjPred, trans, rdc, qry.query(), GridCacheLocalQueryFuture.this, ctx.localNodeId(), cctx.io().nextIoId(), qry.query().includeMetadata(), true, qry.arguments() ); }', 'output': 'private CacheQueryInfo localQueryInfo() throws IgniteCheckedException { CacheQueryBean qry = query(); IgnitePredicate<CacheEntry<Object, Object>> prjPred = qry.query().projectionFilter() == null ? F.<CacheEntry<Object, Object>>alwaysTrue() : qry.query().projectionFilter(); IgniteMarshaller marsh = cctx.marshaller(); IgniteReducer<Object, Object> rdc = qry.reducer() != null ? marsh.<IgniteReducer<Object, Object>>unmarshal(marsh.marshal(qry.reducer()), null) : null; IgniteClosure<Object, Object> trans = qry.transform() != null ? marsh.<IgniteClosure<Object, Object>>unmarshal(marsh.marshal(qry.transform()), null) : null; return new CacheQueryInfo( true, prjPred, trans, rdc, qry.query(), GridCacheLocalQueryFuture.this, ctx.localNodeId(), cctx.io().nextIoId(), qry.query().includeMetadata(), true, qry.arguments() ); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Set<PendingFile> getIncomingFiles(InetAddress host) { Set<PendingFile> set = new HashSet<PendingFile>(); for (Map.Entry<Pair<InetAddress, Long>, StreamInSession> entry : sessions.entrySet()) { if (entry.getKey().left.equals(host)) { StreamInSession session = entry.getValue(); if (session.current != null) set.add(session.current); set.addAll(session.files); } } return set; }', 'ground_truth': 'public static Set<PendingFile> getIncomingFiles(InetAddress host) { Set<PendingFile> set = new HashSet<PendingFile>(); for (Map.Entry<UUID, StreamInSession> entry : sessions.entrySet()) { StreamInSession session = entry.getValue(); if (session.getHost().equals(host)) { if (session.current != null) set.add(session.current); set.addAll(session.files); } } return set; }', 'output': 'public static Collection<PendingFile> getIncomingFiles(InetAddress host) { Collection<PendingFile> set = new HashSet<PendingFile>(); for (Map.Entry<Pair<InetAddress, Long>, StreamInSession> entry : sessions.entrySet()) { if (entry.getKey().left.equals(host)) { StreamInSession session = entry.getValue(); if (session.current != null) set.add(session.current); set.addAll(session.files); } } return set; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testDeleteClusterResources() throws Exception{ Resource.Type type = Resource.Type.Cluster; AmbariManagementController managementController = createMock(AmbariManagementController.class); XXX response = createNiceMock(TrackActionResponse.class); Set<ClusterResponse> nameResponse = new HashSet<ClusterResponse>(); nameResponse.add(new ClusterResponse(102L, "Cluster102", null)); expect(managementController.getClusters(Matchers.clusterRequest(null, "Cluster102", null, null))).andReturn(nameResponse).once(); managementController.deleteCluster(Matchers.clusterRequest(102L, null, null, null)); managementController.deleteCluster(Matchers.clusterRequest(103L, null, null, null)); replay(managementController, response); ResourceProvider provider = ResourceProviderImpl.getResourceProvider( type, PropertyHelper.getPropertyIds(type), PropertyHelper.getKeyPropertyIds(type), managementController); Predicate  predicate = new PredicateBuilder().property(ResourceProviderImpl.CLUSTER_NAME_PROPERTY_ID).equals("Cluster102").toPredicate(); provider.deleteResources(predicate); predicate = new PredicateBuilder().property(ResourceProviderImpl.CLUSTER_ID_PROPERTY_ID).equals(103L).toPredicate(); provider.deleteResources(predicate); verify(managementController, response); }', 'ground_truth': 'public void testDeleteClusterResources() throws Exception{ Resource.Type type = Resource.Type.Cluster; AmbariManagementController managementController = createMock(AmbariManagementController.class); RequestStatusResponse response = createNiceMock(RequestStatusResponse.class); Set<ClusterResponse> nameResponse = new HashSet<ClusterResponse>(); nameResponse.add(new ClusterResponse(102L, "Cluster102", null, null)); expect(managementController.getClusters(anyObject(Set.class))).andReturn(nameResponse).once(); managementController.deleteCluster(Matchers.clusterRequest(102L, null, "HDP-0.1", null)); managementController.deleteCluster(Matchers.clusterRequest(103L, null, "HDP-0.1", null)); replay(managementController, response); ResourceProvider provider = ResourceProviderImpl.getResourceProvider( type, PropertyHelper.getPropertyIds(type), PropertyHelper.getKeyPropertyIds(type), managementController); Predicate  predicate = new PredicateBuilder().property(ResourceProviderImpl.CLUSTER_NAME_PROPERTY_ID).equals("Cluster102").toPredicate(); provider.deleteResources(predicate); predicate = new PredicateBuilder().property(ResourceProviderImpl.CLUSTER_ID_PROPERTY_ID).equals(103L).toPredicate(); provider.deleteResources(predicate); verify(managementController, response); }', 'output': 'public void testDeleteClusterResources() throws Exception{ Resource.Type type = Resource.Type.Cluster; AmbariManagementController managementController = createMock(AmbariManagementController.class); TrackActionResponse response = createNiceMock(TrackActionResponse.class); Set<ClusterResponse> nameResponse = new HashSet<ClusterResponse>(); nameResponse.add(new ClusterResponse(102L, "Cluster102", null)); expect(managementController.getClusters(Matchers.clusterRequest(null, "Cluster102", null, null))).andReturn(nameResponse).once(); managementController.deleteCluster(Matchers.clusterRequest(102L, null, null, null)); managementController.deleteCluster(Matchers.clusterRequest(103L, null, null, null)); replay(managementController, response); ResourceProvider provider = ResourceProviderImpl.getResourceProvider( type, PropertyHelper.getPropertyIds(type), PropertyHelper.getKeyPropertyIds(type), managementController); Predicate  predicate = new PredicateBuilder().property(ResourceProviderImpl.CLUSTER_NAME_PROPERTY_ID).equals("Cluster102").toPredicate(); provider.deleteResources(predicate); predicate = new PredicateBuilder().property(ResourceProviderImpl.CLUSTER_ID_PROPERTY_ID).equals(103L).toPredicate(); provider.deleteResources(predicate); verify(managementController, response); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testIteration()  throws Exception { try { final GridCache<String, Integer> cache = cache(); final int entryCnt = 1000; for (int i = 0; i < entryCnt; i++) cache.put(String.valueOf(i), i); Collection<GridCacheEntry<String, Integer>> entries = new ArrayList<>(10_000); for (int i = 0; i < 10_000; i++) entries.add(cache.randomEntry()); startGrid(1); startGrid(2); startGrid(3); for (GridCacheEntry<String, Integer> entry : entries) entry.partition(); for (int i = 0; i < entryCnt; i++) cache.remove(String.valueOf(i)); } finally { stopGrid(3); stopGrid(2); stopGrid(1); } }', 'ground_truth': 'public void testIteration()  throws Exception { try { final Cache<String, Integer> cache = cache(); final int entryCnt = 1000; for (int i = 0; i < entryCnt; i++) cache.put(String.valueOf(i), i); Collection<CacheEntry<String, Integer>> entries = new ArrayList<>(10_000); for (int i = 0; i < 10_000; i++) entries.add(cache.randomEntry()); startGrid(1); startGrid(2); startGrid(3); for (CacheEntry<String, Integer> entry : entries) entry.partition(); for (int i = 0; i < entryCnt; i++) cache.remove(String.valueOf(i)); } finally { stopGrid(3); stopGrid(2); stopGrid(1); } }', 'output': 'public void testIteration()  throws Exception { try { final IgniteCache<String, Integer> cache = cache(); final int entryCnt = 1000; for (int i = 0; i < entryCnt; i++) cache.put(String.valueOf(i), i); Collection<IgniteCacheEntry<String, Integer>> entries = new ArrayList<>(10_000); for (int i = 0; i < 10_000; i++) entries.add(cache.randomEntry()); startGrid(1); startGrid(2); startGrid(3); for (IgniteCacheEntry<String, Integer> entry : entries) entry.partition(); for (int i = 0; i < entryCnt; i++) cache.remove(String.valueOf(i)); } finally { stopGrid(3); stopGrid(2); stopGrid(1); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void setBranches(List<I> branches) { Collections.sort(branches); XXX model = new DefaultListModel(); for (I i : branches) { model.addElement(i); if (i.isDeletion()) { panel.btnAllowDeletes.setVisible(true); } } panel.list.setModel(model); changeSupport.fireChange(); }', 'ground_truth': 'public void setBranches(List<I> branches) { Collections.sort(branches); DefaultListModel<I> model = new DefaultListModel<>(); for (I i : branches) { model.addElement(i); if (i.isDeletion()) { panel.btnAllowDeletes.setVisible(true); } } panel.list.setModel(model); changeSupport.fireChange(); }', 'output': 'public void setBranches(List<I> branches) { Collections.sort(branches); ListModel<I> model = new DefaultListModel<>(); for (I i : branches) { model.addElement(i); if (i.isDeletion()) { panel.btnAllowDeletes.setVisible(true); } } panel.list.setModel(model); changeSupport.fireChange(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public boolean equals(Object obj) { if (obj == this) return true; if (!(obj instanceof FlinkWindow)) return false; XXX thatTindowID = ((FlinkWindow) obj).getWindowID(); return thatTindowID.equals(this.getWindowID()); }', 'ground_truth': 'public boolean equals(Object obj) { if (obj == this) return true; if (!(obj instanceof FlinkWindow)) return false; WID thatTindowID = ((FlinkWindow<WID>) obj).getWindowID(); return thatTindowID.equals(this.getWindowID()); }', 'output': 'public boolean equals(Object obj) { if (obj == this) return true; if (!(obj instanceof FlinkWindow)) return false; String thatTindowID = ((FlinkWindow) obj).getWindowID(); return thatTindowID.equals(this.getWindowID()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGetRootServiceComponents() throws Exception { RootServiceComponentRequest request = new RootServiceComponentRequest(null, null); Set<RootServiceComponentResponse> rootServiceComponents; try { rootServiceComponents = responseFactory.getRootServiceComponents(request); } catch (Exception e) { assertTrue(e instanceof ObjectNotFoundException); } RootServiceResponseFactory.Components ambariServerComponent = RootServiceResponseFactory.Components.AMBARI_SERVER; request = new RootServiceComponentRequest(null, ambariServerComponent.name()); try { rootServiceComponents = responseFactory.getRootServiceComponents(request); } catch (Exception e) { assertTrue(e instanceof ObjectNotFoundException); } String serviceName = RootServiceResponseFactory.Services.AMBARI.name(); request = new RootServiceComponentRequest(serviceName, null); rootServiceComponents = responseFactory.getRootServiceComponents(request); assertEquals( RootServiceResponseFactory.Services.AMBARI.getComponents().length, rootServiceComponents.size()); String ambariVersion = ambariMetaInfo.getServerVersion(); for (int i = 0; i < RootServiceResponseFactory.Services.AMBARI.getComponents().length; i++) { XXX component = RootServiceResponseFactory.Services.AMBARI.getComponents()[i]; if (component.name().equals(ambariServerComponent.name())) { for (RootServiceComponentResponse response : rootServiceComponents) { if (response.getComponentName().equals(ambariServerComponent.name())) { assertEquals(ambariVersion, response.getComponentVersion()); assertEquals(1, response.getProperties().size(), 1); assertTrue(response.getProperties().containsKey("jdk_location")); } } } else { assertTrue(rootServiceComponents.contains(new RootServiceComponentResponse( serviceName, component.name(), RootServiceResponseFactory.NOT_APPLICABLE, Collections.emptyMap()))); } } request = new RootServiceComponentRequest( RootServiceResponseFactory.Services.AMBARI.name(), RootServiceResponseFactory.Services.AMBARI.getComponents()[0].name()); rootServiceComponents = responseFactory.getRootServiceComponents(request); assertEquals(1, rootServiceComponents.size()); for (RootServiceComponentResponse response : rootServiceComponents) { if (response.getComponentName().equals( RootServiceResponseFactory.Services.AMBARI.getComponents()[0].name())) { assertEquals(ambariVersion, response.getComponentVersion()); assertEquals(2, response.getProperties().size()); assertTrue(response.getProperties().containsKey("jdk_location")); assertTrue(response.getProperties().containsKey("java.version")); } } request = new RootServiceComponentRequest( RootServiceResponseFactory.Services.AMBARI.name(), "XXX"); try { rootServiceComponents = responseFactory.getRootServiceComponents(request); } catch (Exception e) { assertTrue(e instanceof ObjectNotFoundException); } }', 'ground_truth': 'public void testGetRootServiceComponents() throws Exception { RootServiceComponentRequest request = new RootServiceComponentRequest(null, null); Set<RootServiceComponentResponse> rootServiceComponents; try { rootServiceComponents = responseFactory.getRootServiceComponents(request); } catch (Exception e) { assertTrue(e instanceof ObjectNotFoundException); } RootComponent ambariServerComponent = RootComponent.AMBARI_SERVER; request = new RootServiceComponentRequest(null, ambariServerComponent.name()); try { rootServiceComponents = responseFactory.getRootServiceComponents(request); } catch (Exception e) { assertTrue(e instanceof ObjectNotFoundException); } String serviceName = RootService.AMBARI.name(); request = new RootServiceComponentRequest(serviceName, null); rootServiceComponents = responseFactory.getRootServiceComponents(request); assertEquals( RootService.AMBARI.getComponents().length, rootServiceComponents.size()); String ambariVersion = ambariMetaInfo.getServerVersion(); for (int i = 0; i < RootService.AMBARI.getComponents().length; i++) { RootComponent component = RootService.AMBARI.getComponents()[i]; if (component.name().equals(ambariServerComponent.name())) { for (RootServiceComponentResponse response : rootServiceComponents) { if (response.getComponentName().equals(ambariServerComponent.name())) { assertEquals(ambariVersion, response.getComponentVersion()); assertEquals(1, response.getProperties().size(), 1); assertTrue(response.getProperties().containsKey("jdk_location")); } } } else { assertTrue(rootServiceComponents.contains(new RootServiceComponentResponse( serviceName, component.name(), RootServiceResponseFactory.NOT_APPLICABLE, Collections.emptyMap()))); } } request = new RootServiceComponentRequest( RootService.AMBARI.name(), RootService.AMBARI.getComponents()[0].name()); rootServiceComponents = responseFactory.getRootServiceComponents(request); assertEquals(1, rootServiceComponents.size()); for (RootServiceComponentResponse response : rootServiceComponents) { if (response.getComponentName().equals( RootService.AMBARI.getComponents()[0].name())) { assertEquals(ambariVersion, response.getComponentVersion()); assertEquals(2, response.getProperties().size()); assertTrue(response.getProperties().containsKey("jdk_location")); assertTrue(response.getProperties().containsKey("java.version")); } } request = new RootServiceComponentRequest( RootService.AMBARI.name(), "XXX"); try { rootServiceComponents = responseFactory.getRootServiceComponents(request); } catch (Exception e) { assertTrue(e instanceof ObjectNotFoundException); } }', 'output': 'public void testGetRootServiceComponents() throws Exception { RootServiceComponentRequest request = new RootServiceComponentRequest(null, null); Set<RootServiceComponentResponse> rootServiceComponents; try { rootServiceComponents = responseFactory.getRootServiceComponents(request); } catch (Exception e) { assertTrue(e instanceof ObjectNotFoundException); } RootServiceResponseFactory.Components ambariServerComponent = RootServiceResponseFactory.Components.AMBARI_SERVER; request = new RootServiceComponentRequest(null, ambariServerComponent.name()); try { rootServiceComponents = responseFactory.getRootServiceComponents(request); } catch (Exception e) { assertTrue(e instanceof ObjectNotFoundException); } String serviceName = RootServiceResponseFactory.Services.AMBARI.name(); request = new RootServiceComponentRequest(serviceName, null); rootServiceComponents = responseFactory.getRootServiceComponents(request); assertEquals( RootServiceResponseFactory.Services.AMBARI.getComponents().length, rootServiceComponents.size()); String ambariVersion = ambariMetaInfo.getServerVersion(); for (int i = 0; i < RootServiceResponseFactory.Services.AMBARI.getComponents().length; i++) { XXX component = RootServiceResponseFactory.Services.AMBARI.getComponents()[i]; if (component.name().equals(ambariServerComponent.name())) { for (RootServiceComponentResponse response : rootServiceComponents) { if (response.getComponentName().equals(ambariServerComponent.name())) { assertEquals(ambariVersion, response.getComponentVersion()); assertEquals(1, response.getProperties().size(), 1); assertTrue(response.getProperties().containsKey("jdk_location")); } } } else { assertTrue(rootServiceComponents.contains(new RootServiceComponentResponse( serviceName, component.name(), RootServiceResponseFactory.NOT_APPLICABLE, Collections.emptyMap()))); } } request = new RootServiceComponentRequest( RootServiceResponseFactory.Services.AMBARI.name(), RootServiceResponseFactory.Services.AMBARI.getComponents()[0].name()); rootServiceComponents = responseFactory.getRootServiceComponents(request); assertEquals(1, rootServiceComponents.size()); for (RootServiceComponentResponse response : rootServiceComponents) { if (response.getComponentName().equals( RootService'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testLateAndAtWatermarkSessionsProcessingTime() throws Exception { TriggerTester<Integer, Iterable<Integer>, IntervalWindow> tester = TriggerTester.nonCombining( Sessions.withGapDuration(Duration.millis(20)), AfterWatermark.<IntervalWindow>pastEndOfWindow() .withLateFirings(AfterProcessingTime .<IntervalWindow>pastFirstElementInPane().plusDelayOf(Duration.millis(10))), AccumulationMode.ACCUMULATING_FIRED_PANES, Duration.millis(100)); tester.advanceProcessingTime(new Instant(0L)); tester.injectElements( TimestampedValue.of(1, new Instant(5L)), TimestampedValue.of(2, new Instant(20L))); tester.advanceInputWatermark(new Instant(70L)); tester.injectElements(TimestampedValue.of(3, new Instant(6L))); tester.advanceProcessingTime(new Instant(100L)); tester.injectElements(TimestampedValue.of(4, new Instant(9L))); tester.advanceProcessingTime(new Instant(100L)); List<WindowedValue<Iterable<Integer>>> output = tester.extractOutput(); assertThat(output, Matchers.contains( isSingleWindowedValue(containsInAnyOrder(1, 2), 5, 5, 40), isSingleWindowedValue(containsInAnyOrder(1, 2, 3), 39, 5, 40))); assertFalse(tester.isMarkedFinished(new IntervalWindow(new Instant(5), new Instant(40)))); }', 'ground_truth': 'public void testLateAndAtWatermarkSessionsProcessingTime() throws Exception { ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining( Sessions.withGapDuration(Duration.millis(20)), AfterWatermark.<IntervalWindow>pastEndOfWindow() .withLateFirings(AfterProcessingTime .<IntervalWindow>pastFirstElementInPane().plusDelayOf(Duration.millis(10))), AccumulationMode.ACCUMULATING_FIRED_PANES, Duration.millis(100)); tester.advanceProcessingTime(new Instant(0L)); tester.injectElements( TimestampedValue.of(1, new Instant(5L)), TimestampedValue.of(2, new Instant(20L))); tester.advanceInputWatermark(new Instant(70L)); tester.injectElements(TimestampedValue.of(3, new Instant(6L))); tester.advanceProcessingTime(new Instant(100L)); tester.injectElements(TimestampedValue.of(4, new Instant(9L))); tester.advanceProcessingTime(new Instant(100L)); List<WindowedValue<Iterable<Integer>>> output = tester.extractOutput(); assertThat(output, Matchers.contains( isSingleWindowedValue(containsInAnyOrder(1, 2), 5, 5, 40), isSingleWindowedValue(containsInAnyOrder(1, 2, 3), 39, 5, 40))); assertFalse(tester.isMarkedFinished(new IntervalWindow(new Instant(5), new Instant(40)))); }', 'output': 'public void testLateAndAtWatermarkSessionsProcessingTime() throws Exception { TriggerTester<Integer, Iterable<Integer>, IntervalWindow> tester = TriggerTester.nonCombining( Sessions.withGapDuration(Duration.millis(20)), AfterWatermark.<IntervalWindow>pastEndOfWindow() .withLateFirings(AfterProcessingTime .<IntervalWindow>pastFirstElementInPane().plusDelayOf(Duration.millis(10))), AccumulationMode.ACCUMULATING_FIRED_PANES, Duration.millis(100)); tester.advanceProcessingTime(new Instant(0L)); tester.injectElements( TimestampedValue.of(1, new Instant(5L)), TimestampedValue.of(2, new Instant(20L))); tester.advanceInputWatermark(new Instant(70L)); tester.injectElements(TimestampedValue.of(3, new Instant(6L))); tester.advanceProcessingTime(new Instant(100L)); tester.injectElements(TimestampedValue.of(4, new Instant(9L))); tester.advanceProcessingTime(new Instant(100L)); List<WindowedValue<Iterable<Integer>>> output = tester.extractOutput(); assertThat(output, Matchers.contains( isSingleWindowedValue(containsInAnyOrder(1, 2), 5, 5, 40), isSingleWindowedValue(containsInAnyOrder(1, 2, 3), 39, 5, 40))); assertFalse(tester.isMarkedFinished(new IntervalWindow(new Instant(5), new Instant(40)))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testBoundsAsClusteringWithSingleEqAndSliceRestrictions() { CFMetaData cfMetaData = newCFMetaData(Sort.ASC, Sort.ASC, Sort.ASC); ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); ByteBuffer value4 = ByteBufferUtil.bytes(4); ByteBuffer value5 = ByteBufferUtil.bytes(5); Restriction singleEq = newSingleEq(cfMetaData, 0, value1); Restriction multiSlice = newMultiSlice(cfMetaData, 1, Bound.START, false, value2, value3); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(cfMetaData); restrictions = restrictions.mergeWith(singleEq).mergeWith(multiSlice); XXX bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1); singleEq = newSingleEq(cfMetaData, 0, value1); multiSlice = newMultiSlice(cfMetaData, 1, Bound.START, false, value2, value3); Restriction multiSlice2 = newMultiSlice(cfMetaData, 1, Bound.END, false, value4); restrictions = new ClusteringColumnRestrictions(cfMetaData); restrictions = restrictions.mergeWith(multiSlice2).mergeWith(singleEq).mergeWith(multiSlice); bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), false, value1, value4); singleEq = newSingleEq(cfMetaData, 0, value1); multiSlice = newMultiSlice(cfMetaData, 1, Bound.START, true, value2, value3); multiSlice2 = newMultiSlice(cfMetaData, 1, Bound.END, true, value4, value5); restrictions = new ClusteringColumnRestrictions(cfMetaData); restrictions = restrictions.mergeWith(multiSlice2).mergeWith(singleEq).mergeWith(multiSlice); bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), true, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value4, value5); }', 'ground_truth': 'public void testBoundsAsClusteringWithSingleEqAndSliceRestrictions() { CFMetaData cfMetaData = newCFMetaData(Sort.ASC, Sort.ASC, Sort.ASC); ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); ByteBuffer value4 = ByteBufferUtil.bytes(4); ByteBuffer value5 = ByteBufferUtil.bytes(5); Restriction singleEq = newSingleEq(cfMetaData, 0, value1); Restriction multiSlice = newMultiSlice(cfMetaData, 1, Bound.START, false, value2, value3); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(cfMetaData); restrictions = restrictions.mergeWith(singleEq).mergeWith(multiSlice); SortedSet<ClusteringBound> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1); singleEq = newSingleEq(cfMetaData, 0, value1); multiSlice = newMultiSlice(cfMetaData, 1, Bound.START, false, value2, value3); Restriction multiSlice2 = newMultiSlice(cfMetaData, 1, Bound.END, false, value4); restrictions = new ClusteringColumnRestrictions(cfMetaData); restrictions = restrictions.mergeWith(multiSlice2).mergeWith(singleEq).mergeWith(multiSlice); bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), false, value1, value4); singleEq = newSingleEq(cfMetaData, 0, value1); multiSlice = newMultiSlice(cfMetaData, 1, Bound.START, true, value2, value3); multiSlice2 = newMultiSlice(cfMetaData, 1, Bound.END, true, value4, value5); restrictions = new ClusteringColumnRestrictions(cfMetaData); restrictions = restrictions.mergeWith(multiSlice2).mergeWith(singleEq).mergeWith(multiSlice); bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), true, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1, value4, value5); }', 'output': 'public void testBoundsAsClusteringWithSingleEqAndSliceRestrictions() { CFMetaData cfMetaData = newCFMetaData(Sort.ASC, Sort.ASC, Sort.ASC); ByteBuffer value1 = ByteBufferUtil.bytes(1); ByteBuffer value2 = ByteBufferUtil.bytes(2); ByteBuffer value3 = ByteBufferUtil.bytes(3); ByteBuffer value4 = ByteBufferUtil.bytes(4); ByteBuffer value5 = ByteBufferUtil.bytes(5); Restriction singleEq = newSingleEq(cfMetaData, 0, value1); Restriction multiSlice = newMultiSlice(cfMetaData, 1, Bound.START, false, value2, value3); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(cfMetaData); restrictions = restrictions.mergeWith(singleEq).mergeWith(multiSlice); Bounds bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), true, value1); singleEq = newSingleEq(cfMetaData, 0, value1); multiSlice = newMultiSlice(cfMetaData, 1, Bound.START, false, value2, value3); Restriction multiSlice2 = newMultiSlice(cfMetaData, 1, Bound.END, false, value4, value5); restrictions = new ClusteringColumnRestrictions(cfMetaData); restrictions = restrictions.mergeWith(multiSlice2).mergeWith(singleEq).mergeWith(multiSlice); Bounds bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertStartBound(get(bounds, 0), false, value1, value2, value3); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEndBound(get(bounds, 0), false, value1, value4, value'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void memoryMetrics(VisorNodeDataCollectorJobResult res) { try { List<VisorMemoryMetrics> memoryMetrics = res.getMemoryMetrics(); for (XXX m : ignite.memoryMetrics()) memoryMetrics.add(new VisorMemoryMetrics(m)); } catch (Exception e) { res.setMemoryMetricsEx(new VisorExceptionWrapper(e)); } }', 'ground_truth': 'protected void memoryMetrics(VisorNodeDataCollectorJobResult res) { try { List<VisorMemoryMetrics> memoryMetrics = res.getMemoryMetrics(); for (DataRegionMetrics m : ignite.dataRegionMetrics()) memoryMetrics.add(new VisorMemoryMetrics(m)); } catch (Exception e) { res.setMemoryMetricsEx(new VisorExceptionWrapper(e)); } }', 'output': 'protected void memoryMetrics(VisorNodeDataCollectorJobResult res) { try { List<VisorMemoryMetrics> memoryMetrics = res.getMemoryMetrics(); for (VisorMemoryMetrics m : ignite.memoryMetrics()) memoryMetrics.add(m); } catch (Exception e) { res.setMemoryMetricsEx(new VisorExceptionWrapper(e)); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testReduceFailed() throws Exception { Ignite ignite = G.grid(getTestGridName()); ignite.compute().localDeployTask(GridTaskFailedTestTask.class, GridTaskFailedTestTask.class.getClassLoader()); XXX fut = executeAsync(ignite.compute(), GridTaskFailedTestTask.class.getName(), FailType.RESULT); try { fut.get(); assert false; } catch (GridException e) { info("Got expected grid exception: " + e); } IgniteUuid sesId = fut.getTaskSession().getId(); List<GridEvent> evts = ignite.events().remoteQuery(new TaskFailedEventFilter(sesId), 0); assert evts.size() == 1; info("Task failed event: " + evts.get(0)); }', 'ground_truth': 'public void testReduceFailed() throws Exception { Ignite ignite = G.grid(getTestGridName()); ignite.compute().localDeployTask(GridTaskFailedTestTask.class, GridTaskFailedTestTask.class.getClassLoader()); ComputeTaskFuture<?> fut = executeAsync(ignite.compute(), GridTaskFailedTestTask.class.getName(), FailType.RESULT); try { fut.get(); assert false; } catch (GridException e) { info("Got expected grid exception: " + e); } IgniteUuid sesId = fut.getTaskSession().getId(); List<GridEvent> evts = ignite.events().remoteQuery(new TaskFailedEventFilter(sesId), 0); assert evts.size() == 1; info("Task failed event: " + evts.get(0)); }', 'output': 'public void testReduceFailed() throws Exception { Ignite ignite = G.grid(getTestGridName()); ignite.compute().localDeployTask(GridTaskFailedTestTask.class, GridTaskFailedTestTask.class.getClassLoader()); IgniteFuture<?> fut = executeAsync(ignite.compute(), GridTaskFailedTestTask.class.getName(), FailType.RESULT); try { fut.get(); assert false; } catch (GridException e) { info("Got expected grid exception: " + e); } IgniteUuid sesId = fut.getTaskSession().getId(); List<GridEvent> evts = ignite.events().remoteQuery(new TaskFailedEventFilter(sesId), 0); assert evts.size() == 1; info("Task failed event: " + evts.get(0)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testNormalizeSort() { List<? extends AbstractBounds> input, expected; input = asList(range("4", "5"), range("1", "3")); expected = asList(range("1", "3"), range("4", "5")); assertNormalize(input, expected); input = asList(bounds("4", "5"), bounds("1", "3")); expected = asList(bounds("1", "3"), bounds("4", "5")); assertNormalize(input, expected); }', 'ground_truth': 'public void testNormalizeSort() { List<? extends AbstractBounds<RowPosition>> input, expected; input = asList(range("4", "5"), range("1", "3")); expected = asList(range("1", "3"), range("4", "5")); assertNormalize(input, expected); input = asList(bounds("4", "5"), bounds("1", "3")); expected = asList(bounds("1", "3"), bounds("4", "5")); assertNormalize(input, expected); }', 'output': 'public void testNormalizeSort() { List<? extends AbstractBounds> input, expected; input = asList(range("4", "5"), range("1", "3")); expected = asList(range("1", "3"), range("4", "5")); assertNormalize(input, expected); input = asList(bounds("4", "5"), bounds("1", "3")); expected = asList(bounds("1", "3"), bounds("4", "5")); assertNormalize(input, expected); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGetHostRoleCommandStatusSummaryCacheSizeDefault() throws  Exception { final Properties ambariProperties = new Properties(); final Configuration configuration = new Configuration(ambariProperties); XXX actualCacheSize = configuration.getHostRoleCommandStatusSummaryCacheSize(); Assert.assertEquals(actualCacheSize, Configuration.SERVER_HRC_STATUS_SUMMARY_CACHE_SIZE_DEFAULT); }', 'ground_truth': 'public void testGetHostRoleCommandStatusSummaryCacheSizeDefault() throws  Exception { final Properties ambariProperties = new Properties(); final Configuration configuration = new Configuration(ambariProperties); Long actualCacheSize = configuration.getHostRoleCommandStatusSummaryCacheSize(); Assert.assertEquals(actualCacheSize, Configuration.SERVER_HRC_STATUS_SUMMARY_CACHE_SIZE.getDefaultValue()); }', 'output': 'public void testGetHostRoleCommandStatusSummaryCacheSizeDefault() throws  Exception { final Properties ambariProperties = new Properties(); final Configuration configuration = new Configuration(ambariProperties); int actualCacheSize = configuration.getHostRoleCommandStatusSummaryCacheSize(); Assert.assertEquals(actualCacheSize, Configuration.SERVER_HRC_STATUS_SUMMARY_CACHE_SIZE_DEFAULT); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void generateParentBody( RelNode rel, StatementList stmtList) { if (stmtList != null) { pushStatementList(stmtList); } XXX frame = mapRel2Frame.get(rel); bindDeferred(frame, rel); ((JavaLoopRel) frame.parent).implementJavaParent(this, frame.ordinal); if (stmtList != null) { popStatementList(stmtList); } }', 'ground_truth': 'public void generateParentBody( RelNode rel, StatementList stmtList) { if (stmtList != null) { pushStatementList(stmtList); } JavaFrame frame = (JavaFrame) mapRel2Frame.get(rel); bindDeferred(frame, rel); ((JavaLoopRel) frame.parent).implementJavaParent(this, frame.ordinal); if (stmtList != null) { popStatementList(stmtList); } }', 'output': 'public void generateParentBody( RelNode rel, StatementList stmtList) { if (stmtList != null) { pushStatementList(stmtList); } JavaFrame frame = mapRel2Frame.get(rel); bindDeferred(frame, rel); ((JavaLoopRel) frame.parent).implementJavaParent(this, frame.ordinal); if (stmtList != null) { popStatementList(stmtList); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSimpleKeyEvict() throws Exception { GridCache<Integer, String> c = grid().cache(CACHE_NAME); for (int i = 0; i < 3; i++) c.putx(i, "value" + i); assertEquals(3, c.size()); assertEquals("value0", c.get(0)); assertEquals("value1", c.get(1)); assertEquals("value2", c.get(2)); svc.simpleKeyEvict(2); assertEquals(2, c.size()); assertEquals("value0", c.get(0)); assertEquals("value1", c.get(1)); assertNull(c.get(2)); }', 'ground_truth': 'public void testSimpleKeyEvict() throws Exception { IgniteCache<Integer, String> c = grid().jcache(CACHE_NAME); for (int i = 0; i < 3; i++) c.put(i, "value" + i); assertEquals(3, c.size()); assertEquals("value0", c.get(0)); assertEquals("value1", c.get(1)); assertEquals("value2", c.get(2)); svc.simpleKeyEvict(2); assertEquals(2, c.size()); assertEquals("value0", c.get(0)); assertEquals("value1", c.get(1)); assertNull(c.get(2)); }', 'output': 'public void testSimpleKeyEvict() throws Exception { Cache<Integer, String> c = grid().cache(CACHE_NAME); for (int i = 0; i < 3; i++) c.putx(i, "value" + i); assertEquals(3, c.size()); assertEquals("value0", c.get(0)); assertEquals("value1", c.get(1)); assertEquals("value2", c.get(2)); svc.simpleKeyEvict(2); assertEquals(2, c.size()); assertEquals("value0", c.get(0)); assertEquals("value1", c.get(1)); assertNull(c.get(2)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAddAll() throws Exception { try { String queueName = UUID.randomUUID().toString(); info("Queue name: " + queueName); GridCache c = grid(0).cache(null); XXX queue = c.dataStructures().queue(queueName, QUEUE_CAPACITY, false, true); assertTrue(queue.isEmpty()); grid(0).compute().call(new AddAllJob(queueName, RETRIES)); assertEquals(GRID_CNT * RETRIES, queue.size()); queue.clear(5); assertEquals(0, queue.size()); c.dataStructures().removeQueue(queueName); } catch (Throwable t) { error("Failure in test: " + t); } }', 'ground_truth': 'public void testAddAll() throws Exception { try { String queueName = UUID.randomUUID().toString(); info("Queue name: " + queueName); Cache c = grid(0).cache(null); CacheQueue<Integer> queue = c.dataStructures().queue(queueName, QUEUE_CAPACITY, false, true); assertTrue(queue.isEmpty()); grid(0).compute().call(new AddAllJob(queueName, RETRIES)); assertEquals(GRID_CNT * RETRIES, queue.size()); queue.clear(5); assertEquals(0, queue.size()); c.dataStructures().removeQueue(queueName); } catch (Throwable t) { error("Failure in test: " + t); } }', 'output': 'public void testAddAll() throws Exception { try { String queueName = UUID.randomUUID().toString(); info("Queue name: " + queueName); IgniteCache<Integer, Integer> c = grid(0).jcache(null); IgniteQueue<Integer, Integer> queue = c.dataStructures().queue(queueName, QUEUE_CAPACITY, false, true); assertTrue(queue.isEmpty()); grid(0).compute().call(new AddAllJob(queueName, RETRIES)); assertEquals(GRID_CNT * RETRIES, queue.size()); queue.clear(5); assertEquals(0, queue.size()); c.dataStructures().removeQueue(queueName); } catch (Throwable t) { error("Failure in test: " + t); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testNullCheckpoint() throws Exception { final int numElements = 20; PipelineOptions options = PipelineOptionsFactory.create(); TestCountingSource source = new TestCountingSource(numElements) { @Override public Coder<CounterMark> getCheckpointMarkCoder() { return null; } }; UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark> flinkWrapper = new UnboundedSourceWrapper<>("stepName", options, source, numSplits); StreamSource< WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>, UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark>> sourceOperator = new StreamSource<>(flinkWrapper); AbstractStreamOperatorTestHarness<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>> testHarness = new AbstractStreamOperatorTestHarness<>( sourceOperator, numTasks /* max parallelism */, numTasks /* parallelism */, 0 /* subtask index */); testHarness.setTimeCharacteristic(TimeCharacteristic.EventTime); testHarness.open(); XXX snapshot = testHarness.snapshot(0, 0); UnboundedSourceWrapper< KV<Integer, Integer>, TestCountingSource.CounterMark> restoredFlinkWrapper = new UnboundedSourceWrapper<>( "stepName", options, new TestCountingSource(numElements), numSplits); StreamSource< WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>, UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark>> restoredSourceOperator = new StreamSource<>(restoredFlinkWrapper); AbstractStreamOperatorTestHarness<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>> restoredTestHarness = new AbstractStreamOperatorTestHarness<>( restoredSourceOperator, numTasks /* max parallelism */, 1 /* parallelism */, 0 /* subtask index */); restoredTestHarness.setup(); restoredTestHarness.initializeState(snapshot); restoredTestHarness.open(); assertEquals(0, restoredFlinkWrapper.getLocalSplitSources().size()); }', 'ground_truth': 'public void testNullCheckpoint() throws Exception { final int numElements = 20; PipelineOptions options = PipelineOptionsFactory.create(); TestCountingSource source = new TestCountingSource(numElements) { @Override public Coder<CounterMark> getCheckpointMarkCoder() { return null; } }; UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark> flinkWrapper = new UnboundedSourceWrapper<>("stepName", options, source, numSplits); StreamSource< WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>, UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark>> sourceOperator = new StreamSource<>(flinkWrapper); AbstractStreamOperatorTestHarness<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>> testHarness = new AbstractStreamOperatorTestHarness<>( sourceOperator, numTasks /* max parallelism */, numTasks /* parallelism */, 0 /* subtask index */); testHarness.setTimeCharacteristic(TimeCharacteristic.EventTime); testHarness.open(); OperatorSubtaskState snapshot = testHarness.snapshot(0, 0); UnboundedSourceWrapper< KV<Integer, Integer>, TestCountingSource.CounterMark> restoredFlinkWrapper = new UnboundedSourceWrapper<>( "stepName", options, new TestCountingSource(numElements), numSplits); StreamSource< WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>, UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark>> restoredSourceOperator = new StreamSource<>(restoredFlinkWrapper); AbstractStreamOperatorTestHarness<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>> restoredTestHarness = new AbstractStreamOperatorTestHarness<>( restoredSourceOperator, numTasks /* max parallelism */, 1 /* parallelism */, 0 /* subtask index */); restoredTestHarness.setup(); restoredTestHarness.initializeState(snapshot); restoredTestHarness.open(); assertEquals(0, restoredFlinkWrapper.getLocalSplitSources().size()); }', 'output': 'public void testNullCheckpoint() throws Exception { final int numElements = 20; PipelineOptions options = PipelineOptionsFactory.create(); TestCountingSource source = new TestCountingSource(numElements) { @Override public Coder<CounterMark> getCheckpointMarkCoder() { return null; } }; UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark> flinkWrapper = new UnboundedSourceWrapper<>("stepName", options, source, numSplits); StreamSource< WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>, UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark>> sourceOperator = new StreamSource<>(flinkWrapper); AbstractStreamOperatorTestHarness<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>> testHarness = new AbstractStreamOperatorTestHarness<>( sourceOperator, numTasks /* max parallelism */, numTasks /* parallelism */, 0 /* subtask index */); testHarness.setTimeCharacteristic(TimeCharacteristic.EventTime); testHarness.open(); Snapshot snapshot = testHarness.snapshot(0, 0); UnboundedSourceWrapper< KV<Integer, Integer>, TestCountingSource.CounterMark> restoredFlinkWrapper = new UnboundedSourceWrapper<>( "stepName", options, new TestCountingSource(numElements), numSplits); StreamSource< WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>, UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark>> restoredSourceOperator = new StreamSource<>(restoredFlinkWrapper); AbstractStreamOperatorTestHarness<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>> restoredTestHarness = new AbstractStreamOperatorTestHarness<>( restoredSourceOperator, numTasks /* max parallelism */, 1 /* parallelism */, 0 /* subtask index */); restoredTestHarness.setup(); restoredTestHarness.initializeState(snapshot); restoredTestHarness.open(); assertEquals(0, restoredFlinkWrapper.getLocalSplitSources().size()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void checkData(IgniteEx grid0) { IgniteCache<Long, String> cache = grid0.cache(cacheName()); assertNotNull(cache); for (long i = 0; i < 100; i++) assertEquals("" + i, cache.get(i)); }', 'ground_truth': 'protected void checkData(IgniteEx grid0) { IgniteCache<Long, Object> cache = grid0.cache(cacheName()); assertNotNull(cache); int size = cache.size(); assertTrue("Cache cannot be empty", size > 0); for (long i = 0; i < size; i++) assertEquals(generateValue(i), cache.get(i)); }', 'output': 'protected void checkData(IgniteEx grid0) { IgniteCache<Long, String> cache = grid0.jcache(cacheName()); assertNotNull(cache); for (long i = 0; i < 100; i++) assertEquals("" + i, cache.get(i)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String getString(ByteBuffer bytes) { try { return LexicalUUIDTerm.instance.getString(bytes); } catch (XXX e) { throw new MarshalException(e.getMessage()); } }', 'ground_truth': 'public String getString(ByteBuffer bytes) { try { return LexicalUUIDTerm.instance.getString(bytes); } catch (org.apache.cassandra.cql.jdbc.MarshalException e) { throw new MarshalException(e.getMessage()); } }', 'output': 'public String getString(ByteBuffer bytes) { try { return LexicalUUIDTerm.instance.getString(bytes); } catch (IOException e) { throw new MarshalException(e.getMessage()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testDateArrayOverride() { GridPortableTestClasses.TestObjectContainer obj = new GridPortableTestClasses.TestObjectContainer(); BinaryObjectBuilderImpl mutableObj = wrap(obj); Date[] arr = { new Date() }; mutableObj.setField("foo", arr); XXX res = mutableObj.build().deserialize(); assertEquals(Date[].class, res.foo.getClass()); assertTrue(Objects.deepEquals(arr, res.foo)); }', 'ground_truth': 'public void testDateArrayOverride() { GridBinaryTestClasses.TestObjectContainer obj = new GridBinaryTestClasses.TestObjectContainer(); BinaryObjectBuilderImpl mutableObj = wrap(obj); Date[] arr = { new Date() }; mutableObj.setField("foo", arr); GridBinaryTestClasses.TestObjectContainer res = mutableObj.build().deserialize(); assertEquals(Date[].class, res.foo.getClass()); assertTrue(Objects.deepEquals(arr, res.foo)); }', 'output': 'public void testDateArrayOverride() { IgnitePortableTestClasses.TestObjectContainer obj = new IgnitePortableTestClasses.TestObjectContainer(); BinaryObjectBuilderImpl mutableObj = wrap(obj); Date[] arr = { new Date() }; mutableObj.setField("foo", arr); BinaryObject res = mutableObj.build().deserialize(); assertEquals(Date[].class, res.foo.getClass()); assertTrue(Objects.deepEquals(arr, res.foo)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Object getResourceHandler(@PathParam("instanceName") String instanceName, @PathParam("resources") String resources) { XXX instanceDefinition = ViewRegistry.getInstance().getInstanceDefinition(m_viewName, instanceName); if (instanceDefinition == null) { throw new IllegalArgumentException("A view instance " + m_viewName + "/" + instanceName + " can not be found."); } Object service = instanceDefinition.getService(resources); if (service == null) { throw new IllegalArgumentException("A resource type " + resources + " for view instance " + m_viewName + "/" + instanceName + " can not be found."); } return service; }', 'ground_truth': 'public Object getResourceHandler(@PathParam("instanceName") String instanceName, @PathParam("resources") String resources) { ViewInstanceEntity instanceDefinition = ViewRegistry.getInstance().getInstanceDefinition(m_viewName, instanceName); if (instanceDefinition == null) { throw new IllegalArgumentException("A view instance " + m_viewName + "/" + instanceName + " can not be found."); } Object service = instanceDefinition.getService(resources); if (service == null) { throw new IllegalArgumentException("A resource type " + resources + " for view instance " + m_viewName + "/" + instanceName + " can not be found."); } return service; }', 'output': 'public Object getResourceHandler(@PathParam("instanceName") String instanceName, @PathParam("resourceType") String resourceType, @PathParam("resources") String resources) { ViewInstanceDefinition instanceDefinition = ViewRegistry.getInstance().getInstanceDefinition(m_viewName, instanceName); if (instanceDefinition == null) { throw new IllegalArgumentException("A view instance " + m_viewName + "/" + instanceName + " can not be found."); } Object service = instanceDefinition.getService(resourceType, resources); if (service == null) { throw new IllegalArgumentException("A resource type " + resourceType + " for view instance " + m_viewName + "/" + instanceName + " can not be found."); } return service; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static GridClient client(String addr) throws IOException { try { IgniteFuture<GridClient> fut = cliMap.get(addr); if (fut == null) { GridFutureAdapter<GridClient> fut0 = new GridFutureAdapter<>(); XXX oldFut = cliMap.putIfAbsent(addr, fut0); if (oldFut != null) return oldFut.get(); else { GridClientConfiguration cliCfg = new GridClientConfiguration(); cliCfg.setProtocol(TCP); cliCfg.setServers(Collections.singletonList(addr)); cliCfg.setMarshaller(new GridClientOptimizedMarshaller()); cliCfg.setDaemon(true); try { GridClient cli = GridClientFactory.start(cliCfg); fut0.onDone(cli); return cli; } catch (GridClientException e) { fut0.onDone(e); throw new IOException("Failed to establish connection with GridGain node: " + addr, e); } } } else return fut.get(); } catch (IgniteCheckedException e) { throw new IOException("Failed to establish connection with GridGain node: " + addr, e); } }', 'ground_truth': 'private static GridClient client(String addr) throws IOException { try { IgniteInternalFuture<GridClient> fut = cliMap.get(addr); if (fut == null) { GridFutureAdapter<GridClient> fut0 = new GridFutureAdapter<>(); IgniteInternalFuture<GridClient> oldFut = cliMap.putIfAbsent(addr, fut0); if (oldFut != null) return oldFut.get(); else { GridClientConfiguration cliCfg = new GridClientConfiguration(); cliCfg.setProtocol(TCP); cliCfg.setServers(Collections.singletonList(addr)); cliCfg.setMarshaller(new GridClientOptimizedMarshaller()); cliCfg.setDaemon(true); try { GridClient cli = GridClientFactory.start(cliCfg); fut0.onDone(cli); return cli; } catch (GridClientException e) { fut0.onDone(e); throw new IOException("Failed to establish connection with GridGain node: " + addr, e); } } } else return fut.get(); } catch (IgniteCheckedException e) { throw new IOException("Failed to establish connection with GridGain node: " + addr, e); } }', 'output': 'private static GridClient client(String addr) throws IOException { try { IgniteFuture<GridClient> fut = cliMap.get(addr); if (fut == null) { GridFutureAdapter<GridClient> fut0 = new GridFutureAdapter<>(); GridClient oldFut = cliMap.putIfAbsent(addr, fut0); if (oldFut != null) return oldFut.get(); else { GridClientConfiguration cliCfg = new GridClientConfiguration(); cliCfg.setProtocol(TCP); cliCfg.setServers(Collections.singletonList(addr)); cliCfg.setMarshaller(new GridClientOptimizedMarshaller()); cliCfg.setDaemon(true); try { GridClient cli = GridClientFactory.start(cliCfg); fut0.onDone(cli); return cli; } catch (GridClientException e) { fut0.onDone(e); throw new IOException("Failed to establish connection with GridGain node: " + addr, e); } } } else return fut.get(); } catch (IgniteCheckedException e) { throw new IOException("Failed to establish connection with GridGain node: " + addr, e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static ColumnIdentifier getColumnId(CFMetaData cfm, int i) { XXX definedNames = cfm.getColumnAliases(); return definedNames == null || i >= definedNames.size() || cfm.getColumnAliases().get(i) == null ? new ColumnIdentifier(DEFAULT_COLUMN_ALIAS + (i + 1), false) : new ColumnIdentifier(cfm.getColumnAliases().get(i), definitionType); }', 'ground_truth': 'private static ColumnIdentifier getColumnId(CFMetaData cfm, int i) { List<ColumnDefinition> definedNames = cfm.clusteringKeyColumns(); return definedNames == null || i >= definedNames.size() || definedNames.get(i) == null ? new ColumnIdentifier(DEFAULT_COLUMN_ALIAS + (i + 1), false) : new ColumnIdentifier(definedNames.get(i).name, definitionType); }', 'output': 'private static ColumnIdentifier getColumnId(CFMetaData cfm, int i) { List<String> definedNames = cfm.getColumnAliases(); return definedNames == null || i >= definedNames.size() || definedNames.get(i) == null ? new ColumnIdentifier(DEFAULT_COLUMN_ALIAS + (i + 1), false) : new ColumnIdentifier(definedNames.get(i), definitionType); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public double getLoad(ClusterNode node, int jobsSentSinceLastUpdate) { XXX metrics = node.metrics(); double k = 1.0d; if (useProcs) { int procs = metrics.getTotalCpus(); if (procs > 1) k = procs * procCoefficient; } double load = (useAvg ? metrics.getAverageCpuLoad() : metrics.getCurrentCpuLoad()) / k; return load < 0 ? 0 : load; } /** {@inheritDoc} */ @Override public String toString() { return S.toString(GridAdaptiveCpuLoadProbe.class, this); } } }', 'ground_truth': '@Override public double getLoad(ClusterNode node, int jobsSentSinceLastUpdate) { ClusterNodeMetrics metrics = node.metrics(); double k = 1.0d; if (useProcs) { int procs = metrics.getTotalCpus(); if (procs > 1) k = procs * procCoefficient; } double load = (useAvg ? metrics.getAverageCpuLoad() : metrics.getCurrentCpuLoad()) / k; return load < 0 ? 0 : load; }', 'output': '@Override public double getLoad(ClusterNode node, int jobsSentSinceLastUpdate) { GridMetrics metrics = node.metrics(); double k = 1.0d; if (useProcs) { int procs = metrics.getTotalCpus(); if (procs > 1) k = procs * procCoefficient; } double load = (useAvg ? metrics.getAverageCpuLoad() : metrics.getCurrentCpuLoad()) / k; return load < 0 ? 0 : load; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAtomicStampedConstantMultipleTopologyChange() throws Exception { try { GridCacheAtomicStamped<Integer, Integer> s = cache().dataStructures().atomicStamped(STRUCTURE_NAME, 1, 1, true); XXX fut = GridTestUtils.runMultiThreadedAsync(new CA() { @Override public void apply() { try { for (int i = 0; i < TOP_CHANGE_CNT; i++) { Collection<String> names = new GridLeanSet<>(3); try { for (int j = 0; j < 3; j++) { String name = UUID.randomUUID().toString(); names.add(name); Ignite g = startGrid(name); IgniteBiTuple<Integer, Integer> t = g.cache(null).dataStructures() .<Integer, Integer>atomicStamped(STRUCTURE_NAME, 1, 1, true).get(); assert t.get1() > 0; assert t.get2() > 0; } } finally { if (i != TOP_CHANGE_CNT - 1) for (String name : names) stopGrid(name); } } } catch (Exception e) { throw F.wrap(e); } } }, TOP_CHANGE_THREAD_CNT, "topology-change-thread"); int val = s.value(); while (!fut.isDone()) { IgniteBiTuple<Integer, Integer> t = s.get(); assert t.get1() == val; assert t.get2() == val; val++; s.set(val, val); } fut.get(); for (Ignite g : G.allGrids()) { IgniteBiTuple<Integer, Integer> t = g.cache(null).dataStructures() .<Integer, Integer>atomicStamped(STRUCTURE_NAME, 1, 1, true).get(); assert t.get1() == val; assert t.get2() == val; } } finally { cache().dataStructures().removeAtomicStamped(STRUCTURE_NAME); } }', 'ground_truth': 'public void testAtomicStampedConstantMultipleTopologyChange() throws Exception { try { GridCacheAtomicStamped<Integer, Integer> s = cache().dataStructures().atomicStamped(STRUCTURE_NAME, 1, 1, true); IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new CA() { @Override public void apply() { try { for (int i = 0; i < TOP_CHANGE_CNT; i++) { Collection<String> names = new GridLeanSet<>(3); try { for (int j = 0; j < 3; j++) { String name = UUID.randomUUID().toString(); names.add(name); Ignite g = startGrid(name); IgniteBiTuple<Integer, Integer> t = g.cache(null).dataStructures() .<Integer, Integer>atomicStamped(STRUCTURE_NAME, 1, 1, true).get(); assert t.get1() > 0; assert t.get2() > 0; } } finally { if (i != TOP_CHANGE_CNT - 1) for (String name : names) stopGrid(name); } } } catch (Exception e) { throw F.wrap(e); } } }, TOP_CHANGE_THREAD_CNT, "topology-change-thread"); int val = s.value(); while (!fut.isDone()) { IgniteBiTuple<Integer, Integer> t = s.get(); assert t.get1() == val; assert t.get2() == val; val++; s.set(val, val); } fut.get(); for (Ignite g : G.allGrids()) { IgniteBiTuple<Integer, Integer> t = g.cache(null).dataStructures() .<Integer, Integer>atomicStamped(STRUCTURE_NAME, 1, 1, true).get(); assert t.get1() == val; assert t.get2() == val; } } finally { cache().dataStructures().removeAtomicStamped(STRUCTURE_NAME); } }', 'output': 'public void testAtomicStampedMultipleTopologyChange() throws Exception { GridCacheAtomicStamped<Integer, Integer> s = cache().dataStructures().atomicStamped(STRUCTURE_NAME, 1, 1, true); IgniteFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new CA() { @Override public void apply() { try { for (int i = 0; i < TOP_CHANGE_CNT; i++) { Collection<String> names = new GridLeanSet<>(3); try { for (int j = 0; j < 3; j++) { String name = UUID.randomUUID().toString(); names.add(name); Ignite g = startGrid(name); IgniteBiTuple<Integer, Integer> t = g.cache(null).dataStructures() .<Integer, Integer>atomicStamped(STRUCTURE_NAME, 1, 1, true).get(); assert t.get1() > 0; assert t.get2() > 0; } } finally { if (i != TOP_CHANGE_CNT - 1) for (String name : names) stopGrid(name); } } } catch (Exception e) { throw F.wrap(e); } } }, TOP_CHANGE_THREAD_CNT, "topology-change-thread"); int val = s.value(); while (!fut.isDone()) { IgniteBiTuple<Integer, Integer> t = s.get(); assert t.get1() == val; assert t.get2() == val; val++; s.set(val, val); } fut.get(); for (Ignite g : G.allGrids()) { IgniteBiTuple<Integer, Integer> t = g.cache(null).dataStructures() .<Integer, Integer>atomicStamped(STRUCTURE_NAME, 1, 1, true).get(); assert t.get1() == val; assert t.get2() == val; } } finally { cache().dataStructures().removeAtomicStamped(STRUCTURE_NAME); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testApply() { Resource resource = new ResourceImpl(Resource.Type.HostComponent); XXX propertyId = new PropertyIdImpl("foo", "category1", false); Predicate predicate = new EqualsPredicate(propertyId, "bar"); resource.setProperty(propertyId, "monkey"); Assert.assertFalse(predicate.evaluate(resource)); resource.setProperty(propertyId, "bar"); Assert.assertTrue(predicate.evaluate(resource)); propertyId = new PropertyIdImpl("fun", "category1", false); predicate = new EqualsPredicate(propertyId, "bar"); Assert.assertFalse(predicate.evaluate(resource)); }', 'ground_truth': 'public void testApply() { Resource resource = new ResourceImpl(Resource.Type.HostComponent); String propertyId = PropertyHelper.getPropertyId("category1", "foo"); Predicate predicate = new EqualsPredicate<String>(propertyId, "bar"); resource.setProperty(propertyId, "monkey"); Assert.assertFalse(predicate.evaluate(resource)); resource.setProperty(propertyId, "bar"); Assert.assertTrue(predicate.evaluate(resource)); propertyId = PropertyHelper.getPropertyId("category1", "fun"); predicate = new EqualsPredicate<String>(propertyId, "bar"); Assert.assertFalse(predicate.evaluate(resource)); }', 'output': 'public void testApply() { Resource resource = new ResourceImpl(Resource.Type.HostComponent); PropertyId propertyId = new PropertyIdImpl("foo", "category1", false); Predicate predicate = new EqualsPredicate(propertyId, "bar"); resource.setProperty(propertyId, "monkey"); Assert.assertFalse(predicate.evaluate(resource)); resource.setProperty(propertyId, "bar"); Assert.assertTrue(predicate.evaluate(resource)); propertyId = new PropertyIdImpl("fun", "category1", false); predicate = new EqualsPredicate(propertyId, "bar"); Assert.assertFalse(predicate.evaluate(resource)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void searchAndTest() { XXX subtypes = reflections.getSubTypesOf(WithReferenceComparable.class); for (Class<? extends WithReferenceComparable> subtype : subtypes) { if(subtype.isInterface() || subtype.isAnonymousClass() || subtype.isLocalClass() || subtype.isMemberClass()) { continue; } subtype = instantiable(subtype); test(subtype); } }', 'ground_truth': 'public void searchAndTest() { for (Class<? extends WithReferenceComparable> subtype : candidates) { if(subtype.isInterface() || subtype.isAnonymousClass() || subtype.isLocalClass() || subtype.isMemberClass()) { continue; } subtype = instantiable(subtype); test(subtype); } }', 'output': 'public void searchAndTest() { List<Class<? extends WithReferenceComparable>> subtypes = reflections.getSubTypesOf(WithReferenceComparable.class); for (Class<? extends WithReferenceComparable> subtype : subtypes) { if(subtype.isInterface() || subtype.isAnonymousClass() || subtype.isLocalClass() || subtype.isMemberClass()) { continue; } subtype = instantiable(subtype); test(subtype); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testQuotes() throws Exception { XXX binding = new HashMap(); binding.put("Christian", "Stein"); XmlTemplateEngine xmlTemplateEngine = new XmlTemplateEngine(); String xmlScript = "<!-- Just a comment. -->\\n" + "<xml xmlns:mygsp=\\"http: + "  ${Christian + \\" \\" + Christian}" + "  <mygsp:expression>Christian + \\" \\" + Christian</mygsp:expression>" + "  <mygsp:scriptlet>println Christian</mygsp:scriptlet>" + "</xml>"; String xmlResult = "<xml>\\n" + "  Stein Stein\\n" + xmlTemplateEngine.getIndentation() + "Stein Stein\\n" + "Stein" + System.lineSeparator() + "</xml>\\n"; Template template = xmlTemplateEngine.createTemplate(xmlScript); assertEquals(xmlResult, template.make(binding).toString()); }', 'ground_truth': 'public void testQuotes() throws Exception { Map<String, String> binding = new HashMap(); binding.put("Christian", "Stein"); XmlTemplateEngine xmlTemplateEngine = new XmlTemplateEngine(); String xmlScript = "<!-- Just a comment. -->\\n" + "<xml xmlns:mygsp=\\"http: + "  ${Christian + \\" \\" + Christian}" + "  <mygsp:expression>Christian + \\" \\" + Christian</mygsp:expression>" + "  <mygsp:scriptlet>println Christian</mygsp:scriptlet>" + "</xml>"; String xmlResult = "<xml>\\n" + "  Stein Stein\\n" + xmlTemplateEngine.getIndentation() + "Stein Stein\\n" + "Stein" + System.lineSeparator() + "</xml>\\n"; Template template = xmlTemplateEngine.createTemplate(xmlScript); assertEquals(xmlResult, template.make(binding).toString()); }', 'output': 'public void testQuotes() throws Exception { Map<String, String> binding = new HashMap<>(); binding.put("Christian", "Stein"); XmlTemplateEngine xmlTemplateEngine = new XmlTemplateEngine(); String xmlScript = "<!-- Just a comment. -->\\n" + "<xml xmlns:mygsp=\\"http://www.springframework.org/schema/gsp\\">\\n" + "  <mygsp:expression>Christian + \\" \\" + Christian</mygsp:expression>" + "  <mygsp:scriptlet>println Christian</mygsp:scriptlet>" + "</xml>"; String xmlResult = "<xml>\\n" + "  Stein Stein\\n" + xmlTemplateEngine.getIndentation() + "Stein Stein\\n" + "Stein" + System.lineSeparator() + "</xml>\\n"; Template template = xmlTemplateEngine.createTemplate(xmlScript); assertEquals(xmlResult, template.make(binding).toString()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testBuildXmlSinkDirect() { XXX sink = XmlFileBasedSink.writeOf(Bird.class, testRootElement, testFilePrefix); assertEquals(testClass, sink.classToBind); assertEquals(testRootElement, sink.rootElementName); assertEquals(testFilePrefix, sink.baseOutputFilename); }', 'ground_truth': 'public void testBuildXmlSinkDirect() { XmlSink.Bound<Bird> sink = XmlSink.writeOf(Bird.class, testRootElement, testFilePrefix); assertEquals(testClass, sink.classToBind); assertEquals(testRootElement, sink.rootElementName); assertEquals(testFilePrefix, sink.baseOutputFilename); }', 'output': 'public void testBuildXmlSinkDirect() { Sink<?> sink = XmlFileBasedSink.writeOf(Bird.class, testRootElement, testFilePrefix); assertEquals(testClass, sink.classToBind); assertEquals(testRootElement, sink.rootElementName); assertEquals(testFilePrefix, sink.baseOutputFilename); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private ColumnFilter gatherQueriedColumns() { if (selection.isWildcard()) return ColumnFilter.all(cfm); ColumnFilter.Builder builder = ColumnFilter.allRegularColumnsBuilder(cfm); for (XXX def : selection.getColumns()) if (!def.isPrimaryKeyColumn()) builder.add(def); builder.addAll(restrictions.nonPKRestrictedColumns(true)); if (cfm.hasStaticColumns() && returnStaticContentOnPartitionWithNoRows()) builder.addAll(cfm.partitionColumns().statics); return builder.build(); }', 'ground_truth': 'private ColumnFilter gatherQueriedColumns() { if (selection.isWildcard()) return ColumnFilter.all(table); ColumnFilter.Builder builder = ColumnFilter.allRegularColumnsBuilder(table); for (ColumnMetadata def : selection.getColumns()) if (!def.isPrimaryKeyColumn()) builder.add(def); builder.addAll(restrictions.nonPKRestrictedColumns(true)); if (table.hasStaticColumns() && returnStaticContentOnPartitionWithNoRows()) builder.addAll(table.staticColumns()); return builder.build(); }', 'output': 'private ColumnFilter gatherQueriedColumns() { if (selection.isWildcard()) return ColumnFilter.all(cfm); ColumnFilter.Builder builder = ColumnFilter.allRegularColumnsBuilder(cfm); for (ColumnDef def : selection.getColumns()) if (!def.isPrimaryKeyColumn()) builder.add(def); builder.addAll(restrictions.nonPKRestrictedColumns(true)); if (cfm.hasStaticColumns() && returnStaticContentOnPartitionWithNoRows()) builder.addAll(cfm.partitionColumns().statics); return builder.build(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void receiveTiming(InetAddress host, long latency) { ExponentiallyDecayingSample sample = samples.get(host); if (sample == null) { XXX maybeNewSample = new ExponentiallyDecayingSample(WINDOW_SIZE, ALPHA); sample = samples.putIfAbsent(host, maybeNewSample); if (sample == null) sample = maybeNewSample; } sample.update(latency); }', 'ground_truth': 'public void receiveTiming(InetAddress host, long latency) { ExponentiallyDecayingReservoir sample = samples.get(host); if (sample == null) { ExponentiallyDecayingReservoir maybeNewSample = new ExponentiallyDecayingReservoir(WINDOW_SIZE, ALPHA); sample = samples.putIfAbsent(host, maybeNewSample); if (sample == null) sample = maybeNewSample; } sample.update(latency); }', 'output': 'public void receiveTiming(InetAddress host, long latency) { DecayingSample sample = samples.get(host); if (sample == null) { DecayingSample maybeNewSample = new DecayingSample(WINDOW_SIZE, ALPHA); sample = samples.putIfAbsent(host, maybeNewSample); if (sample == null) sample = maybeNewSample; } sample.update(latency); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void doCall() throws  StatusException  { if(!isRunning()) { log.debug("ReplicateAndRunCommands is paused"); return; } List<? extends CommandModel> commandsToReplay; while(isRunning()) { commandsToReplay = commandModelRepository.findNotYetReplayed(); if(commandsToReplay.isEmpty()) { XXX hwm = commandModelRepository.findMostRecentReplayed().orElse(null); if (hwm != null) { if (hwm.getReplayState() != null && hwm.getReplayState().isFailed()) { log.info("Command {} hit replay error", hwm.getInteractionId()); return; } } else { hwm = commandModelRepository.findMostRecentCompleted().orElse(null); } val commandDtos = commandFetcher.fetchCommand(hwm); commandsToReplay = commandDtos.stream() .map(dto -> transactionService.callWithinCurrentTransactionElseCreateNew( () -> commandModelRepository.saveForReplay(dto)) .ifFailureFail() .getValue().orElse(null) ) .collect(Collectors.toList()); if(commandsToReplay.isEmpty()) { return; } } replay(commandsToReplay); } }', 'ground_truth': 'private void doCall() throws  StatusException  { if(!isRunning()) { log.debug("ReplicateAndRunCommands is paused"); return; } List<CommandLog> commandsToReplay; while(isRunning()) { commandsToReplay = commandLogRepository.findNotYetReplayed(); if(commandsToReplay.isEmpty()) { CommandLog hwm = commandLogRepository.findMostRecentReplayed().orElse(null); if (hwm != null) { if (hwm.getReplayState() != null && hwm.getReplayState().isFailed()) { log.info("Command {} hit replay error", hwm.getInteractionId()); return; } } else { hwm = commandLogRepository.findMostRecentCompleted().orElse(null); } val commandDtos = commandFetcher.fetchCommand(hwm); commandsToReplay = commandDtos.stream() .map(dto -> transactionService.callWithinCurrentTransactionElseCreateNew( () -> commandLogRepository.saveForReplay(dto)) .ifFailureFail() .getValue().orElse(null) ) .collect(Collectors.toList()); if(commandsToReplay.isEmpty()) { return; } } replay(commandsToReplay); } }', 'output': 'private void doCall() throws  StatusException  { if(!isRunning()) { log.debug("ReplicateAndRunCommands is paused"); return; } List<? extends CommandModel> commandsToReplay; while(isRunning()) { commandsToReplay = commandModelRepository.findNotYetReplayed(); if(commandsToReplay.isEmpty()) { CommandModel hwm = commandModelRepository.findMostRecentReplayed().orElse(null); if (hwm != null) { if (hwm.getReplayState() != null && hwm.getReplayState().isFailed()) { log.info("Command {} hit replay error", hwm.getInteractionId()); return; } } else { hwm = commandModelRepository.findMostRecentCompleted().orElse(null); } val commandDtos = commandFetcher.fetchCommand(hwm); commandsToReplay = commandDtos.stream() .map(dto -> transactionService.callWithinCurrentTransactionElseCreateNew( () -> commandModelRepository.saveForReplay(dto)) .ifFailureFail() .getValue().orElse(null) ) .collect(Collectors.toList()); if(commandsToReplay.isEmpty()) { return; } } replay(commandsToReplay); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void run() { AnticompactionRequest acr = new AnticompactionRequest(parentSession, successfulRanges); XXX peerVersion = SystemKeyspace.getReleaseVersion(neighbor); if (peerVersion != null && peerVersion.compareTo(VERSION_CHECKER) > 0) { MessagingService.instance().sendRR(acr.createMessage(), neighbor, new AnticompactionCallback(this), TimeUnit.DAYS.toMillis(1), true); } else { MessagingService.instance().sendOneWay(acr.createMessage(), neighbor); set(neighbor); } }', 'ground_truth': 'public void run() { AnticompactionRequest acr = new AnticompactionRequest(parentSession, successfulRanges); CassandraVersion peerVersion = SystemKeyspace.getReleaseVersion(neighbor); if (peerVersion != null && peerVersion.compareTo(VERSION_CHECKER) > 0) { MessagingService.instance().sendRR(acr.createMessage(), neighbor, new AnticompactionCallback(this), TimeUnit.DAYS.toMillis(1), true); } else { MessagingService.instance().sendOneWay(acr.createMessage(), neighbor); set(neighbor); } }', 'output': 'public void run() { AnticompactionRequest acr = new AnticompactionRequest(parentSession, successfulRanges); String peerVersion = SystemKeyspace.getReleaseVersion(neighbor); if (peerVersion != null && peerVersion.compareTo(VERSION_CHECKER) > 0) { MessagingService.instance().sendRR(acr.createMessage(), neighbor, new AnticompactionCallback(this), TimeUnit.DAYS.toMillis(1), true); } else { MessagingService.instance().sendOneWay(acr.createMessage(), neighbor); set(neighbor); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testBinding() throws SyntaxException, ClassNotFoundException, IOException { XXX binding = new Binding(); binding.setVariable("sam", "pullara"); Template template = new SimpleTemplateEngine().createTemplate("<%= sam %>"); template.setBinding(binding.getVariables()); assertEquals("pullara", template.toString()); }', 'ground_truth': 'public void testBinding() throws SyntaxException, ClassNotFoundException, IOException { Map binding = new HashMap(); binding.put("sam", "pullara"); Template template = new SimpleTemplateEngine().createTemplate("<%= sam %>"); template.setBinding(binding.getVariables()); assertEquals("pullara", template.toString()); }', 'output': 'public void testBinding() throws SyntaxException, ClassNotFoundException, IOException { Map<String, Object> binding = new HashMap<>(); binding.put("sam", "pullara"); Template template = new SimpleTemplateEngine().createTemplate("<%= sam %>"); template.setBinding(binding); assertEquals("pullara", template.toString()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testDaemonNodeJoin() throws Exception { try { startGridsMultiThreaded(3); final AtomicReference<GridException> err = new AtomicReference<>(); for (int i = 0; i < 3; i++) { Ignite g = grid(i); g.events().localListen(new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent evt) { GridDiscoveryEvent discoEvt = (GridDiscoveryEvent) evt; if (discoEvt.topologyNodes().size() != 3) err.compareAndSet(null, new GridException("Invalid discovery event [evt=" + discoEvt + ", nodes=" + discoEvt.topologyNodes() + \']\')); return true; } }, IgniteEventType.EVT_NODE_JOINED); } daemon = true; GridKernal daemon = (GridKernal)startGrid(3); XXX join = daemon.context().discovery().localJoinEvent(); assertEquals(3, join.topologyNodes().size()); U.sleep(100); if (err.get() != null) throw err.get(); } finally { stopAllGrids(); } }', 'ground_truth': 'public void testDaemonNodeJoin() throws Exception { try { startGridsMultiThreaded(3); final AtomicReference<GridException> err = new AtomicReference<>(); for (int i = 0; i < 3; i++) { Ignite g = grid(i); g.events().localListen(new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent evt) { IgniteDiscoveryEvent discoEvt = (IgniteDiscoveryEvent) evt; if (discoEvt.topologyNodes().size() != 3) err.compareAndSet(null, new GridException("Invalid discovery event [evt=" + discoEvt + ", nodes=" + discoEvt.topologyNodes() + \']\')); return true; } }, IgniteEventType.EVT_NODE_JOINED); } daemon = true; GridKernal daemon = (GridKernal)startGrid(3); IgniteDiscoveryEvent join = daemon.context().discovery().localJoinEvent(); assertEquals(3, join.topologyNodes().size()); U.sleep(100); if (err.get() != null) throw err.get(); } finally { stopAllGrids(); } }', 'output': 'public void testDaemonNodeJoin() throws Exception { try { startGridsMultiThreaded(3); final AtomicReference<GridException> err = new AtomicReference<>(); for (int i = 0; i < 3; i++) { Ignite g = grid(i); g.events().localListen(new IgnitePredicate<IgniteEvent>() { @Override public boolean apply(IgniteEvent evt) { GridDiscoveryEvent discoEvt = (GridDiscoveryEvent) evt; if (discoEvt.topologyNodes().size() != 3) err.compareAndSet(null, new GridException("Invalid discovery event [evt=" + discoEvt + ", nodes=" + discoEvt.topologyNodes() + \']\')); return true; } }, IgniteEventType.EVT_NODE_JOINED); } daemon = true; GridKernal daemon = (GridKernal)startGrid(3); IgniteInternalFuture<?> join = daemon.context().discovery().localJoinEvent(); assertEquals(3, join.get().topologyNodes().size()); U.sleep(100); if (err.get() != null) throw err.get(); } finally { stopAllGrids(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCreationOfPipelineOptions() throws Exception { ObjectMapper mapper = new ObjectMapper(); String stringOptions = mapper.writeValueAsString(new String[]{ "--runner=DataflowPipelineRunner", "--project=testProject", "--apiRootUrl=testApiRootUrl", "--dataflowEndpoint=testDataflowEndpoint", "--tempLocation=testTempLocation", "--serviceAccountName=testServiceAccountName", "--serviceAccountKeyfile=testServiceAccountKeyfile", "--zone=testZone", "--numWorkers=1", "--diskSizeGb=2" }); System.getProperties().put("dataflowOptions", stringOptions); XXX options = TestPipeline.getPipelineOptions(); assertEquals(DataflowPipelineRunner.class, options.getRunner()); assertThat(options.getJobName(), startsWith("testpipelinetest0testcreationofpipelineoptions-")); assertEquals("testProject", options.getProject()); assertEquals("testApiRootUrl", options.getApiRootUrl()); assertEquals("testDataflowEndpoint", options.getDataflowEndpoint()); assertEquals("testTempLocation", options.getTempLocation()); assertEquals("testServiceAccountName", options.getServiceAccountName()); assertEquals("testServiceAccountKeyfile", options.getServiceAccountKeyfile()); assertEquals("testZone", options.getZone()); assertEquals(2, options.getDiskSizeGb()); } public void testCreationOfPipelineOptionsFromReallyVerboselyNamedTestCase() throws Exception { ObjectMapper mapper = new ObjectMapper(); String stringOptions = mapper.writeValueAsString(new String[]{}); System.getProperties().put("dataflowOptions", stringOptions); XXX options = TestPipeline.getPipelineOptions(); assertThat(options.getAppName(), startsWith( "TestPipelineTest-testCreationOfPipelineOptionsFromReallyVerboselyNamedTestCase")); assertThat(options.getJobName(), startsWith( "testpipelinetest0testcreationofpipelineoptionsfrom")); }', 'ground_truth': 'public void testCreationOfPipelineOptions() throws Exception { ObjectMapper mapper = new ObjectMapper(); String stringOptions = mapper.writeValueAsString(new String[]{ "--runner=DataflowPipelineRunner", "--project=testProject", "--apiRootUrl=testApiRootUrl", "--dataflowEndpoint=testDataflowEndpoint", "--tempLocation=testTempLocation", "--serviceAccountName=testServiceAccountName", "--serviceAccountKeyfile=testServiceAccountKeyfile", "--zone=testZone", "--numWorkers=1", "--diskSizeGb=2" }); System.getProperties().put("dataflowOptions", stringOptions); DataflowPipelineOptions options = TestPipeline.testingPipelineOptions().as(DataflowPipelineOptions.class); assertEquals(DataflowPipelineRunner.class, options.getRunner()); assertThat(options.getJobName(), startsWith("testpipelinetest0testcreationofpipelineoptions-")); assertEquals("testProject", options.as(GcpOptions.class).getProject()); assertEquals("testApiRootUrl", options.getApiRootUrl()); assertEquals("testDataflowEndpoint", options.getDataflowEndpoint()); assertEquals("testTempLocation", options.getTempLocation()); assertEquals("testServiceAccountName", options.getServiceAccountName()); assertEquals( "testServiceAccountKeyfile", options.as(GcpOptions.class).getServiceAccountKeyfile()); assertEquals("testZone", options.getZone()); assertEquals(2, options.getDiskSizeGb()); }', 'output': 'public void testCreationOfPipelineOptions() throws Exception { ObjectMapper mapper = new ObjectMapper(); String stringOptions = mapper.writeValueAsString(new String[]{ "--runner=DataflowPipelineRunner", "--project=testProject", "--apiRootUrl=testApiRootUrl", "--dataflowEndpoint=testDataflowEndpoint", "--tempLocation=testTempLocation", "--serviceAccountName=testServiceAccountName", "--serviceAccountKeyfile=testServiceAccountKeyfile", "--zone=testZone", "--numWorkers=1", "--diskSizeGb=2" }); System.getProperties().put("dataflowOptions", stringOptions); PipelineOptions options = TestPipeline.getPipelineOptions(); assertEquals(DataflowPipelineRunner.class, options.getRunner()); assertThat(options.getJobName(), startsWith("testpipelinetest0testcreationofpipelineoptions-")); assertEquals("testProject", options.getProject()); assertEquals("testApiRootUrl", options.getApiRootUrl()); assertEquals("testDataflowEndpoint", options.getDataflowEndpoint()); assertEquals("testTempLocation", options.getTempLocation()); assertEquals("testServiceAccountName", options.getServiceAccountName()); assertEquals("testServiceAccountKeyfile", options.getServiceAccountKeyfile()); assertEquals("testZone", options.getZone()); assertEquals(2, options.getDiskSizeGb()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void receiveMessage(IMessage message) { sync(() -> { try (DataInputBuffer in = new DataInputBuffer(message.bytes())) { XXX messageIn = MessageInHandler.deserialize(in, message.id(), message.version(), message.from()); Runnable deliver = new MessageDeliveryTask(messageIn, message.id()); deliver.run(); } catch (Throwable t) { throw new RuntimeException("Exception occurred on node " + broadcastAddressAndPort(), t); } }).run(); }', 'ground_truth': 'public void receiveMessage(IMessage message) { sync(() -> { try (DataInputBuffer in = new DataInputBuffer(message.bytes())) { Message<?> messageIn = Message.serializer.deserialize(in, message.from(), message.version()); messageIn.verb().handler().doVerb((Message<Object>) messageIn); } catch (Throwable t) { throw new RuntimeException("Exception occurred on node " + broadcastAddressAndPort(), t); } }).run(); }', 'output': 'public void receiveMessage(IMessage message) { sync(() -> { try (DataInputBuffer in = new DataInputBuffer(message.bytes())) { MessageIn messageIn = MessageInHandler.deserialize(in, message.id(), message.version(), message.from()); Runnable deliver = new MessageDeliveryTask(messageIn, message.id()); deliver.run(); } catch (Throwable t) { throw new RuntimeException("Exception occurred on node " + broadcastAddressAndPort(), t); } }).run(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void finalChecks() throws Exception { for (int i = 1; i <= maxKeyValue(); i++) { for (int k = 0; k < 3; k++) { try { GridCacheEntry<Integer, String> e1 = null; String v1 = null; for (int j = 0; j < gridCount(); j++) { GridCache<Integer, String> cache = cache(j); IgniteTx tx = cache.tx(); assertNull("Transaction is not completed: " + tx, tx); if (j == 0) { e1 = cache.entry(i); v1 = e1.get(); } else { GridCacheEntry<Integer, String> e2 = cache.entry(i); String v2 = e2.get(); if (!F.eq(v2, v1)) { v1 = e1.get(); v2 = e2.get(); } assert F.eq(v2, v1) : "Invalid cached value [key=" + i + ", v1=" + v1 + ", v2=" + v2 + ", e1=" + e1 + ", e2=" + e2 + ", grid=" + j + \']\'; } } break; } catch (AssertionError e) { if (k == 2) throw e; else Thread.sleep(500); } } } for (int i = 1; i <= maxKeyValue(); i++) { for (int k = 0; k < 3; k++) { try { for (int j = 0; j < gridCount(); j++) { GridCacheProjection<Integer, String> cache = cache(j); cache.removeAll(); } break; } catch (AssertionError e) { if (k == 2) throw e; else Thread.sleep(500); } } } }', 'ground_truth': 'protected void finalChecks() throws Exception { for (int i = 1; i <= maxKeyValue(); i++) { for (int k = 0; k < 3; k++) { try { CacheEntry<Integer, String> e1 = null; String v1 = null; for (int j = 0; j < gridCount(); j++) { Cache<Integer, String> cache = cache(j); IgniteTx tx = cache.tx(); assertNull("Transaction is not completed: " + tx, tx); if (j == 0) { e1 = cache.entry(i); v1 = e1.get(); } else { CacheEntry<Integer, String> e2 = cache.entry(i); String v2 = e2.get(); if (!F.eq(v2, v1)) { v1 = e1.get(); v2 = e2.get(); } assert F.eq(v2, v1) : "Invalid cached value [key=" + i + ", v1=" + v1 + ", v2=" + v2 + ", e1=" + e1 + ", e2=" + e2 + ", grid=" + j + \']\'; } } break; } catch (AssertionError e) { if (k == 2) throw e; else Thread.sleep(500); } } } for (int i = 1; i <= maxKeyValue(); i++) { for (int k = 0; k < 3; k++) { try { for (int j = 0; j < gridCount(); j++) { CacheProjection<Integer, String> cache = cache(j); cache.removeAll(); } break; } catch (AssertionError e) { if (k == 2) throw e; else Thread.sleep(500); } } } }', 'output': 'protected void finalChecks() throws Exception { for (int i = 1; i <= maxKeyValue(); i++) { for (int k = 0; k < 3; k++) { try { GridCacheEntry<Integer, String> e1 = null; String v1 = null; for (int j = 0; j < gridCount(); j++) { GridCache<Integer, String> cache = cache(j); IgniteTx tx = cache.tx(); assertNull("Transaction is not completed: " + tx, tx); if (j == 0) { e1 = cache.entry(i); v1 = e1.get(); } else { GridCacheEntry<Integer, String> e2 = cache.entry(i); String v2 = e2.get(); if (!F.eq(v2, v1)) { v1 = e1.get(); v2 = e2.get(); } assert F.eq(v2, v1) : "Invalid cached value [key=" + i + ", v1=" + v1 + ", v2=" + v2 + ", e1=" + e1 + ", e2=" + e2 + ", grid=" + j + \']\'; } } break; } catch (AssertionError e) { if (k == 2) throw e; else Thread.sleep(500); } } } for (int i = 1; i <= maxKeyValue(); i++) { for (int k = 0; k < 3; k++) { try { for (int j = 0; j < gridCount(); j++) { GridCacheProjection<Integer, String> cache = cache(j); cache.removeAll(); } break; } catch (AssertionError e) { if (k == 2) throw e; else Thread.sleep(500); } } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void fireTableChanged(DatabaseConnection dbconn, String tableName) { for (XXX i = listeners.allInstances().iterator(); i.hasNext();) { ((DbMetaDataListener)i.next()).tableChanged(dbconn, tableName); } }', 'ground_truth': 'public static void fireTableChanged(DatabaseConnection dbconn, String tableName) { for (Iterator<DbMetaDataListener> i = listeners.allInstances().iterator(); i.hasNext();) { i.next().tableChanged(dbconn, tableName); } }', 'output': 'public static void fireTableChanged(DatabaseConnection dbconn, String tableName) { for (DbMetaDataListener listener : listeners) { listener.tableChanged(dbconn, tableName); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testEnumArrayModification() { TestObjectAllTypes obj = new TestObjectAllTypes(); obj.enumArr = new TestObjectEnum[] {TestObjectEnum.A, TestObjectEnum.B}; XXX mutObj = wrap(obj); PortableBuilderEnum[] arr = mutObj.getField("enumArr"); arr[0] = new PortableBuilderEnum(mutObj.typeId(), TestObjectEnum.B); TestObjectAllTypes res = mutObj.build().deserialize(); Assert.assertArrayEquals(new TestObjectEnum[] {TestObjectEnum.A, TestObjectEnum.B}, res.enumArr); }', 'ground_truth': 'public void testEnumArrayModification() { TestObjectAllTypes obj = new TestObjectAllTypes(); obj.enumArr = new TestObjectEnum[] {TestObjectEnum.A, TestObjectEnum.B}; BinaryObjectBuilderImpl mutObj = wrap(obj); PortableBuilderEnum[] arr = mutObj.getField("enumArr"); arr[0] = new PortableBuilderEnum(mutObj.typeId(), TestObjectEnum.B); TestObjectAllTypes res = mutObj.build().deserialize(); Assert.assertArrayEquals(new TestObjectEnum[] {TestObjectEnum.A, TestObjectEnum.B}, res.enumArr); }', 'output': 'public void testEnumArrayModification() { TestObjectAllTypes obj = new TestObjectAllTypes(); obj.enumArr = new TestObjectEnum[] {TestObjectEnum.A, TestObjectEnum.B}; BinaryInputArchive mutObj = wrap(obj); PortableBuilderEnum[] arr = mutObj.getField("enumArr"); arr[0] = new PortableBuilderEnum(mutObj.typeId(), TestObjectEnum.B); TestObjectAllTypes res = mutObj.build().deserialize(); Assert.assertArrayEquals(new TestObjectEnum[] {TestObjectEnum.A, TestObjectEnum.B}, res.enumArr); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private List getSorted(int[] index, List unsorted) { XXX sorted = new ArrayList(unsorted.size()); int start = 0; for (int i=0; i<unsorted.size(); i++) { int min = -1; for (int j=0; j<unsorted.size(); j++) { if (index[j]==-1) continue; if (min==-1) { min = j; } else if (index[j]<index[min]) { min = j; } } if (min==-1) break; sorted.add(unsorted.get(min)); index[min] = -1; } return sorted; }', 'ground_truth': 'private List getSorted(int[] index, List unsorted) { List sorted = new ArrayList(unsorted.size()); int start = 0; for (int i=0; i<unsorted.size(); i++) { int min = -1; for (int j=0; j<unsorted.size(); j++) { if (index[j]==-1) continue; if (min==-1) { min = j; } else if (index[j]<index[min]) { min = j; } } if (min==-1) break; sorted.add(unsorted.get(min)); index[min] = -1; } return sorted; }', 'output': 'private List getSorted(int[] index, List unsorted) { List sorted = new ArrayList(unsorted.size()); int start = 0; for (int i=0; i<unsorted.size(); i++) { int min = -1; for (int j=0; j<unsorted.size(); j++) { if (index[j]==-1) continue; if (min==-1) { min = j; } else if (index[j]<index[min]) { min = j; } } if (min==-1) break; sorted.add(unsorted.get(min)); index[min] = -1; } return sorted; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private long getFreeSpaceJ(String s) { try { setEnvironmentVariable( "LANG", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_COLLATE", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_CTYPE", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_MESSAGES", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_MONETARY", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_NUMERIC", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_TIME", "C", EnvironmentScope.PROCESS, false); } catch (NativeException e) { LogManager.log(e); } try { final String stdout = SystemUtils.executeCommand(getDfCommand(s)).getStdOut().trim(); final String[] lines = StringUtils.splitByLines(stdout); for (int i = 1; i < lines.length; i++) { int index = lines[i].indexOf("%"); if (index != -1) { String parts[] = lines[i].substring(0, index).split("[ ]+"); if (parts.length > 1) { return new Long(parts[parts.length - 2]).longValue() * 1024L; } } } } catch (XXX e) { LogManager.log(e); } catch (NumberFormatException e) { LogManager.log(e); } return 0L; }', 'ground_truth': 'private long getFreeSpaceJ(String s) { try { setEnvironmentVariable( "LANG", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_COLLATE", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_CTYPE", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_MESSAGES", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_MONETARY", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_NUMERIC", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_TIME", "C", EnvironmentScope.PROCESS, false); } catch (NativeException e) { LogManager.log(e); } try { final String stdout = SystemUtils.executeCommand(getDfCommand(s)).getStdOut().trim(); final String[] lines = StringUtils.splitByLines(stdout); for (int i = 1; i < lines.length; i++) { int index = lines[i].indexOf("%"); if (index != -1) { String parts[] = lines[i].substring(0, index).split("[ ]+"); if (parts.length > 1) { return Long.parseLong(parts[parts.length - 2]) * 1024L; } } } } catch (IOException | NumberFormatException e) { LogManager.log(e); } return 0L; }', 'output': 'private long getFreeSpaceJ(String s) { try { setEnvironmentVariable( "LANG", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_COLLATE", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_CTYPE", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_MESSAGES", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_MONETARY", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_NUMERIC", "C", EnvironmentScope.PROCESS, false); setEnvironmentVariable( "LC_TIME", "C", EnvironmentScope.PROCESS, false); } catch (NativeException e) { LogManager.log(e); } try { final String stdout = SystemUtils.executeCommand(getDfCommand(s)).getStdOut().trim(); final String[] lines = StringUtils.splitByLines(stdout); for (int i = 1; i < lines.length; i++) { int index = lines[i].indexOf("%"); if (index != -1) { String parts[] = lines[i].substring(0, index).split("[ ]+"); if (parts.length > 1) { return new Long(parts[parts.length - 2]).longValue() * 1024L; } } } } catch (IOException e) { LogManager.log(e); } catch (NumberFormatException e) { LogManager.log(e); } return 0L; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void send_get_column(String keyspace, String key, ColumnPath column_path, int consistency_level) throws TException { oprot_.writeMessageBegin(new TMessage("get_column", TMessageType.CALL, seqid_)); XXX args = new get_column_args(); args.keyspace = keyspace; args.key = key; args.column_path = column_path; args.consistency_level = consistency_level; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); } public void send_get_super_column(String keyspace, String key, SuperColumnPath super_column_path, int consistency_level) throws TException\n    { { oprot_.writeMessageBegin(new TMessage("get_super_column", TMessageType.CALL, seqid_)); get_super_column_args args = new get_super_column_args(); args.keyspace = keyspace; args.key = key; args.super_column_path = super_column_path; args.consistency_level = consistency_level; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'ground_truth': 'public void send_get(String keyspace, String key, ColumnPath column_path, int consistency_level) throws TException { oprot_.writeMessageBegin(new TMessage("get", TMessageType.CALL, seqid_)); get_args args = new get_args(); args.keyspace = keyspace; args.key = key; args.column_path = column_path; args.consistency_level = consistency_level; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'output': 'public void send_get_column(String keyspace, String key, ColumnPath column_path, int consistency_level) throws TException { oprot_.writeMessageBegin(new TMessage("get_column", TMessageType.CALL, seqid_)); get_column_args args = new get_column_args(); args.keyspace = keyspace; args.key = key; args.column_path = column_path; args.consistency_level = consistency_level; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void loadTestIgnite(final CIX1<Ignite> writeClos, final CIX1<Ignite> readClos) { info("Read threads: " + readThreads()); info("Write threads: " + writeThreads()); info("Test duration (ms): " + testDuration); final Ignite ignite = G.ignite(); final GridCache<Integer, Integer> cache = ignite.cache(null); assert cache != null; try { IgniteFuture<?> f1 = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() { @Nullable @Override public Object call() throws Exception { long start = System.currentTimeMillis(); while (!done.get()) { if (tx) { try (IgniteTx tx = cache.txStart()) { writeClos.apply(ignite); tx.commit(); } } else writeClos.apply(ignite); } writeTime.addAndGet(System.currentTimeMillis() - start); return null; } }, writeThreads(), "cache-load-test-worker"); XXX f2 = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() { @Nullable @Override public Object call() throws Exception { long start = System.currentTimeMillis(); while(!done.get()) { if (tx) { try (IgniteTx tx = cache.txStart()) { readClos.apply(ignite); tx.commit(); } } else readClos.apply(ignite); } readTime.addAndGet(System.currentTimeMillis() - start); return null; } }, readThreads(), "cache-load-test-worker"); Thread.sleep(testDuration); done.set(true); f1.get(); f2.get(); info("Test stats: "); info("    total-threads = " + threads); info("    write-ratio = " + writeRatio); info("    total-runs = " + (reads.get() + writes.get())); info("    total-reads = " + reads); info("    total-writes = " + writes); info("    read-time (ms) = " + readTime); info("    write-time (ms) = " + writeTime); info("    avg-read-time (ms) = " + ((double)readTime.get() / reads.get())); info("    avg-write-time (ms) = " + ((double)writeTime.get() / writes.get())); } catch (Exception e) { throw new RuntimeException(e); } }', 'ground_truth': 'protected void loadTestIgnite(final CIX1<Ignite> writeClos, final CIX1<Ignite> readClos) { info("Read threads: " + readThreads()); info("Write threads: " + writeThreads()); info("Test duration (ms): " + testDuration); final Ignite ignite = G.ignite(); final GridCache<Integer, Integer> cache = ignite.cache(null); assert cache != null; try { IgniteInternalFuture<?> f1 = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() { @Nullable @Override public Object call() throws Exception { long start = System.currentTimeMillis(); while (!done.get()) { if (tx) { try (IgniteTx tx = cache.txStart()) { writeClos.apply(ignite); tx.commit(); } } else writeClos.apply(ignite); } writeTime.addAndGet(System.currentTimeMillis() - start); return null; } }, writeThreads(), "cache-load-test-worker"); IgniteInternalFuture<?> f2 = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() { @Nullable @Override public Object call() throws Exception { long start = System.currentTimeMillis(); while(!done.get()) { if (tx) { try (IgniteTx tx = cache.txStart()) { readClos.apply(ignite); tx.commit(); } } else readClos.apply(ignite); } readTime.addAndGet(System.currentTimeMillis() - start); return null; } }, readThreads(), "cache-load-test-worker"); Thread.sleep(testDuration); done.set(true); f1.get(); f2.get(); info("Test stats: "); info("    total-threads = " + threads); info("    write-ratio = " + writeRatio); info("    total-runs = " + (reads.get() + writes.get())); info("    total-reads = " + reads); info("    total-writes = " + writes); info("    read-time (ms) = " + readTime); info("    write-time (ms) = " + writeTime); info("    avg-read-time (ms) = " + ((double)readTime.get() / reads.get())); info("    avg-write-time (ms) = " + ((double)writeTime.get() / writes.get())); } catch (Exception e) { throw new RuntimeException(e); } }', 'output': 'protected void loadTestIgnite(final CIX1<Ignite> writeClos, final CIX1<Ignite> readClos) { info("Read threads: " + readThreads()); info("Write threads: " + writeThreads()); info("Test duration (ms): " + testDuration); final Ignite ignite = G.ignite(); final GridCache<Integer, Integer> cache = ignite.cache(null); assert cache != null; try { IgniteFuture<?> f1 = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() { @Nullable @Override public Object call() throws Exception { long start = System.currentTimeMillis(); while (!done.get()) { if (tx) { try (IgniteTx tx = cache.txStart()) { writeClos.apply(ignite); tx.commit(); } } else writeClos.apply(ignite); } writeTime.addAndGet(System.currentTimeMillis() - start); return null; } }, writeThreads(), "cache-load-test-worker"); IgniteFuture<?> f2 = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() { @Nullable @Override public Object call() throws Exception { long start = System.currentTimeMillis(); while (!done.get()) { if (tx) { try (IgniteTx tx = cache.txStart()) { readClos.apply(ignite); tx.commit(); } } else readClos.apply(ignite); } readTime.addAndGet(System.currentTimeMillis() - start); return null; } }, readThreads(), "cache-load-test-worker"); Thread.sleep(testDuration); done.set(true); f1.get(); f2.get(); info("Test stats: "); info("    total-threads = " + threads); info("    write-ratio = " + writeRatio); info("    total-runs = " + (reads.get() + writes.get())); info("    total-reads = " + reads); info("    total-writes = " + writes); info("    read-time (ms) = " + readTime); info("    write-time (ms) = " + writeTime); info("    avg-read-time (ms) = "'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSameConfiguration() throws Exception { String name = "dupService"; IgniteManaged svcs1 = randomGrid().managed().withAsync(); XXX svcs2 = randomGrid().managed().withAsync(); svcs1.deployClusterSingleton(name, new DummyService()); IgniteFuture<?> fut1 = svcs1.future(); svcs2.deployClusterSingleton(name, new DummyService()); IgniteFuture<?> fut2 = svcs2.future(); info("Deployed service: " + name); fut1.get(); info("Finished waiting for service future1: " + name); fut2.get(); info("Finished waiting for service future2: " + name); }', 'ground_truth': 'public void testSameConfiguration() throws Exception { String name = "dupService"; IgniteServices svcs1 = randomGrid().services().withAsync(); IgniteServices svcs2 = randomGrid().services().withAsync(); svcs1.deployClusterSingleton(name, new DummyService()); IgniteFuture<?> fut1 = svcs1.future(); svcs2.deployClusterSingleton(name, new DummyService()); IgniteFuture<?> fut2 = svcs2.future(); info("Deployed service: " + name); fut1.get(); info("Finished waiting for service future1: " + name); fut2.get(); info("Finished waiting for service future2: " + name); }', 'output': 'public void testSameConfiguration() throws Exception { String name = "dupService"; IgniteManaged svcs1 = randomGrid().managed().withAsync(); IgniteManaged svcs2 = randomGrid().managed().withAsync(); svcs1.deployClusterSingleton(name, new DummyService()); IgniteFuture<?> fut1 = svcs1.future(); svcs2.deployClusterSingleton(name, new DummyService()); IgniteFuture<?> fut2 = svcs2.future(); info("Deployed service: " + name); fut1.get(); info("Finished waiting for service future1: " + name); fut2.get(); info("Finished waiting for service future2: " + name); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void sortClasses() throws CompilationFailedException { Iterator modules = this.ast.getModules().iterator(); while (modules.hasNext()) { ModuleNode module = (ModuleNode) modules.next(); List classes = module.getClasses(); for (Iterator iter = classes.iterator(); iter.hasNext();) { ClassNode start = (ClassNode) iter.next(); ClassNode cn = start; XXX parents = new HashSet(); do { if (parents.contains(cn.getName())) { getErrorCollector().addErrorAndContinue( new SimpleMessage("cyclic inheritance involving "+cn.getName()+" in class "+start.getName(),this) ); cn=null; } else { parents.add(cn.getName()); cn = cn.getSuperClass(); } } while (cn!=null); } errorCollector.failIfErrors(); module.sortClasses(); } }', 'ground_truth': 'private void sortClasses() throws CompilationFailedException { Iterator modules = this.ast.getModules().iterator(); while (modules.hasNext()) { ModuleNode module = (ModuleNode) modules.next(); List classes = module.getClasses(); for (Iterator iter = classes.iterator(); iter.hasNext();) { ClassNode start = (ClassNode) iter.next(); ClassNode cn = start; Set parents = new HashSet(); do { if (parents.contains(cn.getName())) { getErrorCollector().addErrorAndContinue( new SimpleMessage("cyclic inheritance involving "+cn.getName()+" in class "+start.getName(),this) ); cn=null; } else { parents.add(cn.getName()); cn = cn.getSuperClass(); } } while (cn!=null); } errorCollector.failIfErrors(); module.sortClasses(); } }', 'output': 'private void sortClasses() throws CompilationFailedException { Iterator modules = this.ast.getModules().iterator(); while (modules.hasNext()) { ModuleNode module = (ModuleNode) modules.next(); List classes = module.getClasses(); for (Iterator iter = classes.iterator(); iter.hasNext();) { ClassNode start = (ClassNode) iter.next(); ClassNode cn = start; Set parents = new HashSet(); do { if (parents.contains(cn.getName())) { getErrorCollector().addErrorAndContinue( new SimpleMessage("cyclic inheritance involving "+cn.getName()+" in class "+start.getName(),this) ); cn=null; } else { parents.add(cn.getName()); cn = cn.getSuperClass(); } } while (cn!=null); } errorCollector.failIfErrors(); module.sortClasses(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private UUID pingNode(InetSocketAddress addr) throws GridException { assert addr != null; if (F.contains(locNodeAddrs, addr)) return locNodeId; XXX fut = new GridFutureAdapter<>(); GridFuture<UUID> oldFut = pingMap.putIfAbsent(addr, fut); if (oldFut != null) return oldFut.get(); else { Collection<Throwable> errs = null; Socket sock = null; for (int i = 0; i < reconCnt; i++) { try { if (addr.isUnresolved()) addr = new InetSocketAddress(InetAddress.getByName(addr.getHostName()), addr.getPort()); long tstamp = U.currentTimeMillis(); sock = openSocket(addr); writeToSocket(sock, new GridTcpDiscoveryHandshakeRequest(locNodeId)); GridTcpDiscoveryHandshakeResponse res = readMessage(sock, null, netTimeout); if (locNodeId.equals(res.creatorNodeId())) { if (log.isDebugEnabled()) log.debug("Handshake response from local node: " + res); break; } stats.onClientSocketInitialized(U.currentTimeMillis() - tstamp); fut.onDone(res.creatorNodeId()); pingMap.remove(addr); return res.creatorNodeId(); } catch (IOException | GridException e) { if (errs == null) errs = new ArrayList<>(); errs.add(e); } finally { U.closeQuiet(sock); } } GridException e = new GridSpiException("Failed to ping node by address: " + addr, U.exceptionWithSuppressed("Failed to ping node by address: " + addr, errs)); fut.onDone(e); pingMap.remove(addr); throw e; } }', 'ground_truth': 'private UUID pingNode(InetSocketAddress addr) throws GridException { assert addr != null; if (F.contains(locNodeAddrs, addr)) return locNodeId; GridFutureAdapterEx<UUID> fut = new GridFutureAdapterEx<>(); GridFuture<UUID> oldFut = pingMap.putIfAbsent(addr, fut); if (oldFut != null) return oldFut.get(); else { Collection<Throwable> errs = null; try { Socket sock = null; for (int i = 0; i < reconCnt; i++) { try { if (addr.isUnresolved()) addr = new InetSocketAddress(InetAddress.getByName(addr.getHostName()), addr.getPort()); long tstamp = U.currentTimeMillis(); sock = openSocket(addr); writeToSocket(sock, new GridTcpDiscoveryHandshakeRequest(locNodeId)); GridTcpDiscoveryHandshakeResponse res = readMessage(sock, null, netTimeout); if (locNodeId.equals(res.creatorNodeId())) { if (log.isDebugEnabled()) log.debug("Handshake response from local node: " + res); break; } stats.onClientSocketInitialized(U.currentTimeMillis() - tstamp); fut.onDone(res.creatorNodeId()); return res.creatorNodeId(); } catch (IOException | GridException e) { if (errs == null) errs = new ArrayList<>(); errs.add(e); } finally { U.closeQuiet(sock); } } } catch (Throwable t) { fut.onDone(t); throw U.cast(t); } finally { if (!fut.isDone()) fut.onDone(U.exceptionWithSuppressed("Failed to ping node by address: " + addr, errs)); boolean b = pingMap.remove(addr, fut); assert b; } return fut.get(); } }', 'output': 'private UUID pingNode(InetSocketAddress addr) throws GridException { assert addr != null; if (F.contains(locNodeAddrs, addr)) return locNodeId; IgniteInternalFuture<?> fut = new GridFutureAdapter<>(); GridFuture<UUID> oldFut = pingMap.putIfAbsent(addr, fut); if (oldFut != null) return oldFut.get(); else { Collection<Throwable> errs = null; Socket sock = null; for (int i = 0; i < reconCnt; i++) { try { if (addr.isUnresolved()) addr = new InetSocketAddress(InetAddress.getByName(addr.getHostName()), addr.getPort()); long tstamp = U.currentTimeMillis(); sock = openSocket(addr); writeToSocket(sock, new GridTcpDiscoveryHandshakeRequest(locNodeId)); GridTcpDiscoveryHandshakeResponse res = readMessage(sock, null, netTimeout); if (locNodeId.equals(res.creatorNodeId())) { if (log.isDebugEnabled()) log.debug("Handshake response from local node: " + res); break; } stats.onClientSocketInitialized(U.currentTimeMillis() - tstamp); fut.onDone(res.creatorNodeId()); pingMap.remove(addr); return res.creatorNodeId(); } catch (IOException | GridException e) { if (errs == null) errs = new ArrayList<>(); errs.add(e); } finally { U.closeQuiet(sock); } } GridException e = new GridSpiException("Failed to ping node by address: " + addr, U.exceptionWithSuppressed("Failed to ping node by address: " + addr, errs)); fut.onDone(e); pingMap.remove(addr); throw e; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static JoinRelType simplifyJoin(RelNode joinRel, ImmutableList<RexNode> aboveFilters, JoinRelType joinType) { final int nTotalFields = joinRel.getRowType().getFieldCount(); final int nSysFields = 0; final int nFieldsLeft = joinRel.getInputs().get(0).getRowType().getFieldCount(); final int nFieldsRight = joinRel.getInputs().get(1).getRowType().getFieldCount(); assert nTotalFields == nSysFields + nFieldsLeft + nFieldsRight; BitSet leftBitmap = BitSets.range(nSysFields, nSysFields + nFieldsLeft); XXX rightBitmap = BitSets.range(nSysFields + nFieldsLeft, nTotalFields); for (RexNode filter : aboveFilters) { if (joinType.generatesNullsOnLeft() && Strong.is(filter, leftBitmap)) { joinType = joinType.cancelNullsOnLeft(); } if (joinType.generatesNullsOnRight() && Strong.is(filter, rightBitmap)) { joinType = joinType.cancelNullsOnRight(); } if (joinType == JoinRelType.INNER) { break; } } return joinType; }', 'ground_truth': 'public static JoinRelType simplifyJoin(RelNode joinRel, ImmutableList<RexNode> aboveFilters, JoinRelType joinType) { final int nTotalFields = joinRel.getRowType().getFieldCount(); final int nSysFields = 0; final int nFieldsLeft = joinRel.getInputs().get(0).getRowType().getFieldCount(); final int nFieldsRight = joinRel.getInputs().get(1).getRowType().getFieldCount(); assert nTotalFields == nSysFields + nFieldsLeft + nFieldsRight; ImmutableBitSet leftBitmap = ImmutableBitSet.range(nSysFields, nSysFields + nFieldsLeft); ImmutableBitSet rightBitmap = ImmutableBitSet.range(nSysFields + nFieldsLeft, nTotalFields); for (RexNode filter : aboveFilters) { if (joinType.generatesNullsOnLeft() && Strong.is(filter, leftBitmap)) { joinType = joinType.cancelNullsOnLeft(); } if (joinType.generatesNullsOnRight() && Strong.is(filter, rightBitmap)) { joinType = joinType.cancelNullsOnRight(); } if (joinType == JoinRelType.INNER) { break; } } return joinType; }', 'output': 'public static JoinRelType simplifyJoin(RelNode joinRel, ImmutableList<RexNode> aboveFilters, JoinRelType joinType) { final int nTotalFields = joinRel.getRowType().getFieldCount(); final int nSysFields = 0; final int nFieldsLeft = joinRel.getInputs().get(0).getRowType().getFieldCount(); final int nFieldsRight = joinRel.getInputs().get(1).getRowType().getFieldCount(); assert nTotalFields == nSysFields + nFieldsLeft + nFieldsRight; BitSet leftBitmap = BitSets.range(nSysFields, nSysFields + nFieldsLeft); BitSet rightBitmap = BitSets.range(nSysFields + nFieldsLeft, nTotalFields); for (RexNode filter : aboveFilters) { if (joinType.generatesNullsOnLeft() && Strong.is(filter, leftBitmap)) { joinType = joinType.cancelNullsOnLeft(); } if (joinType.generatesNullsOnRight() && Strong.is(filter, rightBitmap)) { joinType = joinType.cancelNullsOnRight(); } if (joinType == JoinRelType.INNER) { break; } } return joinType; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void makeSwingUseSpecialClipboard (java.awt.datatransfer.Clipboard clip) { try { synchronized (TopSecurityManager.class) { if (! (System.getSecurityManager() instanceof TopSecurityManager)) { LOG.warning("Our manager has to be active: " + System.getSecurityManager()); return; } if (CLIPBOARD_FORBIDDEN != null) { return; } CLIPBOARD_FORBIDDEN = new ThreadLocal<Object>(); CLIPBOARD_FORBIDDEN.set (clip); } javax.swing.JComponent source = new javax.swing.JPanel (); javax.swing.TransferHandler.getPasteAction ().actionPerformed ( new java.awt.event.ActionEvent (source, 0, "") ); javax.swing.TransferHandler.getCopyAction ().actionPerformed ( new java.awt.event.ActionEvent (source, 0, "") ); javax.swing.TransferHandler.getCutAction ().actionPerformed ( new java.awt.event.ActionEvent (source, 0, "") ); Object forb = CLIPBOARD_FORBIDDEN.get (); CLIPBOARD_FORBIDDEN.set(null); if (! (forb instanceof TopSecurityManager) ) { System.err.println("Cannot install our clipboard to swing components, TopSecurityManager is not the security manager: " + forb); return; } Class<?> appContextClass = ClassLoader.getSystemClassLoader().loadClass("sun.awt.AppContext"); Method getAppContext = appContextClass.getMethod ("getAppContext"); Object appContext = getAppContext.invoke (null, new Object[0]); XXX actionClass = javax.swing.TransferHandler.getCopyAction ().getClass (); java.lang.reflect.Field sandboxKeyField = actionClass.getDeclaredField ("SandboxClipboardKey"); sandboxKeyField.setAccessible (true); Object value = sandboxKeyField.get (null); Method put = appContextClass.getMethod ("put", Object.class, Object.class); put.invoke (appContext, new Object[] { value, clip }); } catch (ThreadDeath ex) { throw ex; } catch (Throwable t) { t.printStackTrace(); } finally { if (CLIPBOARD_FORBIDDEN != null) { CLIPBOARD_FORBIDDEN.set (null); } } }', 'ground_truth': 'public static void makeSwingUseSpecialClipboard (java.awt.datatransfer.Clipboard clip) { try { synchronized (TopSecurityManager.class) { if (! (System.getSecurityManager() instanceof TopSecurityManager)) { LOG.warning("Our manager has to be active: " + System.getSecurityManager()); return; } if (CLIPBOARD_FORBIDDEN != null) { return; } CLIPBOARD_FORBIDDEN = new ThreadLocal<Object>(); CLIPBOARD_FORBIDDEN.set (clip); } javax.swing.JComponent source = new javax.swing.JPanel (); javax.swing.TransferHandler.getPasteAction ().actionPerformed ( new java.awt.event.ActionEvent (source, 0, "") ); javax.swing.TransferHandler.getCopyAction ().actionPerformed ( new java.awt.event.ActionEvent (source, 0, "") ); javax.swing.TransferHandler.getCutAction ().actionPerformed ( new java.awt.event.ActionEvent (source, 0, "") ); Object forb = CLIPBOARD_FORBIDDEN.get (); CLIPBOARD_FORBIDDEN.set(null); if (! (forb instanceof TopSecurityManager) ) { System.err.println("Cannot install our clipboard to swing components, TopSecurityManager is not the security manager: " + forb); return; } Class<?> appContextClass = ClassLoader.getSystemClassLoader().loadClass("sun.awt.AppContext"); Method getAppContext = appContextClass.getMethod ("getAppContext"); Object appContext = getAppContext.invoke (null, new Object[0]); Class<?> actionClass = javax.swing.TransferHandler.getCopyAction ().getClass (); java.lang.reflect.Field sandboxKeyField = actionClass.getDeclaredField ("SandboxClipboardKey"); sandboxKeyField.setAccessible (true); Object value = sandboxKeyField.get (null); Method put = appContextClass.getMethod ("put", Object.class, Object.class); put.invoke (appContext, new Object[] { value, clip }); } catch (ThreadDeath ex) { throw ex; } catch (Throwable t) { t.printStackTrace(); } finally { if (CLIPBOARD_FORBIDDEN != null) { CLIPBOARD_FORBIDDEN.set (null); } } }', 'output': 'public static void makeSwingUseSpecialClipboard (java.awt.datatransfer.Clipboard clip) { try { synchronized (TopSecurityManager.class) { if (! (System.getSecurityManager() instanceof TopSecurityManager)) { LOG.warning("Our manager has to be active: " + System.getSecurityManager()); return; } if (CLIPBOARD_FORBIDDEN != null) { return; } CLIPBOARD_FORBIDDEN = new ThreadLocal<Object>(); CLIPBOARD_FORBIDDEN.set (clip); } javax.swing.JComponent source = new javax.swing.JPanel (); javax.swing.TransferHandler.getPasteAction ().actionPerformed ( new java.awt.event.ActionEvent (source, 0, "") ); javax.swing.TransferHandler.getCopyAction ().actionPerformed ( new java.awt.event.ActionEvent (source, 0, "") ); javax.swing.TransferHandler.getCutAction ().actionPerformed ( new java.awt.event.ActionEvent (source, 0, "") ); Object forb = CLIPBOARD_FORBIDDEN.get (); CLIPBOARD_FORBIDDEN.set(null); if (! (forb instanceof TopSecurityManager) ) { System.err.println("Cannot install our clipboard to swing components, TopSecurityManager is not the security manager: " + forb); return; } Class<?> appContextClass = ClassLoader.getSystemClassLoader().loadClass("sun.awt.AppContext"); Method getAppContext = appContextClass.getMethod ("getAppContext"); Object appContext = getAppContext.invoke (null, new Object[0]); Class<?> actionClass = javax.swing.TransferHandler.getCopyAction ().getClass (); java.lang.reflect.Field sandboxKeyField = actionClass.getDeclaredField ("SandboxClipboardKey"); sandboxKeyField.setAccessible (true); Object value = sandboxKeyField.get (null); Method put = appContextClass.getMethod ("put", Object.class, Object.class); put.invoke (appContext, new Object[] { value, clip }); } catch (ThreadDeath ex) { throw ex; } catch (Throwable t) { t.printStackTrace();'}
{'instruction': 'Please refactor the following code:', 'input': 'public void execute(TestCase tc) throws Exception { for (int i = 0; i < tc.getNumRuns(); i++) { XXX sink = ListDataSink.get(tc.getNumOutputPartitions()); Flow flow = Flow.create(tc.toString()); tc.getOutput(flow).persist(sink); executor.submit(flow).get(); tc.validate(sink.getOutputs()); } }', 'ground_truth': 'public void execute(TestCase tc) throws Exception { boolean executed = false; for (Processing.Type proc: this.processing.asList()) { for (int i = 0; i < tc.getNumRuns(); i++) { ListDataSink<?> sink = ListDataSink.get(tc.getNumOutputPartitions()); Flow flow = Flow.create(tc.toString()); Dataset output = tc.getOutput(flow, proc.isBounded()); if (output != null) { output.persist(sink); executor.submit(flow).get(); tc.validate(sink.getOutputs()); executed = true; } } } if (!executed) { LOG.warn("{} was skipped due to incompatible processing types.", tc.getClass()); } }', 'output': 'public void execute(TestCase tc) throws Exception { for (int i = 0; i < tc.getNumRuns(); i++) { DataSink sink = ListDataSink.get(tc.getNumOutputPartitions()); Flow flow = Flow.create(tc.toString()); tc.getOutput(flow).persist(sink); executor.submit(flow).get(); tc.validate(sink.getOutputs()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSerializationTopicCreatedByStringAndIntAndLong() throws Exception { for (XXX marsh : getMarshallers()) { info("Test GridTopic externalization [marshaller=" + marsh + \']\'); for (GridTopic topic : GridTopic.values()) { Externalizable msgOut = (Externalizable)topic.topic(A_STRING, AN_INT, A_LONG); assertEquals(msgOut, GridTestIoUtils.externalize(msgOut, marsh)); } } }', 'ground_truth': 'public void testSerializationTopicCreatedByStringAndIntAndLong() throws Exception { for (Marshaller marsh : getMarshallers()) { info("Test GridTopic externalization [marshaller=" + marsh + \']\'); for (GridTopic topic : GridTopic.values()) { Externalizable msgOut = (Externalizable)topic.topic(A_STRING, AN_INT, A_LONG); assertEquals(msgOut, GridTestIoUtils.externalize(msgOut, marsh)); } } }', 'output': 'public void testSerializationTopicCreatedByStringAndIntAndLong() throws Exception { for (IgniteMarshaller marsh : getMarshallers()) { info("Test GridTopic externalization [marshaller=" + marsh + \']\'); for (GridTopic topic : GridTopic.values()) { Externalizable msgOut = (Externalizable)topic.topic(A_STRING, AN_INT, A_LONG); assertEquals(msgOut, GridTestIoUtils.externalize(msgOut, marsh)); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSplitQueryFnWithoutNumSplits() throws Exception { int numSplits = 0; int expectedNumSplits = 20; long entityBytes = expectedNumSplits * DEFAULT_BUNDLE_SIZE_BYTES; long timestamp = 1234L; RunQueryRequest latestTimestampRequest = makeRequest(makeLatestTimestampQuery(NAMESPACE), NAMESPACE); RunQueryResponse latestTimestampResponse = makeLatestTimestampResponse(timestamp); RunQueryRequest statRequest = makeRequest(makeStatKindQuery(NAMESPACE, timestamp), NAMESPACE); RunQueryResponse statResponse = makeStatKindResponse(entityBytes); when(mockDatastore.runQuery(latestTimestampRequest)) .thenReturn(latestTimestampResponse); when(mockDatastore.runQuery(statRequest)) .thenReturn(statResponse); when(mockQuerySplitter.getSplits( eq(QUERY), any(PartitionId.class), eq(expectedNumSplits), any(Datastore.class))) .thenReturn(splitQuery(QUERY, expectedNumSplits)); SplitQueryFn splitQueryFn = new SplitQueryFn(V_1_OPTIONS, numSplits, mockDatastoreFactory); DoFnTester<Query, KV<Integer, Query>> doFnTester = DoFnTester.of(splitQueryFn); doFnTester.setCloningBehavior(CloningBehavior.DO_NOT_CLONE); List<KV<Integer, Query>> queries = doFnTester.processBundle(QUERY); assertEquals(queries.size(), expectedNumSplits); verifyUniqueKeys(queries); verify(mockQuerySplitter, times(1)).getSplits( eq(QUERY), any(PartitionId.class), eq(expectedNumSplits), any(Datastore.class)); verify(mockDatastore, times(1)).runQuery(latestTimestampRequest); verify(mockDatastore, times(1)).runQuery(statRequest); }', 'ground_truth': 'public void testSplitQueryFnWithoutNumSplits() throws Exception { int numSplits = 0; int expectedNumSplits = 20; long entityBytes = expectedNumSplits * DEFAULT_BUNDLE_SIZE_BYTES; long timestamp = 1234L; RunQueryRequest latestTimestampRequest = makeRequest(makeLatestTimestampQuery(NAMESPACE), NAMESPACE); RunQueryResponse latestTimestampResponse = makeLatestTimestampResponse(timestamp); RunQueryRequest statRequest = makeRequest(makeStatKindQuery(NAMESPACE, timestamp), NAMESPACE); RunQueryResponse statResponse = makeStatKindResponse(entityBytes); when(mockDatastore.runQuery(latestTimestampRequest)) .thenReturn(latestTimestampResponse); when(mockDatastore.runQuery(statRequest)) .thenReturn(statResponse); when(mockQuerySplitter.getSplits( eq(QUERY), any(PartitionId.class), eq(expectedNumSplits), any(Datastore.class))) .thenReturn(splitQuery(QUERY, expectedNumSplits)); SplitQueryFn splitQueryFn = new SplitQueryFn(V_1_OPTIONS, numSplits, mockDatastoreFactory); DoFnTester<Query, Query> doFnTester = DoFnTester.of(splitQueryFn); doFnTester.setCloningBehavior(CloningBehavior.DO_NOT_CLONE); List<Query> queries = doFnTester.processBundle(QUERY); assertEquals(queries.size(), expectedNumSplits); verify(mockQuerySplitter, times(1)).getSplits( eq(QUERY), any(PartitionId.class), eq(expectedNumSplits), any(Datastore.class)); verify(mockDatastore, times(1)).runQuery(latestTimestampRequest); verify(mockDatastore, times(1)).runQuery(statRequest); }', 'output': 'public void testSplitQueryFnWithoutNumSplits() throws Exception { int numSplits = 0; int expectedNumSplits = 20; long entityBytes = expectedNumSplits * DEFAULT_BUNDLE_SIZE_BYTES; long timestamp = 1234L; RunQueryRequest latestTimestampRequest = makeRequest(makeLatestTimestampQuery(NAMESPACE), NAMESPACE); RunQueryResponse latestTimestampResponse = makeLatestTimestampResponse(timestamp); RunQueryRequest statRequest = makeRequest(makeStatKindQuery(NAMESPACE, timestamp), NAMESPACE); RunQueryResponse statResponse = makeStatKindResponse(entityBytes); when(mockDatastore.runQuery(latestTimestampRequest)) .thenReturn(latestTimestampResponse); when(mockDatastore.runQuery(statRequest)) .thenReturn(statResponse); when(mockQuerySplitter.getSplits(eq(QUERY), any(PartitionId.class), anyInt(), any(Datastore.class))) .thenReturn(splitQuery(QUERY, expectedNumSplits)); SplitQueryFn splitQueryFn = new SplitQueryFn(V_1_OPTIONS, numSplits, mockDatastoreFactory); DoFnTester<Query, KV<Integer, Query>> doFnTester = DoFnTester.of(splitQueryFn); doFnTester.setCloningBehavior(CloningBehavior.DO_NOT_CLONE); List<KV<Integer, Query>> queries = doFnTester.processBundle(QUERY); assertEquals(queries.size(), expectedNumSplits); verifyUniqueKeys(queries); verify(mockQuerySplitter, times(1)).getSplits(eq(QUERY), any(PartitionId.class), anyInt(), any(Datastore.class)); verify(mockDatastore, times(1)).runQuery(latestTimestampRequest); verify(mockDatastore, times(1)).runQuery(statRequest); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testEventWindowingEarlyTriggered() throws Exception { ListDataSink<WindowedPair<TimeInterval, String, HashSet<String>>> output = ListDataSink.get(1); ListDataSource<Pair<String, Integer>> source = ListDataSource.unbounded( asList( Pair.of("one",     1), Pair.of("two",     2), Pair.of("three",   3), Pair.of("four",    4), Pair.of("five",    5), Pair.of("six",     6), Pair.of("seven",   7), Pair.of("eight",   8), Pair.of("nine",    9), Pair.of("ten",    10), Pair.of("eleven", 11))) .withReadDelay(Duration.ofMillis(200)); Flow f = Flow.create("test-attached-windowing"); ReduceByKey.of(f.createInput(source)) .keyBy(e -> "") .valueBy((UnaryFunction<Pair<String, Integer>, HashSet<String>>) what -> Sets.newHashSet(what.getFirst())) .combineBy((CombinableReduceFunction<HashSet<String>>) what -> { Iterator<HashSet<String>> iter = what.iterator(); HashSet<String> s = iter.next(); while (iter.hasNext()) { s.addAll(iter.next()); } return s; }) .windowBy(Time.of(Duration.ofMillis(6)) .earlyTriggering(Duration.ofMillis(2)) .using(e -> (long) e.getSecond())) .setNumPartitions(1) .outputWindowed() .persist(output); new TestFlinkExecutor() .setAutoWatermarkInterval(Duration.ofMillis(10)) .waitForCompletion(f); assertEquals( asList("00: 02 => one, two", "00: 04 => four, one, three, two", "00: 05 => five, four, one, three, two", "06: 03 => eight, seven, six", "06: 05 => eight, nine, seven, six, ten", "06: 06 => eight, eleven, nine, seven, six, ten"), output.getOutput(0) .stream() .map(p -> { StringBuilder sb = new StringBuilder(); sb.append(String.format("%02d: %02d => ", p.getWindowLabel().getStartMillis(), p.getSecond().size())); ArrayList<String> xs = Lists.newArrayList(p.getSecond()); xs.sort(String::compareTo); Joiner.on(", ").appendTo(sb, xs); return sb.toString(); }) .sorted() .collect(Collectors.toList())); }', 'ground_truth': 'public void testEventWindowingEarlyTriggered() throws Exception { ListDataSink<Triple<TimeInterval, String, HashSet<String>>> output = ListDataSink.get(1); ListDataSource<Pair<String, Integer>> source = ListDataSource.unbounded( asList( Pair.of("one",     1), Pair.of("two",     2), Pair.of("three",   3), Pair.of("four",    4), Pair.of("five",    5), Pair.of("six",     6), Pair.of("seven",   7), Pair.of("eight",   8), Pair.of("nine",    9), Pair.of("ten",    10), Pair.of("eleven", 11))) .withReadDelay(Duration.ofMillis(200)); Flow f = Flow.create("test-attached-windowing"); Dataset<Pair<String, HashSet<String>>> reduced = ReduceByKey.of(f.createInput(source)) .keyBy(e -> "") .valueBy((UnaryFunction<Pair<String, Integer>, HashSet<String>>) what -> Sets.newHashSet(what.getFirst())) .combineBy((CombinableReduceFunction<HashSet<String>>) what -> { Iterator<HashSet<String>> iter = what.iterator(); HashSet<String> s = iter.next(); while (iter.hasNext()) { s.addAll(iter.next()); } return s; }) .windowBy(Time.of(Duration.ofMillis(6)) .earlyTriggering(Duration.ofMillis(2)) .using(e -> (long) e.getSecond())) .setNumPartitions(1) .output(); Util.extractWindows(reduced, TimeInterval.class).persist(output); new TestFlinkExecutor() .setAutoWatermarkInterval(Duration.ofMillis(10)) .waitForCompletion(f); assertEquals( asList("00: 02 => one, two", "00: 04 => four, one, three, two", "00: 05 => five, four, one, three, two", "06: 03 => eight, seven, six", "06: 05 => eight, nine, seven, six, ten", "06: 06 => eight, eleven, nine, seven, six, ten"), output.getOutput(0) .stream() .map(p -> { StringBuilder sb = new StringBuilder(); sb.append(String.format("%02d: %02d => ", p.getFirst().getStartMillis(), p.getThird().size())); ArrayList<String> xs = Lists.newArrayList(p.getThird()); xs.sort(String::compareTo); Joiner.on(", ").appendTo(sb, xs); return sb.toString(); }) .sorted() .collect(Collectors.toList())); }', 'output': 'public void testEventWindowingEarlyTriggered() throws Exception { ListDataSink<WindowedPair<TimeInterval, String, HashSet<String>>> output = ListDataSink.get(1); ListDataSource<Pair<String, Integer>> source = ListDataSource.unbounded( asList( Pair.of("one",     1), Pair.of("two",     2), Pair.of("three",   3), Pair.of("four",    4), Pair.of("five",    5), Pair.of("six",     6), Pair.of("seven",   7), Pair.of("eight",   8), Pair.of("nine",    9), Pair.of("ten",    10), Pair.of("eleven", 11))) .withReadDelay(Duration.ofMillis(200)); Flow f = Flow.create("test-attached-windowing"); ReduceByKey.of(f.createInput(source)) .keyBy(e -> "") .valueBy((UnaryFunction<Pair<String, Integer>, HashSet<String>>) what -> Sets.newHashSet(what.getFirst())) .combineBy((CombinableReduceFunction<HashSet<String>>) what -> { Iterator<HashSet<String>> iter = what.iterator(); HashSet<String> s = iter.next(); while (iter.hasNext()) { s.addAll(iter.next()); } return s; }) .windowBy(Time.of(Duration.ofMillis(6)) .earlyTriggering(Duration.ofMillis(2)) .using(e -> (long) e.getSecond())) .setNumPartitions(1) .outputWindowed() .persist(output); new TestFlinkExecutor() .setAutoWatermarkInterval(Duration.ofMillis(10)) .waitForCompletion(f); assertEquals( asList("00: 02 => one, two", "00: 04 => four, one, three, two", "00: 05 => five, four, one, three, two", "06: 03 => eight, seven, six", "06: 05 => eight, nine, seven, six, ten",'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testExtractionDirectFromTransform() throws Exception { PCollection<String> input = p.apply(Create.of("hello")); PDone output = input.apply(writeFiles); AppliedPTransform<PCollection<String>, PDone, WriteFiles<String>> appliedPTransform = AppliedPTransform.<PCollection<String>, PDone, WriteFiles<String>>of( "foo", input.expand(), output.expand(), writeFiles, p); assertThat( WriteFilesTranslation.isRunnerDeterminedSharding(appliedPTransform), equalTo(writeFiles.getNumShards() == null && writeFiles.getSharding() == null)); assertThat( WriteFilesTranslation.isWindowedWrites(appliedPTransform), equalTo(writeFiles.isWindowedWrites())); assertThat(WriteFilesTranslation.getSink(appliedPTransform), equalTo(writeFiles.getSink())); }', 'ground_truth': 'public void testExtractionDirectFromTransform() throws Exception { PCollection<String> input = p.apply(Create.of("hello")); PDone output = input.apply(writeFiles); AppliedPTransform<PCollection<String>, PDone, WriteFiles<String, Void, String>> appliedPTransform = AppliedPTransform.of("foo", input.expand(), output.expand(), writeFiles, p); assertThat( WriteFilesTranslation.isRunnerDeterminedSharding(appliedPTransform), equalTo(writeFiles.getNumShards() == null && writeFiles.getSharding() == null)); assertThat( WriteFilesTranslation.isWindowedWrites(appliedPTransform), equalTo(writeFiles.isWindowedWrites())); assertThat( WriteFilesTranslation.<String, Void, String>getSink(appliedPTransform), equalTo(writeFiles.getSink())); }', 'output': 'public void testExtractionDirectFromTransform() throws Exception { PCollection<String> input = p.apply(Create.of("hello")); PCollection<String> output = input.apply(writeFiles); AppliedPTransform<PCollection<String>, PCollection<String>, WriteFiles<String>> appliedPTransform = AppliedPTransform.<PCollection<String>, PCollection<String>, WriteFiles<String>>of( "foo", input.expand(), output.expand(), writeFiles, p); assertThat( WriteFilesTranslation.isRunnerDeterminedSharding(appliedPTransform), equalTo(writeFiles.getNumShards() == null && writeFiles.getSharding() == null)); assertThat( WriteFilesTranslation.isWindowedWrites(appliedPTransform), equalTo(writeFiles.isWindowedWrites())); assertThat(WriteFilesTranslation.getSink(appliedPTransform), equalTo(writeFiles.getSink())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void compareAndSet(String cacheName) throws Exception { info("Running test [name=" + getName() + ", cache=" + cacheName + \']\'); GridCache cache = grid().cache(cacheName); assertNotNull(cache); assertEquals(0, cache.primarySize()); XXX atomic = cache.dataStructures().atomicLong("atomic", 0, true); assertEquals(0, cache.primarySize()); long newVal = RND.nextLong(); final long oldVal = atomic.get(); assert !atomic.compareAndSet(oldVal - 1, newVal); assert oldVal == atomic.get(); assert atomic.compareAndSet(oldVal, newVal); assert newVal == atomic.get(); }', 'ground_truth': 'private void compareAndSet(String cacheName) throws Exception { info("Running test [name=" + getName() + ", cache=" + cacheName + \']\'); GridCache cache = grid().cache(cacheName); assertNotNull(cache); assertEquals(0, cache.primarySize()); IgniteAtomicLong atomic = cache.dataStructures().atomicLong("atomic", 0, true); assertEquals(0, cache.primarySize()); long newVal = RND.nextLong(); final long oldVal = atomic.get(); assert !atomic.compareAndSet(oldVal - 1, newVal); assert oldVal == atomic.get(); assert atomic.compareAndSet(oldVal, newVal); assert newVal == atomic.get(); }', 'output': 'private void compareAndSet(String cacheName) throws Exception { info("Running test [name=" + getName() + ", cache=" + cacheName + \']\'); GridCache cache = grid().cache(cacheName); assertNotNull(cache); assertEquals(0, cache.primarySize()); AtomicLong atomic = cache.dataStructures().atomicLong("atomic", 0, true); assertEquals(0, cache.primarySize()); long newVal = RND.nextLong(); final long oldVal = atomic.get(); assert !atomic.compareAndSet(oldVal - 1, newVal); assert oldVal == atomic.get(); assert atomic.compareAndSet(oldVal, newVal); assert newVal == atomic.get(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGetAndAdd() throws Exception { for (int i = 1; i < MAX_LOOPS_NUM; i++) { for (XXX seq : seqArr) getAndAdd(seq, i); if (i % 100 == 0) info("Finished iteration: " + i); } } public void testGetAndAddInTx() throws Exception { try (IgniteTx tx = grid().cache(null).txStart(PESSIMISTIC, REPEATABLE_READ)) { for (int i = 1; i < MAX_LOOPS_NUM; i++) { for (XXX seq : seqArr) getAndAdd(seq, i); if (i % 100 == 0) info("Finished iteration: " + i); } } }', 'ground_truth': 'public void testGetAndAdd() throws Exception { for (int i = 1; i < MAX_LOOPS_NUM; i++) { for (IgniteAtomicSequence seq : seqArr) getAndAdd(seq, i); if (i % 100 == 0) info("Finished iteration: " + i); } }', 'output': 'public void testGetAndAdd() throws Exception { for (int i = 1; i < MAX_LOOPS_NUM; i++) { for (String seq : seqArr) getAndAdd(seq, i); if (i % 100 == 0) info("Finished iteration: " + i); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void maybeAddOrUpdateKeyspace(KeyspaceMetadata expected) { KeyspaceMetadata defined = Schema.instance.getKSMetaData(expected.name); if (defined == null) { maybeAddKeyspace(expected); defined = Schema.instance.getKSMetaData(expected.name); } for (XXX expectedTable : expected.tables) { CFMetaData definedTable = defined.tables.get(expectedTable.cfName).orElse(null); if (definedTable == null || !definedTable.equals(expectedTable)) MigrationManager.forceAnnounceNewColumnFamily(expectedTable); } }', 'ground_truth': 'private void maybeAddOrUpdateKeyspace(KeyspaceMetadata expected) { KeyspaceMetadata defined = Schema.instance.getKeyspaceMetadata(expected.name); if (defined == null) { maybeAddKeyspace(expected); defined = Schema.instance.getKeyspaceMetadata(expected.name); } for (TableMetadata expectedTable : expected.tables) { TableMetadata definedTable = defined.tables.get(expectedTable.name).orElse(null); if (definedTable == null || !definedTable.equals(expectedTable)) MigrationManager.forceAnnounceNewTable(expectedTable); } }', 'output': 'private void maybeAddOrUpdateKeyspace(KeyspaceMetadata expected) { KeyspaceMetadata defined = Schema.instance.getKSMetaData(expected.name); if (defined == null) { maybeAddKeyspace(expected); defined = Schema.instance.getKSMetaData(expected.name); } for (CFMetaData expectedTable : expected.tables) { CFMetaData definedTable = defined.tables.get(expectedTable.cfName).orElse(null); if (definedTable == null || !definedTable.equals(expectedTable)) MigrationManager.forceAnnounceNewColumnFamily(expectedTable); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testMapField() throws Exception { PortableBuilder builder = builder("Class"); builder.hashCode(100); builder.setField("mapField", F.asMap(new Key(1), new Value(1), new Key(2), new Value(2))); PortableObject po = builder.build(); assertEquals("class".hashCode(), po.typeId()); assertEquals(100, po.hashCode()); Map<PortableObject, PortableObject> map = po.field("mapField"); assertEquals(2, map.size()); for (Map.Entry<PortableObject, PortableObject> e : map.entrySet()) assertEquals(e.getKey().<Key>deserialize().i, e.getValue().<Value>deserialize().i); }', 'ground_truth': 'public void testMapField() throws Exception { BinaryObjectBuilder builder = builder("Class"); builder.hashCode(100); builder.setField("mapField", F.asMap(new Key(1), new Value(1), new Key(2), new Value(2))); BinaryObject po = builder.build(); assertEquals("class".hashCode(), po.typeId()); assertEquals(100, po.hashCode()); Map<BinaryObject, BinaryObject> map = po.field("mapField"); assertEquals(2, map.size()); for (Map.Entry<BinaryObject, BinaryObject> e : map.entrySet()) assertEquals(e.getKey().<Key>deserialize().i, e.getValue().<Value>deserialize().i); }', 'output': 'public void testMapField() throws Exception { PortableBuilder builder = builder("Class"); builder.hashCode(100); builder.setField("mapField", ImmutableMap.of(new Key(1), new Value(1), new Key(2), new Value(2))); PortableObject po = builder.build(); assertEquals("class".hashCode(), po.typeId()); assertEquals(100, po.hashCode()); Map<PortableObject, PortableObject> map = po.field("mapField"); assertEquals(2, map.size()); for (Map.Entry<PortableObject, PortableObject> e : map.entrySet()) assertEquals(e.getKey().<Key>deserialize().i, e.getValue().<Value>deserialize().i); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testStringArrayModification() { GridPortableTestClasses.TestObjectAllTypes obj = new GridPortableTestClasses.TestObjectAllTypes(); obj.strArr = new String[] {"a", "a", "a"}; BinaryObjectBuilderImpl mutObj = wrap(obj); String[] arr = mutObj.getField("strArr"); arr[0] = "b"; XXX res = mutObj.build().deserialize(); Assert.assertArrayEquals(new String[] {"b", "a", "a"}, res.strArr); }', 'ground_truth': 'public void testStringArrayModification() { GridBinaryTestClasses.TestObjectAllTypes obj = new GridBinaryTestClasses.TestObjectAllTypes(); obj.strArr = new String[] {"a", "a", "a"}; BinaryObjectBuilderImpl mutObj = wrap(obj); String[] arr = mutObj.getField("strArr"); arr[0] = "b"; GridBinaryTestClasses.TestObjectAllTypes res = mutObj.build().deserialize(); Assert.assertArrayEquals(new String[] {"b", "a", "a"}, res.strArr); }', 'output': 'public void testStringArrayModification() { GridPortableTestClasses.TestObjectAllTypes obj = new GridPortableTestClasses.TestObjectAllTypes(); obj.strArr = new String[] {"a", "a", "a"}; BinaryObjectBuilderImpl mutObj = wrap(obj); String[] arr = mutObj.getField("strArr"); arr[0] = "b"; TestObjectAllTypes res = mutObj.build().deserialize(); Assert.assertArrayEquals(new String[] {"b", "a", "a"}, res.strArr); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testWriteThrough() throws Exception { GridCache<Integer, String> cache = cache(); Map<Integer, String> map = store.getMap(); assert map.isEmpty(); IgniteTx tx = cache.txStart(OPTIMISTIC, REPEATABLE_READ); try { for (int i = 1; i <= 10; i++) { cache.putx(i, Integer.toString(i)); checkLastMethod(null); } tx.commit(); } finally { tx.close(); } U.sleep(WRITE_FROM_BEHIND_FLUSH_FREQUENCY + 100); checkLastMethod("putAll"); assert cache.size() == 10; for (int i = 1; i <= 10; i++) { String val = map.get(i); assert val != null; assert val.equals(Integer.toString(i)); } store.resetLastMethod(); tx = cache.txStart(); try { for (int i = 1; i <= 10; i++) { String val = cache.remove(i); checkLastMethod(null); assert val != null; assert val.equals(Integer.toString(i)); } tx.commit(); } finally { tx.close(); } U.sleep(WRITE_FROM_BEHIND_FLUSH_FREQUENCY + 100); checkLastMethod("removeAll"); assert map.isEmpty(); }', 'ground_truth': 'public void testWriteThrough() throws Exception { IgniteCache<Integer, String> cache = jcache(); Map<Integer, String> map = store.getMap(); assert map.isEmpty(); IgniteTx tx = grid().transactions().txStart(OPTIMISTIC, REPEATABLE_READ); try { for (int i = 1; i <= 10; i++) { cache.put(i, Integer.toString(i)); checkLastMethod(null); } tx.commit(); } finally { tx.close(); } U.sleep(WRITE_FROM_BEHIND_FLUSH_FREQUENCY + 100); checkLastMethod("putAll"); assert cache.size() == 10; for (int i = 1; i <= 10; i++) { String val = map.get(i); assert val != null; assert val.equals(Integer.toString(i)); } store.resetLastMethod(); tx = grid().transactions().txStart(); try { for (int i = 1; i <= 10; i++) { String val = cache.getAndRemove(i); checkLastMethod(null); assert val != null; assert val.equals(Integer.toString(i)); } tx.commit(); } finally { tx.close(); } U.sleep(WRITE_FROM_BEHIND_FLUSH_FREQUENCY + 100); checkLastMethod("removeAll"); assert map.isEmpty(); }', 'output': 'public void testWriteThrough() throws Exception { Cache<Integer, String> cache = cache(); Map<Integer, String> map = store.getMap(); assert map.isEmpty(); IgniteTx tx = cache.txStart(OPTIMISTIC, REPEATABLE_READ); try { for (int i = 1; i <= 10; i++) { cache.putx(i, Integer.toString(i)); checkLastMethod(null); } tx.commit(); } finally { tx.close(); } U.sleep(WRITE_FROM_BEHIND_FLUSH_FREQUENCY + 100); checkLastMethod("putAll"); assert cache.size() == 10; for (int i = 1; i <= 10; i++) { String val = map.get(i); assert val != null; assert val.equals(Integer.toString(i)); } store.resetLastMethod(); tx = cache.txStart(); try { for (int i = 1; i <= 10; i++) { String val = cache.remove(i); checkLastMethod(null); assert val != null; assert val.equals(Integer.toString(i)); } tx.commit(); } finally { tx.close(); } U.sleep(WRITE_FROM_BEHIND_FLUSH_FREQUENCY + 100); checkLastMethod("removeAll"); assert map.isEmpty(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static synchronized List<Installation> getInstallations(Collection loadedInstallations) { if ( INSTALLATIONS == null ) { Installation bundled = BundledInstallation.getDefault(); if (bundled.isInstalled()) { ArrayList<Installation> bundledList = new ArrayList<Installation>(); bundledList.add(bundled); return bundledList; } ArrayList<Installation> stackInstalls = new ArrayList<Installation>(); ArrayList<Installation> stdInstalls = new ArrayList<Installation>(); for ( XXX it = loadedInstallations.iterator() ; it.hasNext() ; ) { Installation installation = (Installation)it.next(); if ( installation.isStackInstall() ) { stackInstalls.add(installation); } else { stdInstalls.add(installation); } } INSTALLATIONS = new ArrayList<Installation>(); INSTALLATIONS.addAll(stackInstalls); INSTALLATIONS.addAll(stdInstalls); } return INSTALLATIONS; }', 'ground_truth': 'public static synchronized List<Installation> getInstallations(Collection loadedInstallations) { if ( INSTALLATIONS == null ) { Installation bundled = BundledInstallation.getDefault(); if (bundled.isInstalled()) { ArrayList<Installation> bundledList = new ArrayList<Installation>(); bundledList.add(bundled); return bundledList; } ArrayList<Installation> stackInstalls = new ArrayList<Installation>(); ArrayList<Installation> stdInstalls = new ArrayList<Installation>(); for ( Iterator<Installation> it = loadedInstallations.iterator() ; it.hasNext() ; ) { Installation installation = it.next(); if ( installation.isStackInstall() ) { stackInstalls.add(installation); } else { stdInstalls.add(installation); } } INSTALLATIONS = new ArrayList<Installation>(); INSTALLATIONS.addAll(stackInstalls); INSTALLATIONS.addAll(stdInstalls); } return INSTALLATIONS; }', 'output': 'public static synchronized List<Installation> getInstallations(Collection loadedInstallations) { if ( INSTALLATIONS == null ) { Installation bundled = BundledInstallation.getDefault(); if (bundled.isInstalled()) { ArrayList<Installation> bundledList = new ArrayList<Installation>(); bundledList.add(bundled); return bundledList; } ArrayList<Installation> stackInstalls = new ArrayList<Installation>(); ArrayList<Installation> stdInstalls = new ArrayList<Installation>(); for ( Iterator<Installation> it = loadedInstallations.iterator() ; it.hasNext() ; ) { Installation installation = it.next(); if ( installation.isStackInstall() ) { stackInstalls.add(installation); } else { stdInstalls.add(installation); } } INSTALLATIONS = new ArrayList<Installation>(); INSTALLATIONS.addAll(stackInstalls); INSTALLATIONS.addAll(stdInstalls); } return INSTALLATIONS; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override protected Void run(String cacheName) throws IgniteCheckedException { XXX cache = g.cachex(cacheName); if (cache != null) cache.resetMetrics(); return null; } /** {@inheritDoc} */ @Override public String toString() { return S.toString(VisorCacheResetMetricsJob.class, this); } } } }', 'ground_truth': '@Override protected Void run(String cacheName) throws IgniteCheckedException { Cache cache = g.cachex(cacheName); if (cache != null) cache.resetMetrics(); return null; }', 'output': '@Override protected Void run(String cacheName) throws IgniteCheckedException { Cache<K, V> cache = g.cachex(cacheName); if (cache != null) cache.resetMetrics(); return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSize() throws Exception { assert cache().isEmpty(); int size = 10; Map<String, Integer> map = new HashMap<>(size); for (int i = 0; i < size; i++) map.put("key" + i, i); Map<GridNode, Collection<String>> mapped = grid(0).mapKeysToNodes(null, map.keySet()); for (int i = 0; i < gridCount(); i++) { Collection<String> keys = mapped.get(grid(i).localNode()); if (!F.isEmpty(keys)) { for (String key : keys) cache(i).put(key, map.get(key)); } } map.remove("key0"); mapped = grid(0).mapKeysToNodes(null, map.keySet()); for (int i = 0; i < gridCount(); i++) { CU.invalidate(cache(i), "key0"); assertNull("Failed check for grid: " + i, cache(i).peek("key0")); Collection<String> keysCol = mapped.get(grid(i).localNode()); assert !cache(i).isEmpty() || F.isEmpty(keysCol); } for (int i = 0; i < gridCount(); i++) { GridCacheContext<String, Integer> ctx = context(i); int sum = 0; for (String key : map.keySet()) if (ctx.affinity().localNode(key, ctx.discovery().topologyVersion())) sum++; assertEquals("Incorrect key size on cache #" + i, sum, cache(i).keySet().size()); assertEquals("Incorrect key size on cache #" + i, sum, cache(i).size()); } for (int i = 0; i < gridCount(); i++) { Collection<String> keysCol = mapped.get(grid(i).localNode()); assertEquals("Failed check for grid: " + i, !F.isEmpty(keysCol) ? keysCol.size() : 0, cache(i).primarySize()); } int globalPrimarySize = map.size(); for (int i = 0; i < gridCount(); i++) assertEquals(globalPrimarySize, cache(i).globalPrimarySize()); int times = 1; if (cacheMode() == REPLICATED) times = gridCount(); else if (cacheMode() == PARTITIONED) times = Math.min(gridCount(), cache().configuration().getBackups() + 1); int globalSize = globalPrimarySize * times; for (int i = 0; i < gridCount(); i++) assertEquals(globalSize, cache(i).globalSize()); }', 'ground_truth': 'public void testSize() throws Exception { assert cache().isEmpty(); int size = 10; Map<String, Integer> map = new HashMap<>(size); for (int i = 0; i < size; i++) map.put("key" + i, i); Map<ClusterNode, Collection<String>> mapped = grid(0).mapKeysToNodes(null, map.keySet()); for (int i = 0; i < gridCount(); i++) { Collection<String> keys = mapped.get(grid(i).localNode()); if (!F.isEmpty(keys)) { for (String key : keys) cache(i).put(key, map.get(key)); } } map.remove("key0"); mapped = grid(0).mapKeysToNodes(null, map.keySet()); for (int i = 0; i < gridCount(); i++) { CU.invalidate(cache(i), "key0"); assertNull("Failed check for grid: " + i, cache(i).peek("key0")); Collection<String> keysCol = mapped.get(grid(i).localNode()); assert !cache(i).isEmpty() || F.isEmpty(keysCol); } for (int i = 0; i < gridCount(); i++) { GridCacheContext<String, Integer> ctx = context(i); int sum = 0; for (String key : map.keySet()) if (ctx.affinity().localNode(key, ctx.discovery().topologyVersion())) sum++; assertEquals("Incorrect key size on cache #" + i, sum, cache(i).keySet().size()); assertEquals("Incorrect key size on cache #" + i, sum, cache(i).size()); } for (int i = 0; i < gridCount(); i++) { Collection<String> keysCol = mapped.get(grid(i).localNode()); assertEquals("Failed check for grid: " + i, !F.isEmpty(keysCol) ? keysCol.size() : 0, cache(i).primarySize()); } int globalPrimarySize = map.size(); for (int i = 0; i < gridCount(); i++) assertEquals(globalPrimarySize, cache(i).globalPrimarySize()); int times = 1; if (cacheMode() == REPLICATED) times = gridCount(); else if (cacheMode() == PARTITIONED) times = Math.min(gridCount(), cache().configuration().getBackups() + 1); int globalSize = globalPrimarySize * times; for (int i = 0; i < gridCount(); i++) assertEquals(globalSize, cache(i).globalSize()); }', 'output': 'public void testSize() throws Exception { assert cache().isEmpty(); int size = 10; Map<String, Integer> map = new HashMap<>(size); for (int i = 0; i < size; i++) map.put("key" + i, i); Map<GridNode, Collection<String>> mapped = grid(0).mapKeysToNodes(null, map.keySet()); for (int i = 0; i < gridCount(); i++) { Collection<String> keys = mapped.get(grid(i).localNode()); if (!F.isEmpty(keys)) { for (String key : keys) cache(i).put(key, map.get(key)); } } map.remove("key0"); mapped = grid(0).mapKeysToNodes(null, map.keySet()); for (int i = 0; i < gridCount(); i++) { CU.invalidate(cache(i), "key0"); assertNull("Failed check for grid: " + i, cache(i).peek("key0")); Collection<String> keysCol = mapped.get(grid(i).localNode()); assert !cache(i).isEmpty() || F.isEmpty(keysCol); } for (int i = 0; i < gridCount(); i++) { GridCacheContext<String, Integer> ctx = context(i); int sum = 0; for (String key : map.keySet()) if (ctx.affinity().localNode(key, ctx.discovery().topologyVersion())) sum++; assertEquals("Incorrect key size on cache #" + i, sum, cache(i).keySet().size()); assertEquals("Incorrect key size on cache #" + i, sum, cache(i).size()); } for (int i = 0; i < gridCount(); i++) { Collection<String> keysCol = mapped.get(grid(i).localNode()); assertEquals("Failed check for grid: " + i, !F.isEmpty(keysCol) ? keysCol.size() : 0, cache(i).primarySize()); } int globalPrimarySize = map.size(); for (int i = 0; i < gridCount(); i++) assertEquals(globalPrimarySize, cache(i).globalPrimarySize()); int times = 1; if (cacheMode() == REPLICATED) times ='}
{'instruction': 'Please refactor the following code:', 'input': 'public void testInvokeProcessElementTimeBounded() throws Exception { SplittableProcessElementInvoker<Void, String, OffsetRange, Long>.Result res = runTest(10000, Duration.ZERO, Integer.MAX_VALUE, Duration.millis(100)); assertFalse(res.getContinuation().shouldResume()); OffsetRange residualRange = res.getResidualRestriction(); assertThat(residualRange.getFrom(), greaterThan(10L)); assertThat(residualRange.getFrom(), lessThan(100L)); assertEquals(10000, residualRange.getTo()); } public void testInvokeProcessElementTimeBoundedWithStartupDelay() throws Exception { SplittableProcessElementInvoker<Void, String, OffsetRange, Long>.Result res = runTest(10000, Duration.standardSeconds(3), Integer.MAX_VALUE, Duration.millis(100)); assertFalse(res.getContinuation().shouldResume()); OffsetRange residualRange = res.getResidualRestriction(); assertThat(residualRange.getFrom(), greaterThan(10L)); assertThat(residualRange.getFrom(), lessThan(100L)); assertEquals(10000, residualRange.getTo()); }', 'ground_truth': 'public void testInvokeProcessElementTimeBounded() throws Exception { SplittableProcessElementInvoker<Void, String, OffsetRange, OffsetRangeTracker>.Result res = runTest(10000, Duration.ZERO, Integer.MAX_VALUE, Duration.millis(100)); assertFalse(res.getContinuation().shouldResume()); OffsetRange residualRange = res.getResidualRestriction(); assertThat(residualRange.getFrom(), greaterThan(10L)); assertThat(residualRange.getFrom(), lessThan(100L)); assertEquals(10000, residualRange.getTo()); }', 'output': 'public void testInvokeProcessElementTimeBounded() throws Exception { SplittableProcessElementInvoker.Result res = runTest(10000, Duration.ZERO, Integer.MAX_VALUE, Duration.millis(100)); assertFalse(res.getContinuation().shouldResume()); OffsetRange residualRange = res.getResidualRestriction(); assertThat(residualRange.getFrom(), greaterThan(10L)); assertThat(residualRange.getFrom(), lessThan(100L)); assertEquals(10000, residualRange.getTo()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testRemoteNodeEventStorage() throws Exception { IgnitePredicate<IgniteEvent> filter = new TestEventFilter(); generateEvents(ignite2); ClusterGroup prj = ignite1.cluster().forPredicate(F.remoteNodes(ignite1.cluster().localNode().id())); XXX evts = events(prj).remoteQuery(filter, 0); assert evts != null; assert evts.size() == 1; }', 'ground_truth': 'public void testRemoteNodeEventStorage() throws Exception { IgnitePredicate<Event> filter = new TestEventFilter(); generateEvents(ignite2); ClusterGroup prj = ignite1.cluster().forPredicate(F.remoteNodes(ignite1.cluster().localNode().id())); Collection<Event> evts = events(prj).remoteQuery(filter, 0); assert evts != null; assert evts.size() == 1; }', 'output': 'public void testRemoteNodeEventStorage() throws Exception { IgnitePredicate<IgniteEvent> filter = new TestEventFilter(); generateEvents(ignite2); ClusterGroup prj = ignite1.cluster().forPredicate(F.remoteNodes(ignite1.cluster().localNode().id())); List<IgniteEvent> evts = events(prj).remoteQuery(filter, 0); assert evts != null; assert evts.size() == 1; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private TreeElement getRootElement() { if (doc == null) { return null; } XXX it; it = doc.getChildNodes().iterator(); while (it.hasNext()) { Object next = it.next(); if (next instanceof TreeElement) { return (TreeElement) next; } } return null; }', 'ground_truth': 'private TreeElement getRootElement() { if (doc == null) { return null; } Iterator<?> it; it = doc.getChildNodes().iterator(); while (it.hasNext()) { Object next = it.next(); if (next instanceof TreeElement) { return (TreeElement) next; } } return null; }', 'output': 'private TreeElement getRootElement() { if (doc == null) { return null; } Iterator<Object> it = doc.getChildNodes().iterator(); while (it.hasNext()) { Object next = it.next(); if (next instanceof TreeElement) { return (TreeElement) next; } } return null; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGrowthTrackerOutputFullyBeforeCheckpointIncomplete() { Instant now = Instant.now(); GrowthTracker<String, Integer> tracker = newTracker(); tracker.addNewAsPending( PollResult.incomplete( Arrays.asList( TimestampedValue.of("d", now.plus(standardSeconds(4))), TimestampedValue.of("c", now.plus(standardSeconds(3))), TimestampedValue.of("a", now.plus(standardSeconds(1))), TimestampedValue.of("b", now.plus(standardSeconds(2))))) .withWatermark(now.plus(standardSeconds(7)))); assertEquals("a", tracker.tryClaimNextPending().getValue()); assertEquals("b", tracker.tryClaimNextPending().getValue()); assertEquals("c", tracker.tryClaimNextPending().getValue()); assertEquals("d", tracker.tryClaimNextPending().getValue()); assertFalse(tracker.hasPending()); assertEquals(now.plus(standardSeconds(7)), tracker.getWatermark()); GrowthTracker<String, Integer> residualTracker = newTracker(tracker.checkpoint()); GrowthTracker<String, Integer> primaryTracker = newTracker(tracker.currentRestriction()); assertEquals(now.plus(standardSeconds(1)), primaryTracker.getWatermark()); assertTrue(primaryTracker.hasPending()); assertEquals("a", primaryTracker.tryClaimNextPending().getValue()); assertTrue(primaryTracker.hasPending()); assertEquals("b", primaryTracker.tryClaimNextPending().getValue()); assertTrue(primaryTracker.hasPending()); assertEquals("c", primaryTracker.tryClaimNextPending().getValue()); assertTrue(primaryTracker.hasPending()); assertEquals("d", primaryTracker.tryClaimNextPending().getValue()); assertFalse(primaryTracker.hasPending()); assertFalse(primaryTracker.shouldPollMore()); primaryTracker.checkDone(); assertEquals(BoundedWindow.TIMESTAMP_MAX_VALUE, primaryTracker.getWatermark()); assertFalse(residualTracker.hasPending()); assertTrue(residualTracker.shouldPollMore()); assertEquals(now.plus(standardSeconds(7)), residualTracker.getWatermark()); tracker.checkDone(); assertFalse(tracker.hasPending()); assertFalse(tracker.shouldPollMore()); assertEquals(BoundedWindow.TIMESTAMP_MAX_VALUE, tracker.getWatermark()); }', 'ground_truth': 'public void testGrowthTrackerOutputFullyBeforeCheckpointIncomplete() { Instant now = Instant.now(); GrowthTracker<String, String, Integer> tracker = newTracker(); tracker.addNewAsPending( PollResult.incomplete( Arrays.asList( TimestampedValue.of("d", now.plus(standardSeconds(4))), TimestampedValue.of("c", now.plus(standardSeconds(3))), TimestampedValue.of("a", now.plus(standardSeconds(1))), TimestampedValue.of("b", now.plus(standardSeconds(2))))) .withWatermark(now.plus(standardSeconds(7)))); assertEquals("a", tracker.tryClaimNextPending().getValue()); assertEquals("b", tracker.tryClaimNextPending().getValue()); assertEquals("c", tracker.tryClaimNextPending().getValue()); assertEquals("d", tracker.tryClaimNextPending().getValue()); assertFalse(tracker.hasPending()); assertEquals(now.plus(standardSeconds(7)), tracker.getWatermark()); GrowthTracker<String, String, Integer> residualTracker = newTracker(tracker.checkpoint()); GrowthTracker<String, String, Integer> primaryTracker = newTracker(tracker.currentRestriction()); assertEquals(now.plus(standardSeconds(1)), primaryTracker.getWatermark()); assertTrue(primaryTracker.hasPending()); assertEquals("a", primaryTracker.tryClaimNextPending().getValue()); assertTrue(primaryTracker.hasPending()); assertEquals("b", primaryTracker.tryClaimNextPending().getValue()); assertTrue(primaryTracker.hasPending()); assertEquals("c", primaryTracker.tryClaimNextPending().getValue()); assertTrue(primaryTracker.hasPending()); assertEquals("d", primaryTracker.tryClaimNextPending().getValue()); assertFalse(primaryTracker.hasPending()); assertFalse(primaryTracker.shouldPollMore()); primaryTracker.checkDone(); assertEquals(BoundedWindow.TIMESTAMP_MAX_VALUE, primaryTracker.getWatermark()); assertFalse(residualTracker.hasPending()); assertTrue(residualTracker.shouldPollMore()); assertEquals(now.plus(standardSeconds(7)), residualTracker.getWatermark()); tracker.checkDone(); assertFalse(tracker.hasPending()); assertFalse(tracker.shouldPollMore()); assertEquals(BoundedWindow.TIMESTAMP_MAX_VALUE, tracker.getWatermark()); }', 'output': 'public void testGrowthTrackerOutputFullyBeforeCheckpointIncomplete() { Instant now = Instant.now(); GrowthTracker<String, Integer> tracker = newTracker(); tracker.addNewAsPending( PollResult.incomplete( Arrays.asList( TimestampedValue.of("d", now.plus(standardSeconds(4))), TimestampedValue.of("c", now.plus(standardSeconds(3))), TimestampedValue.of("a", now.plus(standardSeconds(1))), TimestampedValue.of("b", now.plus(standardSeconds(2))))) .withWatermark(now.plus(standardSeconds(7)))); assertEquals("a", tracker.tryClaimNextPending().getValue()); assertEquals("b", tracker.tryClaimNextPending().getValue()); assertEquals("c", tracker.tryClaimNextPending().getValue()); assertEquals("d", tracker.tryClaimNextPending().getValue()); assertFalse(tracker.hasPending()); assertEquals(now.plus(standardSeconds(7)), tracker.getWatermark()); GrowthTracker<String, Integer> residualTracker = newTracker(tracker.checkpoint()); GrowthTracker<String, Integer> primaryTracker = newTracker(tracker.currentRestriction()); assertEquals(now.plus(standardSeconds(1)), primaryTracker.getWatermark()); assertTrue(primaryTracker.hasPending()); assertEquals("a", primaryTracker.tryClaimNextPending().getValue()); assertTrue(primaryTracker.hasPending()); assertEquals("b", primaryTracker.tryClaimNextPending().getValue()); assertTrue(primaryTracker.hasPending()); assertEquals("c", primaryTracker.tryClaimNextPending().getValue()); assertTrue(primaryTracker.hasPending()); assertEquals("d", primaryTracker.tryClaimNextPending().getValue()); assertFalse(primaryTracker.hasPending()); assertFalse(primaryTracker.shouldPollMore()); primaryTracker.checkDone(); assertEquals(BoundedWindow.TIMESTAMP_MAX_VALUE, primaryTracker.getWatermark()); assertFalse(residualTracker.has'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCancel() throws Exception { Ignite ignite = G.grid(getTestGridName()); ignite.compute().localDeployTask(GridCancelTestTask.class, U.detectClassLoader(GridCancelTestTask.class)); XXX fut = executeAsync(ignite.compute(), GridCancelTestTask.class.getName(), null); boolean await = startSignal.await(WAIT_TIME, TimeUnit.MILLISECONDS); assert await : "Jobs did not start."; info("Test task result: " + fut); assert fut != null; Object res = fut.get(); assert (Integer)res == 1; await = stopSignal.await(WAIT_TIME, TimeUnit.MILLISECONDS); assert await : "Jobs did not stop."; assert (cancelCnt + processedCnt) >= SPLIT_COUNT && (cancelCnt + processedCnt) <= (SPLIT_COUNT - 1) * 2 +1 : "Invalid cancel count value: " + cancelCnt; }', 'ground_truth': 'public void testCancel() throws Exception { Ignite ignite = G.grid(getTestGridName()); ignite.compute().localDeployTask(GridCancelTestTask.class, U.detectClassLoader(GridCancelTestTask.class)); ComputeTaskFuture<?> fut = executeAsync(ignite.compute(), GridCancelTestTask.class.getName(), null); boolean await = startSignal.await(WAIT_TIME, TimeUnit.MILLISECONDS); assert await : "Jobs did not start."; info("Test task result: " + fut); assert fut != null; Object res = fut.get(); assert (Integer)res == 1; await = stopSignal.await(WAIT_TIME, TimeUnit.MILLISECONDS); assert await : "Jobs did not stop."; assert (cancelCnt + processedCnt) >= SPLIT_COUNT && (cancelCnt + processedCnt) <= (SPLIT_COUNT - 1) * 2 +1 : "Invalid cancel count value: " + cancelCnt; }', 'output': 'public void testCancel() throws Exception { Ignite ignite = G.grid(getTestGridName()); ignite.compute().localDeployTask(GridCancelTestTask.class, U.detectClassLoader(GridCancelTestTask.class)); IgniteFuture<?> fut = executeAsync(ignite.compute(), GridCancelTestTask.class.getName(), null); boolean await = startSignal.await(WAIT_TIME, TimeUnit.MILLISECONDS); assert await : "Jobs did not start."; info("Test task result: " + fut); assert fut != null; Object res = fut.get(); assert (Integer)res == 1; await = stopSignal.await(WAIT_TIME, TimeUnit.MILLISECONDS); assert await : "Jobs did not stop."; assert (cancelCnt + processedCnt) >= SPLIT_COUNT && (cancelCnt + processedCnt) <= (SPLIT_COUNT - 1) * 2 +1 : "Invalid cancel count value: " + cancelCnt; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkInverses(CFMetaData cfm) throws Exception { DecoratedKey k = StorageService.getPartitioner().decorateKey(ByteBufferUtil.bytes(cfm.ksName)); XXX keyspace = Schema.instance.getKSMetaData(cfm.ksName); CFMetaData before = cfm; CFMetaData after = ThriftConversion.fromThriftForUpdate(ThriftConversion.toThrift(before), before); assert before.equals(after) : String.format("%n%s%n!=%n%s", before, after); Mutation rm = LegacySchemaTables.makeCreateTableMutation(keyspace, cfm, FBUtilities.timestampMicros()); PartitionUpdate serializedCf = rm.getPartitionUpdate(Schema.instance.getId(SystemKeyspace.NAME, LegacySchemaTables.COLUMNFAMILIES)); PartitionUpdate serializedCD = rm.getPartitionUpdate(Schema.instance.getId(SystemKeyspace.NAME, LegacySchemaTables.COLUMNS)); CFMetaData newCfm = LegacySchemaTables.createTableFromTablePartitionAndColumnsPartition(UnfilteredRowIterators.filter(serializedCf.unfilteredIterator(), FBUtilities.nowInSeconds()), UnfilteredRowIterators.filter(serializedCD.unfilteredIterator(), FBUtilities.nowInSeconds())); assert cfm.equals(newCfm) : String.format("%n%s%n!=%n%s", cfm, newCfm); }', 'ground_truth': 'private void checkInverses(CFMetaData cfm) throws Exception { DecoratedKey k = StorageService.getPartitioner().decorateKey(ByteBufferUtil.bytes(cfm.ksName)); KeyspaceMetadata keyspace = Schema.instance.getKSMetaData(cfm.ksName); CFMetaData before = cfm; CFMetaData after = ThriftConversion.fromThriftForUpdate(ThriftConversion.toThrift(before), before); assert before.equals(after) : String.format("%n%s%n!=%n%s", before, after); Mutation rm = LegacySchemaTables.makeCreateTableMutation(keyspace, cfm, FBUtilities.timestampMicros()); PartitionUpdate serializedCf = rm.getPartitionUpdate(Schema.instance.getId(SystemKeyspace.NAME, LegacySchemaTables.COLUMNFAMILIES)); PartitionUpdate serializedCD = rm.getPartitionUpdate(Schema.instance.getId(SystemKeyspace.NAME, LegacySchemaTables.COLUMNS)); CFMetaData newCfm = LegacySchemaTables.createTableFromTablePartitionAndColumnsPartition(UnfilteredRowIterators.filter(serializedCf.unfilteredIterator(), FBUtilities.nowInSeconds()), UnfilteredRowIterators.filter(serializedCD.unfilteredIterator(), FBUtilities.nowInSeconds())); assert cfm.equals(newCfm) : String.format("%n%s%n!=%n%s", cfm, newCfm); }', 'output': 'private void checkInverses(CFMetaData cfm) throws Exception { DecoratedKey k = StorageService.getPartitioner().decorateKey(ByteBufferUtil.bytes(cfm.ksName)); Keyspace keyspace = Schema.instance.getKSMetaData(cfm.ksName); CFMetaData before = cfm; CFMetaData after = ThriftConversion.fromThriftForUpdate(ThriftConversion.toThrift(before), before); assert before.equals(after) : String.format("%n%s%n!=%n%s", before, after); Mutation rm = LegacySchemaTables.makeCreateTableMutation(keyspace, cfm, FBUtilities.timestampMicros()); PartitionUpdate serializedCf = rm.getPartitionUpdate(Schema.instance.getId(SystemKeyspace.NAME, LegacySchemaTables.COLUMNFAMILIES)); PartitionUpdate serializedCD = rm.getPartitionUpdate(Schema.instance.getId(SystemKeyspace.NAME, LegacySchemaTables.COLUMNS)); CFMetaData newCfm = LegacySchemaTables.createTableFromTablePartitionAndColumnsPartition(UnfilteredRowIterators.filter(serializedCf.unfilteredIterator(), FBUtilities.nowInSeconds()), UnfilteredRowIterators.filter(serializedCD.unfilteredIterator(), FBUtilities.nowInSeconds())); assert cfm.equals(newCfm) : String.format("%n%s%n!=%n%s", cfm, newCfm); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCreateFileColocated() throws Exception { IgfsPath path = new IgfsPath("/colocated"); UUID uuid = UUID.randomUUID(); IgniteUuid affKey; long idx = 0; while (true) { affKey = new IgniteUuid(uuid, idx); if (grid(0).mapKeyToNode(DATA_CACHE_NAME, affKey).id().equals(grid(0).localNode().id())) break; idx++; } try (XXX out = fs.create(path, 1024, true, affKey, 0, 1024, null)) { for (int i = 0; i < 15; i++) out.write(new byte[1024 * 1024]); } IgniteFsFile info = fs.info(path); Collection<IgniteFsBlockLocation> affNodes = fs.affinity(path, 0, info.length()); assertEquals(1, affNodes.size()); Collection<UUID> nodeIds = F.first(affNodes).nodeIds(); assertEquals(1, nodeIds.size()); assertEquals(grid(0).localNode().id(), F.first(nodeIds)); }', 'ground_truth': 'public void testCreateFileColocated() throws Exception { IgfsPath path = new IgfsPath("/colocated"); UUID uuid = UUID.randomUUID(); IgniteUuid affKey; long idx = 0; while (true) { affKey = new IgniteUuid(uuid, idx); if (grid(0).mapKeyToNode(DATA_CACHE_NAME, affKey).id().equals(grid(0).localNode().id())) break; idx++; } try (IgfsOutputStream out = fs.create(path, 1024, true, affKey, 0, 1024, null)) { for (int i = 0; i < 15; i++) out.write(new byte[1024 * 1024]); } IgfsFile info = fs.info(path); Collection<IgfsBlockLocation> affNodes = fs.affinity(path, 0, info.length()); assertEquals(1, affNodes.size()); Collection<UUID> nodeIds = F.first(affNodes).nodeIds(); assertEquals(1, nodeIds.size()); assertEquals(grid(0).localNode().id(), F.first(nodeIds)); }', 'output': 'public void testCreateFileColocated() throws Exception { IgfsPath path = new IgfsPath("/colocated"); UUID uuid = UUID.randomUUID(); IgniteUuid affKey; long idx = 0; while (true) { affKey = new IgniteUuid(uuid, idx); if (grid(0).mapKeyToNode(DATA_CACHE_NAME, affKey).id().equals(grid(0).localNode().id())) break; idx++; } try (IgfsOutputStream out = fs.create(path, 1024, true, affKey, 0, 1024, null)) { for (int i = 0; i < 15; i++) out.write(new byte[1024 * 1024]); } IgniteFsFile info = fs.info(path); Collection<IgniteFsBlockLocation> affNodes = fs.affinity(path, 0, info.length()); assertEquals(1, affNodes.size()); Collection<UUID> nodeIds = F.first(affNodes).nodeIds(); assertEquals(1, nodeIds.size()); assertEquals(grid(0).localNode().id(), F.first(nodeIds)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void execute(Ignite node, Consumer<IgniteScheduler> consumer, boolean isForbiddenCase) { XXX r = () -> node.compute().run(() -> { error = null; latch = new CountDownLatch(1); consumer.accept(Ignition.localIgnite().scheduler()); try { latch.await(10, TimeUnit.SECONDS); } catch (InterruptedException e) { throw new RuntimeException(e); } if (error != null) throw error; }); if (isForbiddenCase) runForbiddenOperation(r, AccessControlException.class); else runOperation(r); }', 'ground_truth': 'private void execute(Ignite node, Consumer<IgniteScheduler> consumer, boolean isForbiddenCase) { RunnableX r = () -> node.compute().run(() -> { error = null; latch = new CountDownLatch(1); consumer.accept(Ignition.localIgnite().scheduler()); try { latch.await(10, TimeUnit.SECONDS); } catch (InterruptedException e) { throw new RuntimeException(e); } if (error != null) throw error; }); if (isForbiddenCase) runForbiddenOperation(r, AccessControlException.class); else runOperation(r); }', 'output': 'private void execute(Ignite node, Consumer<IgniteScheduler> consumer, boolean isForbiddenCase) { IgniteFuture<Void> r = node.compute().runAsync(() -> { error = null; latch = new CountDownLatch(1); consumer.accept(Ignition.localIgnite().scheduler()); try { latch.await(10, TimeUnit.SECONDS); } catch (InterruptedException e) { throw new RuntimeException(e); } if (error != null) throw error; }); if (isForbiddenCase) runForbiddenOperation(r, AccessControlException.class); else runOperation(r); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testExceptAll() throws Exception { String sql = ""; sql += "SELECT order_id, site_id, price " + "FROM ORDER_DETAILS1 " + " EXCEPT ALL " + "SELECT order_id, site_id, price " + "FROM ORDER_DETAILS2 "; XXX rows = compilePipeline(sql, pipeline, sqlEnv); PAssert.that(rows).satisfies(new CheckSize(2)); PAssert.that(rows).containsInAnyOrder( TestUtils.RowsBuilder.of( SqlTypeCoders.BIGINT, "order_id", SqlTypeCoders.INTEGER, "site_id", SqlTypeCoders.DOUBLE, "price" ).addRows( 4L, 4, 4.0, 4L, 4, 4.0 ).getRows()); pipeline.run(); }', 'ground_truth': 'public void testExceptAll() throws Exception { String sql = ""; sql += "SELECT order_id, site_id, price " + "FROM ORDER_DETAILS1 " + " EXCEPT ALL " + "SELECT order_id, site_id, price " + "FROM ORDER_DETAILS2 "; PCollection<Row> rows = compilePipeline(sql, pipeline, sqlEnv); PAssert.that(rows).satisfies(new CheckSize(2)); PAssert.that(rows).containsInAnyOrder( TestUtils.RowsBuilder.of( SqlTypeCoders.BIGINT, "order_id", SqlTypeCoders.INTEGER, "site_id", SqlTypeCoders.DOUBLE, "price" ).addRows( 4L, 4, 4.0, 4L, 4, 4.0 ).getRows()); pipeline.run(); }', 'output': 'public void testExceptAll() throws Exception { String sql = ""; sql += "SELECT order_id, site_id, price " + "FROM ORDER_DETAILS1 " + " EXCEPT ALL " + "SELECT order_id, site_id, price " + "FROM ORDER_DETAILS2 "; List<Row> rows = compilePipeline(sql, pipeline, sqlEnv); PAssert.that(rows).satisfies(new CheckSize(2)); PAssert.that(rows).containsInAnyOrder( TestUtils.RowsBuilder.of( SqlTypeCoders.BIGINT, "order_id", SqlTypeCoders.INTEGER, "site_id", SqlTypeCoders.DOUBLE, "price" ).addRows( 4L, 4, 4.0, 4L, 4, 4.0 ).getRows()); pipeline.run(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static FlinkJobServerDriver fromParams(String[] args) { XXX configuration = new ServerConfiguration(); CmdLineParser parser = new CmdLineParser(configuration); try { parser.parseArgument(args); } catch (CmdLineException e) { LOG.error("Unable to parse command line arguments.", e); printUsage(parser); throw new IllegalArgumentException("Unable to parse command line arguments.", e); } return fromConfig(configuration); }', 'ground_truth': 'public static FlinkJobServerDriver fromParams(String[] args) { FlinkServerConfiguration configuration = new FlinkServerConfiguration(); CmdLineParser parser = new CmdLineParser(configuration); try { parser.parseArgument(args); } catch (CmdLineException e) { LOG.error("Unable to parse command line arguments.", e); printUsage(parser); throw new IllegalArgumentException("Unable to parse command line arguments.", e); } return fromConfig(configuration); }', 'output': 'public static FlinkJobServerDriver fromParams(String[] args) { Configuration configuration = new ServerConfiguration(); CmdLineParser parser = new CmdLineParser(configuration); try { parser.parseArgument(args); } catch (CmdLineException e) { LOG.error("Unable to parse command line arguments.", e); printUsage(parser); throw new IllegalArgumentException("Unable to parse command line arguments.", e); } return fromConfig(configuration); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void passThisReference(ConstructorCallExpression call) { ClassNode cn = call.getType().redirect(); if(!shouldHandleImplicitThisForInnerClass(cn)) return; boolean isInStaticContext = true; if(currentMethod != null) isInStaticContext = currentMethod.getVariableScope().isInStaticContext(); else if(currentField != null) isInStaticContext = currentField.isStatic(); if(isInStaticContext) return; if(classNode != ((InnerClassNode) cn).getOuterClass()) return; Expression argsExp = call.getArguments(); if(argsExp instanceof ArgumentListExpression) { XXX argsListExp = (ArgumentListExpression) argsExp; argsListExp.getExpressions().add(0, VariableExpression.THIS_EXPRESSION); } }', 'ground_truth': 'private void passThisReference(ConstructorCallExpression call) { ClassNode cn = call.getType().redirect(); if(!shouldHandleImplicitThisForInnerClass(cn)) return; boolean isInStaticContext = true; if(currentMethod != null) isInStaticContext = currentMethod.getVariableScope().isInStaticContext(); else if(currentField != null) isInStaticContext = currentField.isStatic(); if(isInStaticContext) return; if(classNode != ((InnerClassNode) cn).getOuterClass()) return; Expression argsExp = call.getArguments(); if(argsExp instanceof TupleExpression) { TupleExpression argsListExp = (TupleExpression) argsExp; argsListExp.getExpressions().add(0, VariableExpression.THIS_EXPRESSION); } }', 'output': 'private void passThisReference(ConstructorCallExpression call) { ClassNode cn = call.getType().redirect(); if(shouldHandleImplicitThisForInnerClass(cn)) return; boolean isInStaticContext = true; if(currentMethod != null) isInStaticContext = currentMethod.getVariableScope().isInStaticContext(); else if(currentField != null) isInStaticContext = currentField.isStatic(); if(isInStaticContext) return; if(classNode != ((InnerClassNode) cn).getOuterClass()) return; Expression argsExp = call.getArguments(); if(argsExp instanceof ArgumentListExpression) { ArgumentListExpression argsListExp = (ArgumentListExpression) argsExp; argsListExp.getExpressions().add(0, VariableExpression.THIS_EXPRESSION); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public PortableSchemaRegistry schemaRegistry(int typeId) { Map<Integer, PortableSchemaRegistry> schemas0 = schemas; if (schemas0 == null) { synchronized (this) { schemas0 = schemas; if (schemas0 == null) { schemas0 = new HashMap<>(); XXX reg = new PortableSchemaRegistry(); schemas0.put(typeId, reg); schemas = schemas0; return reg; } } } XXX reg = schemas0.get(typeId); if (reg == null) { synchronized (this) { reg = schemas.get(typeId); if (reg == null) { reg = new PortableSchemaRegistry(); schemas0 = new HashMap<>(schemas); schemas0.put(typeId, reg); schemas = schemas0; } } } return reg; }', 'ground_truth': 'public BinarySchemaRegistry schemaRegistry(int typeId) { Map<Integer, BinarySchemaRegistry> schemas0 = schemas; if (schemas0 == null) { synchronized (this) { schemas0 = schemas; if (schemas0 == null) { schemas0 = new HashMap<>(); BinarySchemaRegistry reg = new BinarySchemaRegistry(); schemas0.put(typeId, reg); schemas = schemas0; return reg; } } } BinarySchemaRegistry reg = schemas0.get(typeId); if (reg == null) { synchronized (this) { reg = schemas.get(typeId); if (reg == null) { reg = new BinarySchemaRegistry(); schemas0 = new HashMap<>(schemas); schemas0.put(typeId, reg); schemas = schemas0; } } } return reg; }', 'output': 'public PortableSchemaRegistry schemaRegistry(int typeId) { Map<Integer, PortableSchemaRegistry> schemas0 = schemas; if (schemas0 == null) { synchronized (this) { schemas0 = schemas; if (schemas0 == null) { schemas0 = new HashMap<>(); registry = new PortableSchemaRegistry(); schemas0.put(typeId, registry); schemas = schemas0; return registry; } } } registry = schemas0.get(typeId); if (registry == null) { synchronized (this) { registry = schemas.get(typeId); if (registry == null) { registry = new PortableSchemaRegistry(); schemas0 = new HashMap<>(schemas); schemas0.put(typeId, registry); schemas = schemas0; } } } return registry; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void remove(int voffset, int vlength) { int offset = getDocumentOffset(voffset); if (offset < 0) { return; } int length = computeLength(voffset, vlength); if (length <= 0) { return; } XXX doc = getDocument(); try { if(doc.getText(offset + offsetDiff, length).contains("\\n")) { LOGGER.log(Level.WARNING, "Tried to remove EOL"); } if (SAFE_DELETE_PATTERN.matcher(doc.getText(offset + offsetDiff, length)).matches()) { doc.remove(offset + offsetDiff, length); setOffsetDiff(offsetDiff - length); } else { LOGGER.log(Level.WARNING, "Tried to remove non empty text: {0}", doc.getText(offset + offsetDiff, length)); } } catch (BadLocationException ex) { LOGGER.log(Level.INFO, null, ex); } }', 'ground_truth': 'public void remove(int voffset, int vlength) { int offset = getDocumentOffset(voffset); if (offset < 0) { return; } int length = computeLength(voffset, vlength); if (length <= 0) { return; } Document doc = getDocument(); try { if(doc.getText(offset + offsetDiff, length).contains("\\n")) { LOGGER.log(Level.WARNING, "Tried to remove EOL"); } if (SAFE_DELETE_PATTERN.matcher(doc.getText(offset + offsetDiff, length)).matches()) { doc.remove(offset + offsetDiff, length); setOffsetDiff(offsetDiff - length); } else { LOGGER.log(Level.WARNING, "Tried to remove non empty text: {0}", doc.getText(offset + offsetDiff, length)); } } catch (BadLocationException ex) { LOGGER.log(Level.INFO, null, ex); } }', 'output': 'public void remove(int voffset, int vlength) { int offset = getDocumentOffset(voffset); if (offset < 0) { return; } int length = computeLength(voffset, vlength); if (length <= 0) { return; } Document doc = getDocument(); try { if(doc.getText(offset + offsetDiff, length).contains("\\n")) { LOGGER.log(Level.WARNING, "Tried to remove EOL"); } if (SAFE_DELETE_PATTERN.matcher(doc.getText(offset + offsetDiff, length)).matches()) { doc.remove(offset + offsetDiff, length); setOffsetDiff(offsetDiff - length); } else { LOGGER.log(Level.WARNING, "Tried to remove non empty text: {0}", doc.getText(offset + offsetDiff, length)); } } catch (BadLocationException ex) { LOGGER.log(Level.INFO, null, ex); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static List<Interval<RowPosition, SSTableReader>> buildIntervals(Iterable<SSTableReader> sstables) { List<Interval<RowPosition, SSTableReader>> intervals = new ArrayList<>(Iterables.size(sstables)); for (SSTableReader sstable : sstables) intervals.add(Interval.<RowPosition, SSTableReader>create(sstable.first, sstable.last, sstable)); return intervals; }', 'ground_truth': 'public static List<Interval<PartitionPosition, SSTableReader>> buildIntervals(Iterable<SSTableReader> sstables) { List<Interval<PartitionPosition, SSTableReader>> intervals = new ArrayList<>(Iterables.size(sstables)); for (SSTableReader sstable : sstables) intervals.add(Interval.<PartitionPosition, SSTableReader>create(sstable.first, sstable.last, sstable)); return intervals; }', 'output': 'public static Iterable<Interval<RowPosition, SSTableReader>> buildIntervals(Iterable<SSTableReader> sstables) { Iterable<Interval<RowPosition, SSTableReader>> intervals = new ArrayList<>(Iterables.size(sstables)); for (SSTableReader sstable : sstables) intervals.add(Interval.<RowPosition, SSTableReader>create(sstable.first, sstable.last, sstable)); return intervals; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPaginationIteratorKeepAll() throws Exception { GridCache<Integer, Integer> cache = ignite.cache(null); for (int i = 0; i < 50; i++) assertTrue(cache.putx(i, i)); GridCacheQuery<Map.Entry<Integer, Integer>> q = cache.queries().createSqlQuery(Integer.class, "_key >= 0"); q.pageSize(10); q.enableDedup(true); q.keepAll(true); GridCacheQueryFuture<Map.Entry<Integer, Integer>> f = q.execute(); int cnt = 0; Map.Entry<Integer, Integer> e; while ((e = f.next()) != null) { assertTrue(e.getKey() >= 0 && e.getKey() < 50); assertTrue(e.getValue() >= 0 && e.getValue() < 50); cnt++; } assertEquals(50, cnt); assertTrue(f.isDone()); List<Map.Entry<Integer, Integer>> list = new ArrayList<>(f.get()); Collections.sort(list, new Comparator<Map.Entry<Integer, Integer>>() { @Override public int compare(Map.Entry<Integer, Integer> e1, Map.Entry<Integer, Integer> e2) { return e1.getKey().compareTo(e2.getKey()); } }); for (int i = 0; i < 50; i++) { Map.Entry<Integer, Integer> e0 = list.get(i); assertEquals(i, (int)e0.getKey()); assertEquals(i, (int)e0.getValue()); } }', 'ground_truth': 'public void testPaginationIteratorKeepAll() throws Exception { Cache<Integer, Integer> cache = ignite.cache(null); for (int i = 0; i < 50; i++) assertTrue(cache.putx(i, i)); CacheQuery<Map.Entry<Integer, Integer>> q = cache.queries().createSqlQuery(Integer.class, "_key >= 0"); q.pageSize(10); q.enableDedup(true); q.keepAll(true); CacheQueryFuture<Map.Entry<Integer, Integer>> f = q.execute(); int cnt = 0; Map.Entry<Integer, Integer> e; while ((e = f.next()) != null) { assertTrue(e.getKey() >= 0 && e.getKey() < 50); assertTrue(e.getValue() >= 0 && e.getValue() < 50); cnt++; } assertEquals(50, cnt); assertTrue(f.isDone()); List<Map.Entry<Integer, Integer>> list = new ArrayList<>(f.get()); Collections.sort(list, new Comparator<Map.Entry<Integer, Integer>>() { @Override public int compare(Map.Entry<Integer, Integer> e1, Map.Entry<Integer, Integer> e2) { return e1.getKey().compareTo(e2.getKey()); } }); for (int i = 0; i < 50; i++) { Map.Entry<Integer, Integer> e0 = list.get(i); assertEquals(i, (int)e0.getKey()); assertEquals(i, (int)e0.getValue()); } }', 'output': 'public void testPaginationIteratorKeepAll() throws Exception { IgniteCache<Integer, Integer> cache = ignite.jcache(null); for (int i = 0; i < 50; i++) assertTrue(cache.putx(i, i)); GridCacheQuery<Map.Entry<Integer, Integer>> q = cache.queries().createSqlQuery(Integer.class, "_key >= 0"); q.pageSize(10); q.enableDedup(true); q.keepAll(true); GridCacheQueryFuture<Map.Entry<Integer, Integer>> f = q.execute(); int cnt = 0; Map.Entry<Integer, Integer> e; while ((e = f.next()) != null) { assertTrue(e.getKey() >= 0 && e.getKey() < 50); assertTrue(e.getValue() >= 0 && e.getValue() < 50); cnt++; } assertEquals(50, cnt); assertTrue(f.isDone()); List<Map.Entry<Integer, Integer>> list = new ArrayList<>(f.get()); Collections.sort(list, new Comparator<Map.Entry<Integer, Integer>>() { @Override public int compare(Map.Entry<Integer, Integer> e1, Map.Entry<Integer, Integer> e2) { return e1.getKey().compareTo(e2.getKey()); } }); for (int i = 0; i < 50; i++) { Map.Entry<Integer, Integer> e0 = list.get(i); assertEquals(i, (int)e0.getKey()); assertEquals(i, (int)e0.getValue()); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPageTearingInner() throws Exception { PageMemory mem = memory(); mem.start(); try { FullPageId fullId1 = allocatePage(mem); FullPageId fullId2 = allocatePage(mem); Page page1 = mem.page(fullId1.cacheId(), fullId1.pageId()); try { XXX page2 = mem.page(fullId2.cacheId(), fullId2.pageId()); info("Allocated pages [page1=" + page1 + ", page2=" + page2 + \']\'); try { writePage(page1, 1); writePage(page2, 2); readPage(page1, 1); readPage(page2, 2); readPage(page1, 1); readPage(page2, 2); } finally { mem.releasePage(page2); } } finally { mem.releasePage(page1); } } finally { mem.stop(); } }', 'ground_truth': 'public void testPageTearingInner() throws Exception { PageMemory mem = memory(); mem.start(); try { FullPageId fullId1 = allocatePage(mem); FullPageId fullId2 = allocatePage(mem); long page1 = mem.acquirePage(fullId1.cacheId(), fullId1.pageId()); try { long page2 = mem.acquirePage(fullId2.cacheId(), fullId2.pageId()); info("Allocated pages [page1Id=" + fullId1.pageId() + ", page1=" + page1 + ", page2Id=" + fullId2.pageId() + ", page2=" + page2 + \']\'); try { writePage(mem, fullId1.pageId(), page1, 1); writePage(mem, fullId2.pageId(), page2, 2); readPage(mem, fullId1.pageId(), page1, 1); readPage(mem, fullId2.pageId(), page2, 2); readPage(mem, fullId1.pageId(), page1, 1); readPage(mem, fullId2.pageId(), page2, 2); } finally { mem.releasePage(fullId2.cacheId(), fullId2.pageId(), page2); } } finally { mem.releasePage(fullId1.cacheId(), fullId1.pageId(), page1); } } finally { mem.stop(); } }', 'output': 'public void testPageTearingInner() throws Exception { PageMemory mem = memory(); mem.start(); try { FullPageId fullId1 = allocatePage(mem); FullPageId fullId2 = allocatePage(mem); Page page1 = mem.page(fullId1.cacheId(), fullId1.pageId()); try { Page page2 = mem.page(fullId2.cacheId(), fullId2.pageId()); info("Allocated pages [page1=" + page1 + ", page2=" + page2 + \']\'); try { writePage(page1, 1); writePage(page2, 2); readPage(page1, 1); readPage(page2, 2); readPage(page1, 1); readPage(page2, 2); } finally { mem.releasePage(page2); } } finally { mem.releasePage(page1); } } finally { mem.stop(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public IndexLookupBatch createLookupBatch(TableFilter[] filters, int filter) { IndexLookupBatch batch = idx.createLookupBatch(filters, filter); if (batch == null) return null; XXX rowDesc = ((GridH2Table)idx.getTable()).rowDescriptor(); return new ProxyDistributedLookupBatch(batch, rowDesc); } /** {@inheritDoc} */ @Override public void removeChildrenAndResources(Session session) { } } }', 'ground_truth': '@Override public IndexLookupBatch createLookupBatch(TableFilter[] filters, int filter) { IndexLookupBatch batch = idx.createLookupBatch(filters, filter); if (batch == null) return null; GridQueryRowDescriptor rowDesc = ((GridH2Table)idx.getTable()).rowDescriptor(); return new ProxyDistributedLookupBatch(batch, rowDesc); }', 'output': '@Override public IndexLookupBatch createLookupBatch(TableFilter[] filters, int filter) { IndexLookupBatch batch = idx.createLookupBatch(filters, filter); if (batch == null) return null; RowDescriptor rowDesc = ((GridH2Table)idx.getTable()).rowDescriptor(); return new ProxyDistributedLookupBatch(batch, rowDesc); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void fireTablesChanged(DatabaseConnection dbconn) { for (XXX i = listeners.allInstances().iterator(); i.hasNext();) { ((DbMetaDataListener)i.next()).tablesChanged(dbconn); } }', 'ground_truth': 'public static void fireTablesChanged(DatabaseConnection dbconn) { for (Iterator<DbMetaDataListener> i = listeners.allInstances().iterator(); i.hasNext();) { i.next().tablesChanged(dbconn); } }', 'output': 'public static void fireTablesChanged(DatabaseConnection dbconn) { for (DbMetaDataListener listener : listeners()) listener.tablesChanged(dbconn); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public String contents() { StringBuilder builder = new StringBuilder(); builder.append("{"); for (Map.Entry<DecoratedKey, ColumnFamily> entry : columnFamilies.entrySet()) { builder.append(entry.getKey()).append(": ").append(entry.getValue()).append(", "); } builder.append("}"); return builder.toString(); }', 'ground_truth': 'public String contents() { StringBuilder builder = new StringBuilder(); builder.append("{"); for (Map.Entry<RowPosition, ColumnFamily> entry : columnFamilies.entrySet()) { builder.append(entry.getKey()).append(": ").append(entry.getValue()).append(", "); } builder.append("}"); return builder.toString(); }', 'output': 'public String contents() { StringBuilder builder = new StringBuilder(); builder.append("{"); for (Map.Entry<DecoratedKey, ColumnFamily> entry : columnFamilies.entrySet()) { builder.append(entry.getKey()).append(": ").append(entry.getValue()).append(", "); } builder.append("}"); return builder.toString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static boolean maybeLoad() { try { logger.trace("Will try to load mx4j now, if it\'s in the classpath"); XXX mbs = ManagementFactory.getPlatformMBeanServer(); ObjectName processorName = new ObjectName("Server:name=XSLTProcessor"); Class<?> httpAdaptorClass = Class.forName("mx4j.tools.adaptor.http.HttpAdaptor"); Object httpAdaptor = httpAdaptorClass.newInstance(); httpAdaptorClass.getMethod("setHost", String.class).invoke(httpAdaptor, getAddress()); httpAdaptorClass.getMethod("setPort", Integer.TYPE).invoke(httpAdaptor, getPort()); ObjectName httpName = new ObjectName("system:name=http"); mbs.registerMBean(httpAdaptor, httpName); Class<?> xsltProcessorClass = Class.forName("mx4j.tools.adaptor.http.XSLTProcessor"); Object xsltProcessor = xsltProcessorClass.newInstance(); httpAdaptorClass.getMethod("setProcessor", Class.forName("mx4j.tools.adaptor.http.ProcessorMBean")). invoke(httpAdaptor, xsltProcessor); mbs.registerMBean(xsltProcessor, processorName); httpAdaptorClass.getMethod("start").invoke(httpAdaptor); logger.info("mx4j successfuly loaded"); return true; } catch (ClassNotFoundException e) { logger.trace("Will not load MX4J, mx4j-tools.jar is not in the classpath"); } catch(Exception e) { logger.warn("Could not start register mbean in JMX", e); } return false; }', 'ground_truth': 'public static boolean maybeLoad() { try { logger.trace("Will try to load mx4j now, if it\'s in the classpath"); MBeanWrapper mbs = MBeanWrapper.instance; ObjectName processorName = new ObjectName("Server:name=XSLTProcessor"); Class<?> httpAdaptorClass = Class.forName("mx4j.tools.adaptor.http.HttpAdaptor"); Object httpAdaptor = httpAdaptorClass.newInstance(); httpAdaptorClass.getMethod("setHost", String.class).invoke(httpAdaptor, getAddress()); httpAdaptorClass.getMethod("setPort", Integer.TYPE).invoke(httpAdaptor, getPort()); ObjectName httpName = new ObjectName("system:name=http"); mbs.registerMBean(httpAdaptor, httpName); Class<?> xsltProcessorClass = Class.forName("mx4j.tools.adaptor.http.XSLTProcessor"); Object xsltProcessor = xsltProcessorClass.newInstance(); httpAdaptorClass.getMethod("setProcessor", Class.forName("mx4j.tools.adaptor.http.ProcessorMBean")). invoke(httpAdaptor, xsltProcessor); mbs.registerMBean(xsltProcessor, processorName); httpAdaptorClass.getMethod("start").invoke(httpAdaptor); logger.info("mx4j successfuly loaded"); return true; } catch (ClassNotFoundException e) { logger.trace("Will not load MX4J, mx4j-tools.jar is not in the classpath"); } catch(Exception e) { logger.warn("Could not start register mbean in JMX", e); } return false; }', 'output': 'public static boolean maybeLoad() { try { logger.trace("Will try to load mx4j now, if it\'s in the classpath"); PlatformMBeanServer mbs = ManagementFactory.getPlatformMBeanServer(); ObjectName processorName = new ObjectName("Server:name=XSLTProcessor"); Class<?> httpAdaptorClass = Class.forName("mx4j.tools.adaptor.http.HttpAdaptor"); Object httpAdaptor = httpAdaptorClass.newInstance(); httpAdaptorClass.getMethod("setHost", String.class).invoke(httpAdaptor, getAddress()); httpAdaptorClass.getMethod("setPort", Integer.TYPE).invoke(httpAdaptor, getPort()); ObjectName httpName = new ObjectName("system:name=http"); mbs.registerMBean(httpAdaptor, httpName); Class<?> xsltProcessorClass = Class.forName("mx4j.tools.adaptor.http.XSLTProcessor"); Object xsltProcessor = xsltProcessorClass.newInstance(); httpAdaptorClass.getMethod("setProcessor", Class.forName("mx4j.tools.adaptor.http.ProcessorMBean")). invoke(httpAdaptor, xsltProcessor); mbs.registerMBean(xsltProcessor, processorName); httpAdaptorClass.getMethod("start").invoke(httpAdaptor); logger.info("mx4j successfuly loaded"); return true; } catch (ClassNotFoundException e) { logger.trace("Will not load MX4J, mx4j-tools.jar is not in the classpath"); } catch(Exception e) { logger.warn("Could not start register mbean in JMX", e); } return false; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testUpdateDigest() throws Exception { ColumnFamilyStore cfs = Keyspace.open(KEYSPACE1).getColumnFamilyStore(COUNTER1); ByteBuffer col = ByteBufferUtil.bytes("val"); MessageDigest digest1 = MessageDigest.getInstance("md5"); MessageDigest digest2 = MessageDigest.getInstance("md5"); CounterContext.ContextState state = CounterContext.ContextState.allocate(0, 2, 2); state.writeRemote(CounterId.fromInt(1), 4L, 4L); state.writeLocal(CounterId.fromInt(2), 4L, 4L); state.writeRemote(CounterId.fromInt(3), 4L, 4L); state.writeLocal(CounterId.fromInt(4), 4L, 4L); Cell original = createCounterCellFromContext(cfs, col, state, 5); XXX cDef = cfs.metadata.getColumnDefinition(col); Cell cleared = BufferCell.live(cDef, 5, CounterContext.instance().clearAllLocal(state.context)); CounterContext.instance().updateDigest(digest1, original.value()); CounterContext.instance().updateDigest(digest2, cleared.value()); assert Arrays.equals(digest1.digest(), digest2.digest()); }', 'ground_truth': 'public void testUpdateDigest() throws Exception { ColumnFamilyStore cfs = Keyspace.open(KEYSPACE1).getColumnFamilyStore(COUNTER1); ByteBuffer col = ByteBufferUtil.bytes("val"); MessageDigest digest1 = MessageDigest.getInstance("md5"); MessageDigest digest2 = MessageDigest.getInstance("md5"); CounterContext.ContextState state = CounterContext.ContextState.allocate(0, 2, 2); state.writeRemote(CounterId.fromInt(1), 4L, 4L); state.writeLocal(CounterId.fromInt(2), 4L, 4L); state.writeRemote(CounterId.fromInt(3), 4L, 4L); state.writeLocal(CounterId.fromInt(4), 4L, 4L); Cell original = createCounterCellFromContext(cfs, col, state, 5); ColumnMetadata cDef = cfs.metadata().getColumn(col); Cell cleared = BufferCell.live(cDef, 5, CounterContext.instance().clearAllLocal(state.context)); CounterContext.instance().updateDigest(digest1, original.value()); CounterContext.instance().updateDigest(digest2, cleared.value()); assert Arrays.equals(digest1.digest(), digest2.digest()); }', 'output': 'public void testUpdateDigest() throws Exception { ColumnFamilyStore cfs = Keyspace.open(KEYSPACE1).getColumnFamilyStore(COUNTER1); ByteBuffer col = ByteBufferUtil.bytes("val"); MessageDigest digest1 = MessageDigest.getInstance("md5"); MessageDigest digest2 = MessageDigest.getInstance("md5"); CounterContext.ContextState state = CounterContext.ContextState.allocate(0, 2, 2); state.writeRemote(CounterId.fromInt(1), 4L, 4L); state.writeLocal(CounterId.fromInt(2), 4L, 4L); state.writeRemote(CounterId.fromInt(3), 4L, 4L); state.writeLocal(CounterId.fromInt(4), 4L, 4L); Cell original = createCounterCellFromContext(cfs, col, state, 5); ColumnDefinition cDef = cfs.metadata.getColumnDefinition(col); Cell cleared = BufferCell.live(cDef, 5, CounterContext.instance().clearAllLocal(state.context)); CounterContext.instance().updateDigest(digest1, original.value()); CounterContext.instance().updateDigest(digest2, cleared.value()); assert Arrays.equals(digest1.digest(), digest2.digest()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void givenMember() throws Exception { final ApplicationFeatureId applicationFeatureId = ApplicationFeatureId.newMember("com.mycompany.Bar", "foo"); expectedException.expect(IllegalStateException.class); applicationFeatureId.getParentPackageId(); } public void givenMember() throws Exception { final ApplicationFeatureId applicationFeatureId = ApplicationFeatureId.newMember("com.mycompany.Bar", "foo"); final XXX parentClassId = applicationFeatureId.getParentClassId(); assertThat(parentClassId.getType(), is(ApplicationFeatureType.TYPE)); assertThat(parentClassId.getNamespace(), is("com.mycompany")); assertThat(parentClassId.getTypeSimpleName(), is("Bar")); assertThat(parentClassId.getMemberName(), is(nullValue())); }', 'ground_truth': 'public void givenMember() throws Exception { val applicationFeatureId = ApplicationFeatureId.newMember("com.mycompany.Bar", "foo"); val parentClassId = applicationFeatureId.getParentClassId(); assertThat(parentClassId.getSort(), is(ApplicationFeatureSort.TYPE)); assertThat(parentClassId.getNamespace(), is("com.mycompany")); assertThat(parentClassId.getTypeSimpleName(), is("Bar")); assertThat(parentClassId.getMemberName(), is(nullValue())); }', 'output': 'public void givenMember() throws Exception { final ApplicationFeatureId applicationFeatureId = ApplicationFeatureId.newMember("com.mycompany.Bar", "foo"); expectedException.expect(IllegalStateException.class); applicationFeatureId.getParentPackageId(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ParsedStatement parseStatement(String queryStr) throws InvalidRequestException { try { CharStream stream = new ANTLRStringStream(queryStr); CqlLexer lexer = new CqlLexer(stream); TokenStream tokenStream = new CommonTokenStream(lexer); CqlParser parser = new CqlParser(tokenStream); ParsedStatement statement = parser.query(); lexer.throwLastRecognitionError(); parser.throwLastRecognitionError(); return statement; } catch (RuntimeException re) { XXX ire = new InvalidRequestException("Failed parsing statement: [" + queryStr + "] reason: " + re.getClass().getSimpleName() + " " + re.getMessage()); ire.initCause(re); throw ire; } catch (RecognitionException e) { XXX ire = new InvalidRequestException("Invalid or malformed CQL query string"); ire.initCause(e); throw ire; } }', 'ground_truth': 'public static ParsedStatement parseStatement(String queryStr) throws SyntaxException { try { CharStream stream = new ANTLRStringStream(queryStr); CqlLexer lexer = new CqlLexer(stream); TokenStream tokenStream = new CommonTokenStream(lexer); CqlParser parser = new CqlParser(tokenStream); ParsedStatement statement = parser.query(); lexer.throwLastRecognitionError(); parser.throwLastRecognitionError(); return statement; } catch (RuntimeException re) { SyntaxException ire = new SyntaxException("Failed parsing statement: [" + queryStr + "] reason: " + re.getClass().getSimpleName() + " " + re.getMessage()); throw ire; } catch (RecognitionException e) { SyntaxException ire = new SyntaxException("Invalid or malformed CQL query string: " + e.getMessage()); throw ire; } }', 'output': 'public static ParsedStatement parseStatement(String queryStr) throws InvalidRequestException { try { CharStream stream = new ANTLRStringStream(queryStr); CqlLexer lexer = new CqlLexer(stream); TokenStream tokenStream = new CommonTokenStream(lexer); CqlParser parser = new CqlParser(tokenStream); ParsedStatement statement = parser.query(); lexer.throwLastRecognitionError(); parser.throwLastRecognitionError(); return statement; } catch (RuntimeException re) { InvalidRequestException ire = new InvalidRequestException("Failed parsing statement: [" + queryStr + "] reason: " + re.getClass().getSimpleName() + " " + re.getMessage()); ire.initCause(re); throw ire; } catch (RecognitionException e) { InvalidRequestException ire = new InvalidRequestException("Invalid or malformed CQL query string"); ire.initCause(e); throw ire; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private GridCache<Object, Object> regionCache(String regionName) throws CacheException { String cacheName = regionCaches.get(regionName); if (cacheName == null) { if (dfltCache != null) return dfltCache; cacheName = regionName; } GridCache<Object, Object> cache = ignite.cache(cacheName); if (cache == null) throw new CacheException("Cache \'" + cacheName + "\' for region \'" + regionName + "\' is not configured."); return cache; }', 'ground_truth': 'private Cache<Object, Object> regionCache(String regionName) throws CacheException { String cacheName = regionCaches.get(regionName); if (cacheName == null) { if (dfltCache != null) return dfltCache; cacheName = regionName; } Cache<Object, Object> cache = ignite.cache(cacheName); if (cache == null) throw new CacheException("Cache \'" + cacheName + "\' for region \'" + regionName + "\' is not configured."); return cache; }', 'output': 'private IgniteCache<Object, Object> regionCache(String regionName) throws CacheException { String cacheName = regionCaches.get(regionName); if (cacheName == null) { if (dfltCache != null) return dfltCache; cacheName = regionName; } IgniteCache<Object, Object> cache = ignite.jcache(cacheName); if (cache == null) throw new CacheException("Cache \'" + cacheName + "\' for region \'" + regionName + "\' is not configured."); return cache; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Map<ByteBuffer,List<Counter>> recv_multiget_counter_slice() throws InvalidRequestException, UnavailableException, TimedOutException, TException { TMessage msg = iprot_.readMessageBegin(); if (msg.type == TMessageType.EXCEPTION) { XXX x = TApplicationException.read(iprot_); iprot_.readMessageEnd(); throw x; } if (msg.seqid != seqid_) { throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "multiget_counter_slice failed: out of sequence response"); } multiget_counter_slice_result result = new multiget_counter_slice_result(); result.read(iprot_); iprot_.readMessageEnd(); if (result.isSetSuccess()) { return result.success; } if (result.ire != null) { throw result.ire; } if (result.ue != null) { throw result.ue; } if (result.te != null) { throw result.te; } throw new TApplicationException(TApplicationException.MISSING_RESULT, "multiget_counter_slice failed: unknown result"); }', 'ground_truth': 'public Map<ByteBuffer,List<Counter>> recv_multiget_counter_slice() throws InvalidRequestException, UnavailableException, TimedOutException, org.apache.thrift.TException { org.apache.thrift.protocol.TMessage msg = iprot_.readMessageBegin(); if (msg.type == org.apache.thrift.protocol.TMessageType.EXCEPTION) { org.apache.thrift.TApplicationException x = org.apache.thrift.TApplicationException.read(iprot_); iprot_.readMessageEnd(); throw x; } if (msg.seqid != seqid_) { throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.BAD_SEQUENCE_ID, "multiget_counter_slice failed: out of sequence response"); } multiget_counter_slice_result result = new multiget_counter_slice_result(); result.read(iprot_); iprot_.readMessageEnd(); if (result.isSetSuccess()) { return result.success; } if (result.ire != null) { throw result.ire; } if (result.ue != null) { throw result.ue; } if (result.te != null) { throw result.te; } throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.MISSING_RESULT, "multiget_counter_slice failed: unknown result"); }', 'output': 'public Map<ByteBuffer,List<Counter>> recv_multiget_counter_slice() throws InvalidRequestException, UnavailableException, TimedOutException, TException { TMessage msg = iprot_.readMessageBegin(); if (msg.type == TMessageType.EXCEPTION) { TApplicationException x = TApplicationException.read(iprot_); iprot_.readMessageEnd(); throw x; } if (msg.seqid != seqid_) { throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "multiget_counter_slice failed: out of sequence response"); } multiget_counter_slice_result result = new multiget_counter_slice_result(); result.read(iprot_); iprot_.readMessageEnd(); if (result.isSetSuccess()) { return result.success; } if (result.ire != null) { throw result.ire; } if (result.ue != null) { throw result.ue; } if (result.te != null) { throw result.te; } throw new TApplicationException(TApplicationException.MISSING_RESULT, "multiget_counter_slice failed: unknown result"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testBoundedSizeDequeueWindowUniqueMultithreaded() throws Exception { XXX win = new GridStreamerBoundedSizeWindow<>(); win.setMaximumSize(500); win.setUnique(true); win.start(); checkWindowMultithreaded(win, 100000, 10, 1000); win.consistencyCheck(); finalChecks(win, 500); }', 'ground_truth': 'public void testBoundedSizeDequeueWindowUniqueMultithreaded() throws Exception { StreamerBoundedSizeWindow<Integer> win = new StreamerBoundedSizeWindow<>(); win.setMaximumSize(500); win.setUnique(true); win.start(); checkWindowMultithreaded(win, 100000, 10, 1000); win.consistencyCheck(); finalChecks(win, 500); }', 'output': 'public void testBoundedSizeDequeueWindowUniqueMultithreaded() throws Exception { StreamerBoundedSizeWindow win = new StreamerBoundedSizeWindow<>(); win.setMaximumSize(500); win.setUnique(true); win.start(); checkWindowMultithreaded(win, 100000, 10, 1000); win.consistencyCheck(); finalChecks(win, 500); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void send_get_slice_from(String tablename, String key, String columnParent, boolean isAscending, int count) throws TException { oprot_.writeMessageBegin(new TMessage("get_slice_from", TMessageType.CALL, seqid_)); XXX args = new get_slice_from_args(); args.tablename = tablename; args.key = key; args.columnParent = columnParent; args.isAscending = isAscending; args.count = count; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'ground_truth': 'public void send_get_slice(String tablename, String key, String columnParent, boolean isAscending, int count) throws TException { oprot_.writeMessageBegin(new TMessage("get_slice", TMessageType.CALL, seqid_)); get_slice_args args = new get_slice_args(); args.tablename = tablename; args.key = key; args.columnParent = columnParent; args.isAscending = isAscending; args.count = count; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }', 'output': 'public void send_get_slice_from(String tablename, String key, String columnParent, boolean isAscending, int count) throws TException { oprot_.writeMessageBegin(new TMessage("get_slice_from", TMessageType.CALL, seqid_)); get_slice_from_args args = new get_slice_from_args(); args.tablename = tablename; args.key = key; args.columnParent = columnParent; args.isAscending = isAscending; args.count = count; args.write(oprot_); oprot_.writeMessageEnd(); oprot_.getTransport().flush(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private Unfiltered fixNegativeLocalExpirationTime(Row row) { Row.Builder builder = HeapAllocator.instance.cloningBTreeRowBuilder(); builder.newRow(row.clustering()); builder.addPrimaryKeyLivenessInfo(row.primaryKeyLivenessInfo().isExpiring() && row.primaryKeyLivenessInfo().localExpirationTime() < 0 ? row.primaryKeyLivenessInfo().withUpdatedTimestampAndLocalDeletionTime(row.primaryKeyLivenessInfo().timestamp() + 1, AbstractCell.MAX_DELETION_TIME) :row.primaryKeyLivenessInfo()); builder.addRowDeletion(row.deletion()); for (ColumnData cd : row) { if (cd.column().isSimple()) { XXX cell = (Cell)cd; builder.addCell(cell.isExpiring() && cell.localDeletionTime() < 0 ? cell.withUpdatedTimestampAndLocalDeletionTime(cell.timestamp() + 1, AbstractCell.MAX_DELETION_TIME) : cell); } else { ComplexColumnData complexData = (ComplexColumnData)cd; builder.addComplexDeletion(complexData.column(), complexData.complexDeletion()); for (XXX cell : complexData) { builder.addCell(cell.isExpiring() && cell.localDeletionTime() < 0 ? cell.withUpdatedTimestampAndLocalDeletionTime(cell.timestamp() + 1, AbstractCell.MAX_DELETION_TIME) : cell); } } } return builder.build(); }', 'ground_truth': 'private Unfiltered fixNegativeLocalExpirationTime(Row row) { Row.Builder builder = HeapAllocator.instance.cloningBTreeRowBuilder(); builder.newRow(row.clustering()); builder.addPrimaryKeyLivenessInfo(row.primaryKeyLivenessInfo().isExpiring() && row.primaryKeyLivenessInfo().localExpirationTime() < 0 ? row.primaryKeyLivenessInfo().withUpdatedTimestampAndLocalDeletionTime(row.primaryKeyLivenessInfo().timestamp() + 1, AbstractCell.MAX_DELETION_TIME) :row.primaryKeyLivenessInfo()); builder.addRowDeletion(row.deletion()); for (ColumnData cd : row) { if (cd.column().isSimple()) { Cell<?> cell = (Cell<?>)cd; builder.addCell(cell.isExpiring() && cell.localDeletionTime() < 0 ? cell.withUpdatedTimestampAndLocalDeletionTime(cell.timestamp() + 1, AbstractCell.MAX_DELETION_TIME) : cell); } else { ComplexColumnData complexData = (ComplexColumnData)cd; builder.addComplexDeletion(complexData.column(), complexData.complexDeletion()); for (Cell<?> cell : complexData) { builder.addCell(cell.isExpiring() && cell.localDeletionTime() < 0 ? cell.withUpdatedTimestampAndLocalDeletionTime(cell.timestamp() + 1, AbstractCell.MAX_DELETION_TIME) : cell); } } } return builder.build(); }', 'output': 'private Unfiltered fixNegativeLocalExpirationTime(Row row) { Row.Builder builder = HeapAllocator.instance.cloningBTreeRowBuilder(); builder.newRow(row.clustering()); builder.addPrimaryKeyLivenessInfo(row.primaryKeyLivenessInfo().isExpiring() && row.primaryKeyLivenessInfo().localExpirationTime() < 0 ? row.primaryKeyLivenessInfo().withUpdatedTimestampAndLocalDeletionTime(row.primaryKeyLivenessInfo().timestamp() + 1, AbstractCell.MAX_DELETION_TIME) :row.primaryKeyLivenessInfo()); builder.addRowDeletion(row.deletion()); for (ColumnData cd : row) { if (cd.column().isSimple()) { Cell cell = (Cell)cd; builder.addCell(cell.isExpiring() && cell.localDeletionTime() < 0 ? cell.withUpdatedTimestampAndLocalDeletionTime(cell.timestamp() + 1, AbstractCell.MAX_DELETION_TIME) : cell); } else { ComplexColumnData complexData = (ComplexColumnData)cd; builder.addComplexDeletion(complexData.column(), complexData.complexDeletion()); for (Cell cell : complexData) { builder.addCell(cell.isExpiring() && cell.localDeletionTime() < 0 ? cell.withUpdatedTimestampAndLocalDeletionTime(cell.timestamp() + 1, AbstractCell.MAX_DELETION_TIME) : cell); } } } return builder.build(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void setMetadata(IndexMetadata indexDef) { metadata = indexDef; Pair<ColumnDefinition, IndexTarget.Type> target = TargetParser.parse(baseCfs.metadata, indexDef); functions = getFunctions(indexDef, target); CFMetaData cfm = indexCfsMetadata(baseCfs.metadata, indexDef); indexCfs = ColumnFamilyStore.createColumnFamilyStore(baseCfs.keyspace, cfm.cfName, cfm, baseCfs.getTracker().loadsstables); indexedColumn = target.left; }', 'ground_truth': 'private void setMetadata(IndexMetadata indexDef) { metadata = indexDef; Pair<ColumnMetadata, IndexTarget.Type> target = TargetParser.parse(baseCfs.metadata(), indexDef); functions = getFunctions(indexDef, target); TableMetadataRef tableRef = TableMetadataRef.forOfflineTools(indexCfsMetadata(baseCfs.metadata(), indexDef)); indexCfs = ColumnFamilyStore.createColumnFamilyStore(baseCfs.keyspace, tableRef.name, tableRef, baseCfs.getTracker().loadsstables); indexedColumn = target.left; }', 'output': 'private void setMetadata(IndexMetadata indexDef) { metadata = indexDef; Pair<String, IndexTarget.Type> target = TargetParser.parse(baseCfs.metadata, indexDef); functions = getFunctions(indexDef, target); CFMetaData cfm = indexCfsMetadata(baseCfs.metadata, indexDef); indexCfs = ColumnFamilyStore.createColumnFamilyStore(baseCfs.keyspace, cfm.cfName, cfm, baseCfs.getTracker().loadsstables); indexedColumn = target.left; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public final PersistenceQuery createPersistenceQueryFor( final Query<?> query, final QueryCardinality cardinality) { if (log.isDebugEnabled()) { log.debug("createPersistenceQueryFor: {}", query.getDescription()); } final ObjectSpecification noSpec = specFor(query); if (query instanceof AllInstancesQuery) { final AllInstancesQuery<?> queryFindAllInstances = (AllInstancesQuery<?>) query; return new PersistenceQueryFindAllInstances(noSpec, queryFindAllInstances.getStart(), queryFindAllInstances.getCount()); } if (query instanceof NamedQuery) { final NamedQuery<?> queryDefault = (NamedQuery<?>) query; final String queryName = queryDefault.getName(); final Map<String, ObjectAdapter> parametersByName = wrap(queryDefault.getParametersByName()); return new PersistenceQueryFindUsingApplibQueryDefault(noSpec, queryName, parametersByName, cardinality, specificationLoader, queryDefault.getStart(), queryDefault.getCount()); } throw _Exceptions.unsupportedOperation("query type %s not supported by this persistence implementation", query.getClass()); }', 'ground_truth': 'public final PersistenceQuery createPersistenceQueryFor( final Query<?> query, final QueryCardinality cardinality) { if (log.isDebugEnabled()) { log.debug("createPersistenceQueryFor: {}", query.getDescription()); } final ObjectSpecification noSpec = specFor(query); if (query instanceof AllInstancesQuery) { final AllInstancesQuery<?> queryFindAllInstances = (AllInstancesQuery<?>) query; return new PersistenceQueryFindAllInstances(noSpec, queryFindAllInstances.getStart(), queryFindAllInstances.getCount()); } if (query instanceof NamedQuery) { final NamedQuery<?> queryDefault = (NamedQuery<?>) query; final String queryName = queryDefault.getName(); final Map<String, ManagedObject> parametersByName = wrap(queryDefault.getParametersByName()); return new PersistenceQueryFindUsingApplibQueryDefault(noSpec, queryName, parametersByName, cardinality, specificationLoader, queryDefault.getStart(), queryDefault.getCount()); } throw _Exceptions.unsupportedOperation("query type %s not supported by this persistence implementation", query.getClass()); }', 'output': 'public final PersistenceQuery createPersistenceQueryFor( final Query<?> query, final Cardinality cardinality) { if (log.isDebugEnabled()) { log.debug("createPersistenceQueryFor: {}", query.getDescription()); } final ObjectSpecification noSpec = specFor(query); if (query instanceof AllInstancesQuery) { final AllInstancesQuery<?> queryFindAllInstances = (AllInstancesQuery<?>) query; return new PersistenceQueryFindAllInstances(noSpec, queryFindAllInstances.getStart(), queryFindAllInstances.getCount()); } if (query instanceof NamedQuery) { final NamedQuery<?> queryDefault = (NamedQuery<?>) query; final String queryName = queryDefault.getName(); final Map<String, ObjectAdapter> parametersByName = wrap(queryDefault.getParametersByName()); return new PersistenceQueryFindUsingApplibQueryDefault(noSpec, queryName, parametersByName, cardinality, specificationLoader, queryDefault.getStart(), queryDefault.getCount()); } throw _Exceptions.unsupportedOperation("query type %s not supported by this persistence implementation", query.getClass()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static IgniteConfiguration configuration() throws IgniteException { IgniteConfiguration cfg = new IgniteConfiguration(); cfg.setLocalHost("127.0.0.1"); cfg.setDeploymentMode(SHARED); cfg.setPeerClassLoadingEnabled(true); XXX marsh = new IgniteOptimizedMarshaller(); marsh.setRequireSerializable(false); cfg.setMarshaller(marsh); CacheConfiguration cacheCfg = new CacheConfiguration(); cacheCfg.setAtomicityMode(TRANSACTIONAL); cacheCfg.setWriteSynchronizationMode(FULL_SYNC); cacheCfg.setPreloadMode(SYNC); cacheCfg.setAtomicityMode(TRANSACTIONAL); CacheQueryConfiguration qryCfg = new CacheQueryConfiguration(); qryCfg.setIndexPrimitiveKey(true); qryCfg.setIndexFixedTyping(false); cacheCfg.setQueryConfiguration(qryCfg); cfg.setCacheConfiguration(cacheCfg); TcpDiscoverySpi discoSpi = new TcpDiscoverySpi(); TcpDiscoveryVmIpFinder ipFinder = new TcpDiscoveryVmIpFinder(); ipFinder.setAddresses(Collections.singletonList("127.0.0.1:47500..47509")); discoSpi.setIpFinder(ipFinder); cfg.setDiscoverySpi(discoSpi); return cfg; }', 'ground_truth': 'public static IgniteConfiguration configuration() throws IgniteException { IgniteConfiguration cfg = new IgniteConfiguration(); cfg.setLocalHost("127.0.0.1"); cfg.setDeploymentMode(SHARED); cfg.setPeerClassLoadingEnabled(true); OptimizedMarshaller marsh = new OptimizedMarshaller(); marsh.setRequireSerializable(false); cfg.setMarshaller(marsh); CacheConfiguration cacheCfg = new CacheConfiguration(); cacheCfg.setAtomicityMode(TRANSACTIONAL); cacheCfg.setWriteSynchronizationMode(FULL_SYNC); cacheCfg.setPreloadMode(SYNC); cacheCfg.setAtomicityMode(TRANSACTIONAL); CacheQueryConfiguration qryCfg = new CacheQueryConfiguration(); qryCfg.setIndexPrimitiveKey(true); qryCfg.setIndexFixedTyping(false); cacheCfg.setQueryConfiguration(qryCfg); cfg.setCacheConfiguration(cacheCfg); TcpDiscoverySpi discoSpi = new TcpDiscoverySpi(); TcpDiscoveryVmIpFinder ipFinder = new TcpDiscoveryVmIpFinder(); ipFinder.setAddresses(Collections.singletonList("127.0.0.1:47500..47509")); discoSpi.setIpFinder(ipFinder); cfg.setDiscoverySpi(discoSpi); return cfg; }', 'output': 'public static IgniteConfiguration configuration() throws IgniteException { IgniteConfiguration cfg = new IgniteConfiguration(); cfg.setLocalHost("127.0.0.1"); cfg.setDeploymentMode(SHARED); cfg.setPeerClassLoadingEnabled(true); OptimizedMarshaller marsh = new OptimizedMarshaller(); marsh.setRequireSerializable(false); cfg.setMarshaller(marsh); CacheConfiguration cacheCfg = new CacheConfiguration(); cacheCfg.setAtomicityMode(TRANSACTIONAL); cacheCfg.setWriteSynchronizationMode(FULL_SYNC); cacheCfg.setPreloadMode(SYNC); cacheCfg.setAtomicityMode(TRANSACTIONAL); CacheQueryConfiguration qryCfg = new CacheQueryConfiguration(); qryCfg.setIndexPrimitiveKey(true); qryCfg.setIndexFixedTyping(false); cacheCfg.setQueryConfiguration(qryCfg); cfg.setCacheConfiguration(cacheCfg); TcpDiscoverySpi discoSpi = new TcpDiscoverySpi(); TcpDiscoveryVmIpFinder ipFinder = new TcpDiscoveryVmIpFinder(); ipFinder.setAddresses(Collections.singletonList("127.0.0.1:47500..47509")); discoSpi.setIpFinder(ipFinder); cfg.setDiscoverySpi(discoSpi); return cfg; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private static IgniteCache<Integer, double[]> getTestCache(Ignite ignite) { CacheConfiguration<Integer, double[]> cacheConfiguration = new CacheConfiguration<>(); cacheConfiguration.setName("TEST_" + UUID.randomUUID()); cacheConfiguration.setAffinity(new RendezvousAffinityFunction(false, 10)); IgniteCache<Integer, double[]> cache = ignite.createCache(cacheConfiguration); for (int i = 0; i < data.length; i++) cache.put(i, data[i]); return cache; }', 'ground_truth': 'private static IgniteCache<Integer, Vector> getTestCache(Ignite ignite) { CacheConfiguration<Integer, Vector> cacheConfiguration = new CacheConfiguration<>(); cacheConfiguration.setName("TEST_" + UUID.randomUUID()); cacheConfiguration.setAffinity(new RendezvousAffinityFunction(false, 10)); IgniteCache<Integer, Vector> cache = ignite.createCache(cacheConfiguration); for (int i = 0; i < data.length; i++) cache.put(i, VectorUtils.of(data[i])); return cache; }', 'output': 'private static IgniteCache<Long, double[]> getTestCache(Ignite ignite) { CacheConfiguration<Long, double[]> cacheConfiguration = new CacheConfiguration<>(); cacheConfiguration.setName("TEST_" + UUID.randomUUID()); cacheConfiguration.setAffinity(new RendezvousAffinityFunction(false, 10)); IgniteCache<Long, double[]> cache = ignite.createCache(cacheConfiguration); for (long i = 0; i < data.length; i++) cache.put(i, data[i]); return cache; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public IgniteInternalFuture<IgniteTxEx> commitAsyncLocal() { if (log.isDebugEnabled()) log.debug("Committing colocated tx locally: " + this); if (pessimistic()) prepareAsync(); IgniteInternalFuture<IgniteTxEx<K, V>> prep = prepFut.get(); if (F.isEmpty(dhtMap) && F.isEmpty(nearMap)) { if (prep != null) return (IgniteInternalFuture<IgniteTxEx>)(IgniteInternalFuture)prep; return new GridFinishedFuture<IgniteTxEx>(cctx.kernalContext(), this); } final GridDhtTxFinishFuture<K, V> fut = new GridDhtTxFinishFuture<>(cctx, this, /*commit*/true); cctx.mvcc().addFuture(fut); if (prep == null || prep.isDone()) { assert prep != null || optimistic(); try { if (prep != null) prep.get(); fut.finish(); } catch (XXX e) { if (log.isDebugEnabled()) log.debug("Failed optimistically to prepare transaction [tx=" + this + ", e=" + e + \']\'); fut.onError(e); } catch (IgniteCheckedException e) { U.error(log, "Failed to prepare transaction: " + this, e); fut.onError(e); } } else prep.listenAsync(new CI1<IgniteInternalFuture<IgniteTxEx<K, V>>>() { @Override public void apply(IgniteInternalFuture<IgniteTxEx<K, V>> f) { try { f.get(); fut.finish(); } catch (XXX e) { if (log.isDebugEnabled()) log.debug("Failed optimistically to prepare transaction [tx=" + this + ", e=" + e + \']\'); fut.onError(e); } catch (IgniteCheckedException e) { U.error(log, "Failed to prepare transaction: " + this, e); fut.onError(e); } } }); return fut; }', 'ground_truth': 'public IgniteInternalFuture<IgniteTxEx> commitAsyncLocal() { if (log.isDebugEnabled()) log.debug("Committing colocated tx locally: " + this); if (pessimistic()) prepareAsync(); IgniteInternalFuture<IgniteTxEx<K, V>> prep = prepFut.get(); if (F.isEmpty(dhtMap) && F.isEmpty(nearMap)) { if (prep != null) return (IgniteInternalFuture<IgniteTxEx>)(IgniteInternalFuture)prep; return new GridFinishedFuture<IgniteTxEx>(cctx.kernalContext(), this); } final GridDhtTxFinishFuture<K, V> fut = new GridDhtTxFinishFuture<>(cctx, this, /*commit*/true); cctx.mvcc().addFuture(fut); if (prep == null || prep.isDone()) { assert prep != null || optimistic(); try { if (prep != null) prep.get(); fut.finish(); } catch (IgniteTxOptimisticCheckedException e) { if (log.isDebugEnabled()) log.debug("Failed optimistically to prepare transaction [tx=" + this + ", e=" + e + \']\'); fut.onError(e); } catch (IgniteCheckedException e) { U.error(log, "Failed to prepare transaction: " + this, e); fut.onError(e); } } else prep.listenAsync(new CI1<IgniteInternalFuture<IgniteTxEx<K, V>>>() { @Override public void apply(IgniteInternalFuture<IgniteTxEx<K, V>> f) { try { f.get(); fut.finish(); } catch (IgniteTxOptimisticCheckedException e) { if (log.isDebugEnabled()) log.debug("Failed optimistically to prepare transaction [tx=" + this + ", e=" + e + \']\'); fut.onError(e); } catch (IgniteCheckedException e) { U.error(log, "Failed to prepare transaction: " + this, e); fut.onError(e); } } }); return fut; }', 'output': 'public IgniteInternalFuture<IgniteTxEx> commitAsyncLocal() { if (log.isDebugEnabled()) log.debug("Committing colocated tx locally: " + this); if (pessimistic()) prepareAsync(); IgniteInternalFuture<IgniteTxEx<K, V>> prep = prepFut.get(); if (F.isEmpty(dhtMap) && F.isEmpty(nearMap)) { if (prep != null) return (IgniteInternalFuture<IgniteTxEx>)prep; return new GridFinishedFuture<IgniteTxEx>(cctx.kernalContext(), this); } final GridDhtTxFinishFuture<K, V> fut = new GridDhtTxFinishFuture<>(cctx, this, /*commit*/true); cctx.mvcc().addFuture(fut); if (prep == null || prep.isDone()) { assert prep != null || optimistic(); try { if (prep != null) prep.get(); fut.finish(); } catch (Throwable e) { if (log.isDebugEnabled()) log.debug("Failed optimistically to prepare transaction [tx=" + this + ", e=" + e + \']\'); fut.onError(e); } catch (IgniteCheckedException e) { U.error(log, "Failed to prepare transaction: " + this, e); fut.onError(e); } } else prep.listenAsync(new CI1<IgniteInternalFuture<IgniteTxEx<K, V>>>() { @Override public void apply(IgniteInternalFuture<IgniteTxEx<K, V>> f) { try { f.get(); fut.finish(); } catch (Throwable e) { if (log.isDebugEnabled()) log.debug("Failed optimistically to prepare transaction [tx=" + this + ", e=" + e + \']\'); fut.onError(e); } catch (IgniteCheckedException e) { U.error(log, "Failed to prepare transaction: " + this, e); fut.onError(e); } } }); return fut; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void run() throws UnknownHostException, GridException { GridNioServerListener<ByteBuffer> lsnr = new GridNioServerListenerAdapter<ByteBuffer>() { @Override public void onConnected(GridNioSession ses) { X.print("New connection accepted."); } @Override public void onDisconnected(GridNioSession ses, @Nullable Exception e) { } @Override public void onMessage(GridNioSession ses, ByteBuffer msg) { ByteBuffer buf = ByteBuffer.allocate(msg.remaining()).put(msg); buf.position(0); ses.send(buf); } @Override public void onSessionWriteTimeout(GridNioSession ses) { X.error("Session write timeout. Closing."); } @Override public void onSessionIdleTimeout(GridNioSession ses) { X.error("Session idle timeout. Closing."); } }; XXX log  = new GridTestLog4jLogger(U.resolveGridGainUrl("config/gridgain-log4j.xml")); GridNioServer.<ByteBuffer>builder() .address(InetAddress.getByName("localhost")) .port(port) .listener(lsnr) .logger(log) .selectorCount(selectorCnt) .gridName("") .tcpNoDelay(false) .directBuffer(false) .byteOrder(ByteOrder.nativeOrder()) .socketSendBufferSize(0) .socketReceiveBufferSize(0) .sendQueueLimit(0) .build() .start(); }', 'ground_truth': 'public void run() throws UnknownHostException, GridException { GridNioServerListener<ByteBuffer> lsnr = new GridNioServerListenerAdapter<ByteBuffer>() { @Override public void onConnected(GridNioSession ses) { X.print("New connection accepted."); } @Override public void onDisconnected(GridNioSession ses, @Nullable Exception e) { } @Override public void onMessage(GridNioSession ses, ByteBuffer msg) { ByteBuffer buf = ByteBuffer.allocate(msg.remaining()).put(msg); buf.position(0); ses.send(buf); } @Override public void onSessionWriteTimeout(GridNioSession ses) { X.error("Session write timeout. Closing."); } @Override public void onSessionIdleTimeout(GridNioSession ses) { X.error("Session idle timeout. Closing."); } }; IgniteLogger log  = new GridTestLog4jLogger(U.resolveGridGainUrl("config/gridgain-log4j.xml")); GridNioServer.<ByteBuffer>builder() .address(InetAddress.getByName("localhost")) .port(port) .listener(lsnr) .logger(log) .selectorCount(selectorCnt) .gridName("") .tcpNoDelay(false) .directBuffer(false) .byteOrder(ByteOrder.nativeOrder()) .socketSendBufferSize(0) .socketReceiveBufferSize(0) .sendQueueLimit(0) .build() .start(); }', 'output': 'public void run() throws UnknownHostException, GridException { GridNioServerListener<ByteBuffer> lsnr = new GridNioServerListenerAdapter<ByteBuffer>() { @Override public void onConnected(GridNioSession ses) { X.print("New connection accepted."); } @Override public void onDisconnected(GridNioSession ses, @Nullable Exception e) { } @Override public void onMessage(GridNioSession ses, ByteBuffer msg) { ByteBuffer buf = ByteBuffer.allocate(msg.remaining()).put(msg); buf.position(0); ses.send(buf); } @Override public void onSessionWriteTimeout(GridNioSession ses) { X.error("Session write timeout. Closing."); } @Override public void onSessionIdleTimeout(GridNioSession ses) { X.error("Session idle timeout. Closing."); } }; IgniteLogger log = new GridTestLog4jLogger(U.resolveGridGainUrl("config/gridgain-log4j.xml")); GridNioServer.<ByteBuffer>builder() .address(InetAddress.getByName("localhost")) .port(port) .logger(log) .selectorCount(selectorCnt) .gridName("") .tcpNoDelay(false) .directBuffer(false) .byteOrder(ByteOrder.nativeOrder()) .socketSendBufferSize(0) .socketReceiveBufferSize(0) .sendQueueLimit(0) .build() .start(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGetRepositoryUpdatedBaseUrl() throws Exception { String buildDir = tmpFolder.getRoot().getAbsolutePath(); XXX ambariMetaInfo = setupTempAmbariMetaInfo(buildDir); String newBaseUrl = "http: ambariMetaInfo.updateRepoBaseURL(STACK_NAME_HDP, "2.1.1", "redhat6", STACK_NAME_HDP + "-2.1.1", newBaseUrl); RepositoryInfo repoInfo = ambariMetaInfo.getRepository(STACK_NAME_HDP, "2.1.1", "redhat6", STACK_NAME_HDP + "-2.1.1"); assertEquals(newBaseUrl, repoInfo.getBaseUrl()); String prevBaseUrl = repoInfo.getDefaultBaseUrl(); ambariMetaInfo.init(); List<RepositoryInfo> redhat6Repo = ambariMetaInfo.getRepositories( STACK_NAME_HDP, "2.1.1", "redhat6"); assertNotNull(redhat6Repo); for (RepositoryInfo ri : redhat6Repo) { if (STACK_NAME_HDP.equals(ri.getRepoName())) { assertEquals(newBaseUrl, ri.getBaseUrl()); assertFalse(ri.getBaseUrl().equals(ri.getDefaultBaseUrl())); } } ambariMetaInfo.updateRepoBaseURL(STACK_NAME_HDP, "2.1.1", "redhat6", STACK_NAME_HDP + "-2.1.1", prevBaseUrl); }', 'ground_truth': 'public void testGetRepositoryUpdatedBaseUrl() throws Exception { String buildDir = tmpFolder.getRoot().getAbsolutePath(); TestAmbariMetaInfo ambariMetaInfo = setupTempAmbariMetaInfo(buildDir, true); String newBaseUrl = "http: ambariMetaInfo.updateRepoBaseURL(STACK_NAME_HDP, "2.1.1", "redhat6", STACK_NAME_HDP + "-2.1.1", newBaseUrl); RepositoryInfo repoInfo = ambariMetaInfo.getRepository(STACK_NAME_HDP, "2.1.1", "redhat6", STACK_NAME_HDP + "-2.1.1"); assertEquals(newBaseUrl, repoInfo.getBaseUrl()); String prevBaseUrl = repoInfo.getDefaultBaseUrl(); MetainfoDAO metainfoDAO = ambariMetaInfo.metaInfoDAO; reset(metainfoDAO); MetainfoEntity entity = createNiceMock(MetainfoEntity.class); expect(metainfoDAO.findByKey("repo:/HDP/2.1.1/redhat6/HDP-2.1.1:baseurl")).andReturn(entity).atLeastOnce(); expect(entity.getMetainfoValue()).andReturn(newBaseUrl).atLeastOnce(); replay(metainfoDAO, entity); ambariMetaInfo.init(); waitForAllReposToBeResolved(ambariMetaInfo); List<RepositoryInfo> redhat6Repo = ambariMetaInfo.getRepositories( STACK_NAME_HDP, "2.1.1", "redhat6"); assertNotNull(redhat6Repo); for (RepositoryInfo ri : redhat6Repo) { if (STACK_NAME_HDP.equals(ri.getRepoName())) { assertEquals(newBaseUrl, ri.getBaseUrl()); assertFalse(ri.getBaseUrl().equals(ri.getDefaultBaseUrl())); } } ambariMetaInfo.updateRepoBaseURL(STACK_NAME_HDP, "2.1.1", "redhat6", STACK_NAME_HDP + "-2.1.1", prevBaseUrl); }', 'output': 'public void testGetRepositoryUpdatedBaseUrl() throws Exception { String buildDir = tmpFolder.getRoot().getAbsolutePath(); AmbariMetaInfo ambariMetaInfo = setupTempAmbariMetaInfo(buildDir); String newBaseUrl = "http: ambariMetaInfo.updateRepoBaseURL(STACK_NAME_HDP, "2.1.1", "redhat6", STACK_NAME_HDP + "-2.1.1", newBaseUrl); RepositoryInfo repoInfo = ambariMetaInfo.getRepository(STACK_NAME_HDP, "2.1.1", "redhat6", STACK_NAME_HDP + "-2.1.1"); assertEquals(newBaseUrl, repoInfo.getBaseUrl()); String prevBaseUrl = repoInfo.getDefaultBaseUrl(); ambariMetaInfo.init(); List<RepositoryInfo> redhat6Repo = ambariMetaInfo.getRepositories( STACK_NAME_HDP, "2.1.1", "redhat6"); assertNotNull(redhat6Repo); for (RepositoryInfo ri : redhat6Repo) { if (STACK_NAME_HDP.equals(ri.getRepoName())) { assertEquals(newBaseUrl, ri.getBaseUrl()); assertFalse(ri.getBaseUrl().equals(ri.getDefaultBaseUrl())); } } ambariMetaInfo.updateRepoBaseURL(STACK_NAME_HDP, "2.1.1", "redhat6", STACK_NAME_HDP + "-2.1.1", prevBaseUrl); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public ResultMessage executeWithCondition(QueryState queryState, QueryOptions options) throws RequestExecutionException, RequestValidationException { List<ByteBuffer> variables = options.getValues(); List<ByteBuffer> keys = buildPartitionKeyNames(variables); if (keys.size() > 1) throw new InvalidRequestException("IN on the partition key is not supported with conditional updates"); ByteBuffer key = keys.get(0); CQL3CasConditions conditions = new CQL3CasConditions(cfm, queryState.getTimestamp()); XXX prefix = createClusteringPrefixBuilder(variables); ColumnFamily updates = UnsortedColumns.factory.create(cfm); addUpdatesAndConditions(key, prefix, updates, conditions, variables, getTimestamp(queryState.getTimestamp(), variables)); ColumnFamily result = StorageProxy.cas(keyspace(), columnFamily(), key, conditions, updates, options.getSerialConsistency(), options.getConsistency()); return new ResultMessage.Rows(buildCasResultSet(key, result)); }', 'ground_truth': 'public ResultMessage executeWithCondition(QueryState queryState, QueryOptions options) throws RequestExecutionException, RequestValidationException { List<ByteBuffer> keys = buildPartitionKeyNames(options); if (keys.size() > 1) throw new InvalidRequestException("IN on the partition key is not supported with conditional updates"); ByteBuffer key = keys.get(0); long now = options.getTimestamp(queryState); CQL3CasConditions conditions = new CQL3CasConditions(cfm, now); Composite prefix = createClusteringPrefix(options); ColumnFamily updates = ArrayBackedSortedColumns.factory.create(cfm); addUpdatesAndConditions(key, prefix, updates, conditions, options, getTimestamp(now, options)); ColumnFamily result = StorageProxy.cas(keyspace(), columnFamily(), key, conditions, updates, options.getSerialConsistency(), options.getConsistency()); return new ResultMessage.Rows(buildCasResultSet(key, result)); }', 'output': 'public ResultMessage executeWithCondition(QueryState queryState, QueryOptions options) throws RequestExecutionException, RequestValidationException { List<ByteBuffer> variables = options.getValues(); List<ByteBuffer> keys = buildPartitionKeyNames(variables); if (keys.size() > 1) throw new InvalidRequestException("IN on the partition key is not supported with conditional updates"); ByteBuffer key = keys.get(0); CQL3CasConditions conditions = new CQL3CasConditions(cfm, queryState.getTimestamp()); ClusteringPrefixBuilder prefix = createClusteringPrefixBuilder(variables); ColumnFamily updates = UnsortedColumns.factory.create(cfm); addUpdatesAndConditions(key, prefix, updates, conditions, variables, getTimestamp(queryState.getTimestamp(), variables)); ColumnFamily result = StorageProxy.cas(keyspace(), columnFamily(), key, conditions, updates, options.getSerialConsistency(), options.getConsistency()); return new ResultMessage.Rows(buildCasResultSet(key, result)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testEncodeDecodeFields() throws Exception { SdkComponents sdkComponents = SdkComponents.create(); RunnerApi.PCollection protoCollection = PCollectionTranslation .toProto(testCollection, sdkComponents); XXX protoComponents = sdkComponents.toComponents(); Coder<?> decodedCoder = PCollectionTranslation.getCoder(protoCollection, protoComponents); WindowingStrategy<?, ?> decodedStrategy = PCollectionTranslation.getWindowingStrategy(protoCollection, protoComponents); IsBounded decodedIsBounded = PCollectionTranslation.isBounded(protoCollection); assertThat(decodedCoder, Matchers.<Coder<?>>equalTo(testCollection.getCoder())); assertThat( decodedStrategy, Matchers.<WindowingStrategy<?, ?>>equalTo( testCollection.getWindowingStrategy().fixDefaults())); assertThat(decodedIsBounded, equalTo(testCollection.isBounded())); }', 'ground_truth': 'public void testEncodeDecodeFields() throws Exception { SdkComponents sdkComponents = SdkComponents.create(); RunnerApi.PCollection protoCollection = PCollectionTranslation .toProto(testCollection, sdkComponents); RehydratedComponents protoComponents = RehydratedComponents.forComponents(sdkComponents.toComponents()); Coder<?> decodedCoder = protoComponents.getCoder(protoCollection.getCoderId()); WindowingStrategy<?, ?> decodedStrategy = protoComponents.getWindowingStrategy(protoCollection.getWindowingStrategyId()); IsBounded decodedIsBounded = PCollectionTranslation.isBounded(protoCollection); assertThat(decodedCoder, Matchers.<Coder<?>>equalTo(testCollection.getCoder())); assertThat( decodedStrategy, Matchers.<WindowingStrategy<?, ?>>equalTo( testCollection.getWindowingStrategy().fixDefaults())); assertThat(decodedIsBounded, equalTo(testCollection.isBounded())); }', 'output': 'public void testEncodeDecodeFields() throws Exception { SdkComponents sdkComponents = SdkComponents.create(); RunnerApi.PCollection protoCollection = PCollectionTranslation .toProto(testCollection, sdkComponents); List<RunnerApi.PCollection> protoComponents = sdkComponents.toComponents(); Coder<?> decodedCoder = PCollectionTranslation.getCoder(protoCollection, protoComponents); WindowingStrategy<?, ?> decodedStrategy = PCollectionTranslation.getWindowingStrategy(protoCollection, protoComponents); IsBounded decodedIsBounded = PCollectionTranslation.isBounded(protoCollection); assertThat(decodedCoder, Matchers.<Coder<?>>equalTo(testCollection.getCoder())); assertThat( decodedStrategy, Matchers.<WindowingStrategy<?, ?>>equalTo( testCollection.getWindowingStrategy().fixDefaults())); assertThat(decodedIsBounded, equalTo(testCollection.isBounded())); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testBoundsAsClusteringWithNoRestrictions() { TableMetadata tableMetadata = newTableMetadata(Sort.ASC); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(tableMetadata); XXX bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEmptyStart(get(bounds, 0)); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEmptyEnd(get(bounds, 0)); }', 'ground_truth': 'public void testBoundsAsClusteringWithNoRestrictions() { TableMetadata tableMetadata = newTableMetadata(Sort.ASC); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(tableMetadata); SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEmptyStart(get(bounds, 0)); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEmptyEnd(get(bounds, 0)); }', 'output': 'public void testBoundsAsClusteringWithNoRestrictions() { TableMetadata tableMetadata = newTableMetadata(Sort.ASC); ClusteringColumnRestrictions restrictions = new ClusteringColumnRestrictions(tableMetadata); Bounds bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEmptyStart(get(bounds, 0)); bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT); assertEquals(1, bounds.size()); assertEmptyEnd(get(bounds, 0)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected void register(SystemView<?> sysView) { if (filter != null && !filter.test(sysView)) { if (log.isDebugEnabled()) log.debug("System view filtered and will not be registered [name=" + sysView.name() + \']\'); return; } else if (log.isDebugEnabled()) log.debug("Found new system view [name=" + sysView.name() + \']\'); try { XXX mlBean = new SystemViewMBean<>(sysView); ObjectName mbean = U.registerMBean( ignite().configuration().getMBeanServer(), igniteInstanceName, VIEWS, sysView.name(), mlBean, SystemViewMBean.class); mBeans.add(mbean); if (log.isDebugEnabled()) log.debug("MetricRegistry MBean created [mbean=" + mbean + \']\'); } catch (JMException e) { log.error("MBean for system view \'" + sysView.name() + "\' can\'t be created.", e); } }', 'ground_truth': 'protected void register(SystemView<?> sysView) { if (log.isDebugEnabled()) log.debug("Found new system view [name=" + sysView.name() + \']\'); try { SystemViewMBean<?> mlBean = new SystemViewMBean<>(sysView); ObjectName mbean = U.registerMBean( ignite().configuration().getMBeanServer(), igniteInstanceName, VIEWS, sysView.name(), mlBean, SystemViewMBean.class); mBeans.add(mbean); if (log.isDebugEnabled()) log.debug("MetricRegistry MBean created [mbean=" + mbean + \']\'); } catch (JMException e) { log.error("MBean for system view \'" + sysView.name() + "\' can\'t be created.", e); } }', 'output': 'protected void register(SystemView<?> sysView) { if (filter != null && !filter.test(sysView)) { if (log.isDebugEnabled()) log.debug("System view filtered and will not be registered [name=" + sysView.name() + \']\'); return; } else if (log.isDebugEnabled()) log.debug("Found new system view [name=" + sysView.name() + \']\'); try { SystemViewMBean<T> mlBean = new SystemViewMBean<>(sysView); ObjectName mbean = U.registerMBean( ignite().configuration().getMBeanServer(), igniteInstanceName, VIEWS, sysView.name(), mlBean, SystemViewMBean.class); mBeans.add(mbean); if (log.isDebugEnabled()) log.debug("MetricRegistry MBean created [mbean=" + mbean + \']\'); } catch (JMException e) { log.error("MBean for system view \'" + sysView.name() + "\' can\'t be created.", e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void stateChangeFailover3(boolean activate) throws Exception { fail("https: testReconnectSpi = true; startNodesAndBlockStatusChange(4, 0, 0, !activate); client = false; IgniteInternalFuture startFut1 = GridTestUtils.runAsync((Callable)() -> { startGrid(4); return null; }, "start-node1"); XXX startFut2 = GridTestUtils.runAsync((Callable)() -> { startGrid(5); return null; }, "start-node2"); U.sleep(1000); for (int i = 0; i < 4; i++) ((IgniteDiscoverySpi)ignite(i).configuration().getDiscoverySpi()).simulateNodeFailure(); for (int i = 0; i < 4; i++) stopGrid(getTestIgniteInstanceName(i), true, false); startFut1.get(); startFut2.get(); assertFalse(ignite(4).cluster().active()); assertFalse(ignite(5).cluster().active()); ignite(4).cluster().active(true); for (int i = 0; i < 4; i++) startGrid(i); checkCaches1(6); }', 'ground_truth': 'private void stateChangeFailover3(boolean activate) throws Exception { fail("https: testReconnectSpi = true; startNodesAndBlockStatusChange(4, 0, 0, !activate); client = false; IgniteInternalFuture<?> startFut1 = GridTestUtils.runAsync(() -> { startGrid(4); return null; }, "start-node1"); IgniteInternalFuture<?> startFut2 = GridTestUtils.runAsync(() -> { startGrid(5); return null; }, "start-node2"); U.sleep(1000); for (int i = 0; i < 4; i++) ((IgniteDiscoverySpi)ignite(i).configuration().getDiscoverySpi()).simulateNodeFailure(); for (int i = 0; i < 4; i++) stopGrid(getTestIgniteInstanceName(i), true, false); startFut1.get(); startFut2.get(); assertFalse(ignite(4).cluster().active()); assertFalse(ignite(5).cluster().active()); ignite(4).cluster().active(true); for (int i = 0; i < 4; i++) startGrid(i); checkCaches1(6); }', 'output': 'private void stateChangeFailover3(boolean activate) throws Exception { fail("https: testReconnectSpi = true; startNodesAndBlockStatusChange(4, 0, 0, !activate); client = false; IgniteInternalFuture startFut1 = GridTestUtils.runAsync((Callable)() -> { startGrid(4); return null; }, "start-node1"); IgniteInternalFuture startFut2 = GridTestUtils.runAsync((Callable)() -> { startGrid(5); return null; }, "start-node2"); U.sleep(1000); for (int i = 0; i < 4; i++) ((IgniteDiscoverySpi)ignite(i).configuration().getDiscoverySpi()).simulateNodeFailure(); for (int i = 0; i < 4; i++) stopGrid(getTestIgniteInstanceName(i), true, false); startFut1.get(); startFut2.get(); assertFalse(ignite(4).cluster().active()); assertFalse(ignite(5).cluster().active()); ignite(4).cluster().active(true); for (int i = 0; i < 4; i++) startGrid(i); checkCaches1(6); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public RowCacheKey deserialize(ByteBuffer buf) { long msb = buf.getLong(); long lsb = buf.getLong(); byte[] key = new byte[buf.getInt()]; buf.get(key); return new RowCacheKey(new UUID(msb, lsb), key); } public IRowCacheEntry deserialize(ByteBuffer buf)\n        { { try { XXX in = new DataInputBuffer(buf, false); boolean isSentinel = in.readBoolean(); if (isSentinel) return new RowCacheSentinel(in.readLong()); return CachedPartition.cacheSerializer.deserialize(in); } catch (IOException e) { throw new RuntimeException(e); } }', 'ground_truth': 'public IRowCacheEntry deserialize(ByteBuffer buf) { try { RebufferingInputStream in = new DataInputBuffer(buf, false); boolean isSentinel = in.readBoolean(); if (isSentinel) return new RowCacheSentinel(in.readLong()); return CachedPartition.cacheSerializer.deserialize(in); } catch (IOException e) { throw new RuntimeException(e); } }', 'output': 'public RowCacheKey deserialize(ByteBuffer buf) { long msb = buf.getLong(); long lsb = buf.getLong(); byte[] key = new byte[buf.getInt()]; buf.get(key); return new RowCacheKey(new UUID(msb, lsb), key); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCounterCacheInvalidate() { ColumnFamilyStore cfs = Keyspace.open(KEYSPACE1).getColumnFamilyStore(COUNTER1); cfs.truncateBlocking(); CacheService.instance.invalidateCounterCache(); Clustering c1 = CBuilder.create(cfs.metadata().comparator).add(ByteBufferUtil.bytes(1)).build(); XXX c2 = CBuilder.create(cfs.metadata().comparator).add(ByteBufferUtil.bytes(2)).build(); ColumnMetadata cd = cfs.metadata().getColumn(ByteBufferUtil.bytes("c")); assertEquals(0, CacheService.instance.counterCache.size()); assertNull(cfs.getCachedCounter(bytes(1), c1, cd, null)); assertNull(cfs.getCachedCounter(bytes(1), c2, cd, null)); assertNull(cfs.getCachedCounter(bytes(2), c1, cd, null)); assertNull(cfs.getCachedCounter(bytes(2), c2, cd, null)); assertNull(cfs.getCachedCounter(bytes(3), c1, cd, null)); assertNull(cfs.getCachedCounter(bytes(3), c2, cd, null)); cfs.putCachedCounter(bytes(1), c1, cd, null, ClockAndCount.create(1L, 1L)); cfs.putCachedCounter(bytes(1), c2, cd, null, ClockAndCount.create(1L, 2L)); cfs.putCachedCounter(bytes(2), c1, cd, null, ClockAndCount.create(2L, 1L)); cfs.putCachedCounter(bytes(2), c2, cd, null, ClockAndCount.create(2L, 2L)); cfs.putCachedCounter(bytes(3), c1, cd, null, ClockAndCount.create(3L, 1L)); cfs.putCachedCounter(bytes(3), c2, cd, null, ClockAndCount.create(3L, 2L)); assertEquals(ClockAndCount.create(1L, 1L), cfs.getCachedCounter(bytes(1), c1, cd, null)); assertEquals(ClockAndCount.create(1L, 2L), cfs.getCachedCounter(bytes(1), c2, cd, null)); assertEquals(ClockAndCount.create(2L, 1L), cfs.getCachedCounter(bytes(2), c1, cd, null)); assertEquals(ClockAndCount.create(2L, 2L), cfs.getCachedCounter(bytes(2), c2, cd, null)); assertEquals(ClockAndCount.create(3L, 1L), cfs.getCachedCounter(bytes(3), c1, cd, null)); assertEquals(ClockAndCount.create(3L, 2L), cfs.getCachedCounter(bytes(3), c2, cd, null)); cfs.invalidateCounterCache(Collections.singleton(new Bounds<Token>(cfs.decorateKey(bytes(1)).getToken(), cfs.decorateKey(bytes(2)).getToken()))); assertEquals(2, CacheService.instance.counterCache.size()); assertNull(cfs.getCachedCounter(bytes(1), c1, cd, null)); assertNull(cfs.getCachedCounter(bytes(1), c2, cd, null)); assertNull(cfs.getCachedCounter(bytes(2), c1, cd, null)); assertNull(cfs.getCachedCounter(bytes(2), c2, cd, null)); assertEquals(ClockAndCount.create(3L, 1L), cfs.getCachedCounter(bytes(3), c1, cd, null)); assertEquals(ClockAndCount.create(3L, 2L), cfs.getCachedCounter(bytes(3), c2, cd, null)); }', 'ground_truth': 'public void testCounterCacheInvalidate() { ColumnFamilyStore cfs = Keyspace.open(KEYSPACE1).getColumnFamilyStore(COUNTER1); cfs.truncateBlocking(); CacheService.instance.invalidateCounterCache(); Clustering<?> c1 = CBuilder.create(cfs.metadata().comparator).add(ByteBufferUtil.bytes(1)).build(); Clustering<?> c2 = CBuilder.create(cfs.metadata().comparator).add(ByteBufferUtil.bytes(2)).build(); ColumnMetadata cd = cfs.metadata().getColumn(ByteBufferUtil.bytes("c")); assertEquals(0, CacheService.instance.counterCache.size()); assertNull(cfs.getCachedCounter(bytes(1), c1, cd, null)); assertNull(cfs.getCachedCounter(bytes(1), c2, cd, null)); assertNull(cfs.getCachedCounter(bytes(2), c1, cd, null)); assertNull(cfs.getCachedCounter(bytes(2), c2, cd, null)); assertNull(cfs.getCachedCounter(bytes(3), c1, cd, null)); assertNull(cfs.getCachedCounter(bytes(3), c2, cd, null)); cfs.putCachedCounter(bytes(1), c1, cd, null, ClockAndCount.create(1L, 1L)); cfs.putCachedCounter(bytes(1), c2, cd, null, ClockAndCount.create(1L, 2L)); cfs.putCachedCounter(bytes(2), c1, cd, null, ClockAndCount.create(2L, 1L)); cfs.putCachedCounter(bytes(2), c2, cd, null, ClockAndCount.create(2L, 2L)); cfs.putCachedCounter(bytes(3), c1, cd, null, ClockAndCount.create(3L, 1L)); cfs.putCachedCounter(bytes(3), c2, cd, null, ClockAndCount.create(3L, 2L)); assertEquals(ClockAndCount.create(1L, 1L), cfs.getCachedCounter(bytes(1), c1, cd, null)); assertEquals(ClockAndCount.create(1L, 2L), cfs.getCachedCounter(bytes(1), c2, cd, null)); assertEquals(ClockAndCount.create(2L, 1L), cfs.getCachedCounter(bytes(2), c1, cd, null)); assertEquals(ClockAndCount.create(2L, 2L), cfs.getCachedCounter(bytes(2), c2, cd, null)); assertEquals(ClockAndCount.create(3L, 1L), cfs.getCachedCounter(bytes(3), c1, cd, null)); assertEquals(ClockAndCount.create(3L, 2L), cfs.getCachedCounter(bytes(3), c2, cd, null)); cfs.invalidateCounterCache(Collections.singleton(new Bounds<Token>(cfs.decorateKey(bytes(1)).getToken(), cfs.decorateKey(bytes(2)).getToken()))); assertEquals(2, CacheService.instance.counterCache.size()); assertNull(cfs.getCachedCounter(bytes(1), c1, cd, null)); assertNull(cfs.getCachedCounter(bytes(1), c2, cd, null)); assertNull(cfs.getCachedCounter(bytes(2), c1, cd, null)); assertNull(cfs.getCachedCounter(bytes(2), c2, cd, null)); assertEquals(ClockAndCount.create(3L, 1L), cfs.getCachedCounter(bytes(3), c1, cd, null)); assertEquals(ClockAndCount.create(3L, 2L), cfs.getCachedCounter(bytes(3), c2, cd, null)); }', 'output': 'public void testCounterCacheInvalidate() { ColumnFamilyStore cfs = Keyspace.open(KEYSPACE1).getColumnFamilyStore(COUNTER1); cfs.truncateBlocking(); CacheService.instance.invalidateCounterCache(); Clustering c1 = CBuilder.create(cfs.metadata().comparator).add(ByteBufferUtil.bytes(1)).build(); XXX c2 = CBuilder.create(cfs.metadata().comparator).add(ByteBufferUtil.bytes(2)).build(); ColumnMetadata cd = cfs.metadata().getColumn(ByteBufferUtil.bytes("c")); assertEquals(0, CacheService.instance.counterCache.size()); assertNull(cfs.getCachedCounter(bytes(1), c1, cd, null)); assertNull(cfs.getCachedCounter(bytes(1), c2, cd, null)); assertNull(cfs.getCachedCounter(bytes(2), c1, cd, null)); assertNull(cfs.getCachedCounter(bytes(2), c2, cd, null)); assertNull(cfs.getCachedCounter(bytes(3), c1, cd, null)); assertNull(cfs.getCachedCounter(bytes(3), c2, cd, null)); cfs.putCachedCounter(bytes(1), c1, cd, null, ClockAndCount.create(1L, 1L)); cfs.putCachedCounter(bytes(1), c2, cd, null, ClockAndCount.create(1L, 2L)); cfs.putCachedCounter(bytes(2), c1, cd, null, ClockAndCount.create(2L, 1L)); cfs.putCachedCounter(bytes(2), c2, cd, null, ClockAndCount.create(2L, 2L)); cfs.putCachedCounter(bytes(3), c1, cd, null, ClockAndCount.create(3L, 1L)); cfs.putCachedCounter(bytes(3), c2, cd, null, ClockAndCount.create(3L, 2L)); assertEquals(ClockAndCount.create(1L, 1L), cfs.getCachedCounter(bytes(1), c1,'}
{'instruction': 'Please refactor the following code:', 'input': 'public static synchronized ClassPathAccessor getDefault() { ClassPathAccessor instance = DEFAULT; if (instance == null) { XXX c = ClassPath.class; try { Class.forName(c.getName(), true, c.getClassLoader()); instance = DEFAULT; assert instance != null; } catch (Exception ex) { Exceptions.printStackTrace(ex); } } return instance; }', 'ground_truth': 'public static synchronized ClassPathAccessor getDefault() { ClassPathAccessor instance = DEFAULT; if (instance == null) { Class<?> c = ClassPath.class; try { Class.forName(c.getName(), true, c.getClassLoader()); instance = DEFAULT; assert instance != null; } catch (Exception ex) { Exceptions.printStackTrace(ex); } } return instance; }', 'output': 'public static synchronized ClassPathAccessor getDefault() { ClassPathAccessor instance = DEFAULT; if (instance == null) { ClassPath classPath = ClassPath.class; try { Class.forName(classPath.getName(), true, classPath.getClassLoader()); instance = DEFAULT; assert instance != null; } catch (Exception ex) { Exceptions.printStackTrace(ex); } } return instance; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testFields() { Inspector insp = new Inspector(this); XXX fields = insp.getPublicFields(); assertEquals(2, fields.length); String[] names = { "someField","SOME_CONST" }; assertNameEquals(names, fields); String[] details = {"JAVA","public","InspectorTest","String","someField","only for testing"}; assertContains(fields, details); }', 'ground_truth': 'public void testFields() { Inspector insp = new Inspector(this); Object[] fields = insp.getPublicFields(); assertEquals(2, fields.length); String[] names = { "someField","SOME_CONST" }; assertNameEquals(names, fields); String[] details = {"JAVA","public","InspectorTest","String","someField","only for testing"}; assertContains(fields, details); }', 'output': 'public void testFields() { Inspector insp = new Inspector(this); List<Field> fields = insp.getPublicFields(); assertEquals(2, fields.size()); String[] names = { "someField","SOME_CONST" }; assertNameEquals(names, fields); String[] details = {"JAVA","public","InspectorTest","String","someField","only for testing"}; assertContains(fields, details); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': '@Override protected GridConfiguration getConfiguration() throws Exception { XXX cfg = super.getConfiguration(); cfg.setDeploymentSpi(new GridLocalDeploymentSpi()); ((GridTcpDiscoverySpi)cfg.getDiscoverySpi()).setHeartbeatFrequency(500); cfg.setDeploymentMode(depMode); cfg.setDeploymentMode(depMode); return cfg; } /** * @return Test grid name. */ @Override public String getTestGridName() { return "GridExternalAopTarget"; } } }', 'ground_truth': '@Override protected IgniteConfiguration getConfiguration() throws Exception { IgniteConfiguration cfg = super.getConfiguration(); cfg.setDeploymentSpi(new GridLocalDeploymentSpi()); ((GridTcpDiscoverySpi)cfg.getDiscoverySpi()).setHeartbeatFrequency(500); cfg.setDeploymentMode(depMode); cfg.setDeploymentMode(depMode); return cfg; }', 'output': '@Override protected GridConfiguration getConfiguration() throws Exception { IgniteConfiguration cfg = super.getConfiguration(); cfg.setDeploymentSpi(new GridLocalDeploymentSpi()); ((GridTcpDiscoverySpi)cfg.getDiscoverySpi()).setHeartbeatFrequency(500); cfg.setDeploymentMode(depMode); cfg.setDeploymentMode(depMode); return cfg; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void sstableListChangedAddAndRemove() { XXX repairID = registerSession(cfs, true, true); LocalSessionAccessor.prepareUnsafe(repairID, COORDINATOR, PARTICIPANTS); SSTableReader sstable1 = makeSSTable(true); mutateRepaired(sstable1, repairID, false); SSTableReader sstable2 = makeSSTable(true); mutateRepaired(sstable2, repairID, false); Assert.assertFalse(repairedContains(sstable1)); Assert.assertFalse(unrepairedContains(sstable1)); Assert.assertFalse(repairedContains(sstable2)); Assert.assertFalse(unrepairedContains(sstable2)); Assert.assertFalse(hasPendingStrategiesFor(repairID)); Assert.assertFalse(hasTransientStrategiesFor(repairID)); SSTableListChangedNotification notification; notification = new SSTableListChangedNotification(Collections.singleton(sstable1), Collections.emptyList(), OperationType.COMPACTION); csm.handleNotification(notification, cfs.getTracker()); Assert.assertFalse(repairedContains(sstable1)); Assert.assertFalse(unrepairedContains(sstable1)); Assert.assertTrue(pendingContains(sstable1)); Assert.assertFalse(repairedContains(sstable2)); Assert.assertFalse(unrepairedContains(sstable2)); Assert.assertFalse(pendingContains(sstable2)); Assert.assertTrue(hasPendingStrategiesFor(repairID)); Assert.assertFalse(hasTransientStrategiesFor(repairID)); notification = new SSTableListChangedNotification(Collections.singleton(sstable2), Collections.singleton(sstable1), OperationType.COMPACTION); csm.handleNotification(notification, cfs.getTracker()); Assert.assertFalse(repairedContains(sstable1)); Assert.assertFalse(unrepairedContains(sstable1)); Assert.assertFalse(pendingContains(sstable1)); Assert.assertFalse(repairedContains(sstable2)); Assert.assertFalse(unrepairedContains(sstable2)); Assert.assertTrue(pendingContains(sstable2)); }', 'ground_truth': 'public void sstableListChangedAddAndRemove() { TimeUUID repairID = registerSession(cfs, true, true); LocalSessionAccessor.prepareUnsafe(repairID, COORDINATOR, PARTICIPANTS); SSTableReader sstable1 = makeSSTable(true); mutateRepaired(sstable1, repairID, false); SSTableReader sstable2 = makeSSTable(true); mutateRepaired(sstable2, repairID, false); Assert.assertFalse(repairedContains(sstable1)); Assert.assertFalse(unrepairedContains(sstable1)); Assert.assertFalse(repairedContains(sstable2)); Assert.assertFalse(unrepairedContains(sstable2)); Assert.assertFalse(hasPendingStrategiesFor(repairID)); Assert.assertFalse(hasTransientStrategiesFor(repairID)); SSTableListChangedNotification notification; notification = new SSTableListChangedNotification(Collections.singleton(sstable1), Collections.emptyList(), OperationType.COMPACTION); csm.handleNotification(notification, cfs.getTracker()); Assert.assertFalse(repairedContains(sstable1)); Assert.assertFalse(unrepairedContains(sstable1)); Assert.assertTrue(pendingContains(sstable1)); Assert.assertFalse(repairedContains(sstable2)); Assert.assertFalse(unrepairedContains(sstable2)); Assert.assertFalse(pendingContains(sstable2)); Assert.assertTrue(hasPendingStrategiesFor(repairID)); Assert.assertFalse(hasTransientStrategiesFor(repairID)); notification = new SSTableListChangedNotification(Collections.singleton(sstable2), Collections.singleton(sstable1), OperationType.COMPACTION); csm.handleNotification(notification, cfs.getTracker()); Assert.assertFalse(repairedContains(sstable1)); Assert.assertFalse(unrepairedContains(sstable1)); Assert.assertFalse(pendingContains(sstable1)); Assert.assertFalse(repairedContains(sstable2)); Assert.assertFalse(unrepairedContains(sstable2)); Assert.assertTrue(pendingContains(sstable2)); }', 'output': 'public void sstableListChangedAddAndRemove() { String repairID = registerSession(cfs, true, true); LocalSessionAccessor.prepareUnsafe(repairID, COORDINATOR, PARTICIPANTS); SSTableReader sstable1 = makeSSTable(true); mutateRepaired(sstable1, repairID, false); SSTableReader sstable2 = makeSSTable(true); mutateRepaired(sstable2, repairID, false); Assert.assertFalse(repairedContains(sstable1)); Assert.assertFalse(unrepairedContains(sstable1)); Assert.assertFalse(repairedContains(sstable2)); Assert.assertFalse(unrepairedContains(sstable2)); Assert.assertFalse(hasPendingStrategiesFor(repairID)); Assert.assertFalse(hasTransientStrategiesFor(repairID)); SSTableListChangedNotification notification; notification = new SSTableListChangedNotification(Collections.singleton(sstable1), Collections.emptyList(), OperationType.COMPACTION); csm.handleNotification(notification, cfs.getTracker()); Assert.assertFalse(repairedContains(sstable1)); Assert.assertFalse(unrepairedContains(sstable1)); Assert.assertTrue(pendingContains(sstable1)); Assert.assertFalse(repairedContains(sstable2)); Assert.assertFalse(unrepairedContains(sstable2)); Assert.assertFalse(pendingContains(sstable2)); Assert.assertTrue(hasPendingStrategiesFor(repairID)); Assert.assertFalse(hasTransientStrategiesFor(repairID)); notification = new SSTableListChangedNotification(Collections.singleton(sstable2), Collections.singleton(sstable1), OperationType.COMPACTION); csm.handleNotification(notification, cfs.getTracker()); Assert.assertFalse(repairedContains(sstable1)); Assert.assertFalse(unrepairedContains(sstable1)); Assert.assertFalse(pendingContains(sstable1)); Assert.assertFalse(repairedContains(sstable2)); Assert.assertFalse(unrepairedContains(sstable2)); Assert.assertTrue(pendingContains(sstable2)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testEntrySetIteratorMultithreaded() throws Exception { for (int i = 0; i < gridCount(); i++) cache(i).removeAll(); final XXX putFut = GridTestUtils.runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { for (int i = 0; i < entryCount(); i++) cache().put(KEY_PREFIX + i, i); } }, 1, "put-thread"); GridTestUtils.runMultiThreaded(new CA() { @Override public void apply() { while (!putFut.isDone()) { for (GridCacheEntry<String, Integer> entry : cache().entrySet()) { assert entry != null; assert entry.getKey() != null; assert entry.getKey().contains(KEY_PREFIX); } } } }, 3, "iterator-thread"); }', 'ground_truth': 'public void testEntrySetIteratorMultithreaded() throws Exception { for (int i = 0; i < gridCount(); i++) cache(i).removeAll(); final IgniteFuture<?> putFut = GridTestUtils.runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { for (int i = 0; i < entryCount(); i++) cache().put(KEY_PREFIX + i, i); } }, 1, "put-thread"); GridTestUtils.runMultiThreaded(new CA() { @Override public void apply() { while (!putFut.isDone()) { for (GridCacheEntry<String, Integer> entry : cache().entrySet()) { assert entry != null; assert entry.getKey() != null; assert entry.getKey().contains(KEY_PREFIX); } } } }, 3, "iterator-thread"); }', 'output': 'public void testEntrySetIteratorMultithreaded() throws Exception { for (int i = 0; i < gridCount(); i++) cache(i).removeAll(); final IgniteInternalFuture<?> putFut = GridTestUtils.runMultiThreadedAsync(new CAX() { @Override public void applyx() throws GridException { for (int i = 0; i < entryCount(); i++) cache().put(KEY_PREFIX + i, i); } }, 1, "put-thread"); GridTestUtils.runMultiThreaded(new CA() { @Override public void apply() { while (!putFut.isDone()) { for (GridCacheEntry<String, Integer> entry : cache().entrySet()) { assert entry != null; assert entry.getKey() != null; assert entry.getKey().contains(KEY_PREFIX); } } } }, 3, "iterator-thread"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Collection<ApplicationUser> find(final @Nullable String _search) { final XXX regex = asRegex(_search); return repository.allMatches(Query.named(this.applicationUserClass, ApplicationUser.NAMED_QUERY_FIND) .withParameter("regex", regex)) .stream() .collect(_Sets.toUnmodifiableSorted()); }', 'ground_truth': 'public Collection<ApplicationUser> find(final @Nullable String _search) { val regex = regexReplacer.asRegex(_search); return repository.allMatches(Query.named(this.applicationUserClass, ApplicationUser.NAMED_QUERY_FIND) .withParameter("regex", regex)) .stream() .collect(_Sets.toUnmodifiableSorted()); }', 'output': 'public Collection<ApplicationUser> find(final @Nullable String _search) { final Pattern regex = asRegex(_search); return repository.allMatches(Query.named(this.applicationUserClass, ApplicationUser.NAMED_QUERY_FIND) .withParameter("regex", regex)) .stream() .collect(_Sets.toUnmodifiableSorted()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testWatermarkEarliestState() throws Exception { XXX value = underTest.state(NAMESPACE_1, WATERMARK_EARLIEST_ADDR); assertEquals(value, underTest.state(NAMESPACE_1, WATERMARK_EARLIEST_ADDR)); assertFalse(value.equals(underTest.state(NAMESPACE_2, WATERMARK_EARLIEST_ADDR))); assertThat(value.read(), Matchers.nullValue()); value.add(new Instant(2000)); assertThat(value.read(), equalTo(new Instant(2000))); value.add(new Instant(3000)); assertThat(value.read(), equalTo(new Instant(2000))); value.add(new Instant(1000)); assertThat(value.read(), equalTo(new Instant(1000))); value.clear(); assertThat(value.read(), equalTo(null)); assertThat(underTest.state(NAMESPACE_1, WATERMARK_EARLIEST_ADDR), Matchers.sameInstance(value)); }', 'ground_truth': 'public void testWatermarkEarliestState() throws Exception { WatermarkHoldState value = underTest.state(NAMESPACE_1, WATERMARK_EARLIEST_ADDR); assertEquals(value, underTest.state(NAMESPACE_1, WATERMARK_EARLIEST_ADDR)); assertFalse(value.equals(underTest.state(NAMESPACE_2, WATERMARK_EARLIEST_ADDR))); assertThat(value.read(), Matchers.nullValue()); value.add(new Instant(2000)); assertThat(value.read(), equalTo(new Instant(2000))); value.add(new Instant(3000)); assertThat(value.read(), equalTo(new Instant(2000))); value.add(new Instant(1000)); assertThat(value.read(), equalTo(new Instant(1000))); value.clear(); assertThat(value.read(), equalTo(null)); assertThat(underTest.state(NAMESPACE_1, WATERMARK_EARLIEST_ADDR), Matchers.sameInstance(value)); }', 'output': 'public void testWatermarkEarliestState() throws Exception { WatermarkEarliestState value = underTest.state(NAMESPACE_1, WATERMARK_EARLIEST_ADDR); assertEquals(value, underTest.state(NAMESPACE_1, WATERMARK_EARLIEST_ADDR)); assertFalse(value.equals(underTest.state(NAMESPACE_2, WATERMARK_EARLIEST_ADDR))); assertThat(value.read(), Matchers.nullValue()); value.add(new Instant(2000)); assertThat(value.read(), equalTo(new Instant(2000))); value.add(new Instant(3000)); assertThat(value.read(), equalTo(new Instant(2000))); value.add(new Instant(1000)); assertThat(value.read(), equalTo(new Instant(1000))); value.clear(); assertThat(value.read(), equalTo(null)); assertThat(underTest.state(NAMESPACE_1, WATERMARK_EARLIEST_ADDR), Matchers.sameInstance(value)); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testGetHostComponents() { XXX resourceDef = createStrictMock(ResourceDefinition.class); ResultSerializer resultSerializer = createStrictMock(ResultSerializer.class); Object serializedResult = new Object(); RequestFactory requestFactory = createStrictMock(RequestFactory.class); ResponseFactory responseFactory = createStrictMock(ResponseFactory.class); Request request = createNiceMock(Request.class); RequestHandler requestHandler = createStrictMock(RequestHandler.class); Result result = createStrictMock(Result.class); Response response = createStrictMock(Response.class); HttpHeaders httpHeaders = createNiceMock(HttpHeaders.class); UriInfo uriInfo = createNiceMock(UriInfo.class); String clusterName = "clusterName"; String hostName = "hostName"; expect(requestFactory.createRequest(eq(httpHeaders), isNull(String.class), eq(uriInfo), eq(Request.Type.GET), eq(resourceDef))).andReturn(request); expect(requestHandler.handleRequest(request)).andReturn(result); expect(request.getResultSerializer()).andReturn(resultSerializer); expect(resultSerializer.serialize(result, uriInfo)).andReturn(serializedResult); expect(result.isSynchronous()).andReturn(true).atLeastOnce(); expect(responseFactory.createResponse(Request.Type.GET, serializedResult, true)).andReturn(response); replay(resourceDef, resultSerializer, requestFactory, responseFactory, request, requestHandler, result, response, httpHeaders, uriInfo); HostComponentService componentService = new TestHostComponentService(resourceDef, clusterName, hostName, null, requestFactory, responseFactory, requestHandler); assertSame(response, componentService.getHostComponents(httpHeaders, uriInfo)); verify(resourceDef, resultSerializer, requestFactory, responseFactory, request, requestHandler, result, response, httpHeaders, uriInfo); }', 'ground_truth': 'public void testGetHostComponents() { ResourceInstance resourceDef = createStrictMock(ResourceInstance.class); ResultSerializer resultSerializer = createStrictMock(ResultSerializer.class); Object serializedResult = new Object(); RequestFactory requestFactory = createStrictMock(RequestFactory.class); ResponseFactory responseFactory = createStrictMock(ResponseFactory.class); Request request = createNiceMock(Request.class); RequestHandler requestHandler = createStrictMock(RequestHandler.class); Result result = createStrictMock(Result.class); Response response = createStrictMock(Response.class); HttpHeaders httpHeaders = createNiceMock(HttpHeaders.class); UriInfo uriInfo = createNiceMock(UriInfo.class); String clusterName = "clusterName"; String hostName = "hostName"; expect(requestFactory.createRequest(eq(httpHeaders), isNull(String.class), eq(uriInfo), eq(Request.Type.GET), eq(resourceDef))).andReturn(request); expect(requestHandler.handleRequest(request)).andReturn(result); expect(request.getResultSerializer()).andReturn(resultSerializer); expect(resultSerializer.serialize(result, uriInfo)).andReturn(serializedResult); expect(result.isSynchronous()).andReturn(true).atLeastOnce(); expect(responseFactory.createResponse(Request.Type.GET, serializedResult, true)).andReturn(response); replay(resourceDef, resultSerializer, requestFactory, responseFactory, request, requestHandler, result, response, httpHeaders, uriInfo); HostComponentService componentService = new TestHostComponentService(resourceDef, clusterName, hostName, null, requestFactory, responseFactory, requestHandler); assertSame(response, componentService.getHostComponents(httpHeaders, uriInfo)); verify(resourceDef, resultSerializer, requestFactory, responseFactory, request, requestHandler, result, response, httpHeaders, uriInfo); }', 'output': 'public void testGetHostComponents() { ResourceDefinition resourceDef = createStrictMock(ResourceDefinition.class); ResultSerializer resultSerializer = createStrictMock(ResultSerializer.class); Object serializedResult = new Object(); RequestFactory requestFactory = createStrictMock(RequestFactory.class); ResponseFactory responseFactory = createStrictMock(ResponseFactory.class); Request request = createNiceMock(Request.class); RequestHandler requestHandler = createStrictMock(RequestHandler.class); Result result = createStrictMock(Result.class); Response response = createStrictMock(Response.class); HttpHeaders httpHeaders = createNiceMock(HttpHeaders.class); UriInfo uriInfo = createNiceMock(UriInfo.class); String clusterName = "clusterName"; String hostName = "hostName"; expect(requestFactory.createRequest(eq(httpHeaders), isNull(String.class), eq(uriInfo), eq(Request.Type.GET), eq(resourceDef))).andReturn(request); expect(requestHandler.handleRequest(request)).andReturn(result); expect(request.getResultSerializer()).andReturn(resultSerializer); expect(resultSerializer.serialize(result, uriInfo)).andReturn(serializedResult); expect(result.isSynchronous()).andReturn(true).atLeastOnce(); expect(responseFactory.createResponse(Request.Type.GET, serializedResult, true)).andReturn(response); replay(resourceDef, resultSerializer, requestFactory, responseFactory, request, requestHandler, result, response, httpHeaders, uriInfo); HostComponentService componentService = new TestHostComponentService(resourceDef, clusterName, hostName, null, requestFactory, responseFactory, requestHandler); assertSame(response, componentService.getHostComponents(httpHeaders, uriInfo)); verify(resourceDef, resultSerializer, requestFactory, responseFactory, request, requestHandler, result, response, httpHeaders, uriInfo); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPathModeSecondaryNoCfg() throws Exception { pathModes(F.t("dir", PROXY)); String errMsg = null; try { startUp(); } catch (XXX e) { errMsg = e.getCause().getMessage(); } assertTrue(errMsg.startsWith( "Grid configuration parameter invalid: secondaryFileSystem cannot be null when mode is SECONDARY")); }', 'ground_truth': 'public void testPathModeSecondaryNoCfg() throws Exception { pathModes(F.t("dir", PROXY)); String errMsg = null; try { startUp(); } catch (IgniteException e) { errMsg = e.getCause().getMessage(); } assertTrue(errMsg.startsWith( "Grid configuration parameter invalid: secondaryFileSystem cannot be null when mode is SECONDARY")); }', 'output': 'public void testPathModeSecondaryNoCfg() throws Exception { pathModes(F.t("dir", PROXY)); String errMsg = null; try { startUp(); } catch (IgniteException e) { errMsg = e.getCause().getMessage(); } assertTrue(errMsg.startsWith( "Grid configuration parameter invalid: secondaryFileSystem cannot be null when mode is SECONDARY")); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public final FileResult close() throws Exception { checkState(outputFile != null, "FileResult.close cannot be called with a null outputFile"); LOG.debug("Writing footer to {}.", outputFile); try { writeFooter(); } catch (Exception e) { LOG.error("Writing footer to {} failed, closing channel.", outputFile, e); closeChannelAndThrow(channel, outputFile, e); } LOG.debug("Finishing write to {}.", outputFile); try { finishWrite(); } catch (Exception e) { LOG.error("Finishing write to {} failed, closing channel.", outputFile, e); closeChannelAndThrow(channel, outputFile, e); } checkState( channel.isOpen(), "Channel %s to %s should only be closed by its owner: %s", channel, outputFile); LOG.debug("Closing channel to {}.", outputFile); try { channel.close(); } catch (Exception e) { throw new IOException(String.format("Failed closing channel to %s", outputFile), e); } XXX result = new FileResult(outputFile, shard, window, paneInfo); LOG.debug("Result for bundle {}: {}", this.id, outputFile); return result; }', 'ground_truth': 'public final FileResult<DestinationT> close() throws Exception { checkState(outputFile != null, "FileResult.close cannot be called with a null outputFile"); LOG.debug("Writing footer to {}.", outputFile); try { writeFooter(); } catch (Exception e) { LOG.error("Writing footer to {} failed, closing channel.", outputFile, e); closeChannelAndThrow(channel, outputFile, e); } LOG.debug("Finishing write to {}.", outputFile); try { finishWrite(); } catch (Exception e) { LOG.error("Finishing write to {} failed, closing channel.", outputFile, e); closeChannelAndThrow(channel, outputFile, e); } checkState( channel.isOpen(), "Channel %s to %s should only be closed by its owner: %s", channel, outputFile); LOG.debug("Closing channel to {}.", outputFile); try { channel.close(); } catch (Exception e) { throw new IOException(String.format("Failed closing channel to %s", outputFile), e); } FileResult<DestinationT> result = new FileResult<>(outputFile, shard, window, paneInfo, destination); LOG.debug("Result for bundle {}: {}", this.id, outputFile); return result; }', 'output': 'public final FileResult close() throws Exception { checkState(outputFile != null, "FileResult.close cannot be called with a null outputFile"); LOG.debug("Writing footer to {}.", outputFile); try { writeFooter(); } catch (Exception e) { LOG.error("Writing footer to {} failed, closing channel.", outputFile, e); closeChannelAndThrow(channel, outputFile, e); } LOG.debug("Finishing write to {}.", outputFile); try { finishWrite(); } catch (Exception e) { LOG.error("Finishing write to {} failed, closing channel.", outputFile, e); closeChannelAndThrow(channel, outputFile, e); } checkState( channel.isOpen(), "Channel %s to %s should only be closed by its owner: %s", channel, outputFile); LOG.debug("Closing channel to {}.", outputFile); try { channel.close(); } catch (Exception e) { throw new IOException(String.format("Failed closing channel to %s", outputFile), e); } XXX result = new FileResult(outputFile, shard, window, paneInfo); LOG.debug("Result for bundle {}: {}", this.id, outputFile); return result; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': "public static String URLEnc(String s, String charset) throws UnsupportedEncodingException { int ln = s.length(); int i; for (i = 0; i < ln; i++) { char c = s.charAt(i); if (!(c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c >= '0' && c <= '9' || c == '_' || c == '-' || c == '.' || c == '!' || c == '~' || c >= '\\'' && c <= '*')) { break; } } if (i == ln) { return s; } XXX b = new StringBuffer(ln + ln / 3 + 2); b.append(s.substring(0, i)); int encstart = i; for (i++; i < ln; i++) { char c = s.charAt(i); if (c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c >= '0' && c <= '9' || c == '_' || c == '-' || c == '.' || c == '!' || c == '~' || c >= '\\'' && c <= '*') { if (encstart != -1) { byte[] o = s.substring(encstart, i).getBytes(charset); for (int j = 0; j < o.length; j++) { b.append('%'); byte bc = o[j]; int c1 = bc & 0x0F; int c2 = (bc >> 4) & 0x0F; b.append((char) (c2 < 10 ? c2 + '0' : c2 - 10 + 'A')); b.append((char) (c1 < 10 ? c1 + '0' : c1 - 10 + 'A')); } encstart = -1; } b.append(c); } else { if (encstart == -1) { encstart = i; } } } if (encstart != -1) { byte[] o = s.substring(encstart, i).getBytes(charset); for (int j = 0; j < o.length; j++) { b.append('%'); byte bc = o[j]; int c1 = bc & 0x0F; int c2 = (bc >> 4) & 0x0F; b.append((char) (c2 < 10 ? c2 + '0' : c2 - 10 + 'A')); b.append((char) (c1 < 10 ? c1 + '0' : c1 - 10 + 'A')); } } return b.toString(); }", 'ground_truth': "public static String URLEnc(String s, String charset) throws UnsupportedEncodingException { int ln = s.length(); int i; for (i = 0; i < ln; i++) { char c = s.charAt(i); if (!(c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c >= '0' && c <= '9' || c == '_' || c == '-' || c == '.' || c == '!' || c == '~' || c >= '\\'' && c <= '*')) { break; } } if (i == ln) { return s; } StringBuilder b = new StringBuilder(ln + ln / 3 + 2); b.append(s.substring(0, i)); int encstart = i; for (i++; i < ln; i++) { char c = s.charAt(i); if (c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c >= '0' && c <= '9' || c == '_' || c == '-' || c == '.' || c == '!' || c == '~' || c >= '\\'' && c <= '*') { if (encstart != -1) { byte[] o = s.substring(encstart, i).getBytes(charset); for (int j = 0; j < o.length; j++) { b.append('%'); byte bc = o[j]; int c1 = bc & 0x0F; int c2 = (bc >> 4) & 0x0F; b.append((char) (c2 < 10 ? c2 + '0' : c2 - 10 + 'A')); b.append((char) (c1 < 10 ? c1 + '0' : c1 - 10 + 'A')); } encstart = -1; } b.append(c); } else { if (encstart == -1) { encstart = i; } } } if (encstart != -1) { byte[] o = s.substring(encstart, i).getBytes(charset); for (int j = 0; j < o.length; j++) { b.append('%'); byte bc = o[j]; int c1 = bc & 0x0F; int c2 = (bc >> 4) & 0x0F; b.append((char) (c2 < 10 ? c2 + '0' : c2 - 10 + 'A')); b.append((char) (c1 < 10 ? c1 + '0' : c1 - 10 + 'A')); } } return b.toString(); }", 'output': "public static String URLEnc(String s, String charset) throws UnsupportedEncodingException { int ln = s.length(); int i; for (i = 0; i < ln; i++) { char c = s.charAt(i); if (!(c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c >= '0' && c <= '9' || c == '_' || c == '-' || c == '.' || c == '!' || c == '~' || c >= '\\'' && c <= '*')) { break; } } if (i == ln) { return s; } StringBuilder b = new StringBuilder(ln + ln / 3 + 2); b.append(s.substring(0, i)); int encstart = i; for (i++; i < ln; i++) { char c = s.charAt(i); if (c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c >= '0' && c <= '9' || c == '_' || c == '-' || c == '.' || c == '!' || c == '~' || c >= '\\'' && c <= '*') { if (encstart != -1) { byte[] o = s.substring(encstart, i).getBytes(charset); for (int j = 0; j < o.length; j++) { b.append('%'); byte bc = o[j]; int c1 = bc & 0x0F; int c2 = (bc >> 4) & 0x0F; b.append((char) (c2 < 10 ? c2 + '0' : c2 - 10 + 'A')); b.append((char) (c1 < 10 ? c1 + '0' : c1 - 10 + 'A')); } encstart = -1; } b.append(c); } else { if (encstart == -1) { encstart = i; } } } if (encstart != -1) { byte[] o = s.substring(encstart, i).getBytes(charset); for (int j = 0; j < o.length; j++) { b."}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCheckpointsAfterNumOutputs() throws Exception { int max = 100; DoFn<Integer, String> fn = new CounterFn(2 * max + max / 2, 2 * max); Instant base = Instant.now(); int baseIndex = 42; ProcessFnTester<Integer, String, SomeCheckpoint, SomeCheckpointTracker> tester = new ProcessFnTester<>( base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(SomeCheckpoint.class), max, MAX_BUNDLE_DURATION); List<String> elements; tester.startElement(baseIndex, new SomeCheckpoint(0)); elements = tester.takeOutputElements(); assertEquals(max, elements.size()); assertThat(elements, hasItem(String.valueOf(baseIndex))); assertThat(elements, hasItem(String.valueOf(baseIndex + max - 1))); assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1))); elements = tester.takeOutputElements(); assertEquals(max, elements.size()); assertThat(elements, hasItem(String.valueOf(baseIndex + max))); assertThat(elements, hasItem(String.valueOf(baseIndex + 2 * max - 1))); assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1))); elements = tester.takeOutputElements(); assertEquals(max / 2, elements.size()); assertThat(elements, hasItem(String.valueOf(baseIndex + 2 * max))); assertThat(elements, hasItem(String.valueOf(baseIndex + 2 * max + max / 2 - 1))); assertThat(elements, not(hasItem((String.valueOf(baseIndex + 2 * max + max / 2))))); }', 'ground_truth': 'public void testCheckpointsAfterNumOutputs() throws Exception { int max = 100; DoFn<Integer, String> fn = new CounterFn(); Instant base = Instant.now(); int baseIndex = 42; ProcessFnTester<Integer, String, OffsetRange, OffsetRangeTracker> tester = new ProcessFnTester<>( base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(OffsetRange.class), max, MAX_BUNDLE_DURATION); List<String> elements; tester.startElement(baseIndex, new OffsetRange(0, 2 * max + max / 2)); elements = tester.takeOutputElements(); assertEquals(max, elements.size()); assertThat(elements, hasItem(String.valueOf(baseIndex))); assertThat(elements, hasItem(String.valueOf(baseIndex + max - 1))); assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1))); elements = tester.takeOutputElements(); assertEquals(max, elements.size()); assertThat(elements, hasItem(String.valueOf(baseIndex + max))); assertThat(elements, hasItem(String.valueOf(baseIndex + 2 * max - 1))); assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1))); elements = tester.takeOutputElements(); assertEquals(max / 2, elements.size()); assertThat(elements, hasItem(String.valueOf(baseIndex + 2 * max))); assertThat(elements, hasItem(String.valueOf(baseIndex + 2 * max + max / 2 - 1))); assertThat(elements, not(hasItem((String.valueOf(baseIndex + 2 * max + max / 2))))); }', 'output': 'public void testCheckpointsAfterNumOutputs() throws Exception { int max = 100; DoFn<Integer, String> fn = new CounterFn(2 * max + max / 2, 2 * max); Instant base = Instant.now(); int baseIndex = 42; ProcessFnTester<Integer, String, SomeCheckpoint, SomeCheckpointTracker> tester = new ProcessFnTester<>( base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(SomeCheckpoint.class), max, MAX_BUNDLE_DURATION); List<String> elements; tester.startElement(baseIndex, new SomeCheckpoint(0)); elements = tester.takeOutputElements(); assertEquals(max, elements.size()); assertThat(elements, hasItem(String.valueOf(baseIndex))); assertThat(elements, hasItem(String.valueOf(baseIndex + max - 1))); assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1))); elements = tester.takeOutputElements(); assertEquals(max, elements.size()); assertThat(elements, hasItem(String.valueOf(baseIndex + max))); assertThat(elements, hasItem(String.valueOf(baseIndex + 2 * max - 1))); assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1))); elements = tester.takeOutputElements(); assertEquals(max / 2, elements.size()); assertThat(elements, hasItem(String.valueOf(baseIndex + 2 * max))); assertThat(elements, hasItem(String.valueOf(baseIndex + 2 * max + max / 2 - 1))); assertThat(elements, not(hasItem((String.valueOf(baseIndex + 2 * max + max / 2))))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void doVerb(Message message) { byte[] body = message.getMessageBody(); ByteArrayInputStream bufIn = new ByteArrayInputStream(body); if (logger.isDebugEnabled()) logger.debug(String.format("StreamInitiateVerbeHandler.doVerb %s %s %s", message.getVerb(), message.getMessageId(), message.getMessageType())); try { StreamInitiateMessage biMsg = StreamInitiateMessage.serializer().deserialize(new DataInputStream(bufIn)); InitiatedFile[] initiatedFiles = biMsg.getStreamContext(); if (initiatedFiles.length == 0) { if (logger.isDebugEnabled()) logger.debug("no data needed from " + message.getFrom()); if (StorageService.instance.isBootstrapMode()) StorageService.instance.removeBootstrapSource(message.getFrom(), new String(message.getHeader(StreamOut.TABLE_NAME))); return; } Map<String, String> fileNames = getNewNames(initiatedFiles); Map<String, String> pathNames = new HashMap<String, String>(); for (String ssName : fileNames.keySet()) pathNames.put(ssName, DatabaseDescriptor.getNextAvailableDataLocation()); /* * For each of stream context\'s in the incoming message * generate the new file names and store the new file names * in the StreamContextManager. */ for (InitiatedFile initiatedFile : initiatedFiles) { XXX streamStatus = new StreamInManager.StreamStatus(initiatedFile.getTargetFile(), initiatedFile.getExpectedBytes() ); String file = getNewFileNameFromOldContextAndNames(fileNames, pathNames, initiatedFile); if (logger.isDebugEnabled()) logger.debug("Received Data from  : " + message.getFrom() + " " + initiatedFile.getTargetFile() + " " + file); initiatedFile.setTargetFile(file); addStreamContext(message.getFrom(), initiatedFile, streamStatus); } StreamInManager.registerStreamCompletionHandler(message.getFrom(), new StreamCompletionHandler()); if (logger.isDebugEnabled()) logger.debug("Sending a stream initiate done message ..."); Message doneMessage = new Message(FBUtilities.getLocalAddress(), "", StorageService.Verb.STREAM_INITIATE_DONE, new byte[0] ); MessagingService.instance.sendOneWay(doneMessage, message.getFrom()); } catch (IOException ex) { throw new IOError(ex); } }', 'ground_truth': 'public void doVerb(Message message) { byte[] body = message.getMessageBody(); ByteArrayInputStream bufIn = new ByteArrayInputStream(body); if (logger.isDebugEnabled()) logger.debug(String.format("StreamInitiateVerbeHandler.doVerb %s %s %s", message.getVerb(), message.getMessageId(), message.getMessageType())); try { StreamInitiateMessage biMsg = StreamInitiateMessage.serializer().deserialize(new DataInputStream(bufIn)); InitiatedFile[] initiatedFiles = biMsg.getStreamContext(); if (initiatedFiles.length == 0) { if (logger.isDebugEnabled()) logger.debug("no data needed from " + message.getFrom()); if (StorageService.instance.isBootstrapMode()) StorageService.instance.removeBootstrapSource(message.getFrom(), new String(message.getHeader(StreamOut.TABLE_NAME))); return; } Map<String, String> fileNames = getNewNames(initiatedFiles); Map<String, String> pathNames = new HashMap<String, String>(); for (String ssName : fileNames.keySet()) pathNames.put(ssName, DatabaseDescriptor.getNextAvailableDataLocation()); /* * For each of stream context\'s in the incoming message * generate the new file names and store the new file names * in the StreamContextManager. */ for (InitiatedFile initiatedFile : initiatedFiles) { CompletedFileStatus streamStatus = new CompletedFileStatus(initiatedFile.getTargetFile(), initiatedFile.getExpectedBytes() ); String file = getNewFileNameFromOldContextAndNames(fileNames, pathNames, initiatedFile); if (logger.isDebugEnabled()) logger.debug("Received Data from  : " + message.getFrom() + " " + initiatedFile.getTargetFile() + " " + file); initiatedFile.setTargetFile(file); addStreamContext(message.getFrom(), initiatedFile, streamStatus); } StreamInManager.registerStreamCompletionHandler(message.getFrom(), new StreamCompletionHandler()); if (logger.isDebugEnabled()) logger.debug("Sending a stream initiate done message ..."); Message doneMessage = new Message(FBUtilities.getLocalAddress(), "", StorageService.Verb.STREAM_INITIATE_DONE, new byte[0] ); MessagingService.instance.sendOneWay(doneMessage, message.getFrom()); } catch (IOException ex) { throw new IOError(ex); } }', 'output': 'public void doVerb(Message message) { byte[] body = message.getMessageBody(); ByteArrayInputStream bufIn = new ByteArrayInputStream(body); if (logger.isDebugEnabled()) logger.debug(String.format("StreamInitiateVerbeHandler.doVerb %s %s %s", message.getVerb(), message.getMessageId(), message.getMessageType())); try { StreamInitiateMessage biMsg = StreamInitiateMessage.serializer().deserialize(new DataInputStream(bufIn)); InitiatedFile[] initiatedFiles = biMsg.getStreamContext(); if (initiatedFiles.length == 0) { if (logger.isDebugEnabled()) logger.debug("no data needed from " + message.getFrom()); if (StorageService.instance.isBootstrapMode()) StorageService.instance.removeBootstrapSource(message.getFrom(), new String(message.getHeader(StreamOut.TABLE_NAME))); return; } Map<String, String> fileNames = getNewNames(initiatedFiles); Map<String, String> pathNames = new HashMap<String, String>(); for (String ssName : fileNames.keySet()) pathNames.put(ssName, DatabaseDescriptor.getNextAvailableDataLocation()); /* * For each of stream context\'s in the incoming message * generate the new file names and store the new file names * in the StreamContextManager. */ for (InitiatedFile initiatedFile : initiatedFiles) { XXX streamStatus = new StreamInManager.StreamStatus(initiatedFile.getTargetFile(), initiatedFile.getExpectedBytes() ); String file = getNewFileNameFromOldContextAndNames(fileNames, pathNames, initiatedFile); if (logger.isDebugEnabled()) logger.debug("Received Data from  : " + message.getFrom() + " " + initiatedFile.getTargetFile() + " " + file); initiatedFile.setTargetFile(file); addStreamContext(message.getFrom(), initiatedFile, streamStatus); } StreamInManager.registerStreamCompletionHandler(message.getFrom(), new StreamCompletionHandler()); if (logger.isDebugEnabled()) logger.debug("Sending a stream initiate done message ..."); Message doneMessage = new Message(FBUtilities.getLocalAddress(), "", StorageService.Verb.STREAM_INITIATE'}
{'instruction': 'Please refactor the following code:', 'input': 'public void deleteUDF() { Response createdUDF = doCreateUDF(); XXX createdUdfId = ((UDF) ((JSONObject) createdUDF.getEntity()).get("udf")).getId(); Response response = udfService.delete(String.valueOf(createdUdfId)); Assert.assertEquals(204, response.getStatus()); thrown.expect(NotFoundFormattedException.class); udfService.getOne(String.valueOf(createdUdfId)); }', 'ground_truth': 'public void deleteUDF() { Response createdUDF = doCreateUDF(); Object createdUdfId = ((UDF) ((JSONObject) createdUDF.getEntity()).get("udf")).getId(); Response response = udfService.delete(String.valueOf(createdUdfId)); Assert.assertEquals(204, response.getStatus()); thrown.expect(NotFoundFormattedException.class); udfService.getOne(String.valueOf(createdUdfId)); }', 'output': 'public void deleteUDF() { Response createdUDF = doCreateUDF(); String createdUdfId = ((UDF) ((JSONObject) createdUDF.getEntity()).get("udf")).getId(); Response response = udfService.delete(createdUdfId); Assert.assertEquals(204, response.getStatus()); thrown.expect(NotFoundFormattedException.class); udfService.getOne(createdUdfId); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPartitionSpreading() throws Exception { System.out.printf("%6s, %6s, %6s, %6s, %8s\\n", "Nodes", "Reps", "Min", "Max", "Dev"); for (int i = 5; i < NODES_CNT; i = i * 3 / 2) { for (int replicas = 128; replicas <= 4096; replicas*=2) { Collection<ClusterNode> nodes = createNodes(i, replicas); XXX aff = new GridCacheConsistentHashAffinityFunction(false, 10000); checkDistribution(aff, nodes); } System.out.println(); } }', 'ground_truth': 'public void testPartitionSpreading() throws Exception { System.out.printf("%6s, %6s, %6s, %6s, %8s\\n", "Nodes", "Reps", "Min", "Max", "Dev"); for (int i = 5; i < NODES_CNT; i = i * 3 / 2) { for (int replicas = 128; replicas <= 4096; replicas*=2) { Collection<ClusterNode> nodes = createNodes(i, replicas); CacheConsistentHashAffinityFunction aff = new CacheConsistentHashAffinityFunction(false, 10000); checkDistribution(aff, nodes); } System.out.println(); } }', 'output': 'public void testPartitionSpreading() throws Exception { System.out.printf("%6s, %6s, %6s, %6s, %8s\\n", "Nodes", "Reps", "Min", "Max", "Dev"); for (int i = 5; i < NODES_CNT; i = i * 3 / 2) { for (int replicas = 128; replicas <= 4096; replicas*=2) { Collection<ClusterNode> nodes = createNodes(i, replicas); AffinityFunction aff = new GridCacheConsistentHashAffinityFunction(false, 10000); checkDistribution(aff, nodes); } System.out.println(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAnonymousInnerClassMethodsNotIncluded() throws Exception { XXX srcList = new ArrayList(); srcList.add("groovy/swing/binding/AbstractButtonProperties.java"); xmlTool.add(srcList); MockOutputTool output = new MockOutputTool(); xmlTool.renderToOutput(output, MOCK_DIR); String abstractButtonPropertiesDoc = output.getText(MOCK_DIR + "/groovy/swing/binding/AbstractButtonProperties.html"); assertTrue(abstractButtonPropertiesDoc.indexOf("createBinding") < 0); }', 'ground_truth': 'public void testAnonymousInnerClassMethodsNotIncluded() throws Exception { List<String> srcList = new ArrayList<String>(); srcList.add("groovy/swing/binding/AbstractButtonProperties.java"); xmlTool.add(srcList); MockOutputTool output = new MockOutputTool(); xmlTool.renderToOutput(output, MOCK_DIR); String abstractButtonPropertiesDoc = output.getText(MOCK_DIR + "/groovy/swing/binding/AbstractButtonProperties.html"); assertTrue(abstractButtonPropertiesDoc.indexOf("createBinding") < 0); }', 'output': 'public void testAnonymousInnerClassMethodsNotIncluded() throws Exception { List<String> srcList = new ArrayList<>(); srcList.add("groovy/swing/binding/AbstractButtonProperties.java"); xmlTool.add(srcList); MockOutputTool output = new MockOutputTool(); xmlTool.renderToOutput(output, MOCK_DIR); String abstractButtonPropertiesDoc = output.getText(MOCK_DIR + "/groovy/swing/binding/AbstractButtonProperties.html"); assertTrue(abstractButtonPropertiesDoc.indexOf("createBinding") < 0); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void checkTransaction(IgniteTxConcurrency concurrency, IgniteTxIsolation isolation) throws Throwable { int idx = RAND.nextInt(GRID_CNT); info("Grid will be stopped: " + idx); Ignite g = grid(idx); GridCache<Integer, String> cache = cache(idx); IgniteTx tx = cache.txStart(concurrency, isolation); try { cache.put(KEY, VALUE); int checkIdx = (idx + 1) % G.allGrids().size(); info("Check grid index: " + checkIdx); IgniteFuture<?> f = waitForLocalEvent(grid(checkIdx).events(), new P1<IgniteEvent>() { @Override public boolean apply(IgniteEvent e) { info("Received grid event: " + e); return true; } }, EVT_NODE_LEFT); stopGrid(idx); f.get(); U.sleep(getInteger(GG_TX_SALVAGE_TIMEOUT, 3000)); GridCache<Integer, String> checkCache = cache(checkIdx); boolean locked = false; for (int i = 0; !locked && i < 3; i++) { locked = checkCache.lock(KEY, -1); if (!locked) U.sleep(500); else break; } try { assert locked : "Failed to lock key on cache [idx=" + checkIdx + ", key=" + KEY + \']\'; } finally { checkCache.unlockAll(F.asList(KEY)); } } catch (XXX e) { U.warn(log, "Optimistic transaction failure (will rollback) [msg=" + e.getMessage() + ", tx=" + tx + \']\'); if (G.state(g.name()) == IgniteState.STARTED) tx.rollback(); assert concurrency == OPTIMISTIC && isolation == SERIALIZABLE; } catch (Throwable e) { error("Transaction failed (will rollback): " + tx, e); if (G.state(g.name()) == IgniteState.STARTED) tx.rollback(); throw e; } }', 'ground_truth': 'private void checkTransaction(IgniteTxConcurrency concurrency, IgniteTxIsolation isolation) throws Throwable { int idx = RAND.nextInt(GRID_CNT); info("Grid will be stopped: " + idx); Ignite g = grid(idx); GridCache<Integer, String> cache = cache(idx); IgniteTx tx = cache.txStart(concurrency, isolation); try { cache.put(KEY, VALUE); int checkIdx = (idx + 1) % G.allGrids().size(); info("Check grid index: " + checkIdx); IgniteFuture<?> f = waitForLocalEvent(grid(checkIdx).events(), new P1<IgniteEvent>() { @Override public boolean apply(IgniteEvent e) { info("Received grid event: " + e); return true; } }, EVT_NODE_LEFT); stopGrid(idx); f.get(); U.sleep(getInteger(GG_TX_SALVAGE_TIMEOUT, 3000)); GridCache<Integer, String> checkCache = cache(checkIdx); boolean locked = false; for (int i = 0; !locked && i < 3; i++) { locked = checkCache.lock(KEY, -1); if (!locked) U.sleep(500); else break; } try { assert locked : "Failed to lock key on cache [idx=" + checkIdx + ", key=" + KEY + \']\'; } finally { checkCache.unlockAll(F.asList(KEY)); } } catch (IgniteTxOptimisticException e) { U.warn(log, "Optimistic transaction failure (will rollback) [msg=" + e.getMessage() + ", tx=" + tx + \']\'); if (G.state(g.name()) == IgniteState.STARTED) tx.rollback(); assert concurrency == OPTIMISTIC && isolation == SERIALIZABLE; } catch (Throwable e) { error("Transaction failed (will rollback): " + tx, e); if (G.state(g.name()) == IgniteState.STARTED) tx.rollback(); throw e; } }', 'output': 'private void checkTransaction(IgniteTxConcurrency concurrency, IgniteTxIsolation isolation) throws Throwable { int idx = RAND.nextInt(GRID_CNT); info("Grid will be stopped: " + idx); Ignite g = grid(idx); GridCache<Integer, String> cache = cache(idx); IgniteTx tx = cache.txStart(concurrency, isolation); try { cache.put(KEY, VALUE); int checkIdx = (idx + 1) % G.allGrids().size(); info("Check grid index: " + checkIdx); IgniteFuture<?> f = waitForLocalEvent(grid(checkIdx).events(), new P1<IgniteEvent>() { @Override public boolean apply(IgniteEvent e) { info("Received grid event: " + e); return true; } }, EVT_NODE_LEFT); stopGrid(idx); f.get(); U.sleep(getInteger(GG_TX_SALVAGE_TIMEOUT, 3000)); GridCache<Integer, String> checkCache = cache(checkIdx); boolean locked = false; for (int i = 0; !locked && i < 3; i++) { locked = checkCache.lock(KEY, -1); if (!locked) U.sleep(500); else break; } try { assert locked : "Failed to lock key on cache [idx=" + checkIdx + ", key=" + KEY + \']\'; } finally { checkCache.unlockAll(F.asList(KEY)); } } catch (Exception e) { U.warn(log, "Optimistic transaction failure (will rollback) [msg=" + e.getMessage() + ", tx=" + tx + \']\'); if (G.state(g.name()) == IgniteState.STARTED) tx.rollback(); assert concurrency == OPTIMISTIC && isolation == SERIALIZABLE; } catch (Throwable e) { error("Transaction failed (will rollback): " + tx, e); if (G.state(g.name()) == IgniteState.STARTED) tx.rollback(); throw e; } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private boolean hasPendingAddMessage(UUID nodeId) { if (pendingMsgs.msgs.isEmpty()) return false; for (XXX pendingMsg : pendingMsgs.msgs) { if (pendingMsg instanceof TcpDiscoveryNodeAddedMessage) { TcpDiscoveryNodeAddedMessage addMsg = (TcpDiscoveryNodeAddedMessage)pendingMsg; if (addMsg.node().id().equals(nodeId) && addMsg.id().compareTo(pendingMsgs.discardId) > 0) return true; } } return false; }', 'ground_truth': 'private boolean hasPendingAddMessage(UUID nodeId) { if (pendingMsgs.msgs.isEmpty()) return false; for (PendingMessage pendingMsg : pendingMsgs.msgs) { if (pendingMsg.msg instanceof TcpDiscoveryNodeAddedMessage) { TcpDiscoveryNodeAddedMessage addMsg = (TcpDiscoveryNodeAddedMessage)pendingMsg.msg; if (addMsg.node().id().equals(nodeId) && addMsg.id().compareTo(pendingMsgs.discardId) > 0) return true; } } return false; }', 'output': 'private boolean hasPendingAddMessage(UUID nodeId) { if (pendingMsgs.msgs.isEmpty()) return false; for (TcpDiscoveryMessage pendingMsg : pendingMsgs.msgs) { if (pendingMsg instanceof TcpDiscoveryNodeAddedMessage) { TcpDiscoveryNodeAddedMessage addMsg = (TcpDiscoveryNodeAddedMessage)pendingMsg; if (addMsg.node().id().equals(nodeId) && addMsg.id().compareTo(pendingMsgs.discardId) > 0) return true; } } return false; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void timeit(FilterFactory.Type type) { int size = 300 * FilterTestHelper.ELEMENTS; XXX bf = FilterFactory.getFilter(size, FilterTestHelper.spec.bucketsPerElement, type, false); double sumfp = 0; for (int i = 0; i < 10; i++) { FilterTestHelper.testFalsePositives(bf, new KeyGenerator.RandomStringGenerator(new Random().nextInt(), size), new KeyGenerator.RandomStringGenerator(new Random().nextInt(), size)); bf.clear(); } logger.info("Bloom filter mean false positive: {}", sumfp/10); }', 'ground_truth': 'public void timeit(FilterFactory.Type type) { int size = 300 * FilterTestHelper.ELEMENTS; IFilter bf = FilterFactory.getFilter(size, FilterTestHelper.spec.bucketsPerElement, type, false); double sumfp = 0; for (int i = 0; i < 10; i++) { FilterTestHelper.testFalsePositives(bf, new KeyGenerator.RandomStringGenerator(new Random().nextInt(), size), new KeyGenerator.RandomStringGenerator(new Random().nextInt(), size)); bf.clear(); } logger.info("Bloom filter mean false positive: {}", sumfp/10); }', 'output': 'public void timeit(FilterFactory.Type type) { int size = 300 * FilterTestHelper.ELEMENTS; BloomFilter bf = FilterFactory.getFilter(size, FilterTestHelper.spec.bucketsPerElement, type, false); double sumfp = 0; for (int i = 0; i < 10; i++) { FilterTestHelper.testFalsePositives(bf, new KeyGenerator.RandomStringGenerator(new Random().nextInt(), size), new KeyGenerator.RandomStringGenerator(new Random().nextInt(), size)); bf.clear(); } logger.info("Bloom filter mean false positive: {}", sumfp/10); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void submitUpdate() { Row row = currentViewEntryBuilder.build(); if (row.isEmpty()) return; DecoratedKey partitionKey = makeCurrentPartitionKey(); XXX update = updates.get(partitionKey); if (update == null) { update = new PartitionUpdate(viewMetadata, partitionKey, viewMetadata.regularAndStaticColumns(), 4); updates.put(partitionKey, update); } update.add(row); }', 'ground_truth': 'private void submitUpdate() { Row row = currentViewEntryBuilder.build(); if (row.isEmpty()) return; DecoratedKey partitionKey = makeCurrentPartitionKey(); PartitionUpdate.Builder update = updates.computeIfAbsent(partitionKey, k -> new PartitionUpdate.Builder(viewMetadata, partitionKey, viewMetadata.regularAndStaticColumns(), 4)); update.add(row); }', 'output': 'private void submitUpdate() { Row row = currentViewEntryBuilder.build(); if (row.isEmpty()) return; DecoratedKey partitionKey = makeCurrentPartitionKey(); PartitionUpdate update = updates.get(partitionKey); if (update == null) { update = new PartitionUpdate(viewMetadata, partitionKey, viewMetadata.regularAndStaticColumns(), 4); updates.put(partitionKey, update); } update.add(row); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void moveUpPath (int[] indices) { XXX data = getData (); for (int i=0; i<indices.length; i++) { URL p2 = data.get (indices[i]); URL p1 = data.set (indices[i]-1,p2); data.set (indices[i],p1); } updatePlatform (); fireContentsChanged(this,indices[0]-1,indices[indices.length-1]); }', 'ground_truth': 'private void moveUpPath (int[] indices) { List<URL> data = getData (); for (int i=0; i<indices.length; i++) { URL p2 = data.get (indices[i]); URL p1 = data.set (indices[i]-1,p2); data.set (indices[i],p1); } updatePlatform (); fireContentsChanged(this,indices[0]-1,indices[indices.length-1]); }', 'output': 'private void moveUpPath (int[] indices) { List<URL> data = getData (); for (int i=0; i<indices.length; i++) { URL p2 = data.get (indices[i]); URL p1 = data.set (indices[i]-1,p2); data.set (indices[i],p1); } updatePlatform (); fireContentsChanged(this,indices[0]-1,indices[indices.length-1]); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'private void useStar(OptiqSchema schema, Materialization materialization) { List<OptiqSchema.TableEntry> starTables = getStarTables(schema); if (starTables.isEmpty()) { return; } final RelNode rel2 = RelOptMaterialization.toLeafJoinForm(materialization.queryRel); for (OptiqSchema.TableEntry starTable : starTables) { final Table table = starTable.getTable(); assert table instanceof StarTable; XXX starRelOptTable = new OptiqPrepareImpl.RelOptTableImpl(catalogReader, table.getRowType(typeFactory), starTable); final RelNode rel3 = RelOptMaterialization.tryUseStar(rel2, starRelOptTable); if (rel3 != null) { materialization.materialize(rel3, starRelOptTable); System.out.println("Materialization " + materialization.materializedTable + " matched star table " + starTable + "; query after re-write: " + RelOptUtil.toString(materialization.queryRel)); return; } } }', 'ground_truth': 'private void useStar(OptiqSchema schema, Materialization materialization) { List<OptiqSchema.TableEntry> starTables = getStarTables(schema); if (starTables.isEmpty()) { return; } final RelNode rel2 = RelOptMaterialization.toLeafJoinForm(materialization.queryRel); for (OptiqSchema.TableEntry starTable : starTables) { final Table table = starTable.getTable(); assert table instanceof StarTable; RelOptTableImpl starRelOptTable = RelOptTableImpl.create(catalogReader, table.getRowType(typeFactory), starTable); final RelNode rel3 = RelOptMaterialization.tryUseStar(rel2, starRelOptTable); if (rel3 != null) { materialization.materialize(rel3, starRelOptTable); System.out.println("Materialization " + materialization.materializedTable + " matched star table " + starTable + "; query after re-write: " + RelOptUtil.toString(materialization.queryRel)); return; } } }', 'output': 'private void useStar(OptiqSchema schema, Materialization materialization) { List<OptiqSchema.TableEntry> starTables = getStarTables(schema); if (starTables.isEmpty()) { return; } final RelNode rel2 = RelOptMaterialization.toLeafJoinForm(materialization.queryRel); for (OptiqSchema.TableEntry starTable : starTables) { final Table table = starTable.getTable(); assert table instanceof StarTable; OptiqPrepareImpl.RelOptTableImpl starRelOptTable = new OptiqPrepareImpl.RelOptTableImpl(catalogReader, table.getRowType(typeFactory), starTable); final RelNode rel3 = RelOptMaterialization.tryUseStar(rel2, starRelOptTable); if (rel3 != null) { materialization.materialize(rel3, starRelOptTable); System.out.println("Materialization " + materialization.materializedTable + " matched star table " + starTable + "; query after re-write: " + RelOptUtil.toString(materialization.queryRel)); return; } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testPrimaryPartitionsOneNode() throws Exception { XXX ctx = new GridCacheAffinityFunctionContextImpl(new ArrayList<>(grid(0).nodes()), null, null, 1, 1); List<List<ClusterNode>> assignment = affinity().assignPartitions(ctx); for (ClusterNode node : grid(0).nodes()) { int[] parts = cache().affinity().primaryPartitions(node); assert !F.isEmpty(parts); for (int p : parts) { Collection<ClusterNode> owners = nodes(assignment, p); assert !F.isEmpty(owners); ClusterNode primary = F.first(owners); assert F.eqNodes(node, primary); } } }', 'ground_truth': 'public void testPrimaryPartitionsOneNode() throws Exception { CacheAffinityFunctionContext ctx = new GridCacheAffinityFunctionContextImpl(new ArrayList<>(grid(0).nodes()), null, null, 1, 1); List<List<ClusterNode>> assignment = affinity().assignPartitions(ctx); for (ClusterNode node : grid(0).nodes()) { int[] parts = cache().affinity().primaryPartitions(node); assert !F.isEmpty(parts); for (int p : parts) { Collection<ClusterNode> owners = nodes(assignment, p); assert !F.isEmpty(owners); ClusterNode primary = F.first(owners); assert F.eqNodes(node, primary); } } }', 'output': 'public void testPrimaryPartitionsOneNode() throws Exception { AffinityFunctionContext ctx = new AffinityFunctionContextImpl(new ArrayList<>(grid(0).nodes()), null, null, 1, 1); List<List<ClusterNode>> assignment = affinity().assignPartitions(ctx); for (ClusterNode node : grid(0).nodes()) { int[] parts = cache().affinity().primaryPartitions(node); assert !F.isEmpty(parts); for (int p : parts) { Collection<ClusterNode> owners = nodes(assignment, p); assert !F.isEmpty(owners); ClusterNode primary = F.first(owners); assert F.eqNodes(node, primary); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testRenameFileParentRootSourceMissing() throws Exception { IgniteFsPath file1 = new IgniteFsPath("/file1"); XXX file2 = new IgniteFsPath("/file2"); create(ggfsSecondary, null, paths(file1)); create(ggfs, null, null); ggfs.rename(file1, file2); checkExist(ggfs, ggfsSecondary, file2); checkNotExist(ggfs, ggfsSecondary, file1); }', 'ground_truth': 'public void testRenameFileParentRootSourceMissing() throws Exception { IgfsPath file1 = new IgfsPath("/file1"); IgfsPath file2 = new IgfsPath("/file2"); create(ggfsSecondary, null, paths(file1)); create(ggfs, null, null); ggfs.rename(file1, file2); checkExist(ggfs, ggfsSecondary, file2); checkNotExist(ggfs, ggfsSecondary, file1); }', 'output': 'public void testRenameFileParentRootSourceMissing() throws Exception { IgniteFsPath file1 = new IgniteFsPath("/file1"); IgniteFsPath file2 = new IgniteFsPath("/file2"); create(ggfsSecondary, null, paths(file1)); create(ggfs, null, null); ggfs.rename(file1, file2); checkExist(ggfs, ggfsSecondary, file2); checkNotExist(ggfs, ggfsSecondary, file1); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public <T> T refresh(final T domainObject) { final XXX adapter = getPersistenceSession().adapterFor(domainObject); getPersistenceSession().refreshRoot(adapter); return domainObject; }', 'ground_truth': 'public <T> T refresh(final T domainObject) { final ManagedObject adapter = getPersistenceSession().adapterFor(domainObject); getPersistenceSession().refreshRoot(adapter); return domainObject; }', 'output': 'public <T> T refresh(final T domainObject, final Class<T> clazz) { final PersistenceAdapter<T> adapter = getPersistenceSession().adapterFor(clazz); getPersistenceSession().refreshRoot(adapter); return domainObject; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public Multimap<InetAddress, InetAddress> getHintedEndpoints(Collection<InetAddress> targets) { Multimap<InetAddress, InetAddress> map = HashMultimap.create(targets.size(), 1); XXX endpointSnitch = DatabaseDescriptor.getEndpointSnitch(); for (InetAddress ep : targets) { if (FailureDetector.instance.isAlive(ep)) map.put(ep, ep); } if (map.size() == targets.size()) return map; InetAddress localAddress = FBUtilities.getLocalAddress(); for (InetAddress ep : targets) { if (map.containsKey(ep)) continue; InetAddress destination = map.isEmpty() ? localAddress : endpointSnitch.getSortedListByProximity(localAddress, map.keySet()).get(0); map.put(destination, ep); } return map; }', 'ground_truth': 'public Multimap<InetAddress, InetAddress> getHintedEndpoints(Collection<InetAddress> targets) { Multimap<InetAddress, InetAddress> map = HashMultimap.create(targets.size(), 1); IEndpointSnitch endpointSnitch = DatabaseDescriptor.getEndpointSnitch(); for (InetAddress ep : targets) { if (FailureDetector.instance.isAlive(ep)) map.put(ep, ep); } if (map.size() == targets.size()) return map; InetAddress localAddress = FBUtilities.getLocalAddress(); for (InetAddress ep : targets) { if (map.containsKey(ep)) continue; InetAddress destination = map.isEmpty() ? localAddress : endpointSnitch.getSortedListByProximity(localAddress, map.keySet()).get(0); map.put(destination, ep); } return map; }', 'output': 'public Multimap<InetAddress, InetAddress> getHintedEndpoints(Collection<InetAddress> targets) { Multimap<InetAddress, InetAddress> map = HashMultimap.create(targets.size(), 1); EndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch(); for (InetAddress ep : targets) { if (FailureDetector.instance.isAlive(ep)) map.put(ep, ep); } if (map.size() == targets.size()) return map; InetAddress localAddress = FBUtilities.getLocalAddress(); for (InetAddress ep : targets) { if (map.containsKey(ep)) continue; InetAddress destination = map.isEmpty() ? localAddress : snitch.getSortedListByProximity(localAddress, map.keySet()).get(0); map.put(destination, ep); } return map; }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testShutdown() throws Exception { for (int i = 0; i < 5; i++) { final GridHadoopExecutorService exec = new GridHadoopExecutorService(log, "_GRID_NAME_", 10, 5); final LongAdder sum = new LongAdder(); final AtomicBoolean finish = new AtomicBoolean(); XXX fut = multithreadedAsync(new Callable<Object>() { @Override public Object call() throws Exception { while (!finish.get()) { exec.submit(new Callable<Void>() { @Override public Void call() throws Exception { sum.increment(); return null; } }); } return null; } }, 19); Thread.sleep(200); assertTrue(exec.shutdown(50)); long res = sum.sum(); assertTrue(res > 0); finish.set(true); fut.get(); assertEquals(res, sum.sum()); X.println("_ ok"); } }', 'ground_truth': 'public void testShutdown() throws Exception { for (int i = 0; i < 5; i++) { final GridHadoopExecutorService exec = new GridHadoopExecutorService(log, "_GRID_NAME_", 10, 5); final LongAdder sum = new LongAdder(); final AtomicBoolean finish = new AtomicBoolean(); IgniteFuture<?> fut = multithreadedAsync(new Callable<Object>() { @Override public Object call() throws Exception { while (!finish.get()) { exec.submit(new Callable<Void>() { @Override public Void call() throws Exception { sum.increment(); return null; } }); } return null; } }, 19); Thread.sleep(200); assertTrue(exec.shutdown(50)); long res = sum.sum(); assertTrue(res > 0); finish.set(true); fut.get(); assertEquals(res, sum.sum()); X.println("_ ok"); } }', 'output': 'public void testShutdown() throws Exception { for (int i = 0; i < 5; i++) { final GridHadoopExecutorService exec = new GridHadoopExecutorService(log, "_GRID_NAME_", 10, 5); final LongAdder sum = new LongAdder(); final AtomicBoolean finish = new AtomicBoolean(); IgniteFuture<?> fut = multithreadedAsync(new Callable<Object>() { @Override public Object call() throws Exception { while (!finish.get()) { exec.submit(new Callable<Void>() { @Override public Void call() throws Exception { sum.increment(); return null; } }); } return null; } }, 19); Thread.sleep(200); assertTrue(exec.shutdown(50)); long res = sum.sum(); assertTrue(res > 0); finish.set(true); fut.get(); assertEquals(res, sum.sum()); X.println("_ ok"); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testThrowsForUnsupportedTypes() throws Exception { thrown.expect(UnsupportedOperationException.class); XXX factory = new DefaultRowTypeFactory(); factory.createRowType( Arrays.<FieldValueGetter>asList(getter("unsupportedGetter", UnsupportedClass.class))); }', 'ground_truth': 'public void testThrowsForUnsupportedTypes() throws Exception { thrown.expect(UnsupportedOperationException.class); DefaultSchemaFactory factory = new DefaultSchemaFactory(); factory.createSchema( Arrays.<FieldValueGetter>asList(getter("unsupportedGetter", UnsupportedClass.class))); }', 'output': 'public void testThrowsForUnsupportedTypes() throws Exception { thrown.expect(UnsupportedOperationException.class); RowTypeFactory factory = new DefaultRowTypeFactory(); factory.createRowType( Arrays.<FieldValueGetter>asList(getter("unsupportedGetter", UnsupportedClass.class))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static String capitalize(String s) { StringTokenizer st = new StringTokenizer(s, " \\t\\r\\n", true); XXX buf = new StringBuffer(s.length()); while (st.hasMoreTokens()) { String tok = st.nextToken(); buf.append(tok.substring(0, 1).toUpperCase()); buf.append(tok.substring(1).toLowerCase()); } return buf.toString(); }', 'ground_truth': 'public static String capitalize(String s) { StringTokenizer st = new StringTokenizer(s, " \\t\\r\\n", true); StringBuilder buf = new StringBuilder(s.length()); while (st.hasMoreTokens()) { String tok = st.nextToken(); buf.append(tok.substring(0, 1).toUpperCase()); buf.append(tok.substring(1).toLowerCase()); } return buf.toString(); }', 'output': 'public static String capitalize(String s) { StringTokenizer st = new StringTokenizer(s, " \\t\\r\\n", true); StringBuilder buf = new StringBuilder(s.length()); while (st.hasMoreTokens()) { String tok = st.nextToken(); buf.append(tok.substring(0, 1).toUpperCase()); buf.append(tok.substring(1).toLowerCase()); } return buf.toString(); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static Value createValueFromMirror(Object mirror, boolean isObject, JPDADebuggerImpl debugger) throws InvalidObjectException, InternalExceptionWrapper, VMDisconnectedExceptionWrapper, ObjectCollectedExceptionWrapper, InvalidTypeException, ClassNotLoadedException, ClassNotPreparedExceptionWrapper, IllegalArgumentExceptionWrapper { VirtualMachine vm = debugger.getVirtualMachine(); if (vm == null) { throw new VMDisconnectedExceptionWrapper(new VMDisconnectedException()); } Class<?> clazz = mirror.getClass(); JPDAThreadImpl currentThread; Lock lock; if (willInvokeMethods(isObject, clazz)) { currentThread = (JPDAThreadImpl) debugger.getCurrentThread(); if (currentThread == null) { throw new InvalidObjectException (NbBundle.getMessage(JPDADebuggerImpl.class, "MSG_NoCurrentContext")); } lock = currentThread.accessLock.writeLock(); lock.lock(); } else { currentThread = null; lock = null; } boolean invoking = false; try { ThreadReference threadReference; if (currentThread != null) { threadReference = currentThread.getThreadReference(); currentThread.notifyMethodInvoking(); invoking = true; } else { threadReference = null; } return createValueFromMirror(mirror, clazz, isObject, vm, threadReference); } catch (XXX ex) { InvalidObjectException ioex = new InvalidObjectException(ex.getLocalizedMessage()); ioex.initCause(ex); throw ioex; } catch (UnsupportedOperationExceptionWrapper ex) { InvalidObjectException ioex = new InvalidObjectException(ex.getLocalizedMessage()); ioex.initCause(ex); throw ioex; } catch (PropertyVetoException pvex) { InvalidObjectException ioex = new InvalidObjectException(pvex.getLocalizedMessage()); ioex.initCause(pvex); throw ioex; } finally { if (invoking) { currentThread.notifyMethodInvokeDone(); } if (lock != null) { lock.unlock(); } } }', 'ground_truth': 'public static Value createValueFromMirror(Object mirror, boolean isObject, JPDADebuggerImpl debugger) throws InvalidObjectException, InternalExceptionWrapper, VMDisconnectedExceptionWrapper, ObjectCollectedExceptionWrapper, InvalidTypeException, ClassNotLoadedException, ClassNotPreparedExceptionWrapper, IllegalArgumentExceptionWrapper { VirtualMachine vm = debugger.getVirtualMachine(); if (vm == null) { throw new VMDisconnectedExceptionWrapper(new VMDisconnectedException()); } Class<?> clazz = mirror.getClass(); JPDAThreadImpl currentThread; Lock lock; if (willInvokeMethods(isObject, clazz)) { currentThread = (JPDAThreadImpl) debugger.getCurrentThread(); if (currentThread == null) { throw new InvalidObjectException (NbBundle.getMessage(JPDADebuggerImpl.class, "MSG_NoCurrentContext")); } lock = currentThread.accessLock.writeLock(); lock.lock(); } else { currentThread = null; lock = null; } boolean invoking = false; try { ThreadReference threadReference; if (currentThread != null) { threadReference = currentThread.getThreadReference(); currentThread.notifyMethodInvoking(); invoking = true; } else { threadReference = null; } return createValueFromMirror(mirror, clazz, isObject, vm, threadReference); } catch (IncompatibleThreadStateException | UnsupportedOperationExceptionWrapper | PropertyVetoException ex) { InvalidObjectException ioex = new InvalidObjectException(ex.getLocalizedMessage()); ioex.initCause(ex); throw ioex; } finally { if (invoking) { currentThread.notifyMethodInvokeDone(); } if (lock != null) { lock.unlock(); } } }', 'output': 'public static Value createValueFromMirror(Object mirror, boolean isObject, JPDADebuggerImpl debugger) throws InvalidObjectException, InternalExceptionWrapper, VMDisconnectedExceptionWrapper, ObjectCollectedExceptionWrapper, InvalidTypeException, ClassNotLoadedException, ClassNotPreparedExceptionWrapper, IllegalArgumentExceptionWrapper { VirtualMachine vm = debugger.getVirtualMachine(); if (vm == null) { throw new VMDisconnectedExceptionWrapper(new VMDisconnectedException()); } Class<?> clazz = mirror.getClass(); JPDAThreadImpl currentThread; Lock lock; if (willInvokeMethods(isObject, clazz)) { currentThread = (JPDAThreadImpl) debugger.getCurrentThread(); if (currentThread == null) { throw new InvalidObjectException (NbBundle.getMessage(JPDADebuggerImpl.class, "MSG_NoCurrentContext")); } lock = currentThread.accessLock.writeLock(); lock.lock(); } else { currentThread = null; lock = null; } boolean invoking = false; try { ThreadReference threadReference; if (currentThread != null) { threadReference = currentThread.getThreadReference(); currentThread.notifyMethodInvoking(); invoking = true; } else { threadReference = null; } return createValueFromMirror(mirror, clazz, isObject, vm, threadReference); } catch (IOException ex) { InvalidObjectException ioex = new InvalidObjectException(ex.getLocalizedMessage()); ioex.initCause(ex); throw ioex; } catch (UnsupportedOperationExceptionWrapper ex) { InvalidObjectException ioex = new InvalidObjectException(ex.getLocalizedMessage()); ioex.initCause(ex); throw ioex; } catch (PropertyVetoException pvex) { InvalidObjectException ioex = new InvalidObjectException(pvex.getLocalizedMessage()); ioex.initCause(pvex); throw ioex; } finally { if (invoking) { currentThread.notifyMethodInvokeDone(); } if (lock != null) { lock.unlock(); } } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static ISSTableScanner getScanner(SSTableReader sstable, Collection<Range<Token>> tokenRanges) { List<Pair<Long, Long>> positions = sstable.getPositionsForRanges(tokenRanges); if (positions.isEmpty()) return new EmptySSTableScanner(sstable); return getScanner(sstable, makeBounds(sstable, tokenRanges).iterator()); }', 'ground_truth': 'public static ISSTableScanner getScanner(SSTableReader sstable, Collection<Range<Token>> tokenRanges) { List<SSTableReader.PartitionPositionBounds> positions = sstable.getPositionsForRanges(tokenRanges); if (positions.isEmpty()) return new EmptySSTableScanner(sstable); return getScanner(sstable, makeBounds(sstable, tokenRanges).iterator()); }', 'output': 'public static SSTableScanner getScanner(SSTableReader sstable, Collection<Range<Token>> tokenRanges) { List<Pair<Long, Long>> positions = sstable.getPositionsForRanges(tokenRanges); if (positions.isEmpty()) return new EmptySSTableScanner(sstable); return getScanner(sstable, makeBounds(sstable, tokenRanges).iterator()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testNotEqualTo() { GridPredicate<Object> p1 = F.notEqualTo(null); assert p1 != null; assert p1.apply(new Object()); assert !p1.apply(null); XXX p2 = F.notEqualTo("test"); assert p2 != null; assert p2.apply("test1"); assert p2.apply(null); assert !p2.apply("test"); }', 'ground_truth': 'public void testNotEqualTo() { IgnitePredicate<Object> p1 = F.notEqualTo(null); assert p1 != null; assert p1.apply(new Object()); assert !p1.apply(null); IgnitePredicate<String> p2 = F.notEqualTo("test"); assert p2 != null; assert p2.apply("test1"); assert p2.apply(null); assert !p2.apply("test"); }', 'output': 'public void testNotEqualTo() { IgnitePredicate<Object> p1 = F.notEqualTo(null); assert p1 != null; assert p1.apply(new Object()); assert !p1.apply(null); IgnitePredicate<Object> p2 = F.notEqualTo("test"); assert p2 != null; assert p2.apply("test1"); assert p2.apply(null); assert !p2.apply("test"); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testNamedCache() throws Exception { GridCache<Integer, Integer> cache = grid(0).cache(CACHE); for (int i = 0; i < 200; i++) assert cache.putx(i, i); XXX qry = cache.queries().createSqlFieldsQuery("select * from Integer").projection(grid(0)); Collection<List<?>> res = qry.execute().get(); assert res != null; assert res.size() == (cacheMode() == REPLICATED ? 200 * gridCount() : 200); }', 'ground_truth': 'public void testNamedCache() throws Exception { Cache<Integer, Integer> cache = grid(0).cache(CACHE); for (int i = 0; i < 200; i++) assert cache.putx(i, i); CacheQuery<List<?>> qry = cache.queries().createSqlFieldsQuery("select * from Integer").projection(grid(0)); Collection<List<?>> res = qry.execute().get(); assert res != null; assert res.size() == (cacheMode() == REPLICATED ? 200 * gridCount() : 200); }', 'output': 'public void testNamedCache() throws Exception { IgniteCache<Integer, Integer> cache = grid(0).jcache(CACHE); for (int i = 0; i < 200; i++) assert cache.putx(i, i); IgniteJdbcQuery qry = cache.queries().createSqlFieldsQuery("select * from Integer").projection(grid(0)); Collection<List<?>> res = qry.execute().get(); assert res != null; assert res.size() == (cacheMode() == REPLICATED ? 200 * gridCount() : 200); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void inputOutputSameEvents() throws Exception { NexmarkConfiguration config = NexmarkConfiguration.DEFAULT.copy(); config.sideInputType = NexmarkUtils.SideInputType.DIRECT; config.numEventGenerators = 1; config.numEvents = 5000; config.sideInputRowCount = 10; config.sideInputNumShards = 3; PCollection<KV<Long, String>> sideInput = NexmarkUtils.prepareSideInput(p, config); try { PCollection<Event> input = p.apply(NexmarkUtils.batchEventsSource(config)); PCollection<Bid> justBids = input.apply(NexmarkQueryUtil.JUST_BIDS); PCollection<Long> bidCount = justBids.apply("Count Bids", Count.globally()); XXX query = new BoundedSideInputJoin(config); query.setSideInput(sideInput); PCollection<TimestampedValue<KnownSize>> output = input.apply(query); PCollection<Long> outputCount = output.apply("Count outputs", Count.globally()); PAssert.that(PCollectionList.of(bidCount).and(outputCount).apply(Flatten.pCollections())) .satisfies( counts -> { assertThat(Iterables.size(counts), equalTo(2)); assertThat(Iterables.get(counts, 0), greaterThan(0L)); assertThat(Iterables.get(counts, 0), equalTo(Iterables.get(counts, 1))); return null; }); p.run(); } finally { NexmarkUtils.cleanUpSideInput(config); } }', 'ground_truth': 'public void inputOutputSameEvents() throws Exception { NexmarkConfiguration config = NexmarkConfiguration.DEFAULT.copy(); config.sideInputType = NexmarkUtils.SideInputType.DIRECT; config.numEventGenerators = 1; config.numEvents = 5000; config.sideInputRowCount = 10; config.sideInputNumShards = 3; PCollection<KV<Long, String>> sideInput = NexmarkUtils.prepareSideInput(p, config); try { PCollection<Event> input = p.apply(NexmarkUtils.batchEventsSource(config)); PCollection<Bid> justBids = input.apply(NexmarkQueryUtil.JUST_BIDS); PCollection<Long> bidCount = justBids.apply("Count Bids", Count.globally()); NexmarkQueryTransform query = new BoundedSideInputJoin(config); query.setSideInput(sideInput); PCollection<TimestampedValue<KnownSize>> output = input.apply(query); PCollection<Long> outputCount = output.apply("Count outputs", Count.globally()); PAssert.that(PCollectionList.of(bidCount).and(outputCount).apply(Flatten.pCollections())) .satisfies( counts -> { assertThat(Iterables.size(counts), equalTo(2)); assertThat(Iterables.get(counts, 0), greaterThan(0L)); assertThat(Iterables.get(counts, 0), equalTo(Iterables.get(counts, 1))); return null; }); p.run(); } finally { NexmarkUtils.cleanUpSideInput(config); } }', 'output': 'public void inputOutputSameEvents() throws Exception { NexmarkConfiguration config = NexmarkConfiguration.DEFAULT.copy(); config.sideInputType = NexmarkUtils.SideInputType.DIRECT; config.numEventGenerators = 1; config.numEvents = 5000; config.sideInputRowCount = 10; config.sideInputNumShards = 3; PCollection<KV<Long, String>> sideInput = NexmarkUtils.prepareSideInput(p, config); try { PCollection<Event> input = p.apply(NexmarkUtils.batchEventsSource(config)); PCollection<Bid> justBids = input.apply(NexmarkQueryUtil.JUST_BIDS); PCollection<Long> bidCount = justBids.apply("Count Bids", Count.globally()); BoundedSideInputJoin query = new BoundedSideInputJoin(config); query.setSideInput(sideInput); PCollection<TimestampedValue<KnownSize>> output = input.apply(query); PCollection<Long> outputCount = output.apply("Count outputs", Count.globally()); PAssert.that(PCollectionList.of(bidCount).and(outputCount).apply(Flatten.pCollections())) .satisfies( counts -> { assertThat(Iterables.size(counts), equalTo(2)); assertThat(Iterables.get(counts, 0), greaterThan(0L)); assertThat(Iterables.get(counts, 0), equalTo(Iterables.get(counts, 1))); return null; }); p.run(); } finally { NexmarkUtils.cleanUpSideInput(config); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testCompareAndSetSimpleValue() throws Exception { String atomicName = UUID.randomUUID().toString(); String initVal = "qwerty"; XXX atomic = grid().cache(null).dataStructures() .atomicReference(atomicName, initVal, true); assertEquals(initVal, atomic.get()); atomic.compareAndSet("h", "j"); assertEquals(initVal, atomic.get()); atomic.compareAndSet(initVal, null); assertEquals(null, atomic.get()); }', 'ground_truth': 'public void testCompareAndSetSimpleValue() throws Exception { String atomicName = UUID.randomUUID().toString(); String initVal = "qwerty"; CacheAtomicReference<String> atomic = grid().cache(null).dataStructures() .atomicReference(atomicName, initVal, true); assertEquals(initVal, atomic.get()); atomic.compareAndSet("h", "j"); assertEquals(initVal, atomic.get()); atomic.compareAndSet(initVal, null); assertEquals(null, atomic.get()); }', 'output': 'public void testCompareAndSetSimpleValue() throws Exception { String atomicName = UUID.randomUUID().toString(); String initVal = "qwerty"; AtomicReference<String> atomic = grid().cache(null).dataStructures() .atomicReference(atomicName, initVal, true); assertEquals(initVal, atomic.get()); atomic.compareAndSet("h", "j"); assertEquals(initVal, atomic.get()); atomic.compareAndSet(initVal, null); assertEquals(null, atomic.get()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testEmptyFolder() throws Exception { TextRowCountEstimator textRowCountEstimator = TextRowCountEstimator.builder().setFilePattern(temporaryFolder.getRoot() + "/**").build(); XXX rows = textRowCountEstimator.estimateRowCount(PipelineOptionsFactory.create()); }', 'ground_truth': 'public void testEmptyFolder() throws Exception { TextRowCountEstimator textRowCountEstimator = TextRowCountEstimator.builder().setFilePattern(temporaryFolder.getRoot() + "/**").build(); Double rows = textRowCountEstimator.estimateRowCount(PipelineOptionsFactory.create()); }', 'output': 'public void testEmptyFolder() throws Exception { TextRowCountEstimator textRowCountEstimator = TextRowCountEstimator.builder().setFilePattern(temporaryFolder.getRoot() + "/**").build(); long rows = textRowCountEstimator.estimateRowCount(PipelineOptionsFactory.create()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testEndOfWindowIgnoresTimer() throws Exception { Duration windowDuration = Duration.millis(10); TriggerTester<Integer, Iterable<Integer>, IntervalWindow> tester = TriggerTester.nonCombining( FixedWindows.of(windowDuration), AfterWatermark.<IntervalWindow>pastEndOfWindow(), AccumulationMode.DISCARDING_FIRED_PANES, Duration.millis(100)); tester.fireTimer(new IntervalWindow(new Instant(0), new Instant(10)), new Instant(15), TimeDomain.EVENT_TIME); tester.injectElements( TimestampedValue.of(1, new Instant(1))); tester.fireTimer(new IntervalWindow(new Instant(0), new Instant(10)), new Instant(9), TimeDomain.EVENT_TIME); }', 'ground_truth': 'public void testEndOfWindowIgnoresTimer() throws Exception { Duration windowDuration = Duration.millis(10); ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining( FixedWindows.of(windowDuration), AfterWatermark.<IntervalWindow>pastEndOfWindow(), AccumulationMode.DISCARDING_FIRED_PANES, Duration.millis(100)); tester.fireTimer(new IntervalWindow(new Instant(0), new Instant(10)), new Instant(15), TimeDomain.EVENT_TIME); tester.injectElements( TimestampedValue.of(1, new Instant(1))); tester.fireTimer(new IntervalWindow(new Instant(0), new Instant(10)), new Instant(9), TimeDomain.EVENT_TIME); }', 'output': 'public void testEndOfWindowIgnoresTimer() throws Exception { Duration windowDuration = Duration.millis(10); TriggerTester<Integer, Iterable<Integer>, IntervalWindow> tester = TriggerTester.nonCombining( FixedWindows.of(windowDuration), AfterWatermark.<IntervalWindow>pastEndOfWindow(), AccumulationMode.DISCARDING_FIRED_PANES, Duration.millis(100)); tester.fireTimer(new IntervalWindow(new Instant(0), new Instant(10)), new Instant(15), TimeDomain.EVENT_TIME); tester.injectElements( TimestampedValue.of(1, new Instant(1))); tester.fireTimer(new IntervalWindow(new Instant(0), new Instant(10)), new Instant(9), TimeDomain.EVENT_TIME); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void verifyRestApiCall(@Named("jsonResult") String jsonResult) throws IOException, URISyntaxException { ObjectMapper mapper = new ObjectMapper(); Path jsonFilePath = new File(Resources.getResource("test-output/" + jsonResult).toURI()).toPath(); String jsonExpected = new String(Files.readAllBytes(jsonFilePath)); JsonNode expected = mapper.readTree(jsonExpected); JsonNode result = mapper.readTree(response); JsonNode patch = JsonDiff.asJson(expected, result); XXX diffObjects = mapper.convertValue(patch, List.class); assertDiffs(diffObjects, expected); }', 'ground_truth': 'public void verifyRestApiCall(@Named("jsonResult") String jsonResult) throws IOException, URISyntaxException { ObjectMapper mapper = new ObjectMapper(); Path jsonFilePath = new File(Resources.getResource("test-output/" + jsonResult).toURI()).toPath(); String jsonExpected = new String(Files.readAllBytes(jsonFilePath)); JsonNode expected = mapper.readTree(jsonExpected); JsonNode result = mapper.readTree(response); JsonNode patch = JsonDiff.asJson(expected, result); List<?> diffObjects = mapper.convertValue(patch, List.class); assertDiffs(diffObjects, expected); }', 'output': 'public void verifyRestApiCall(@Named("jsonResult") String jsonResult) throws IOException, URISyntaxException { ObjectMapper mapper = new ObjectMapper(); Path jsonFilePath = new File(Resources.getResource("test-output/" + jsonResult).toURI()).toPath(); String jsonExpected = new String(Files.readAllBytes(jsonFilePath)); JsonNode expected = mapper.readTree(jsonExpected); JsonNode result = mapper.readTree(response); JsonNode patch = JsonDiff.asJson(expected, result); List<JsonNode> diffObjects = mapper.convertValue(patch, List.class); assertDiffs(diffObjects, expected); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testAllEntries() throws Exception { GridCache<Integer, Integer> cache = grid(0).cache(null); GridCacheContinuousQuery<Integer, Integer> qry = cache.queries().createContinuousQuery(); final Map<Integer, List<Integer>> map = new HashMap<>(); final CountDownLatch latch = new CountDownLatch(5); qry.localCallback(new P2<UUID, Collection<org.apache.ignite.cache.query.GridCacheContinuousQueryEntry<Integer, Integer>>>() { @Override public boolean apply(UUID nodeId, Collection<org.apache.ignite.cache.query.GridCacheContinuousQueryEntry<Integer, Integer>> entries) { for (Map.Entry<Integer, Integer> e : entries) { synchronized (map) { List<Integer> vals = map.get(e.getKey()); if (vals == null) { vals = new ArrayList<>(); map.put(e.getKey(), vals); } vals.add(e.getValue()); } latch.countDown(); } return true; } }); try { qry.execute(); cache.putx(1, 1); cache.putx(2, 2); cache.putx(3, 3); cache.removex(2); cache.putx(1, 10); assert latch.await(LATCH_TIMEOUT, MILLISECONDS); assertEquals(3, map.size()); List<Integer> vals = map.get(1); assertNotNull(vals); assertEquals(2, vals.size()); assertEquals(1, (int)vals.get(0)); assertEquals(10, (int)vals.get(1)); vals = map.get(2); assertNotNull(vals); assertEquals(2, vals.size()); assertEquals(2, (int)vals.get(0)); assertNull(vals.get(1)); vals = map.get(3); assertNotNull(vals); assertEquals(1, vals.size()); assertEquals(3, (int)vals.get(0)); } finally { qry.close(); } }', 'ground_truth': 'public void testAllEntries() throws Exception { Cache<Integer, Integer> cache = grid(0).cache(null); CacheContinuousQuery<Integer, Integer> qry = cache.queries().createContinuousQuery(); final Map<Integer, List<Integer>> map = new HashMap<>(); final CountDownLatch latch = new CountDownLatch(5); qry.localCallback(new P2<UUID, Collection<CacheContinuousQueryEntry<Integer, Integer>>>() { @Override public boolean apply(UUID nodeId, Collection<CacheContinuousQueryEntry<Integer, Integer>> entries) { for (Map.Entry<Integer, Integer> e : entries) { synchronized (map) { List<Integer> vals = map.get(e.getKey()); if (vals == null) { vals = new ArrayList<>(); map.put(e.getKey(), vals); } vals.add(e.getValue()); } latch.countDown(); } return true; } }); try { qry.execute(); cache.putx(1, 1); cache.putx(2, 2); cache.putx(3, 3); cache.removex(2); cache.putx(1, 10); assert latch.await(LATCH_TIMEOUT, MILLISECONDS); assertEquals(3, map.size()); List<Integer> vals = map.get(1); assertNotNull(vals); assertEquals(2, vals.size()); assertEquals(1, (int)vals.get(0)); assertEquals(10, (int)vals.get(1)); vals = map.get(2); assertNotNull(vals); assertEquals(2, vals.size()); assertEquals(2, (int)vals.get(0)); assertNull(vals.get(1)); vals = map.get(3); assertNotNull(vals); assertEquals(1, vals.size()); assertEquals(3, (int)vals.get(0)); } finally { qry.close(); } }', 'output': 'public void testAllEntries() throws Exception { Cache<Integer, Integer> cache = grid(0).cache(null); CacheContinuousQuery<Integer, Integer> qry = cache.queries().createContinuousQuery(); final Map<Integer, List<Integer>> map = new HashMap<>(); final CountDownLatch latch = new CountDownLatch(5); qry.localCallback(new P2<UUID, Collection<org.apache.ignite.cache.query.CacheContinuousQueryEntry<Integer, Integer>>>() { @Override public boolean apply(UUID nodeId, Collection<org.apache.ignite.cache.query.CacheContinuousQueryEntry<Integer, Integer>> entries) { for (Map.Entry<Integer, Integer> e : entries) { synchronized (map) { List<Integer> vals = map.get(e.getKey()); if (vals == null) { vals = new ArrayList<>(); map.put(e.getKey(), vals); } vals.add(e.getValue()); } latch.countDown(); } return true; } }); try { qry.execute(); cache.putx(1, 1); cache.putx(2, 2); cache.putx(3, 3); cache.removex(2); cache.putx(1, 10); assert latch.await(LATCH_TIMEOUT, MILLISECONDS); assertEquals(3, map.size()); List<Integer> vals = map.get(1); assertNotNull(vals); assertEquals(2, vals.size()); assertEquals(1, (int)vals.get(0)); assertEquals(10, (int)vals.get(1)); vals = map.get(2); assertNotNull(vals); assertEquals(2, vals.size()); assertEquals(2, (int)vals.get(0)); assertNull(vals.get(1)); vals = map.get(3); assertNotNull(vals); assertEquals(1, vals.size()); assertEquals(3, (int)vals.get(0)); } finally { qry.close(); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'protected OptiqConnectionImpl(Driver driver, AvaticaFactory factory, String url, Properties info, OptiqRootSchema rootSchema, JavaTypeFactory typeFactory) { super(driver, factory, url, info); this.prepareFactory = driver.prepareFactory; this.typeFactory = typeFactory != null ? typeFactory : new JavaTypeFactoryImpl(); if (rootSchema == null) { rootSchema = new OptiqRootSchema(new RootSchema()); rootSchema.addSchema(MetadataSchema.create(rootSchema.plus())); } this.rootSchema = rootSchema; XXX cfg = connectionConfig(info); this.properties.put(InternalProperty.CASE_SENSITIVE, cfg.caseSensitive()); this.properties.put(InternalProperty.UNQUOTED_CASING, cfg.unquotedCasing()); this.properties.put(InternalProperty.QUOTED_CASING, cfg.quotedCasing()); this.properties.put(InternalProperty.QUOTING, cfg.quoting()); }', 'ground_truth': 'protected OptiqConnectionImpl(Driver driver, AvaticaFactory factory, String url, Properties info, OptiqRootSchema rootSchema, JavaTypeFactory typeFactory) { super(driver, factory, url, info); this.prepareFactory = driver.prepareFactory; this.typeFactory = typeFactory != null ? typeFactory : new JavaTypeFactoryImpl(); if (rootSchema == null) { rootSchema = new OptiqRootSchema(new RootSchema()); rootSchema.addSchema(MetadataSchema.create(rootSchema.plus())); } this.rootSchema = rootSchema; OptiqConnectionConfig cfg = new OptiqConnectionConfigImpl(info); this.properties.put(InternalProperty.CASE_SENSITIVE, cfg.caseSensitive()); this.properties.put(InternalProperty.UNQUOTED_CASING, cfg.unquotedCasing()); this.properties.put(InternalProperty.QUOTED_CASING, cfg.quotedCasing()); this.properties.put(InternalProperty.QUOTING, cfg.quoting()); }', 'output': 'protected OptiqConnectionImpl(Driver driver, AvaticaFactory factory, String url, Properties info, OptiqRootSchema rootSchema, JavaTypeFactory typeFactory) { super(driver, factory, url, info); this.prepareFactory = driver.prepareFactory; this.typeFactory = typeFactory != null ? typeFactory : new JavaTypeFactoryImpl(); if (rootSchema == null) { rootSchema = new OptiqRootSchema(new RootSchema()); rootSchema.addSchema(MetadataSchema.create(rootSchema.plus())); } this.rootSchema = rootSchema; ConnectionConfig cfg = connectionConfig(info); this.properties.put(InternalProperty.CASE_SENSITIVE, cfg.caseSensitive()); this.properties.put(InternalProperty.UNQUOTED_CASING, cfg.unquotedCasing()); this.properties.put(InternalProperty.QUOTED_CASING, cfg.quotedCasing()); this.properties.put(InternalProperty.QUOTING, cfg.quoting()); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void testSequentialForceRebuildIndexes() throws Exception { prepareBeforeNodeStart(); IgniteEx n = startGrid(0); populate(n.cache(DEFAULT_CACHE_NAME), 100); GridCacheContext<?, ?> cacheCtx = n.cachex(DEFAULT_CACHE_NAME).context(); XXX stopRebuildIdxConsumer = addStopRebuildIndexConsumer(n, cacheCtx.name()); assertEqualsCollections(emptyList(), forceRebuildIndexes(n, cacheCtx)); IgniteInternalFuture<?> idxRebFut0 = checkStartRebuildIndexes(n, cacheCtx); stopRebuildIdxConsumer.startRebuildIdxFut.get(getTestTimeout()); assertFalse(idxRebFut0.isDone()); assertEqualsCollections(F.asList(cacheCtx), forceRebuildIndexes(n, cacheCtx)); assertTrue(idxRebFut0 == indexRebuildFuture(n, cacheCtx.cacheId())); stopRebuildIdxConsumer.finishRebuildIdxFut.onDone(); idxRebFut0.get(getTestTimeout()); checkFinishRebuildIndexes(n, cacheCtx, 100); assertEquals(100, stopRebuildIdxConsumer.visitCnt.get()); stopRebuildIdxConsumer.resetFutures(); assertEqualsCollections(emptyList(), forceRebuildIndexes(n, cacheCtx)); IgniteInternalFuture<?> idxRebFut1 = checkStartRebuildIndexes(n, cacheCtx); stopRebuildIdxConsumer.startRebuildIdxFut.get(getTestTimeout()); assertFalse(idxRebFut1.isDone()); stopRebuildIdxConsumer.finishRebuildIdxFut.onDone(); idxRebFut1.get(getTestTimeout()); checkFinishRebuildIndexes(n, cacheCtx, 100); assertEquals(200, stopRebuildIdxConsumer.visitCnt.get()); }', 'ground_truth': 'public void testSequentialForceRebuildIndexes() throws Exception { prepareBeforeNodeStart(); IgniteEx n = startGrid(0); populate(n.cache(DEFAULT_CACHE_NAME), 100); GridCacheContext<?, ?> cacheCtx = n.cachex(DEFAULT_CACHE_NAME).context(); StopBuildIndexConsumer stopRebuildIdxConsumer = addStopRebuildIndexConsumer(n, cacheCtx.name()); assertEqualsCollections(emptyList(), forceRebuildIndexes(n, cacheCtx)); IgniteInternalFuture<?> idxRebFut0 = checkStartRebuildIndexes(n, cacheCtx); stopRebuildIdxConsumer.startBuildIdxFut.get(getTestTimeout()); assertFalse(idxRebFut0.isDone()); assertEqualsCollections(F.asList(cacheCtx), forceRebuildIndexes(n, cacheCtx)); assertTrue(idxRebFut0 == indexRebuildFuture(n, cacheCtx.cacheId())); stopRebuildIdxConsumer.finishBuildIdxFut.onDone(); idxRebFut0.get(getTestTimeout()); checkFinishRebuildIndexes(n, cacheCtx, 100); assertEquals(100, stopRebuildIdxConsumer.visitCnt.get()); stopRebuildIdxConsumer.resetFutures(); assertEqualsCollections(emptyList(), forceRebuildIndexes(n, cacheCtx)); IgniteInternalFuture<?> idxRebFut1 = checkStartRebuildIndexes(n, cacheCtx); stopRebuildIdxConsumer.startBuildIdxFut.get(getTestTimeout()); assertFalse(idxRebFut1.isDone()); stopRebuildIdxConsumer.finishBuildIdxFut.onDone(); idxRebFut1.get(getTestTimeout()); checkFinishRebuildIndexes(n, cacheCtx, 100); assertEquals(200, stopRebuildIdxConsumer.visitCnt.get()); }', 'output': 'public void testSequentialForceRebuildIndexes() throws Exception { prepareBeforeNodeStart(); IgniteEx n = startGrid(0); populate(n.cache(DEFAULT_CACHE_NAME), 100); GridCacheContext<?, ?> cacheCtx = n.cachex(DEFAULT_CACHE_NAME).context(); StopRebuildIndexConsumer stopRebuildIdxConsumer = addStopRebuildIndexConsumer(n, cacheCtx.name()); assertEqualsCollections(emptyList(), forceRebuildIndexes(n, cacheCtx)); IgniteInternalFuture<?> idxRebFut0 = checkStartRebuildIndexes(n, cacheCtx); stopRebuildIdxConsumer.startRebuildIdxFut.get(getTestTimeout()); assertFalse(idxRebFut0.isDone()); assertEqualsCollections(F.asList(cacheCtx), forceRebuildIndexes(n, cacheCtx)); assertTrue(idxRebFut0 == indexRebuildFuture(n, cacheCtx.cacheId())); stopRebuildIdxConsumer.finishRebuildIdxFut.onDone(); idxRebFut0.get(getTestTimeout()); checkFinishRebuildIndexes(n, cacheCtx, 100); assertEquals(100, stopRebuildIdxConsumer.visitCnt.get()); stopRebuildIdxConsumer.resetFutures(); assertEqualsCollections(emptyList(), forceRebuildIndexes(n, cacheCtx)); IgniteInternalFuture<?> idxRebFut1 = checkStartRebuildIndexes(n, cacheCtx); stopRebuildIdxConsumer.startRebuildIdxFut.get(getTestTimeout()); assertFalse(idxRebFut1.isDone()); stopRebuildIdxConsumer.finishRebuildIdxFut.onDone(); idxRebFut1.get(getTestTimeout()); checkFinishRebuildIndexes(n, cacheCtx, 100); assertEquals(200, stopRebuildIdxCons'}
{'instruction': 'Please refactor the following code:', 'input': '@Override public String toString() { try { PortableReaderContext ctx = new PortableReaderContext(); ctx.setPortableHandler(start(), this); return toString(ctx, new IdentityHashMap<PortableObject, Integer>()); } catch (XXX e) { throw new IgniteException("Failed to create string representation of portable object.", e); } } } }', 'ground_truth': '@Override public String toString() { try { PortableReaderContext ctx = new PortableReaderContext(); ctx.setPortableHandler(start(), this); return toString(ctx, new IdentityHashMap<BinaryObject, Integer>()); } catch (BinaryObjectException e) { throw new IgniteException("Failed to create string representation of portable object.", e); } }', 'output': '@Override public String toString() { try { PortableReaderContext ctx = new PortableReaderContext(); ctx.setPortableHandler(start(), this); return toString(ctx, new IdentityHashMap<PortableObject, Integer>()); } catch (IgniteCheckedException e) { throw new IgniteException("Failed to create string representation of portable object.", e); } }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public void boundedSourceInMemoryTransformEvaluatorProducesElements() throws Exception { BoundedSource<Long> source = CountingSource.upTo(10L); TestPipeline p = TestPipeline.create(); PCollection<Long> longs = p.apply(Read.from(source)); TransformEvaluatorFactory factory = new BoundedReadEvaluatorFactory(); InProcessEvaluationContext context = mock(InProcessEvaluationContext.class); XXX output = InProcessBundle.unkeyed(longs); when(context.createRootBundle(longs)).thenReturn(output); TransformEvaluator<?> evaluator = factory.forApplication(longs.getProducingTransformInternal(), null, context); InProcessTransformResult result = evaluator.finishBundle(); assertThat(result.getWatermarkHold(), equalTo(BoundedWindow.TIMESTAMP_MAX_VALUE)); assertThat( output.getElements(), containsInAnyOrder( gw(1L), gw(2L), gw(4L), gw(8L), gw(9L), gw(7L), gw(6L), gw(5L), gw(3L), gw(0L))); }', 'ground_truth': 'public void boundedSourceInMemoryTransformEvaluatorProducesElements() throws Exception { BoundedSource<Long> source = CountingSource.upTo(10L); TestPipeline p = TestPipeline.create(); PCollection<Long> longs = p.apply(Read.from(source)); TransformEvaluatorFactory factory = new BoundedReadEvaluatorFactory(); InProcessEvaluationContext context = mock(InProcessEvaluationContext.class); UncommittedBundle<Long> output = InProcessBundle.unkeyed(longs); when(context.createRootBundle(longs)).thenReturn(output); TransformEvaluator<?> evaluator = factory.forApplication(longs.getProducingTransformInternal(), null, context); InProcessTransformResult result = evaluator.finishBundle(); assertThat(result.getWatermarkHold(), equalTo(BoundedWindow.TIMESTAMP_MAX_VALUE)); assertThat( output.commit(BoundedWindow.TIMESTAMP_MAX_VALUE).getElements(), containsInAnyOrder( gw(1L), gw(2L), gw(4L), gw(8L), gw(9L), gw(7L), gw(6L), gw(5L), gw(3L), gw(0L))); }', 'output': 'public void boundedSourceInMemoryTransformEvaluatorProducesElements() throws Exception { BoundedSource<Long> source = CountingSource.upTo(10L); TestPipeline p = TestPipeline.create(); PCollection<Long> longs = p.apply(Read.from(source)); TransformEvaluatorFactory factory = new BoundedReadEvaluatorFactory(); InProcessEvaluationContext context = mock(InProcessEvaluationContext.class); InProcessBundle output = InProcessBundle.unkeyed(longs); when(context.createRootBundle(longs)).thenReturn(output); TransformEvaluator<?> evaluator = factory.forApplication(longs.getProducingTransformInternal(), null, context); InProcessTransformResult result = evaluator.finishBundle(); assertThat(result.getWatermarkHold(), equalTo(BoundedWindow.TIMESTAMP_MAX_VALUE)); assertThat( output.getElements(), containsInAnyOrder( gw(1L), gw(2L), gw(4L), gw(8L), gw(9L), gw(7L), gw(6L), gw(5L), gw(3L), gw(0L))); }</s>'}
{'instruction': 'Please refactor the following code:', 'input': 'public static void main(String[] args) throws Throwable { int minRow; int maxRow; XXX rowPrefix, keyspace = "Keyspace1"; if (args.length > 0) { keyspace = args[0]; rowPrefix = args[1]; minRow = Integer.parseInt(args[2]); maxRow = minRow + 1; } else { minRow = 1; maxRow = 10; rowPrefix = "row"; } TestRingCache tester = new TestRingCache(keyspace); for (int nRows = minRow; nRows < maxRow; nRows++) { XXX row = rowPrefix + nRows; ColumnPath col = new ColumnPath("Standard1").setSuper_column(null).setColumn("col1".getBytes()); List<InetAddress> endPoints = tester.ringCache.getEndPoint(row.getBytes()); String hosts=""; for (int i = 0; i < endPoints.size(); i++) hosts = hosts + ((i > 0) ? "," : "") + endPoints.get(i); System.out.println("hosts with key " + row + " : " + hosts + "; choose " + endPoints.get(0)); tester.setup(endPoints.get(0).getHostAddress(), DatabaseDescriptor.getRpcPort()); tester.thriftClient.insert(keyspace, row, col, "val1".getBytes(), 1, ConsistencyLevel.ONE); Column column = tester.thriftClient.get(keyspace, row, col, ConsistencyLevel.ONE).column; System.out.println("read row " + row + " " + new String(column.name) + ":" + new String(column.value) + ":" + column.timestamp); } System.exit(1); }', 'ground_truth': 'public static void main(String[] args) throws Throwable { int minRow; int maxRow; String rowPrefix, keyspace = "Keyspace1"; if (args.length > 0) { keyspace = args[0]; rowPrefix = args[1]; minRow = Integer.parseInt(args[2]); maxRow = minRow + 1; } else { minRow = 1; maxRow = 10; rowPrefix = "row"; } TestRingCache tester = new TestRingCache(keyspace); for (int nRows = minRow; nRows < maxRow; nRows++) { byte[] row = (rowPrefix + nRows).getBytes(); ColumnPath col = new ColumnPath("Standard1").setSuper_column(null).setColumn("col1".getBytes()); List<InetAddress> endPoints = tester.ringCache.getEndPoint(row); String hosts=""; for (int i = 0; i < endPoints.size(); i++) hosts = hosts + ((i > 0) ? "," : "") + endPoints.get(i); System.out.println("hosts with key " + new String(row) + " : " + hosts + "; choose " + endPoints.get(0)); tester.setup(endPoints.get(0).getHostAddress(), DatabaseDescriptor.getRpcPort()); tester.thriftClient.insert(keyspace, row, col, "val1".getBytes(), 1, ConsistencyLevel.ONE); Column column = tester.thriftClient.get(keyspace, row, col, ConsistencyLevel.ONE).column; System.out.println("read row " + new String(row) + " " + new String(column.name) + ":" + new String(column.value) + ":" + column.timestamp); } System.exit(1); }', 'output': 'public static void main(String[] args) throws Throwable { int minRow; int maxRow; String rowPrefix, keyspace = "Keyspace1"; if (args.length > 0) { keyspace = args[0]; rowPrefix = args[1]; minRow = Integer.parseInt(args[2]); maxRow = minRow + 1; } else { minRow = 1; maxRow = 10; rowPrefix = "row"; } TestRingCache tester = new TestRingCache(keyspace); for (int nRows = minRow; nRows < maxRow; nRows++) { String row = rowPrefix + nRows; ColumnPath col = new ColumnPath("Standard1").setSuper_column(null).setColumn("col1".getBytes()); List<InetAddress> endPoints = tester.ringCache.getEndPoint(row.getBytes()); String hosts=""; for (int i = 0; i < endPoints.size(); i++) hosts = hosts + ((i > 0) ? "," : "") + endPoints.get(i).getHostAddress(); System.out.println("hosts with key " + row + " : " + hosts + "; choose " + endPoints.get(0)); tester.setup(endPoints.get(0).getHostAddress(), DatabaseDescriptor.getRpcPort()); tester.thriftClient.insert(keyspace, row, col, "val1".getBytes(), 1, ConsistencyLevel.ONE); Column column = tester.thriftClient.get(keyspace, row, col, ConsistencyLevel.ONE).column; System.out.println("read row " + row + " " + new String(column.name) + ":" + new String(column.value) + ":" + column.timestamp); } System.exit(1); }</s>'}
